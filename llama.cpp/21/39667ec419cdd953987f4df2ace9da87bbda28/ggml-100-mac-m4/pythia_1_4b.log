Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.2s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.513s
user	0m0.853s
sys	0m1.200s
++ nproc
+ make -j10
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha256
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Built target xxhash
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Linking CXX executable ../../bin/llama-gguf
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 31%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 32%] Linking C executable ../bin/test-c
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llama-simple-chat
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library libllava_shared.dylib
[ 37%] Built target llama-quantize-stats
[ 37%] Built target common
[ 37%] Built target llava_static
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 40%] Built target llava_shared
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Built target test-tokenizer-0
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Built target test-tokenizer-1-bpe
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-sampling
[ 48%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Built target test-grammar-integration
[ 49%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-model-load-cancel
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Built target test-log
[ 56%] Built target test-arg-parser
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-barrier
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-backend-ops
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-chat-template
[ 62%] Built target test-gguf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-barrier
[ 63%] Built target test-autorelease
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Linking CXX executable ../bin/test-rope
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Built target test-quantize-fns
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-batched
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Built target test-quantize-perf
[ 69%] Built target llama-batched-bench
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Built target test-rope
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target llama-batched
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-embedding
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-gguf-split
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gritlm
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookahead
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Built target llama-imatrix
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Built target llama-infill
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Built target llama-bench
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Built target llama-lookahead
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Built target llama-lookup
[ 81%] Built target llama-cli
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-parallel
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-lookup-stats
[ 82%] Generating index.html.gz.hpp
[ 82%] Generating loading.html.hpp
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 87%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 88%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Built target llama-perplexity
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Built target llama-quantize
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-run
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Built target llama-passkey
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Built target llama-retrieval
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-run
[ 91%] Built target llama-speculative
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Built target llama-tokenize
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Built target llama-tts
[ 96%] Linking CXX executable ../../bin/llama-cvector-generator
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Built target llama-gen-docs
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.036s
user	0m5.997s
sys	0m9.047s

main: quantize time =  2901.99 ms
main:    total time =  2901.99 ms

main: quantize time =  1793.40 ms
main:    total time =  1793.40 ms

main: quantize time =  1382.61 ms
main:    total time =  1382.61 ms

main: quantize time =  2852.15 ms
main:    total time =  2852.15 ms

main: quantize time =  1709.30 ms
main:    total time =  1709.30 ms

main: quantize time =  4907.63 ms
main:    total time =  4907.63 ms

main: quantize time =  5581.64 ms
main:    total time =  5581.64 ms

main: quantize time =  6907.57 ms
main:    total time =  6907.57 ms

main: quantize time =  5862.42 ms
main:    total time =  5862.42 ms

main: quantize time =  4431.31 ms
main:    total time =  4431.31 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.172 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.287 I main: llama backend init
0.00.000.292 I main: load the model and apply lora adapter, if any
0.00.061.405 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.073.756 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.073.769 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.073.772 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.073.773 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.073.774 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.073.774 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.073.775 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.073.778 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.073.778 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.073.779 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.073.780 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.073.781 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.073.781 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.073.782 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.073.794 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.073.794 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.073.797 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.080.598 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.082.737 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.089.468 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.089.474 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.089.475 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.089.475 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.089.476 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.089.477 I llama_model_loader: - type  f32:  194 tensors
0.00.089.478 I llama_model_loader: - type  f16:   98 tensors
0.00.089.479 I print_info: file format = GGUF V3 (latest)
0.00.089.481 I print_info: file type   = all F32 (guessed)
0.00.089.484 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.125.699 I load: special tokens cache size = 25
0.00.133.464 I load: token to piece cache size = 0.2984 MB
0.00.133.468 I print_info: arch             = gptneox
0.00.133.468 I print_info: vocab_only       = 0
0.00.133.468 I print_info: n_ctx_train      = 2048
0.00.133.469 I print_info: n_embd           = 2048
0.00.133.469 I print_info: n_layer          = 24
0.00.133.473 I print_info: n_head           = 16
0.00.133.474 I print_info: n_head_kv        = 16
0.00.133.476 I print_info: n_rot            = 32
0.00.133.477 I print_info: n_swa            = 0
0.00.133.477 I print_info: n_embd_head_k    = 128
0.00.133.477 I print_info: n_embd_head_v    = 128
0.00.133.478 I print_info: n_gqa            = 1
0.00.133.478 I print_info: n_embd_k_gqa     = 2048
0.00.133.479 I print_info: n_embd_v_gqa     = 2048
0.00.133.481 I print_info: f_norm_eps       = 1.0e-05
0.00.133.482 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.133.482 I print_info: f_clamp_kqv      = 0.0e+00
0.00.133.482 I print_info: f_max_alibi_bias = 0.0e+00
0.00.133.482 I print_info: f_logit_scale    = 0.0e+00
0.00.133.483 I print_info: n_ff             = 8192
0.00.133.483 I print_info: n_expert         = 0
0.00.133.484 I print_info: n_expert_used    = 0
0.00.133.484 I print_info: causal attn      = 1
0.00.133.484 I print_info: pooling type     = 0
0.00.133.484 I print_info: rope type        = 2
0.00.133.484 I print_info: rope scaling     = linear
0.00.133.487 I print_info: freq_base_train  = 10000.0
0.00.133.487 I print_info: freq_scale_train = 1
0.00.133.487 I print_info: n_ctx_orig_yarn  = 2048
0.00.133.487 I print_info: rope_finetuned   = unknown
0.00.133.488 I print_info: ssm_d_conv       = 0
0.00.133.488 I print_info: ssm_d_inner      = 0
0.00.133.488 I print_info: ssm_d_state      = 0
0.00.133.489 I print_info: ssm_dt_rank      = 0
0.00.133.490 I print_info: ssm_dt_b_c_rms   = 0
0.00.133.490 I print_info: model type       = 1.4B
0.00.133.490 I print_info: model params     = 1.41 B
0.00.133.490 I print_info: general.name     = 1.4B
0.00.133.491 I print_info: vocab type       = BPE
0.00.133.491 I print_info: n_vocab          = 50304
0.00.133.491 I print_info: n_merges         = 50009
0.00.133.492 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.133.492 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.133.492 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.133.492 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.133.493 I print_info: LF token         = 128 'Ä'
0.00.133.493 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.133.493 I print_info: max token length = 1024
0.00.135.519 I load_tensors: offloading 24 repeating layers to GPU
0.00.135.519 I load_tensors: offloading output layer to GPU
0.00.135.520 I load_tensors: offloaded 25/25 layers to GPU
0.00.135.539 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.135.540 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.135.903 I llama_init_from_model: n_seq_max     = 1
0.00.135.905 I llama_init_from_model: n_ctx         = 2048
0.00.135.905 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.135.905 I llama_init_from_model: n_batch       = 2048
0.00.135.905 I llama_init_from_model: n_ubatch      = 512
0.00.135.905 I llama_init_from_model: flash_attn    = 0
0.00.135.906 I llama_init_from_model: freq_base     = 10000.0
0.00.135.906 I llama_init_from_model: freq_scale    = 1
0.00.135.907 I ggml_metal_init: allocating
0.00.135.911 I ggml_metal_init: found device: Apple M4
0.00.135.913 I ggml_metal_init: picking default device: Apple M4
0.00.136.618 I ggml_metal_init: using embedded metal library
0.00.349.310 I ggml_metal_init: GPU name:   Apple M4
0.00.349.327 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.349.328 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.349.329 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.349.330 I ggml_metal_init: simdgroup reduction   = true
0.00.349.330 I ggml_metal_init: simdgroup matrix mul. = true
0.00.349.331 I ggml_metal_init: has bfloat            = true
0.00.349.331 I ggml_metal_init: use bfloat            = true
0.00.349.333 I ggml_metal_init: hasUnifiedMemory      = true
0.00.349.337 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.387.343 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.416.739 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.416.753 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.416.789 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.417.771 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.417.773 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.417.773 I llama_init_from_model: graph nodes  = 967
0.00.417.774 I llama_init_from_model: graph splits = 2
0.00.417.777 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.417.901 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.417.901 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.500.603 I main: llama threadpool init, n_threads = 4
0.00.500.642 I 
0.00.500.680 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.500.682 I 
0.00.500.750 I sampler seed: 1234
0.00.500.755 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.500.784 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.500.786 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.500.786 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.328.218 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57119.87 tokens per second)
0.02.328.218 I llama_perf_context_print:        load time =     439.19 ms
0.02.328.219 I llama_perf_context_print: prompt eval time =      43.83 ms /     7 tokens (    6.26 ms per token,   159.70 tokens per second)
0.02.328.221 I llama_perf_context_print:        eval time =    1780.72 ms /    63 runs   (   28.27 ms per token,    35.38 tokens per second)
0.02.328.221 I llama_perf_context_print:       total time =    1827.62 ms /    70 tokens
0.02.328.421 I ggml_metal_free: deallocating

real	0m2.640s
user	0m0.163s
sys	0m0.118s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.974 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.920 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.925 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.930 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.930 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.931 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.931 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.932 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.933 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.933 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.934 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.934 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.935 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.935 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.935 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.937 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.938 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.938 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.873 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.948 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.028 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.030 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.030 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.031 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.031 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.031 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.032 I llama_model_loader: - type  f32:  194 tensors
0.00.035.033 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.033 I print_info: file format = GGUF V3 (latest)
0.00.035.034 I print_info: file type   = Q8_0
0.00.035.035 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.056.241 I load: special tokens cache size = 25
0.00.062.257 I load: token to piece cache size = 0.2984 MB
0.00.062.261 I print_info: arch             = gptneox
0.00.062.261 I print_info: vocab_only       = 0
0.00.062.262 I print_info: n_ctx_train      = 2048
0.00.062.262 I print_info: n_embd           = 2048
0.00.062.262 I print_info: n_layer          = 24
0.00.062.268 I print_info: n_head           = 16
0.00.062.269 I print_info: n_head_kv        = 16
0.00.062.269 I print_info: n_rot            = 32
0.00.062.269 I print_info: n_swa            = 0
0.00.062.269 I print_info: n_embd_head_k    = 128
0.00.062.269 I print_info: n_embd_head_v    = 128
0.00.062.270 I print_info: n_gqa            = 1
0.00.062.271 I print_info: n_embd_k_gqa     = 2048
0.00.062.272 I print_info: n_embd_v_gqa     = 2048
0.00.062.272 I print_info: f_norm_eps       = 1.0e-05
0.00.062.273 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.273 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.273 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.273 I print_info: f_logit_scale    = 0.0e+00
0.00.062.277 I print_info: n_ff             = 8192
0.00.062.277 I print_info: n_expert         = 0
0.00.062.277 I print_info: n_expert_used    = 0
0.00.062.278 I print_info: causal attn      = 1
0.00.062.278 I print_info: pooling type     = 0
0.00.062.279 I print_info: rope type        = 2
0.00.062.279 I print_info: rope scaling     = linear
0.00.062.280 I print_info: freq_base_train  = 10000.0
0.00.062.280 I print_info: freq_scale_train = 1
0.00.062.280 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.281 I print_info: rope_finetuned   = unknown
0.00.062.281 I print_info: ssm_d_conv       = 0
0.00.062.281 I print_info: ssm_d_inner      = 0
0.00.062.281 I print_info: ssm_d_state      = 0
0.00.062.281 I print_info: ssm_dt_rank      = 0
0.00.062.281 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.282 I print_info: model type       = 1.4B
0.00.062.282 I print_info: model params     = 1.41 B
0.00.062.283 I print_info: general.name     = 1.4B
0.00.062.283 I print_info: vocab type       = BPE
0.00.062.283 I print_info: n_vocab          = 50304
0.00.062.284 I print_info: n_merges         = 50009
0.00.062.284 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.284 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.284 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.284 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.285 I print_info: LF token         = 128 'Ä'
0.00.062.285 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.285 I print_info: max token length = 1024
0.00.064.677 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.677 I load_tensors: offloading output layer to GPU
0.00.064.678 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.690 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.691 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.065.098 I llama_init_from_model: n_seq_max     = 1
0.00.065.098 I llama_init_from_model: n_ctx         = 2048
0.00.065.098 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.065.099 I llama_init_from_model: n_batch       = 2048
0.00.065.099 I llama_init_from_model: n_ubatch      = 512
0.00.065.099 I llama_init_from_model: flash_attn    = 0
0.00.065.099 I llama_init_from_model: freq_base     = 10000.0
0.00.065.100 I llama_init_from_model: freq_scale    = 1
0.00.065.100 I ggml_metal_init: allocating
0.00.065.103 I ggml_metal_init: found device: Apple M4
0.00.065.105 I ggml_metal_init: picking default device: Apple M4
0.00.065.837 I ggml_metal_init: using embedded metal library
0.00.068.424 I ggml_metal_init: GPU name:   Apple M4
0.00.068.425 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.426 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.426 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.427 I ggml_metal_init: simdgroup reduction   = true
0.00.068.427 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.427 I ggml_metal_init: has bfloat            = true
0.00.068.427 I ggml_metal_init: use bfloat            = true
0.00.068.427 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.428 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.880 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.105.634 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.105.646 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.105.683 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.107.004 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.107.007 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.107.007 I llama_init_from_model: graph nodes  = 967
0.00.107.007 I llama_init_from_model: graph splits = 2
0.00.107.011 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.107.140 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.107.141 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.370.075 I main: llama threadpool init, n_threads = 4
0.01.370.113 I 
0.01.370.147 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.370.149 I 
0.01.370.365 I sampler seed: 1234
0.01.370.369 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.370.407 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.370.408 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.370.408 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.459.481 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62445.03 tokens per second)
0.02.459.482 I llama_perf_context_print:        load time =    1360.10 ms
0.02.459.482 I llama_perf_context_print: prompt eval time =      43.77 ms /     7 tokens (    6.25 ms per token,   159.94 tokens per second)
0.02.459.483 I llama_perf_context_print:        eval time =    1042.51 ms /    63 runs   (   16.55 ms per token,    60.43 tokens per second)
0.02.459.487 I llama_perf_context_print:       total time =    1089.41 ms /    70 tokens
0.02.459.729 I ggml_metal_free: deallocating

real	0m2.478s
user	0m0.114s
sys	0m0.233s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.019.759 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.264 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.029.270 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.272 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.272 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.272 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.273 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.273 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.274 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.274 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.275 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.275 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.275 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.276 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.276 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.278 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.278 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.279 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.266 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.389 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.632 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.633 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.633 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.634 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.634 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.634 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.038.635 I llama_model_loader: - type  f32:  194 tensors
0.00.038.635 I llama_model_loader: - type q4_0:   97 tensors
0.00.038.635 I llama_model_loader: - type q6_K:    1 tensors
0.00.038.636 I print_info: file format = GGUF V3 (latest)
0.00.038.636 I print_info: file type   = Q4_0
0.00.038.637 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.061.756 I load: special tokens cache size = 25
0.00.068.945 I load: token to piece cache size = 0.2984 MB
0.00.068.949 I print_info: arch             = gptneox
0.00.068.949 I print_info: vocab_only       = 0
0.00.068.949 I print_info: n_ctx_train      = 2048
0.00.068.949 I print_info: n_embd           = 2048
0.00.068.950 I print_info: n_layer          = 24
0.00.068.954 I print_info: n_head           = 16
0.00.068.954 I print_info: n_head_kv        = 16
0.00.068.957 I print_info: n_rot            = 32
0.00.068.958 I print_info: n_swa            = 0
0.00.068.958 I print_info: n_embd_head_k    = 128
0.00.068.958 I print_info: n_embd_head_v    = 128
0.00.068.959 I print_info: n_gqa            = 1
0.00.068.961 I print_info: n_embd_k_gqa     = 2048
0.00.068.961 I print_info: n_embd_v_gqa     = 2048
0.00.068.962 I print_info: f_norm_eps       = 1.0e-05
0.00.068.962 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.068.962 I print_info: f_clamp_kqv      = 0.0e+00
0.00.068.963 I print_info: f_max_alibi_bias = 0.0e+00
0.00.068.963 I print_info: f_logit_scale    = 0.0e+00
0.00.068.964 I print_info: n_ff             = 8192
0.00.068.965 I print_info: n_expert         = 0
0.00.068.965 I print_info: n_expert_used    = 0
0.00.068.965 I print_info: causal attn      = 1
0.00.068.965 I print_info: pooling type     = 0
0.00.068.965 I print_info: rope type        = 2
0.00.068.965 I print_info: rope scaling     = linear
0.00.068.966 I print_info: freq_base_train  = 10000.0
0.00.068.966 I print_info: freq_scale_train = 1
0.00.068.966 I print_info: n_ctx_orig_yarn  = 2048
0.00.068.967 I print_info: rope_finetuned   = unknown
0.00.068.967 I print_info: ssm_d_conv       = 0
0.00.068.968 I print_info: ssm_d_inner      = 0
0.00.068.968 I print_info: ssm_d_state      = 0
0.00.068.968 I print_info: ssm_dt_rank      = 0
0.00.068.969 I print_info: ssm_dt_b_c_rms   = 0
0.00.068.969 I print_info: model type       = 1.4B
0.00.068.969 I print_info: model params     = 1.41 B
0.00.068.970 I print_info: general.name     = 1.4B
0.00.068.971 I print_info: vocab type       = BPE
0.00.068.971 I print_info: n_vocab          = 50304
0.00.068.971 I print_info: n_merges         = 50009
0.00.068.971 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.068.971 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.068.971 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.068.972 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.068.973 I print_info: LF token         = 128 'Ä'
0.00.068.973 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.068.973 I print_info: max token length = 1024
0.00.071.525 I load_tensors: offloading 24 repeating layers to GPU
0.00.071.525 I load_tensors: offloading output layer to GPU
0.00.071.525 I load_tensors: offloaded 25/25 layers to GPU
0.00.071.537 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.071.539 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.072.000 I llama_init_from_model: n_seq_max     = 1
0.00.072.000 I llama_init_from_model: n_ctx         = 2048
0.00.072.001 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.072.001 I llama_init_from_model: n_batch       = 2048
0.00.072.001 I llama_init_from_model: n_ubatch      = 512
0.00.072.001 I llama_init_from_model: flash_attn    = 0
0.00.072.002 I llama_init_from_model: freq_base     = 10000.0
0.00.072.002 I llama_init_from_model: freq_scale    = 1
0.00.072.003 I ggml_metal_init: allocating
0.00.072.005 I ggml_metal_init: found device: Apple M4
0.00.072.008 I ggml_metal_init: picking default device: Apple M4
0.00.072.837 I ggml_metal_init: using embedded metal library
0.00.076.128 I ggml_metal_init: GPU name:   Apple M4
0.00.076.130 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.130 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.131 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.131 I ggml_metal_init: simdgroup reduction   = true
0.00.076.131 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.131 I ggml_metal_init: has bfloat            = true
0.00.076.131 I ggml_metal_init: use bfloat            = true
0.00.076.132 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.132 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.302 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.121.425 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.121.440 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.121.469 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.122.706 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.122.708 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.122.708 I llama_init_from_model: graph nodes  = 967
0.00.122.708 I llama_init_from_model: graph splits = 2
0.00.122.714 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.122.849 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.122.849 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.835.889 I main: llama threadpool init, n_threads = 4
0.00.835.931 I 
0.00.835.967 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.835.969 I 
0.00.836.201 I sampler seed: 1234
0.00.836.209 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.836.222 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.836.222 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.836.222 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.522.161 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58101.47 tokens per second)
0.01.522.161 I llama_perf_context_print:        load time =     816.12 ms
0.01.522.162 I llama_perf_context_print: prompt eval time =      43.66 ms /     7 tokens (    6.24 ms per token,   160.34 tokens per second)
0.01.522.163 I llama_perf_context_print:        eval time =     639.27 ms /    63 runs   (   10.15 ms per token,    98.55 tokens per second)
0.01.522.164 I llama_perf_context_print:       total time =     686.28 ms /    70 tokens
0.01.522.372 I ggml_metal_free: deallocating

real	0m1.543s
user	0m0.123s
sys	0m0.158s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.935 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.325 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.329 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.330 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.330 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.331 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.331 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.331 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.332 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.332 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.333 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.333 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.334 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.334 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.337 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.338 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.339 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.339 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.018 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.045 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.745 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.746 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.747 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.747 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.747 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.747 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.748 I llama_model_loader: - type  f32:  194 tensors
0.00.026.748 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.748 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.749 I print_info: file format = GGUF V3 (latest)
0.00.026.750 I print_info: file type   = Q4_1
0.00.026.751 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.286 I load: special tokens cache size = 25
0.00.051.297 I load: token to piece cache size = 0.2984 MB
0.00.051.302 I print_info: arch             = gptneox
0.00.051.302 I print_info: vocab_only       = 0
0.00.051.303 I print_info: n_ctx_train      = 2048
0.00.051.303 I print_info: n_embd           = 2048
0.00.051.303 I print_info: n_layer          = 24
0.00.051.307 I print_info: n_head           = 16
0.00.051.308 I print_info: n_head_kv        = 16
0.00.051.308 I print_info: n_rot            = 32
0.00.051.309 I print_info: n_swa            = 0
0.00.051.309 I print_info: n_embd_head_k    = 128
0.00.051.309 I print_info: n_embd_head_v    = 128
0.00.051.310 I print_info: n_gqa            = 1
0.00.051.310 I print_info: n_embd_k_gqa     = 2048
0.00.051.311 I print_info: n_embd_v_gqa     = 2048
0.00.051.312 I print_info: f_norm_eps       = 1.0e-05
0.00.051.312 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.312 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.312 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.312 I print_info: f_logit_scale    = 0.0e+00
0.00.051.313 I print_info: n_ff             = 8192
0.00.051.313 I print_info: n_expert         = 0
0.00.051.314 I print_info: n_expert_used    = 0
0.00.051.314 I print_info: causal attn      = 1
0.00.051.315 I print_info: pooling type     = 0
0.00.051.315 I print_info: rope type        = 2
0.00.051.315 I print_info: rope scaling     = linear
0.00.051.315 I print_info: freq_base_train  = 10000.0
0.00.051.316 I print_info: freq_scale_train = 1
0.00.051.316 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.316 I print_info: rope_finetuned   = unknown
0.00.051.316 I print_info: ssm_d_conv       = 0
0.00.051.316 I print_info: ssm_d_inner      = 0
0.00.051.316 I print_info: ssm_d_state      = 0
0.00.051.317 I print_info: ssm_dt_rank      = 0
0.00.051.317 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.317 I print_info: model type       = 1.4B
0.00.051.317 I print_info: model params     = 1.41 B
0.00.051.317 I print_info: general.name     = 1.4B
0.00.051.318 I print_info: vocab type       = BPE
0.00.051.319 I print_info: n_vocab          = 50304
0.00.051.320 I print_info: n_merges         = 50009
0.00.051.320 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.320 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.320 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.320 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.320 I print_info: LF token         = 128 'Ä'
0.00.051.321 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.321 I print_info: max token length = 1024
0.00.053.325 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.325 I load_tensors: offloading output layer to GPU
0.00.053.326 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.337 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.339 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.053.779 I llama_init_from_model: n_seq_max     = 1
0.00.053.781 I llama_init_from_model: n_ctx         = 2048
0.00.053.781 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.781 I llama_init_from_model: n_batch       = 2048
0.00.053.781 I llama_init_from_model: n_ubatch      = 512
0.00.053.781 I llama_init_from_model: flash_attn    = 0
0.00.053.782 I llama_init_from_model: freq_base     = 10000.0
0.00.053.782 I llama_init_from_model: freq_scale    = 1
0.00.053.783 I ggml_metal_init: allocating
0.00.053.787 I ggml_metal_init: found device: Apple M4
0.00.053.789 I ggml_metal_init: picking default device: Apple M4
0.00.054.453 I ggml_metal_init: using embedded metal library
0.00.056.974 I ggml_metal_init: GPU name:   Apple M4
0.00.056.976 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.976 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.982 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.982 I ggml_metal_init: simdgroup reduction   = true
0.00.056.982 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.982 I ggml_metal_init: has bfloat            = true
0.00.056.982 I ggml_metal_init: use bfloat            = true
0.00.056.983 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.984 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.378 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.369 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.377 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.397 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.476 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.477 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.477 I llama_init_from_model: graph nodes  = 967
0.00.089.478 I llama_init_from_model: graph splits = 2
0.00.089.485 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.606 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.607 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.367.271 I main: llama threadpool init, n_threads = 4
0.01.367.335 I 
0.01.367.370 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.367.371 I 
0.01.367.613 I sampler seed: 1234
0.01.367.618 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.367.629 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.367.629 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.367.633 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.02.087.402 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54406.13 tokens per second)
0.02.087.403 I llama_perf_context_print:        load time =    1358.33 ms
0.02.087.404 I llama_perf_context_print: prompt eval time =      45.07 ms /     7 tokens (    6.44 ms per token,   155.30 tokens per second)
0.02.087.405 I llama_perf_context_print:        eval time =     671.92 ms /    63 runs   (   10.67 ms per token,    93.76 tokens per second)
0.02.087.405 I llama_perf_context_print:       total time =     720.14 ms /    70 tokens
0.02.087.672 I ggml_metal_free: deallocating

real	0m2.106s
user	0m0.110s
sys	0m0.159s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.011.381 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.595 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.600 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.602 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.607 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.607 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.608 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.608 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.609 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.610 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.610 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.610 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.611 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.611 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.613 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.615 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.615 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.615 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.462 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.456 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.243 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.244 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.245 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.245 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.245 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.246 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.246 I llama_model_loader: - type  f32:  194 tensors
0.00.027.246 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.247 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.247 I print_info: file format = GGUF V3 (latest)
0.00.027.248 I print_info: file type   = Q5_0
0.00.027.248 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.212 I load: special tokens cache size = 25
0.00.052.076 I load: token to piece cache size = 0.2984 MB
0.00.052.079 I print_info: arch             = gptneox
0.00.052.080 I print_info: vocab_only       = 0
0.00.052.080 I print_info: n_ctx_train      = 2048
0.00.052.080 I print_info: n_embd           = 2048
0.00.052.080 I print_info: n_layer          = 24
0.00.052.083 I print_info: n_head           = 16
0.00.052.083 I print_info: n_head_kv        = 16
0.00.052.084 I print_info: n_rot            = 32
0.00.052.084 I print_info: n_swa            = 0
0.00.052.084 I print_info: n_embd_head_k    = 128
0.00.052.084 I print_info: n_embd_head_v    = 128
0.00.052.085 I print_info: n_gqa            = 1
0.00.052.086 I print_info: n_embd_k_gqa     = 2048
0.00.052.086 I print_info: n_embd_v_gqa     = 2048
0.00.052.087 I print_info: f_norm_eps       = 1.0e-05
0.00.052.087 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.087 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.088 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.088 I print_info: f_logit_scale    = 0.0e+00
0.00.052.088 I print_info: n_ff             = 8192
0.00.052.089 I print_info: n_expert         = 0
0.00.052.089 I print_info: n_expert_used    = 0
0.00.052.089 I print_info: causal attn      = 1
0.00.052.089 I print_info: pooling type     = 0
0.00.052.089 I print_info: rope type        = 2
0.00.052.092 I print_info: rope scaling     = linear
0.00.052.092 I print_info: freq_base_train  = 10000.0
0.00.052.092 I print_info: freq_scale_train = 1
0.00.052.093 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.093 I print_info: rope_finetuned   = unknown
0.00.052.093 I print_info: ssm_d_conv       = 0
0.00.052.093 I print_info: ssm_d_inner      = 0
0.00.052.093 I print_info: ssm_d_state      = 0
0.00.052.094 I print_info: ssm_dt_rank      = 0
0.00.052.094 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.094 I print_info: model type       = 1.4B
0.00.052.094 I print_info: model params     = 1.41 B
0.00.052.095 I print_info: general.name     = 1.4B
0.00.052.095 I print_info: vocab type       = BPE
0.00.052.095 I print_info: n_vocab          = 50304
0.00.052.095 I print_info: n_merges         = 50009
0.00.052.096 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.096 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.096 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.096 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.096 I print_info: LF token         = 128 'Ä'
0.00.052.097 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.097 I print_info: max token length = 1024
0.00.053.692 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.692 I load_tensors: offloading output layer to GPU
0.00.053.692 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.702 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.703 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.977 I llama_init_from_model: n_seq_max     = 1
0.00.053.978 I llama_init_from_model: n_ctx         = 2048
0.00.053.978 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.978 I llama_init_from_model: n_batch       = 2048
0.00.053.978 I llama_init_from_model: n_ubatch      = 512
0.00.053.978 I llama_init_from_model: flash_attn    = 0
0.00.053.979 I llama_init_from_model: freq_base     = 10000.0
0.00.053.979 I llama_init_from_model: freq_scale    = 1
0.00.053.979 I ggml_metal_init: allocating
0.00.053.982 I ggml_metal_init: found device: Apple M4
0.00.053.984 I ggml_metal_init: picking default device: Apple M4
0.00.054.559 I ggml_metal_init: using embedded metal library
0.00.056.901 I ggml_metal_init: GPU name:   Apple M4
0.00.056.903 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.903 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.903 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.904 I ggml_metal_init: simdgroup reduction   = true
0.00.056.904 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.904 I ggml_metal_init: has bfloat            = true
0.00.056.904 I ggml_metal_init: use bfloat            = true
0.00.056.904 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.905 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.605 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.496 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.506 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.529 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.595 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.597 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.597 I llama_init_from_model: graph nodes  = 967
0.00.087.597 I llama_init_from_model: graph splits = 2
0.00.087.600 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.726 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.727 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.732.684 I main: llama threadpool init, n_threads = 4
0.00.732.721 I 
0.00.732.754 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.732.756 I 
0.00.732.986 I sampler seed: 1234
0.00.732.990 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.733.001 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.733.002 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.733.002 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.514.609 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55686.27 tokens per second)
0.01.514.609 I llama_perf_context_print:        load time =     721.30 ms
0.01.514.610 I llama_perf_context_print: prompt eval time =      46.13 ms /     7 tokens (    6.59 ms per token,   151.75 tokens per second)
0.01.514.611 I llama_perf_context_print:        eval time =     732.42 ms /    63 runs   (   11.63 ms per token,    86.02 tokens per second)
0.01.514.611 I llama_perf_context_print:       total time =     781.93 ms /    70 tokens
0.01.514.811 I ggml_metal_free: deallocating

real	0m1.532s
user	0m0.108s
sys	0m0.151s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.366 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.584 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.589 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.590 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.591 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.591 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.596 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.596 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.600 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.601 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.601 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.601 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.605 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.605 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.606 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.609 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.610 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.610 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.305 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.274 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.024 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.025 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.025 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.026 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.026 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.026 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.027 I llama_model_loader: - type  f32:  194 tensors
0.00.025.027 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.028 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.028 I print_info: file format = GGUF V3 (latest)
0.00.025.029 I print_info: file type   = Q5_1
0.00.025.030 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.486 I load: special tokens cache size = 25
0.00.049.341 I load: token to piece cache size = 0.2984 MB
0.00.049.344 I print_info: arch             = gptneox
0.00.049.344 I print_info: vocab_only       = 0
0.00.049.344 I print_info: n_ctx_train      = 2048
0.00.049.345 I print_info: n_embd           = 2048
0.00.049.345 I print_info: n_layer          = 24
0.00.049.348 I print_info: n_head           = 16
0.00.049.349 I print_info: n_head_kv        = 16
0.00.049.349 I print_info: n_rot            = 32
0.00.049.351 I print_info: n_swa            = 0
0.00.049.351 I print_info: n_embd_head_k    = 128
0.00.049.351 I print_info: n_embd_head_v    = 128
0.00.049.352 I print_info: n_gqa            = 1
0.00.049.353 I print_info: n_embd_k_gqa     = 2048
0.00.049.354 I print_info: n_embd_v_gqa     = 2048
0.00.049.354 I print_info: f_norm_eps       = 1.0e-05
0.00.049.354 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.355 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.355 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.355 I print_info: f_logit_scale    = 0.0e+00
0.00.049.356 I print_info: n_ff             = 8192
0.00.049.356 I print_info: n_expert         = 0
0.00.049.356 I print_info: n_expert_used    = 0
0.00.049.356 I print_info: causal attn      = 1
0.00.049.356 I print_info: pooling type     = 0
0.00.049.358 I print_info: rope type        = 2
0.00.049.359 I print_info: rope scaling     = linear
0.00.049.360 I print_info: freq_base_train  = 10000.0
0.00.049.360 I print_info: freq_scale_train = 1
0.00.049.360 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.360 I print_info: rope_finetuned   = unknown
0.00.049.362 I print_info: ssm_d_conv       = 0
0.00.049.362 I print_info: ssm_d_inner      = 0
0.00.049.362 I print_info: ssm_d_state      = 0
0.00.049.362 I print_info: ssm_dt_rank      = 0
0.00.049.362 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.362 I print_info: model type       = 1.4B
0.00.049.363 I print_info: model params     = 1.41 B
0.00.049.363 I print_info: general.name     = 1.4B
0.00.049.364 I print_info: vocab type       = BPE
0.00.049.364 I print_info: n_vocab          = 50304
0.00.049.365 I print_info: n_merges         = 50009
0.00.049.369 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.369 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.369 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.370 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.370 I print_info: LF token         = 128 'Ä'
0.00.049.370 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.370 I print_info: max token length = 1024
0.00.051.392 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.392 I load_tensors: offloading output layer to GPU
0.00.051.392 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.403 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.404 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.683 I llama_init_from_model: n_seq_max     = 1
0.00.051.684 I llama_init_from_model: n_ctx         = 2048
0.00.051.684 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.684 I llama_init_from_model: n_batch       = 2048
0.00.051.684 I llama_init_from_model: n_ubatch      = 512
0.00.051.684 I llama_init_from_model: flash_attn    = 0
0.00.051.685 I llama_init_from_model: freq_base     = 10000.0
0.00.051.685 I llama_init_from_model: freq_scale    = 1
0.00.051.686 I ggml_metal_init: allocating
0.00.051.688 I ggml_metal_init: found device: Apple M4
0.00.051.690 I ggml_metal_init: picking default device: Apple M4
0.00.052.283 I ggml_metal_init: using embedded metal library
0.00.054.630 I ggml_metal_init: GPU name:   Apple M4
0.00.054.632 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.632 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.633 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.633 I ggml_metal_init: simdgroup reduction   = true
0.00.054.633 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.633 I ggml_metal_init: has bfloat            = true
0.00.054.633 I ggml_metal_init: use bfloat            = true
0.00.054.634 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.634 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.112 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.256 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.264 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.287 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.280 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.281 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.282 I llama_init_from_model: graph nodes  = 967
0.00.084.282 I llama_init_from_model: graph splits = 2
0.00.084.285 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.401 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.402 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.233 I main: llama threadpool init, n_threads = 4
0.00.693.268 I 
0.00.693.322 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.325 I 
0.00.693.555 I sampler seed: 1234
0.00.693.559 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.693.607 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.693.612 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.693.612 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.534.641 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59814.66 tokens per second)
0.01.534.642 I llama_perf_context_print:        load time =     683.86 ms
0.01.534.642 I llama_perf_context_print: prompt eval time =      48.10 ms /     7 tokens (    6.87 ms per token,   145.52 tokens per second)
0.01.534.643 I llama_perf_context_print:        eval time =     789.96 ms /    63 runs   (   12.54 ms per token,    79.75 tokens per second)
0.01.534.643 I llama_perf_context_print:       total time =     841.41 ms /    70 tokens
0.01.534.846 I ggml_metal_free: deallocating

real	0m1.551s
user	0m0.107s
sys	0m0.157s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.939 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.574 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.580 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.586 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.586 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.587 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.587 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.587 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.588 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.588 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.589 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.589 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.589 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.590 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.590 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.592 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.592 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.592 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.423 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.439 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.210 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.211 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.211 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.212 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.212 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.212 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.213 I llama_model_loader: - type  f32:  194 tensors
0.00.025.213 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.213 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.214 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.214 I print_info: file format = GGUF V3 (latest)
0.00.025.215 I print_info: file type   = Q2_K - Medium
0.00.025.216 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.438 I load: special tokens cache size = 25
0.00.050.394 I load: token to piece cache size = 0.2984 MB
0.00.050.397 I print_info: arch             = gptneox
0.00.050.397 I print_info: vocab_only       = 0
0.00.050.398 I print_info: n_ctx_train      = 2048
0.00.050.398 I print_info: n_embd           = 2048
0.00.050.398 I print_info: n_layer          = 24
0.00.050.401 I print_info: n_head           = 16
0.00.050.401 I print_info: n_head_kv        = 16
0.00.050.402 I print_info: n_rot            = 32
0.00.050.404 I print_info: n_swa            = 0
0.00.050.404 I print_info: n_embd_head_k    = 128
0.00.050.404 I print_info: n_embd_head_v    = 128
0.00.050.405 I print_info: n_gqa            = 1
0.00.050.406 I print_info: n_embd_k_gqa     = 2048
0.00.050.406 I print_info: n_embd_v_gqa     = 2048
0.00.050.407 I print_info: f_norm_eps       = 1.0e-05
0.00.050.407 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.407 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.408 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.408 I print_info: f_logit_scale    = 0.0e+00
0.00.050.408 I print_info: n_ff             = 8192
0.00.050.408 I print_info: n_expert         = 0
0.00.050.409 I print_info: n_expert_used    = 0
0.00.050.409 I print_info: causal attn      = 1
0.00.050.409 I print_info: pooling type     = 0
0.00.050.409 I print_info: rope type        = 2
0.00.050.409 I print_info: rope scaling     = linear
0.00.050.410 I print_info: freq_base_train  = 10000.0
0.00.050.410 I print_info: freq_scale_train = 1
0.00.050.410 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.410 I print_info: rope_finetuned   = unknown
0.00.050.411 I print_info: ssm_d_conv       = 0
0.00.050.411 I print_info: ssm_d_inner      = 0
0.00.050.411 I print_info: ssm_d_state      = 0
0.00.050.411 I print_info: ssm_dt_rank      = 0
0.00.050.411 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.415 I print_info: model type       = 1.4B
0.00.050.417 I print_info: model params     = 1.41 B
0.00.050.417 I print_info: general.name     = 1.4B
0.00.050.418 I print_info: vocab type       = BPE
0.00.050.418 I print_info: n_vocab          = 50304
0.00.050.418 I print_info: n_merges         = 50009
0.00.050.419 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.420 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.421 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.421 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.421 I print_info: LF token         = 128 'Ä'
0.00.050.421 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.421 I print_info: max token length = 1024
0.00.052.337 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.337 I load_tensors: offloading output layer to GPU
0.00.052.337 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.347 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.348 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.629 I llama_init_from_model: n_seq_max     = 1
0.00.052.630 I llama_init_from_model: n_ctx         = 2048
0.00.052.630 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.630 I llama_init_from_model: n_batch       = 2048
0.00.052.630 I llama_init_from_model: n_ubatch      = 512
0.00.052.630 I llama_init_from_model: flash_attn    = 0
0.00.052.631 I llama_init_from_model: freq_base     = 10000.0
0.00.052.631 I llama_init_from_model: freq_scale    = 1
0.00.052.632 I ggml_metal_init: allocating
0.00.052.635 I ggml_metal_init: found device: Apple M4
0.00.052.636 I ggml_metal_init: picking default device: Apple M4
0.00.053.259 I ggml_metal_init: using embedded metal library
0.00.055.668 I ggml_metal_init: GPU name:   Apple M4
0.00.055.670 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.670 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.671 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.671 I ggml_metal_init: simdgroup reduction   = true
0.00.055.671 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.671 I ggml_metal_init: has bfloat            = true
0.00.055.671 I ggml_metal_init: use bfloat            = true
0.00.055.672 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.672 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.331 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.525 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.530 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.548 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.673 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.675 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.675 I llama_init_from_model: graph nodes  = 967
0.00.085.676 I llama_init_from_model: graph splits = 2
0.00.085.678 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.804 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.804 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.431.629 I main: llama threadpool init, n_threads = 4
0.00.431.667 I 
0.00.431.698 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.431.699 I 
0.00.431.924 I sampler seed: 1234
0.00.431.930 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.431.941 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.431.943 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.431.943 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.107.134 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55209.95 tokens per second)
0.01.107.135 I llama_perf_context_print:        load time =     421.69 ms
0.01.107.136 I llama_perf_context_print: prompt eval time =      38.80 ms /     7 tokens (    5.54 ms per token,   180.43 tokens per second)
0.01.107.137 I llama_perf_context_print:        eval time =     633.65 ms /    63 runs   (   10.06 ms per token,    99.42 tokens per second)
0.01.107.138 I llama_perf_context_print:       total time =     675.51 ms /    70 tokens
0.01.107.391 I ggml_metal_free: deallocating

real	0m1.126s
user	0m0.109s
sys	0m0.108s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.181 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.675 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.681 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.682 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.683 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.683 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.684 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.684 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.685 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.686 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.688 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.688 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.688 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.689 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.689 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.692 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.692 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.692 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.610 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.675 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.455 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.456 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.456 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.457 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.457 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.457 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.458 I llama_model_loader: - type  f32:  194 tensors
0.00.024.458 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.458 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.458 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.458 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.459 I print_info: file format = GGUF V3 (latest)
0.00.024.459 I print_info: file type   = Q3_K - Medium
0.00.024.460 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.729 I load: special tokens cache size = 25
0.00.049.792 I load: token to piece cache size = 0.2984 MB
0.00.049.795 I print_info: arch             = gptneox
0.00.049.795 I print_info: vocab_only       = 0
0.00.049.795 I print_info: n_ctx_train      = 2048
0.00.049.795 I print_info: n_embd           = 2048
0.00.049.795 I print_info: n_layer          = 24
0.00.049.799 I print_info: n_head           = 16
0.00.049.800 I print_info: n_head_kv        = 16
0.00.049.800 I print_info: n_rot            = 32
0.00.049.800 I print_info: n_swa            = 0
0.00.049.800 I print_info: n_embd_head_k    = 128
0.00.049.801 I print_info: n_embd_head_v    = 128
0.00.049.801 I print_info: n_gqa            = 1
0.00.049.802 I print_info: n_embd_k_gqa     = 2048
0.00.049.803 I print_info: n_embd_v_gqa     = 2048
0.00.049.803 I print_info: f_norm_eps       = 1.0e-05
0.00.049.804 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.804 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.804 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.804 I print_info: f_logit_scale    = 0.0e+00
0.00.049.805 I print_info: n_ff             = 8192
0.00.049.805 I print_info: n_expert         = 0
0.00.049.806 I print_info: n_expert_used    = 0
0.00.049.806 I print_info: causal attn      = 1
0.00.049.806 I print_info: pooling type     = 0
0.00.049.806 I print_info: rope type        = 2
0.00.049.806 I print_info: rope scaling     = linear
0.00.049.807 I print_info: freq_base_train  = 10000.0
0.00.049.807 I print_info: freq_scale_train = 1
0.00.049.807 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.808 I print_info: rope_finetuned   = unknown
0.00.049.808 I print_info: ssm_d_conv       = 0
0.00.049.808 I print_info: ssm_d_inner      = 0
0.00.049.808 I print_info: ssm_d_state      = 0
0.00.049.808 I print_info: ssm_dt_rank      = 0
0.00.049.808 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.809 I print_info: model type       = 1.4B
0.00.049.809 I print_info: model params     = 1.41 B
0.00.049.809 I print_info: general.name     = 1.4B
0.00.049.810 I print_info: vocab type       = BPE
0.00.049.812 I print_info: n_vocab          = 50304
0.00.049.812 I print_info: n_merges         = 50009
0.00.049.813 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.813 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.813 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.813 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.813 I print_info: LF token         = 128 'Ä'
0.00.049.815 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.815 I print_info: max token length = 1024
0.00.051.781 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.781 I load_tensors: offloading output layer to GPU
0.00.051.781 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.792 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.793 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.083 I llama_init_from_model: n_seq_max     = 1
0.00.052.084 I llama_init_from_model: n_ctx         = 2048
0.00.052.084 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.084 I llama_init_from_model: n_batch       = 2048
0.00.052.084 I llama_init_from_model: n_ubatch      = 512
0.00.052.085 I llama_init_from_model: flash_attn    = 0
0.00.052.085 I llama_init_from_model: freq_base     = 10000.0
0.00.052.085 I llama_init_from_model: freq_scale    = 1
0.00.052.086 I ggml_metal_init: allocating
0.00.052.088 I ggml_metal_init: found device: Apple M4
0.00.052.090 I ggml_metal_init: picking default device: Apple M4
0.00.052.696 I ggml_metal_init: using embedded metal library
0.00.055.055 I ggml_metal_init: GPU name:   Apple M4
0.00.055.057 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.057 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.057 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.057 I ggml_metal_init: simdgroup reduction   = true
0.00.055.058 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.058 I ggml_metal_init: has bfloat            = true
0.00.055.058 I ggml_metal_init: use bfloat            = true
0.00.055.058 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.059 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.835 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.619 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.623 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.643 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.690 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.691 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.691 I llama_init_from_model: graph nodes  = 967
0.00.084.692 I llama_init_from_model: graph splits = 2
0.00.084.694 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.823 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.823 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.520.766 I main: llama threadpool init, n_threads = 4
0.00.520.811 I 
0.00.520.844 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.520.847 I 
0.00.521.083 I sampler seed: 1234
0.00.521.088 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.521.139 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.521.141 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.521.141 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.266.019 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57864.71 tokens per second)
0.01.266.020 I llama_perf_context_print:        load time =     511.58 ms
0.01.266.021 I llama_perf_context_print: prompt eval time =      44.80 ms /     7 tokens (    6.40 ms per token,   156.24 tokens per second)
0.01.266.021 I llama_perf_context_print:        eval time =     697.05 ms /    63 runs   (   11.06 ms per token,    90.38 tokens per second)
0.01.266.021 I llama_perf_context_print:       total time =     745.26 ms /    70 tokens
0.01.266.259 I ggml_metal_free: deallocating

real	0m1.285s
user	0m0.109s
sys	0m0.115s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.770 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.485 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.489 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.491 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.491 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.491 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.491 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.492 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.493 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.493 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.494 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.494 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.494 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.495 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.495 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.496 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.497 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.497 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.324 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.280 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.031 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.033 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.033 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.033 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.033 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.034 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.034 I llama_model_loader: - type  f32:  194 tensors
0.00.024.034 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.034 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.035 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.035 I print_info: file format = GGUF V3 (latest)
0.00.024.035 I print_info: file type   = Q4_K - Medium
0.00.024.036 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.042.635 I load: special tokens cache size = 25
0.00.048.592 I load: token to piece cache size = 0.2984 MB
0.00.048.595 I print_info: arch             = gptneox
0.00.048.595 I print_info: vocab_only       = 0
0.00.048.596 I print_info: n_ctx_train      = 2048
0.00.048.596 I print_info: n_embd           = 2048
0.00.048.596 I print_info: n_layer          = 24
0.00.048.599 I print_info: n_head           = 16
0.00.048.599 I print_info: n_head_kv        = 16
0.00.048.600 I print_info: n_rot            = 32
0.00.048.600 I print_info: n_swa            = 0
0.00.048.600 I print_info: n_embd_head_k    = 128
0.00.048.600 I print_info: n_embd_head_v    = 128
0.00.048.601 I print_info: n_gqa            = 1
0.00.048.602 I print_info: n_embd_k_gqa     = 2048
0.00.048.602 I print_info: n_embd_v_gqa     = 2048
0.00.048.603 I print_info: f_norm_eps       = 1.0e-05
0.00.048.603 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.604 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.604 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.604 I print_info: f_logit_scale    = 0.0e+00
0.00.048.605 I print_info: n_ff             = 8192
0.00.048.605 I print_info: n_expert         = 0
0.00.048.605 I print_info: n_expert_used    = 0
0.00.048.605 I print_info: causal attn      = 1
0.00.048.605 I print_info: pooling type     = 0
0.00.048.605 I print_info: rope type        = 2
0.00.048.606 I print_info: rope scaling     = linear
0.00.048.608 I print_info: freq_base_train  = 10000.0
0.00.048.609 I print_info: freq_scale_train = 1
0.00.048.609 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.609 I print_info: rope_finetuned   = unknown
0.00.048.609 I print_info: ssm_d_conv       = 0
0.00.048.609 I print_info: ssm_d_inner      = 0
0.00.048.611 I print_info: ssm_d_state      = 0
0.00.048.611 I print_info: ssm_dt_rank      = 0
0.00.048.611 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.611 I print_info: model type       = 1.4B
0.00.048.612 I print_info: model params     = 1.41 B
0.00.048.612 I print_info: general.name     = 1.4B
0.00.048.614 I print_info: vocab type       = BPE
0.00.048.614 I print_info: n_vocab          = 50304
0.00.048.618 I print_info: n_merges         = 50009
0.00.048.618 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.619 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.619 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.619 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.620 I print_info: LF token         = 128 'Ä'
0.00.048.620 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.620 I print_info: max token length = 1024
0.00.050.578 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.578 I load_tensors: offloading output layer to GPU
0.00.050.579 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.589 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.050.590 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.050.879 I llama_init_from_model: n_seq_max     = 1
0.00.050.880 I llama_init_from_model: n_ctx         = 2048
0.00.050.880 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.050.881 I llama_init_from_model: n_batch       = 2048
0.00.050.881 I llama_init_from_model: n_ubatch      = 512
0.00.050.881 I llama_init_from_model: flash_attn    = 0
0.00.050.881 I llama_init_from_model: freq_base     = 10000.0
0.00.050.882 I llama_init_from_model: freq_scale    = 1
0.00.050.882 I ggml_metal_init: allocating
0.00.050.885 I ggml_metal_init: found device: Apple M4
0.00.050.887 I ggml_metal_init: picking default device: Apple M4
0.00.051.489 I ggml_metal_init: using embedded metal library
0.00.053.795 I ggml_metal_init: GPU name:   Apple M4
0.00.053.796 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.797 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.797 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.797 I ggml_metal_init: simdgroup reduction   = true
0.00.053.797 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.797 I ggml_metal_init: has bfloat            = true
0.00.053.798 I ggml_metal_init: use bfloat            = true
0.00.053.798 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.799 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.254 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.953 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.959 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.982 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.933 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.934 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.934 I llama_init_from_model: graph nodes  = 967
0.00.084.935 I llama_init_from_model: graph splits = 2
0.00.084.938 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.066 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.067 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.110 I main: llama threadpool init, n_threads = 4
0.00.663.161 I 
0.00.663.201 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.202 I 
0.00.663.439 I sampler seed: 1234
0.00.663.445 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.663.478 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.663.489 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.663.490 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.424.034 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55686.27 tokens per second)
0.01.424.035 I llama_perf_context_print:        load time =     654.33 ms
0.01.424.035 I llama_perf_context_print: prompt eval time =      47.03 ms /     7 tokens (    6.72 ms per token,   148.84 tokens per second)
0.01.424.036 I llama_perf_context_print:        eval time =     710.43 ms /    63 runs   (   11.28 ms per token,    88.68 tokens per second)
0.01.424.036 I llama_perf_context_print:       total time =     760.93 ms /    70 tokens
0.01.424.300 I ggml_metal_free: deallocating

real	0m1.441s
user	0m0.108s
sys	0m0.149s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.010.381 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.946 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.953 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.958 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.959 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.959 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.960 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.960 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.964 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.964 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.965 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.965 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.965 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.966 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.966 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.969 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.970 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.970 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.821 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.852 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.613 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.614 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.614 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.615 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.615 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.615 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.616 I llama_model_loader: - type  f32:  194 tensors
0.00.027.616 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.617 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.617 I print_info: file format = GGUF V3 (latest)
0.00.027.618 I print_info: file type   = Q5_K - Medium
0.00.027.619 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.047.117 I load: special tokens cache size = 25
0.00.052.933 I load: token to piece cache size = 0.2984 MB
0.00.052.937 I print_info: arch             = gptneox
0.00.052.937 I print_info: vocab_only       = 0
0.00.052.937 I print_info: n_ctx_train      = 2048
0.00.052.938 I print_info: n_embd           = 2048
0.00.052.938 I print_info: n_layer          = 24
0.00.052.941 I print_info: n_head           = 16
0.00.052.942 I print_info: n_head_kv        = 16
0.00.052.942 I print_info: n_rot            = 32
0.00.052.942 I print_info: n_swa            = 0
0.00.052.942 I print_info: n_embd_head_k    = 128
0.00.052.943 I print_info: n_embd_head_v    = 128
0.00.052.943 I print_info: n_gqa            = 1
0.00.052.944 I print_info: n_embd_k_gqa     = 2048
0.00.052.944 I print_info: n_embd_v_gqa     = 2048
0.00.052.945 I print_info: f_norm_eps       = 1.0e-05
0.00.052.945 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.945 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.946 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.946 I print_info: f_logit_scale    = 0.0e+00
0.00.052.946 I print_info: n_ff             = 8192
0.00.052.947 I print_info: n_expert         = 0
0.00.052.947 I print_info: n_expert_used    = 0
0.00.052.947 I print_info: causal attn      = 1
0.00.052.947 I print_info: pooling type     = 0
0.00.052.948 I print_info: rope type        = 2
0.00.052.950 I print_info: rope scaling     = linear
0.00.052.951 I print_info: freq_base_train  = 10000.0
0.00.052.951 I print_info: freq_scale_train = 1
0.00.052.952 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.953 I print_info: rope_finetuned   = unknown
0.00.052.953 I print_info: ssm_d_conv       = 0
0.00.052.953 I print_info: ssm_d_inner      = 0
0.00.052.953 I print_info: ssm_d_state      = 0
0.00.052.953 I print_info: ssm_dt_rank      = 0
0.00.052.953 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.953 I print_info: model type       = 1.4B
0.00.052.954 I print_info: model params     = 1.41 B
0.00.052.954 I print_info: general.name     = 1.4B
0.00.052.954 I print_info: vocab type       = BPE
0.00.052.954 I print_info: n_vocab          = 50304
0.00.052.955 I print_info: n_merges         = 50009
0.00.052.955 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.955 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.955 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.955 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.955 I print_info: LF token         = 128 'Ä'
0.00.052.956 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.956 I print_info: max token length = 1024
0.00.054.986 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.986 I load_tensors: offloading output layer to GPU
0.00.054.987 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.998 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.000 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.055.374 I llama_init_from_model: n_seq_max     = 1
0.00.055.375 I llama_init_from_model: n_ctx         = 2048
0.00.055.375 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.376 I llama_init_from_model: n_batch       = 2048
0.00.055.376 I llama_init_from_model: n_ubatch      = 512
0.00.055.376 I llama_init_from_model: flash_attn    = 0
0.00.055.376 I llama_init_from_model: freq_base     = 10000.0
0.00.055.377 I llama_init_from_model: freq_scale    = 1
0.00.055.377 I ggml_metal_init: allocating
0.00.055.382 I ggml_metal_init: found device: Apple M4
0.00.055.384 I ggml_metal_init: picking default device: Apple M4
0.00.056.056 I ggml_metal_init: using embedded metal library
0.00.058.469 I ggml_metal_init: GPU name:   Apple M4
0.00.058.470 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.471 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.471 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.472 I ggml_metal_init: simdgroup reduction   = true
0.00.058.472 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.472 I ggml_metal_init: has bfloat            = true
0.00.058.472 I ggml_metal_init: use bfloat            = true
0.00.058.473 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.473 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.689 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.720 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.733 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.752 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.757 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.758 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.759 I llama_init_from_model: graph nodes  = 967
0.00.088.759 I llama_init_from_model: graph splits = 2
0.00.088.764 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.887 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.888 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.759.264 I main: llama threadpool init, n_threads = 4
0.00.759.347 I 
0.00.759.397 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.759.397 I 
0.00.759.736 I sampler seed: 1234
0.00.759.742 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.759.755 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.759.757 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.759.757 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.609.767 I llama_perf_sampler_print:    sampling time =       1.68 ms /    71 runs   (    0.02 ms per token, 42362.77 tokens per second)
0.01.609.767 I llama_perf_context_print:        load time =     748.88 ms
0.01.609.768 I llama_perf_context_print: prompt eval time =      51.81 ms /     7 tokens (    7.40 ms per token,   135.10 tokens per second)
0.01.609.769 I llama_perf_context_print:        eval time =     795.26 ms /    63 runs   (   12.62 ms per token,    79.22 tokens per second)
0.01.609.769 I llama_perf_context_print:       total time =     850.51 ms /    70 tokens
0.01.610.050 I ggml_metal_free: deallocating

real	0m1.629s
user	0m0.123s
sys	0m0.130s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.156 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.391 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.396 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.398 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.398 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.399 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.399 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.399 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.400 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.401 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.401 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.401 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.402 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.402 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.403 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.404 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.405 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.405 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.155 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.240 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.098 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.099 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.099 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.100 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.100 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.100 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.101 I llama_model_loader: - type  f32:  194 tensors
0.00.025.101 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.101 I print_info: file format = GGUF V3 (latest)
0.00.025.102 I print_info: file type   = Q6_K
0.00.025.103 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.728 I load: special tokens cache size = 25
0.00.049.787 I load: token to piece cache size = 0.2984 MB
0.00.049.790 I print_info: arch             = gptneox
0.00.049.790 I print_info: vocab_only       = 0
0.00.049.790 I print_info: n_ctx_train      = 2048
0.00.049.790 I print_info: n_embd           = 2048
0.00.049.791 I print_info: n_layer          = 24
0.00.049.794 I print_info: n_head           = 16
0.00.049.795 I print_info: n_head_kv        = 16
0.00.049.795 I print_info: n_rot            = 32
0.00.049.795 I print_info: n_swa            = 0
0.00.049.795 I print_info: n_embd_head_k    = 128
0.00.049.795 I print_info: n_embd_head_v    = 128
0.00.049.796 I print_info: n_gqa            = 1
0.00.049.797 I print_info: n_embd_k_gqa     = 2048
0.00.049.801 I print_info: n_embd_v_gqa     = 2048
0.00.049.801 I print_info: f_norm_eps       = 1.0e-05
0.00.049.802 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.802 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.802 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.802 I print_info: f_logit_scale    = 0.0e+00
0.00.049.803 I print_info: n_ff             = 8192
0.00.049.803 I print_info: n_expert         = 0
0.00.049.803 I print_info: n_expert_used    = 0
0.00.049.804 I print_info: causal attn      = 1
0.00.049.804 I print_info: pooling type     = 0
0.00.049.805 I print_info: rope type        = 2
0.00.049.806 I print_info: rope scaling     = linear
0.00.049.806 I print_info: freq_base_train  = 10000.0
0.00.049.806 I print_info: freq_scale_train = 1
0.00.049.806 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.807 I print_info: rope_finetuned   = unknown
0.00.049.807 I print_info: ssm_d_conv       = 0
0.00.049.807 I print_info: ssm_d_inner      = 0
0.00.049.807 I print_info: ssm_d_state      = 0
0.00.049.807 I print_info: ssm_dt_rank      = 0
0.00.049.807 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.807 I print_info: model type       = 1.4B
0.00.049.808 I print_info: model params     = 1.41 B
0.00.049.808 I print_info: general.name     = 1.4B
0.00.049.812 I print_info: vocab type       = BPE
0.00.049.812 I print_info: n_vocab          = 50304
0.00.049.812 I print_info: n_merges         = 50009
0.00.049.812 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.813 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.813 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.814 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.814 I print_info: LF token         = 128 'Ä'
0.00.049.814 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.814 I print_info: max token length = 1024
0.00.051.810 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.810 I load_tensors: offloading output layer to GPU
0.00.051.811 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.821 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.823 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.110 I llama_init_from_model: n_seq_max     = 1
0.00.052.111 I llama_init_from_model: n_ctx         = 2048
0.00.052.111 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.111 I llama_init_from_model: n_batch       = 2048
0.00.052.112 I llama_init_from_model: n_ubatch      = 512
0.00.052.112 I llama_init_from_model: flash_attn    = 0
0.00.052.112 I llama_init_from_model: freq_base     = 10000.0
0.00.052.112 I llama_init_from_model: freq_scale    = 1
0.00.052.113 I ggml_metal_init: allocating
0.00.052.116 I ggml_metal_init: found device: Apple M4
0.00.052.118 I ggml_metal_init: picking default device: Apple M4
0.00.052.706 I ggml_metal_init: using embedded metal library
0.00.055.103 I ggml_metal_init: GPU name:   Apple M4
0.00.055.104 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.105 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.105 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.106 I ggml_metal_init: simdgroup reduction   = true
0.00.055.106 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.106 I ggml_metal_init: has bfloat            = true
0.00.055.106 I ggml_metal_init: use bfloat            = true
0.00.055.107 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.107 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.074 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.569 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.575 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.599 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.530 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.533 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.533 I llama_init_from_model: graph nodes  = 967
0.00.085.533 I llama_init_from_model: graph splits = 2
0.00.085.537 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.665 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.666 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.945.371 I main: llama threadpool init, n_threads = 4
0.00.945.423 I 
0.00.945.460 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.945.461 I 
0.00.945.694 I sampler seed: 1234
0.00.945.700 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.945.712 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.945.715 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.945.715 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.819.863 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59117.40 tokens per second)
0.01.819.864 I llama_perf_context_print:        load time =     936.21 ms
0.01.819.865 I llama_perf_context_print: prompt eval time =      54.43 ms /     7 tokens (    7.78 ms per token,   128.62 tokens per second)
0.01.819.866 I llama_perf_context_print:        eval time =     816.76 ms /    63 runs   (   12.96 ms per token,    77.13 tokens per second)
0.01.819.867 I llama_perf_context_print:       total time =     874.50 ms /    70 tokens
0.01.820.128 I ggml_metal_free: deallocating

real	0m1.837s
user	0m0.109s
sys	0m0.183s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.861 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.034.812 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.048.262 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.048.279 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.048.284 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.048.285 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.048.285 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.048.286 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.048.286 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.048.289 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.048.290 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.048.291 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.048.292 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.048.293 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.048.293 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.048.294 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.048.299 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.048.300 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.048.300 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.055.682 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.058.545 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.067.162 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.067.166 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.067.166 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.067.167 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.067.167 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.067.168 I llama_model_loader: - type  f32:  194 tensors
0.00.067.169 I llama_model_loader: - type  f16:   98 tensors
0.00.067.170 I print_info: file format = GGUF V3 (latest)
0.00.067.171 I print_info: file type   = all F32 (guessed)
0.00.067.173 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.097.557 I load: special tokens cache size = 25
0.00.104.479 I load: token to piece cache size = 0.2984 MB
0.00.104.482 I print_info: arch             = gptneox
0.00.104.482 I print_info: vocab_only       = 0
0.00.104.482 I print_info: n_ctx_train      = 2048
0.00.104.482 I print_info: n_embd           = 2048
0.00.104.482 I print_info: n_layer          = 24
0.00.104.485 I print_info: n_head           = 16
0.00.104.486 I print_info: n_head_kv        = 16
0.00.104.486 I print_info: n_rot            = 32
0.00.104.487 I print_info: n_swa            = 0
0.00.104.487 I print_info: n_embd_head_k    = 128
0.00.104.487 I print_info: n_embd_head_v    = 128
0.00.104.488 I print_info: n_gqa            = 1
0.00.104.488 I print_info: n_embd_k_gqa     = 2048
0.00.104.489 I print_info: n_embd_v_gqa     = 2048
0.00.104.489 I print_info: f_norm_eps       = 1.0e-05
0.00.104.490 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.104.490 I print_info: f_clamp_kqv      = 0.0e+00
0.00.104.490 I print_info: f_max_alibi_bias = 0.0e+00
0.00.104.490 I print_info: f_logit_scale    = 0.0e+00
0.00.104.491 I print_info: n_ff             = 8192
0.00.104.491 I print_info: n_expert         = 0
0.00.104.491 I print_info: n_expert_used    = 0
0.00.104.491 I print_info: causal attn      = 1
0.00.104.491 I print_info: pooling type     = 0
0.00.104.492 I print_info: rope type        = 2
0.00.104.492 I print_info: rope scaling     = linear
0.00.104.495 I print_info: freq_base_train  = 10000.0
0.00.104.495 I print_info: freq_scale_train = 1
0.00.104.495 I print_info: n_ctx_orig_yarn  = 2048
0.00.104.495 I print_info: rope_finetuned   = unknown
0.00.104.495 I print_info: ssm_d_conv       = 0
0.00.104.496 I print_info: ssm_d_inner      = 0
0.00.104.496 I print_info: ssm_d_state      = 0
0.00.104.496 I print_info: ssm_dt_rank      = 0
0.00.104.496 I print_info: ssm_dt_b_c_rms   = 0
0.00.104.496 I print_info: model type       = 1.4B
0.00.104.497 I print_info: model params     = 1.41 B
0.00.104.497 I print_info: general.name     = 1.4B
0.00.104.497 I print_info: vocab type       = BPE
0.00.104.497 I print_info: n_vocab          = 50304
0.00.104.497 I print_info: n_merges         = 50009
0.00.104.498 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.104.498 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.104.498 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.104.498 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.104.502 I print_info: LF token         = 128 'Ä'
0.00.104.502 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.104.502 I print_info: max token length = 1024
0.00.106.612 I load_tensors: offloading 24 repeating layers to GPU
0.00.106.612 I load_tensors: offloading output layer to GPU
0.00.106.613 I load_tensors: offloaded 25/25 layers to GPU
0.00.106.618 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.106.619 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.106.889 I llama_init_from_model: n_seq_max     = 1
0.00.106.890 I llama_init_from_model: n_ctx         = 128
0.00.106.890 I llama_init_from_model: n_ctx_per_seq = 128
0.00.106.890 I llama_init_from_model: n_batch       = 128
0.00.106.890 I llama_init_from_model: n_ubatch      = 128
0.00.106.890 I llama_init_from_model: flash_attn    = 0
0.00.106.891 I llama_init_from_model: freq_base     = 10000.0
0.00.106.891 I llama_init_from_model: freq_scale    = 1
0.00.106.891 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.106.892 I ggml_metal_init: allocating
0.00.106.895 I ggml_metal_init: found device: Apple M4
0.00.106.897 I ggml_metal_init: picking default device: Apple M4
0.00.107.540 I ggml_metal_init: using embedded metal library
0.00.110.250 I ggml_metal_init: GPU name:   Apple M4
0.00.110.252 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.110.253 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.110.253 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.110.253 I ggml_metal_init: simdgroup reduction   = true
0.00.110.253 I ggml_metal_init: simdgroup matrix mul. = true
0.00.110.254 I ggml_metal_init: has bfloat            = true
0.00.110.254 I ggml_metal_init: use bfloat            = true
0.00.110.254 I ggml_metal_init: hasUnifiedMemory      = true
0.00.110.255 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.119.421 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.120.674 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.120.679 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.120.693 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.121.538 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.121.539 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.121.539 I llama_init_from_model: graph nodes  = 967
0.00.121.539 I llama_init_from_model: graph splits = 2
0.00.121.541 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.121.541 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.300.072 I 
0.01.300.110 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.300.115 I perplexity: tokenizing the input ..
0.01.312.460 I perplexity: tokenization took 12.342 ms
0.01.312.465 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.445.787 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.447.285 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.447.336 I llama_perf_context_print:        load time =    1265.25 ms
0.01.447.338 I llama_perf_context_print: prompt eval time =     132.93 ms /   128 tokens (    1.04 ms per token,   962.91 tokens per second)
0.01.447.338 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.447.339 I llama_perf_context_print:       total time =     147.27 ms /   129 tokens
0.01.448.119 I ggml_metal_free: deallocating

real	0m1.642s
user	0m0.124s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.818 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.242 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.247 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.249 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.250 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.250 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.251 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.254 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.255 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.255 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.256 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.256 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.256 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.257 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.257 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.261 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.261 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.262 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.181 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.532 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.454 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.456 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.456 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.457 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.457 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.457 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.458 I llama_model_loader: - type  f32:  194 tensors
0.00.033.458 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.459 I print_info: file format = GGUF V3 (latest)
0.00.033.460 I print_info: file type   = Q8_0
0.00.033.461 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.056.966 I load: special tokens cache size = 25
0.00.063.037 I load: token to piece cache size = 0.2984 MB
0.00.063.040 I print_info: arch             = gptneox
0.00.063.040 I print_info: vocab_only       = 0
0.00.063.040 I print_info: n_ctx_train      = 2048
0.00.063.040 I print_info: n_embd           = 2048
0.00.063.041 I print_info: n_layer          = 24
0.00.063.044 I print_info: n_head           = 16
0.00.063.045 I print_info: n_head_kv        = 16
0.00.063.047 I print_info: n_rot            = 32
0.00.063.047 I print_info: n_swa            = 0
0.00.063.048 I print_info: n_embd_head_k    = 128
0.00.063.048 I print_info: n_embd_head_v    = 128
0.00.063.050 I print_info: n_gqa            = 1
0.00.063.051 I print_info: n_embd_k_gqa     = 2048
0.00.063.052 I print_info: n_embd_v_gqa     = 2048
0.00.063.052 I print_info: f_norm_eps       = 1.0e-05
0.00.063.053 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.053 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.053 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.053 I print_info: f_logit_scale    = 0.0e+00
0.00.063.054 I print_info: n_ff             = 8192
0.00.063.054 I print_info: n_expert         = 0
0.00.063.055 I print_info: n_expert_used    = 0
0.00.063.055 I print_info: causal attn      = 1
0.00.063.055 I print_info: pooling type     = 0
0.00.063.055 I print_info: rope type        = 2
0.00.063.055 I print_info: rope scaling     = linear
0.00.063.056 I print_info: freq_base_train  = 10000.0
0.00.063.056 I print_info: freq_scale_train = 1
0.00.063.056 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.057 I print_info: rope_finetuned   = unknown
0.00.063.057 I print_info: ssm_d_conv       = 0
0.00.063.057 I print_info: ssm_d_inner      = 0
0.00.063.057 I print_info: ssm_d_state      = 0
0.00.063.057 I print_info: ssm_dt_rank      = 0
0.00.063.058 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.058 I print_info: model type       = 1.4B
0.00.063.060 I print_info: model params     = 1.41 B
0.00.063.060 I print_info: general.name     = 1.4B
0.00.063.060 I print_info: vocab type       = BPE
0.00.063.060 I print_info: n_vocab          = 50304
0.00.063.061 I print_info: n_merges         = 50009
0.00.063.061 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.061 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.061 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.061 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.062 I print_info: LF token         = 128 'Ä'
0.00.063.062 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.062 I print_info: max token length = 1024
0.00.065.360 I load_tensors: offloading 24 repeating layers to GPU
0.00.065.360 I load_tensors: offloading output layer to GPU
0.00.065.360 I load_tensors: offloaded 25/25 layers to GPU
0.00.065.372 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.373 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.065.678 I llama_init_from_model: n_seq_max     = 1
0.00.065.679 I llama_init_from_model: n_ctx         = 128
0.00.065.680 I llama_init_from_model: n_ctx_per_seq = 128
0.00.065.680 I llama_init_from_model: n_batch       = 128
0.00.065.680 I llama_init_from_model: n_ubatch      = 128
0.00.065.680 I llama_init_from_model: flash_attn    = 0
0.00.065.681 I llama_init_from_model: freq_base     = 10000.0
0.00.065.681 I llama_init_from_model: freq_scale    = 1
0.00.065.681 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.065.682 I ggml_metal_init: allocating
0.00.065.685 I ggml_metal_init: found device: Apple M4
0.00.065.687 I ggml_metal_init: picking default device: Apple M4
0.00.066.362 I ggml_metal_init: using embedded metal library
0.00.069.011 I ggml_metal_init: GPU name:   Apple M4
0.00.069.013 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.014 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.014 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.014 I ggml_metal_init: simdgroup reduction   = true
0.00.069.014 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.015 I ggml_metal_init: has bfloat            = true
0.00.069.015 I ggml_metal_init: use bfloat            = true
0.00.069.015 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.016 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.942 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.080.393 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.080.395 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.080.411 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.081.504 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.081.505 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.081.505 I llama_init_from_model: graph nodes  = 967
0.00.081.505 I llama_init_from_model: graph splits = 2
0.00.081.507 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.081.507 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.010.337 I 
0.01.010.380 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.010.383 I perplexity: tokenizing the input ..
0.01.018.457 I perplexity: tokenization took 8.072 ms
0.01.018.460 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.142.781 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.143.934 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.143.961 I llama_perf_context_print:        load time =     999.51 ms
0.01.143.962 I llama_perf_context_print: prompt eval time =     124.09 ms /   128 tokens (    0.97 ms per token,  1031.48 tokens per second)
0.01.143.963 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.143.964 I llama_perf_context_print:       total time =     133.62 ms /   129 tokens
0.01.144.449 I ggml_metal_free: deallocating

real	0m1.161s
user	0m0.089s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.192 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.900 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.904 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.906 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.907 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.907 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.907 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.908 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.909 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.911 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.911 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.912 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.912 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.912 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.913 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.914 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.915 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.915 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.701 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.724 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.598 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.600 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.600 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.600 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.601 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.601 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.601 I llama_model_loader: - type  f32:  194 tensors
0.00.025.602 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.602 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.603 I print_info: file format = GGUF V3 (latest)
0.00.025.603 I print_info: file type   = Q4_0
0.00.025.604 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.505 I load: special tokens cache size = 25
0.00.050.428 I load: token to piece cache size = 0.2984 MB
0.00.050.431 I print_info: arch             = gptneox
0.00.050.431 I print_info: vocab_only       = 0
0.00.050.431 I print_info: n_ctx_train      = 2048
0.00.050.431 I print_info: n_embd           = 2048
0.00.050.431 I print_info: n_layer          = 24
0.00.050.434 I print_info: n_head           = 16
0.00.050.435 I print_info: n_head_kv        = 16
0.00.050.436 I print_info: n_rot            = 32
0.00.050.436 I print_info: n_swa            = 0
0.00.050.436 I print_info: n_embd_head_k    = 128
0.00.050.436 I print_info: n_embd_head_v    = 128
0.00.050.437 I print_info: n_gqa            = 1
0.00.050.438 I print_info: n_embd_k_gqa     = 2048
0.00.050.438 I print_info: n_embd_v_gqa     = 2048
0.00.050.439 I print_info: f_norm_eps       = 1.0e-05
0.00.050.439 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.440 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.441 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.442 I print_info: f_logit_scale    = 0.0e+00
0.00.050.443 I print_info: n_ff             = 8192
0.00.050.445 I print_info: n_expert         = 0
0.00.050.445 I print_info: n_expert_used    = 0
0.00.050.445 I print_info: causal attn      = 1
0.00.050.445 I print_info: pooling type     = 0
0.00.050.445 I print_info: rope type        = 2
0.00.050.445 I print_info: rope scaling     = linear
0.00.050.446 I print_info: freq_base_train  = 10000.0
0.00.050.446 I print_info: freq_scale_train = 1
0.00.050.446 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.446 I print_info: rope_finetuned   = unknown
0.00.050.447 I print_info: ssm_d_conv       = 0
0.00.050.447 I print_info: ssm_d_inner      = 0
0.00.050.447 I print_info: ssm_d_state      = 0
0.00.050.448 I print_info: ssm_dt_rank      = 0
0.00.050.449 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.449 I print_info: model type       = 1.4B
0.00.050.449 I print_info: model params     = 1.41 B
0.00.050.449 I print_info: general.name     = 1.4B
0.00.050.450 I print_info: vocab type       = BPE
0.00.050.450 I print_info: n_vocab          = 50304
0.00.050.450 I print_info: n_merges         = 50009
0.00.050.451 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.451 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.451 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.451 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.451 I print_info: LF token         = 128 'Ä'
0.00.050.452 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.452 I print_info: max token length = 1024
0.00.052.372 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.372 I load_tensors: offloading output layer to GPU
0.00.052.372 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.382 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.384 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.664 I llama_init_from_model: n_seq_max     = 1
0.00.052.665 I llama_init_from_model: n_ctx         = 128
0.00.052.665 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.665 I llama_init_from_model: n_batch       = 128
0.00.052.665 I llama_init_from_model: n_ubatch      = 128
0.00.052.665 I llama_init_from_model: flash_attn    = 0
0.00.052.666 I llama_init_from_model: freq_base     = 10000.0
0.00.052.666 I llama_init_from_model: freq_scale    = 1
0.00.052.666 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.667 I ggml_metal_init: allocating
0.00.052.669 I ggml_metal_init: found device: Apple M4
0.00.052.671 I ggml_metal_init: picking default device: Apple M4
0.00.053.241 I ggml_metal_init: using embedded metal library
0.00.055.560 I ggml_metal_init: GPU name:   Apple M4
0.00.055.562 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.562 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.562 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.563 I ggml_metal_init: simdgroup reduction   = true
0.00.055.563 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.563 I ggml_metal_init: has bfloat            = true
0.00.055.563 I ggml_metal_init: use bfloat            = true
0.00.055.564 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.564 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.443 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.695 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.699 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.722 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.562 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.563 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.564 I llama_init_from_model: graph nodes  = 967
0.00.067.564 I llama_init_from_model: graph splits = 2
0.00.067.565 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.565 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.180 I 
0.00.681.224 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.229 I perplexity: tokenizing the input ..
0.00.688.945 I perplexity: tokenization took 7.714 ms
0.00.688.949 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.811.818 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.813.031 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.813.054 I llama_perf_context_print:        load time =     670.98 ms
0.00.813.055 I llama_perf_context_print: prompt eval time =     122.64 ms /   128 tokens (    0.96 ms per token,  1043.68 tokens per second)
0.00.813.057 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.813.058 I llama_perf_context_print:       total time =     131.88 ms /   129 tokens
0.00.813.451 I ggml_metal_free: deallocating

real	0m0.828s
user	0m0.076s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.849 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.992 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.997 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.999 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.000 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.002 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.002 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.002 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.003 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.004 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.007 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.007 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.007 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.008 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.008 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.011 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.011 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.012 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.792 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.804 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.582 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.583 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.583 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.584 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.584 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.584 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.585 I llama_model_loader: - type  f32:  194 tensors
0.00.024.585 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.585 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.586 I print_info: file format = GGUF V3 (latest)
0.00.024.586 I print_info: file type   = Q4_1
0.00.024.587 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.839 I load: special tokens cache size = 25
0.00.049.580 I load: token to piece cache size = 0.2984 MB
0.00.049.583 I print_info: arch             = gptneox
0.00.049.583 I print_info: vocab_only       = 0
0.00.049.583 I print_info: n_ctx_train      = 2048
0.00.049.584 I print_info: n_embd           = 2048
0.00.049.584 I print_info: n_layer          = 24
0.00.049.587 I print_info: n_head           = 16
0.00.049.589 I print_info: n_head_kv        = 16
0.00.049.589 I print_info: n_rot            = 32
0.00.049.589 I print_info: n_swa            = 0
0.00.049.590 I print_info: n_embd_head_k    = 128
0.00.049.590 I print_info: n_embd_head_v    = 128
0.00.049.590 I print_info: n_gqa            = 1
0.00.049.591 I print_info: n_embd_k_gqa     = 2048
0.00.049.592 I print_info: n_embd_v_gqa     = 2048
0.00.049.593 I print_info: f_norm_eps       = 1.0e-05
0.00.049.593 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.593 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.593 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.593 I print_info: f_logit_scale    = 0.0e+00
0.00.049.594 I print_info: n_ff             = 8192
0.00.049.594 I print_info: n_expert         = 0
0.00.049.595 I print_info: n_expert_used    = 0
0.00.049.595 I print_info: causal attn      = 1
0.00.049.595 I print_info: pooling type     = 0
0.00.049.595 I print_info: rope type        = 2
0.00.049.595 I print_info: rope scaling     = linear
0.00.049.599 I print_info: freq_base_train  = 10000.0
0.00.049.599 I print_info: freq_scale_train = 1
0.00.049.599 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.600 I print_info: rope_finetuned   = unknown
0.00.049.600 I print_info: ssm_d_conv       = 0
0.00.049.600 I print_info: ssm_d_inner      = 0
0.00.049.600 I print_info: ssm_d_state      = 0
0.00.049.600 I print_info: ssm_dt_rank      = 0
0.00.049.600 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.601 I print_info: model type       = 1.4B
0.00.049.601 I print_info: model params     = 1.41 B
0.00.049.605 I print_info: general.name     = 1.4B
0.00.049.605 I print_info: vocab type       = BPE
0.00.049.607 I print_info: n_vocab          = 50304
0.00.049.607 I print_info: n_merges         = 50009
0.00.049.607 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.607 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.607 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.607 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.608 I print_info: LF token         = 128 'Ä'
0.00.049.608 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.608 I print_info: max token length = 1024
0.00.051.602 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.602 I load_tensors: offloading output layer to GPU
0.00.051.602 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.613 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.614 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.904 I llama_init_from_model: n_seq_max     = 1
0.00.051.905 I llama_init_from_model: n_ctx         = 128
0.00.051.905 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.905 I llama_init_from_model: n_batch       = 128
0.00.051.905 I llama_init_from_model: n_ubatch      = 128
0.00.051.905 I llama_init_from_model: flash_attn    = 0
0.00.051.906 I llama_init_from_model: freq_base     = 10000.0
0.00.051.906 I llama_init_from_model: freq_scale    = 1
0.00.051.906 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.907 I ggml_metal_init: allocating
0.00.051.910 I ggml_metal_init: found device: Apple M4
0.00.051.911 I ggml_metal_init: picking default device: Apple M4
0.00.052.490 I ggml_metal_init: using embedded metal library
0.00.054.829 I ggml_metal_init: GPU name:   Apple M4
0.00.054.831 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.831 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.831 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.832 I ggml_metal_init: simdgroup reduction   = true
0.00.054.832 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.832 I ggml_metal_init: has bfloat            = true
0.00.054.832 I ggml_metal_init: use bfloat            = true
0.00.054.832 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.833 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.499 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.773 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.777 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.793 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.699 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.700 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.700 I llama_init_from_model: graph nodes  = 967
0.00.066.701 I llama_init_from_model: graph splits = 2
0.00.066.702 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.702 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.687.991 I 
0.00.688.024 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.688.028 I perplexity: tokenizing the input ..
0.00.695.944 I perplexity: tokenization took 7.914 ms
0.00.695.947 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.818.760 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.819.920 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.819.953 I llama_perf_context_print:        load time =     679.14 ms
0.00.819.954 I llama_perf_context_print: prompt eval time =     122.59 ms /   128 tokens (    0.96 ms per token,  1044.16 tokens per second)
0.00.819.955 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.819.955 I llama_perf_context_print:       total time =     131.96 ms /   129 tokens
0.00.820.409 I ggml_metal_free: deallocating

real	0m0.834s
user	0m0.077s
sys	0m0.099s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.994 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.581 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.585 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.587 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.588 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.589 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.590 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.590 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.591 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.591 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.594 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.594 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.595 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.595 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.596 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.598 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.599 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.599 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.332 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.305 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.002 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.003 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.003 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.003 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.004 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.004 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.004 I llama_model_loader: - type  f32:  194 tensors
0.00.025.004 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.005 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.005 I print_info: file format = GGUF V3 (latest)
0.00.025.006 I print_info: file type   = Q5_0
0.00.025.009 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.043.292 I load: special tokens cache size = 25
0.00.049.070 I load: token to piece cache size = 0.2984 MB
0.00.049.073 I print_info: arch             = gptneox
0.00.049.073 I print_info: vocab_only       = 0
0.00.049.073 I print_info: n_ctx_train      = 2048
0.00.049.074 I print_info: n_embd           = 2048
0.00.049.074 I print_info: n_layer          = 24
0.00.049.076 I print_info: n_head           = 16
0.00.049.077 I print_info: n_head_kv        = 16
0.00.049.077 I print_info: n_rot            = 32
0.00.049.077 I print_info: n_swa            = 0
0.00.049.078 I print_info: n_embd_head_k    = 128
0.00.049.078 I print_info: n_embd_head_v    = 128
0.00.049.079 I print_info: n_gqa            = 1
0.00.049.079 I print_info: n_embd_k_gqa     = 2048
0.00.049.080 I print_info: n_embd_v_gqa     = 2048
0.00.049.081 I print_info: f_norm_eps       = 1.0e-05
0.00.049.081 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.081 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.082 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.082 I print_info: f_logit_scale    = 0.0e+00
0.00.049.082 I print_info: n_ff             = 8192
0.00.049.083 I print_info: n_expert         = 0
0.00.049.083 I print_info: n_expert_used    = 0
0.00.049.083 I print_info: causal attn      = 1
0.00.049.083 I print_info: pooling type     = 0
0.00.049.083 I print_info: rope type        = 2
0.00.049.085 I print_info: rope scaling     = linear
0.00.049.087 I print_info: freq_base_train  = 10000.0
0.00.049.087 I print_info: freq_scale_train = 1
0.00.049.087 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.087 I print_info: rope_finetuned   = unknown
0.00.049.088 I print_info: ssm_d_conv       = 0
0.00.049.088 I print_info: ssm_d_inner      = 0
0.00.049.088 I print_info: ssm_d_state      = 0
0.00.049.088 I print_info: ssm_dt_rank      = 0
0.00.049.088 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.088 I print_info: model type       = 1.4B
0.00.049.089 I print_info: model params     = 1.41 B
0.00.049.089 I print_info: general.name     = 1.4B
0.00.049.089 I print_info: vocab type       = BPE
0.00.049.090 I print_info: n_vocab          = 50304
0.00.049.090 I print_info: n_merges         = 50009
0.00.049.090 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.090 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.091 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.091 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.091 I print_info: LF token         = 128 'Ä'
0.00.049.091 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.092 I print_info: max token length = 1024
0.00.051.044 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.044 I load_tensors: offloading output layer to GPU
0.00.051.044 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.055 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.056 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.328 I llama_init_from_model: n_seq_max     = 1
0.00.051.329 I llama_init_from_model: n_ctx         = 128
0.00.051.329 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.329 I llama_init_from_model: n_batch       = 128
0.00.051.329 I llama_init_from_model: n_ubatch      = 128
0.00.051.330 I llama_init_from_model: flash_attn    = 0
0.00.051.330 I llama_init_from_model: freq_base     = 10000.0
0.00.051.330 I llama_init_from_model: freq_scale    = 1
0.00.051.330 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.331 I ggml_metal_init: allocating
0.00.051.333 I ggml_metal_init: found device: Apple M4
0.00.051.335 I ggml_metal_init: picking default device: Apple M4
0.00.051.906 I ggml_metal_init: using embedded metal library
0.00.054.225 I ggml_metal_init: GPU name:   Apple M4
0.00.054.226 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.226 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.227 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.227 I ggml_metal_init: simdgroup reduction   = true
0.00.054.227 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.227 I ggml_metal_init: has bfloat            = true
0.00.054.227 I ggml_metal_init: use bfloat            = true
0.00.054.228 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.228 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.520 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.860 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.862 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.877 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.064.775 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.064.777 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.064.777 I llama_init_from_model: graph nodes  = 967
0.00.064.777 I llama_init_from_model: graph splits = 2
0.00.064.778 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.779 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.682.746 I 
0.00.682.787 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.682.791 I perplexity: tokenizing the input ..
0.00.690.241 I perplexity: tokenization took 7.449 ms
0.00.690.245 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.825.778 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.827.168 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.827.198 I llama_perf_context_print:        load time =     672.75 ms
0.00.827.199 I llama_perf_context_print: prompt eval time =     135.31 ms /   128 tokens (    1.06 ms per token,   946.00 tokens per second)
0.00.827.200 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.827.200 I llama_perf_context_print:       total time =     144.45 ms /   129 tokens
0.00.827.708 I ggml_metal_free: deallocating

real	0m0.842s
user	0m0.074s
sys	0m0.099s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.399 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.810 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.815 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.817 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.817 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.818 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.818 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.818 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.819 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.820 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.823 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.823 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.824 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.824 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.825 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.827 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.827 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.828 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.593 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.628 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.369 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.370 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.371 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.371 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.371 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.371 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.372 I llama_model_loader: - type  f32:  194 tensors
0.00.024.372 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.372 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.373 I print_info: file format = GGUF V3 (latest)
0.00.024.373 I print_info: file type   = Q5_1
0.00.024.374 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.042.813 I load: special tokens cache size = 25
0.00.048.569 I load: token to piece cache size = 0.2984 MB
0.00.048.572 I print_info: arch             = gptneox
0.00.048.572 I print_info: vocab_only       = 0
0.00.048.572 I print_info: n_ctx_train      = 2048
0.00.048.572 I print_info: n_embd           = 2048
0.00.048.573 I print_info: n_layer          = 24
0.00.048.576 I print_info: n_head           = 16
0.00.048.577 I print_info: n_head_kv        = 16
0.00.048.577 I print_info: n_rot            = 32
0.00.048.577 I print_info: n_swa            = 0
0.00.048.577 I print_info: n_embd_head_k    = 128
0.00.048.578 I print_info: n_embd_head_v    = 128
0.00.048.579 I print_info: n_gqa            = 1
0.00.048.580 I print_info: n_embd_k_gqa     = 2048
0.00.048.580 I print_info: n_embd_v_gqa     = 2048
0.00.048.581 I print_info: f_norm_eps       = 1.0e-05
0.00.048.581 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.582 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.582 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.582 I print_info: f_logit_scale    = 0.0e+00
0.00.048.584 I print_info: n_ff             = 8192
0.00.048.584 I print_info: n_expert         = 0
0.00.048.584 I print_info: n_expert_used    = 0
0.00.048.584 I print_info: causal attn      = 1
0.00.048.584 I print_info: pooling type     = 0
0.00.048.585 I print_info: rope type        = 2
0.00.048.585 I print_info: rope scaling     = linear
0.00.048.587 I print_info: freq_base_train  = 10000.0
0.00.048.588 I print_info: freq_scale_train = 1
0.00.048.588 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.588 I print_info: rope_finetuned   = unknown
0.00.048.588 I print_info: ssm_d_conv       = 0
0.00.048.589 I print_info: ssm_d_inner      = 0
0.00.048.589 I print_info: ssm_d_state      = 0
0.00.048.589 I print_info: ssm_dt_rank      = 0
0.00.048.589 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.589 I print_info: model type       = 1.4B
0.00.048.589 I print_info: model params     = 1.41 B
0.00.048.589 I print_info: general.name     = 1.4B
0.00.048.590 I print_info: vocab type       = BPE
0.00.048.590 I print_info: n_vocab          = 50304
0.00.048.591 I print_info: n_merges         = 50009
0.00.048.594 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.595 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.595 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.595 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.595 I print_info: LF token         = 128 'Ä'
0.00.048.595 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.596 I print_info: max token length = 1024
0.00.050.576 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.576 I load_tensors: offloading output layer to GPU
0.00.050.576 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.587 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.588 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.050.928 I llama_init_from_model: n_seq_max     = 1
0.00.050.929 I llama_init_from_model: n_ctx         = 128
0.00.050.929 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.929 I llama_init_from_model: n_batch       = 128
0.00.050.929 I llama_init_from_model: n_ubatch      = 128
0.00.050.930 I llama_init_from_model: flash_attn    = 0
0.00.050.930 I llama_init_from_model: freq_base     = 10000.0
0.00.050.930 I llama_init_from_model: freq_scale    = 1
0.00.050.930 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.931 I ggml_metal_init: allocating
0.00.050.933 I ggml_metal_init: found device: Apple M4
0.00.050.935 I ggml_metal_init: picking default device: Apple M4
0.00.051.529 I ggml_metal_init: using embedded metal library
0.00.053.918 I ggml_metal_init: GPU name:   Apple M4
0.00.053.920 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.920 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.920 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.921 I ggml_metal_init: simdgroup reduction   = true
0.00.053.921 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.921 I ggml_metal_init: has bfloat            = true
0.00.053.921 I ggml_metal_init: use bfloat            = true
0.00.053.921 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.922 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.440 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.697 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.700 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.714 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.064.653 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.064.655 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.064.655 I llama_init_from_model: graph nodes  = 967
0.00.064.655 I llama_init_from_model: graph splits = 2
0.00.064.656 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.656 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.619.141 I 
0.00.619.221 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.619.227 I perplexity: tokenizing the input ..
0.00.627.368 I perplexity: tokenization took 8.139 ms
0.00.627.377 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.762.210 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.763.377 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.763.394 I llama_perf_context_print:        load time =     609.74 ms
0.00.763.395 I llama_perf_context_print: prompt eval time =     134.61 ms /   128 tokens (    1.05 ms per token,   950.92 tokens per second)
0.00.763.396 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.763.397 I llama_perf_context_print:       total time =     144.26 ms /   129 tokens
0.00.763.678 I ggml_metal_free: deallocating

real	0m0.777s
user	0m0.075s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.296 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.978 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.982 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.984 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.984 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.985 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.985 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.985 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.988 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.989 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.989 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.989 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.990 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.990 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.990 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.993 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.994 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.994 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.796 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.791 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.573 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.574 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.574 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.575 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.575 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.575 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.576 I llama_model_loader: - type  f32:  194 tensors
0.00.025.576 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.576 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.577 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.577 I print_info: file format = GGUF V3 (latest)
0.00.025.578 I print_info: file type   = Q2_K - Medium
0.00.025.578 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.663 I load: special tokens cache size = 25
0.00.049.607 I load: token to piece cache size = 0.2984 MB
0.00.049.609 I print_info: arch             = gptneox
0.00.049.609 I print_info: vocab_only       = 0
0.00.049.610 I print_info: n_ctx_train      = 2048
0.00.049.610 I print_info: n_embd           = 2048
0.00.049.610 I print_info: n_layer          = 24
0.00.049.612 I print_info: n_head           = 16
0.00.049.613 I print_info: n_head_kv        = 16
0.00.049.613 I print_info: n_rot            = 32
0.00.049.613 I print_info: n_swa            = 0
0.00.049.614 I print_info: n_embd_head_k    = 128
0.00.049.614 I print_info: n_embd_head_v    = 128
0.00.049.615 I print_info: n_gqa            = 1
0.00.049.615 I print_info: n_embd_k_gqa     = 2048
0.00.049.617 I print_info: n_embd_v_gqa     = 2048
0.00.049.618 I print_info: f_norm_eps       = 1.0e-05
0.00.049.618 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.618 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.619 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.619 I print_info: f_logit_scale    = 0.0e+00
0.00.049.620 I print_info: n_ff             = 8192
0.00.049.620 I print_info: n_expert         = 0
0.00.049.620 I print_info: n_expert_used    = 0
0.00.049.620 I print_info: causal attn      = 1
0.00.049.622 I print_info: pooling type     = 0
0.00.049.622 I print_info: rope type        = 2
0.00.049.622 I print_info: rope scaling     = linear
0.00.049.623 I print_info: freq_base_train  = 10000.0
0.00.049.623 I print_info: freq_scale_train = 1
0.00.049.623 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.623 I print_info: rope_finetuned   = unknown
0.00.049.624 I print_info: ssm_d_conv       = 0
0.00.049.624 I print_info: ssm_d_inner      = 0
0.00.049.624 I print_info: ssm_d_state      = 0
0.00.049.624 I print_info: ssm_dt_rank      = 0
0.00.049.624 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.624 I print_info: model type       = 1.4B
0.00.049.625 I print_info: model params     = 1.41 B
0.00.049.625 I print_info: general.name     = 1.4B
0.00.049.625 I print_info: vocab type       = BPE
0.00.049.629 I print_info: n_vocab          = 50304
0.00.049.629 I print_info: n_merges         = 50009
0.00.049.630 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.631 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.631 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.631 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.631 I print_info: LF token         = 128 'Ä'
0.00.049.632 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.632 I print_info: max token length = 1024
0.00.051.459 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.459 I load_tensors: offloading output layer to GPU
0.00.051.459 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.470 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.471 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.748 I llama_init_from_model: n_seq_max     = 1
0.00.051.749 I llama_init_from_model: n_ctx         = 128
0.00.051.749 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.749 I llama_init_from_model: n_batch       = 128
0.00.051.750 I llama_init_from_model: n_ubatch      = 128
0.00.051.750 I llama_init_from_model: flash_attn    = 0
0.00.051.750 I llama_init_from_model: freq_base     = 10000.0
0.00.051.750 I llama_init_from_model: freq_scale    = 1
0.00.051.751 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.751 I ggml_metal_init: allocating
0.00.051.754 I ggml_metal_init: found device: Apple M4
0.00.051.756 I ggml_metal_init: picking default device: Apple M4
0.00.052.336 I ggml_metal_init: using embedded metal library
0.00.054.683 I ggml_metal_init: GPU name:   Apple M4
0.00.054.685 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.685 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.686 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.686 I ggml_metal_init: simdgroup reduction   = true
0.00.054.686 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.686 I ggml_metal_init: has bfloat            = true
0.00.054.686 I ggml_metal_init: use bfloat            = true
0.00.054.687 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.687 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.966 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.247 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.251 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.268 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.175 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.176 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.177 I llama_init_from_model: graph nodes  = 967
0.00.065.177 I llama_init_from_model: graph splits = 2
0.00.065.178 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.178 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.371.160 I 
0.00.371.209 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.371.215 I perplexity: tokenizing the input ..
0.00.378.627 I perplexity: tokenization took 7.41 ms
0.00.378.630 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.510.770 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.512.001 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.512.019 I llama_perf_context_print:        load time =     360.86 ms
0.00.512.021 I llama_perf_context_print: prompt eval time =     131.91 ms /   128 tokens (    1.03 ms per token,   970.32 tokens per second)
0.00.512.022 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.512.024 I llama_perf_context_print:       total time =     140.86 ms /   129 tokens
0.00.512.328 I ggml_metal_free: deallocating

real	0m0.526s
user	0m0.074s
sys	0m0.065s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.830 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.356 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.363 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.365 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.365 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.365 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.366 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.366 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.367 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.367 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.368 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.368 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.368 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.369 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.369 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.371 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.371 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.371 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.089 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.081 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.814 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.816 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.816 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.816 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.817 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.817 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.817 I llama_model_loader: - type  f32:  194 tensors
0.00.023.818 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.818 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.818 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.818 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.819 I print_info: file format = GGUF V3 (latest)
0.00.023.819 I print_info: file type   = Q3_K - Medium
0.00.023.825 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.042.071 I load: special tokens cache size = 25
0.00.047.923 I load: token to piece cache size = 0.2984 MB
0.00.047.927 I print_info: arch             = gptneox
0.00.047.928 I print_info: vocab_only       = 0
0.00.047.928 I print_info: n_ctx_train      = 2048
0.00.047.928 I print_info: n_embd           = 2048
0.00.047.928 I print_info: n_layer          = 24
0.00.047.931 I print_info: n_head           = 16
0.00.047.932 I print_info: n_head_kv        = 16
0.00.047.932 I print_info: n_rot            = 32
0.00.047.932 I print_info: n_swa            = 0
0.00.047.932 I print_info: n_embd_head_k    = 128
0.00.047.933 I print_info: n_embd_head_v    = 128
0.00.047.933 I print_info: n_gqa            = 1
0.00.047.934 I print_info: n_embd_k_gqa     = 2048
0.00.047.937 I print_info: n_embd_v_gqa     = 2048
0.00.047.937 I print_info: f_norm_eps       = 1.0e-05
0.00.047.937 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.937 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.938 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.939 I print_info: f_logit_scale    = 0.0e+00
0.00.047.939 I print_info: n_ff             = 8192
0.00.047.939 I print_info: n_expert         = 0
0.00.047.940 I print_info: n_expert_used    = 0
0.00.047.940 I print_info: causal attn      = 1
0.00.047.940 I print_info: pooling type     = 0
0.00.047.940 I print_info: rope type        = 2
0.00.047.940 I print_info: rope scaling     = linear
0.00.047.940 I print_info: freq_base_train  = 10000.0
0.00.047.941 I print_info: freq_scale_train = 1
0.00.047.942 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.942 I print_info: rope_finetuned   = unknown
0.00.047.942 I print_info: ssm_d_conv       = 0
0.00.047.942 I print_info: ssm_d_inner      = 0
0.00.047.942 I print_info: ssm_d_state      = 0
0.00.047.942 I print_info: ssm_dt_rank      = 0
0.00.047.943 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.944 I print_info: model type       = 1.4B
0.00.047.944 I print_info: model params     = 1.41 B
0.00.047.944 I print_info: general.name     = 1.4B
0.00.047.945 I print_info: vocab type       = BPE
0.00.047.945 I print_info: n_vocab          = 50304
0.00.047.945 I print_info: n_merges         = 50009
0.00.047.945 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.945 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.945 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.946 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.947 I print_info: LF token         = 128 'Ä'
0.00.047.947 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.947 I print_info: max token length = 1024
0.00.049.861 I load_tensors: offloading 24 repeating layers to GPU
0.00.049.861 I load_tensors: offloading output layer to GPU
0.00.049.861 I load_tensors: offloaded 25/25 layers to GPU
0.00.049.872 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.049.873 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.050.176 I llama_init_from_model: n_seq_max     = 1
0.00.050.177 I llama_init_from_model: n_ctx         = 128
0.00.050.177 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.177 I llama_init_from_model: n_batch       = 128
0.00.050.177 I llama_init_from_model: n_ubatch      = 128
0.00.050.177 I llama_init_from_model: flash_attn    = 0
0.00.050.178 I llama_init_from_model: freq_base     = 10000.0
0.00.050.178 I llama_init_from_model: freq_scale    = 1
0.00.050.178 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.179 I ggml_metal_init: allocating
0.00.050.181 I ggml_metal_init: found device: Apple M4
0.00.050.183 I ggml_metal_init: picking default device: Apple M4
0.00.050.741 I ggml_metal_init: using embedded metal library
0.00.053.173 I ggml_metal_init: GPU name:   Apple M4
0.00.053.174 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.175 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.175 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.175 I ggml_metal_init: simdgroup reduction   = true
0.00.053.176 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.176 I ggml_metal_init: has bfloat            = true
0.00.053.176 I ggml_metal_init: use bfloat            = true
0.00.053.176 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.178 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.517 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.062.800 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.804 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.821 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.063.698 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.063.699 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.063.699 I llama_init_from_model: graph nodes  = 967
0.00.063.699 I llama_init_from_model: graph splits = 2
0.00.063.701 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.063.701 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.462.377 I 
0.00.462.413 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.462.418 I perplexity: tokenizing the input ..
0.00.470.267 I perplexity: tokenization took 7.848 ms
0.00.470.270 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.602.512 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.603.691 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.603.712 I llama_perf_context_print:        load time =     453.54 ms
0.00.603.713 I llama_perf_context_print: prompt eval time =     132.00 ms /   128 tokens (    1.03 ms per token,   969.67 tokens per second)
0.00.603.714 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.603.714 I llama_perf_context_print:       total time =     141.34 ms /   129 tokens
0.00.604.202 I ggml_metal_free: deallocating

real	0m0.617s
user	0m0.075s
sys	0m0.079s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.835 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.263 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.268 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.270 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.271 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.271 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.271 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.274 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.274 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.275 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.275 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.276 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.276 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.276 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.277 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.280 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.280 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.280 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.090 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.035 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.922 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.923 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.923 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.924 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.924 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.924 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.925 I llama_model_loader: - type  f32:  194 tensors
0.00.025.925 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.925 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.926 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.926 I print_info: file format = GGUF V3 (latest)
0.00.025.927 I print_info: file type   = Q4_K - Medium
0.00.025.927 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.090 I load: special tokens cache size = 25
0.00.051.014 I load: token to piece cache size = 0.2984 MB
0.00.051.017 I print_info: arch             = gptneox
0.00.051.017 I print_info: vocab_only       = 0
0.00.051.018 I print_info: n_ctx_train      = 2048
0.00.051.018 I print_info: n_embd           = 2048
0.00.051.018 I print_info: n_layer          = 24
0.00.051.021 I print_info: n_head           = 16
0.00.051.022 I print_info: n_head_kv        = 16
0.00.051.022 I print_info: n_rot            = 32
0.00.051.022 I print_info: n_swa            = 0
0.00.051.022 I print_info: n_embd_head_k    = 128
0.00.051.023 I print_info: n_embd_head_v    = 128
0.00.051.023 I print_info: n_gqa            = 1
0.00.051.024 I print_info: n_embd_k_gqa     = 2048
0.00.051.025 I print_info: n_embd_v_gqa     = 2048
0.00.051.025 I print_info: f_norm_eps       = 1.0e-05
0.00.051.028 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.028 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.028 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.029 I print_info: f_logit_scale    = 0.0e+00
0.00.051.029 I print_info: n_ff             = 8192
0.00.051.030 I print_info: n_expert         = 0
0.00.051.030 I print_info: n_expert_used    = 0
0.00.051.030 I print_info: causal attn      = 1
0.00.051.030 I print_info: pooling type     = 0
0.00.051.030 I print_info: rope type        = 2
0.00.051.030 I print_info: rope scaling     = linear
0.00.051.031 I print_info: freq_base_train  = 10000.0
0.00.051.031 I print_info: freq_scale_train = 1
0.00.051.031 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.032 I print_info: rope_finetuned   = unknown
0.00.051.032 I print_info: ssm_d_conv       = 0
0.00.051.032 I print_info: ssm_d_inner      = 0
0.00.051.032 I print_info: ssm_d_state      = 0
0.00.051.032 I print_info: ssm_dt_rank      = 0
0.00.051.032 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.033 I print_info: model type       = 1.4B
0.00.051.033 I print_info: model params     = 1.41 B
0.00.051.033 I print_info: general.name     = 1.4B
0.00.051.034 I print_info: vocab type       = BPE
0.00.051.034 I print_info: n_vocab          = 50304
0.00.051.034 I print_info: n_merges         = 50009
0.00.051.034 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.034 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.035 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.035 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.035 I print_info: LF token         = 128 'Ä'
0.00.051.035 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.036 I print_info: max token length = 1024
0.00.052.982 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.983 I load_tensors: offloading output layer to GPU
0.00.052.983 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.994 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.995 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.280 I llama_init_from_model: n_seq_max     = 1
0.00.053.280 I llama_init_from_model: n_ctx         = 128
0.00.053.281 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.281 I llama_init_from_model: n_batch       = 128
0.00.053.281 I llama_init_from_model: n_ubatch      = 128
0.00.053.281 I llama_init_from_model: flash_attn    = 0
0.00.053.281 I llama_init_from_model: freq_base     = 10000.0
0.00.053.282 I llama_init_from_model: freq_scale    = 1
0.00.053.282 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.283 I ggml_metal_init: allocating
0.00.053.286 I ggml_metal_init: found device: Apple M4
0.00.053.288 I ggml_metal_init: picking default device: Apple M4
0.00.053.850 I ggml_metal_init: using embedded metal library
0.00.056.245 I ggml_metal_init: GPU name:   Apple M4
0.00.056.246 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.246 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.247 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.247 I ggml_metal_init: simdgroup reduction   = true
0.00.056.247 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.248 I ggml_metal_init: has bfloat            = true
0.00.056.248 I ggml_metal_init: use bfloat            = true
0.00.056.248 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.249 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.892 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.124 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.126 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.150 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.133 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.134 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.134 I llama_init_from_model: graph nodes  = 967
0.00.068.134 I llama_init_from_model: graph splits = 2
0.00.068.136 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.136 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.625.525 I 
0.00.625.561 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.625.571 I perplexity: tokenizing the input ..
0.00.632.495 I perplexity: tokenization took 6.923 ms
0.00.632.499 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.766.003 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.767.555 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.767.577 I llama_perf_context_print:        load time =     616.69 ms
0.00.767.577 I llama_perf_context_print: prompt eval time =     133.26 ms /   128 tokens (    1.04 ms per token,   960.51 tokens per second)
0.00.767.578 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.767.578 I llama_perf_context_print:       total time =     142.05 ms /   129 tokens
0.00.767.964 I ggml_metal_free: deallocating

real	0m0.782s
user	0m0.076s
sys	0m0.095s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.142 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.300 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.284 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.306 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.322 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.322 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.325 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.326 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.326 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.327 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.327 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.328 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.328 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.328 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.329 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.329 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.331 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.331 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.331 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.884 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.857 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.579 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.581 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.581 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.582 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.582 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.582 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.583 I llama_model_loader: - type  f32:  194 tensors
0.00.025.583 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.584 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.585 I print_info: file format = GGUF V3 (latest)
0.00.025.586 I print_info: file type   = Q5_K - Medium
0.00.025.587 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.482 I load: special tokens cache size = 25
0.00.051.334 I load: token to piece cache size = 0.2984 MB
0.00.051.339 I print_info: arch             = gptneox
0.00.051.339 I print_info: vocab_only       = 0
0.00.051.339 I print_info: n_ctx_train      = 2048
0.00.051.339 I print_info: n_embd           = 2048
0.00.051.339 I print_info: n_layer          = 24
0.00.051.344 I print_info: n_head           = 16
0.00.051.345 I print_info: n_head_kv        = 16
0.00.051.345 I print_info: n_rot            = 32
0.00.051.345 I print_info: n_swa            = 0
0.00.051.346 I print_info: n_embd_head_k    = 128
0.00.051.346 I print_info: n_embd_head_v    = 128
0.00.051.346 I print_info: n_gqa            = 1
0.00.051.347 I print_info: n_embd_k_gqa     = 2048
0.00.051.348 I print_info: n_embd_v_gqa     = 2048
0.00.051.348 I print_info: f_norm_eps       = 1.0e-05
0.00.051.349 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.349 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.349 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.349 I print_info: f_logit_scale    = 0.0e+00
0.00.051.350 I print_info: n_ff             = 8192
0.00.051.350 I print_info: n_expert         = 0
0.00.051.350 I print_info: n_expert_used    = 0
0.00.051.351 I print_info: causal attn      = 1
0.00.051.351 I print_info: pooling type     = 0
0.00.051.351 I print_info: rope type        = 2
0.00.051.351 I print_info: rope scaling     = linear
0.00.051.351 I print_info: freq_base_train  = 10000.0
0.00.051.352 I print_info: freq_scale_train = 1
0.00.051.352 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.352 I print_info: rope_finetuned   = unknown
0.00.051.352 I print_info: ssm_d_conv       = 0
0.00.051.352 I print_info: ssm_d_inner      = 0
0.00.051.353 I print_info: ssm_d_state      = 0
0.00.051.353 I print_info: ssm_dt_rank      = 0
0.00.051.353 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.353 I print_info: model type       = 1.4B
0.00.051.354 I print_info: model params     = 1.41 B
0.00.051.354 I print_info: general.name     = 1.4B
0.00.051.354 I print_info: vocab type       = BPE
0.00.051.354 I print_info: n_vocab          = 50304
0.00.051.357 I print_info: n_merges         = 50009
0.00.051.357 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.357 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.358 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.358 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.358 I print_info: LF token         = 128 'Ä'
0.00.051.359 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.360 I print_info: max token length = 1024
0.00.053.382 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.382 I load_tensors: offloading output layer to GPU
0.00.053.382 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.393 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.395 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.699 I llama_init_from_model: n_seq_max     = 1
0.00.053.700 I llama_init_from_model: n_ctx         = 128
0.00.053.700 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.700 I llama_init_from_model: n_batch       = 128
0.00.053.700 I llama_init_from_model: n_ubatch      = 128
0.00.053.700 I llama_init_from_model: flash_attn    = 0
0.00.053.701 I llama_init_from_model: freq_base     = 10000.0
0.00.053.701 I llama_init_from_model: freq_scale    = 1
0.00.053.701 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.702 I ggml_metal_init: allocating
0.00.053.705 I ggml_metal_init: found device: Apple M4
0.00.053.707 I ggml_metal_init: picking default device: Apple M4
0.00.054.294 I ggml_metal_init: using embedded metal library
0.00.056.680 I ggml_metal_init: GPU name:   Apple M4
0.00.056.682 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.682 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.682 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.683 I ggml_metal_init: simdgroup reduction   = true
0.00.056.683 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.683 I ggml_metal_init: has bfloat            = true
0.00.056.683 I ggml_metal_init: use bfloat            = true
0.00.056.684 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.684 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.927 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.208 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.212 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.229 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.109 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.110 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.110 I llama_init_from_model: graph nodes  = 967
0.00.069.111 I llama_init_from_model: graph splits = 2
0.00.069.112 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.112 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.280 I 
0.00.679.322 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.679.325 I perplexity: tokenizing the input ..
0.00.686.615 I perplexity: tokenization took 7.289 ms
0.00.686.619 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.826.812 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.827.983 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.828.016 I llama_perf_context_print:        load time =     668.97 ms
0.00.828.017 I llama_perf_context_print: prompt eval time =     139.97 ms /   128 tokens (    1.09 ms per token,   914.50 tokens per second)
0.00.828.018 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.828.018 I llama_perf_context_print:       total time =     148.74 ms /   129 tokens
0.00.828.547 I ggml_metal_free: deallocating

real	0m0.844s
user	0m0.078s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.608 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.243 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.248 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.250 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.250 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.250 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.251 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.251 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.252 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.252 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.253 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.253 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.253 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.254 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.256 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.257 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.258 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.258 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.941 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.955 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.657 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.659 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.659 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.659 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.660 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.660 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.661 I llama_model_loader: - type  f32:  194 tensors
0.00.023.661 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.661 I print_info: file format = GGUF V3 (latest)
0.00.023.662 I print_info: file type   = Q6_K
0.00.023.663 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.041.798 I load: special tokens cache size = 25
0.00.047.750 I load: token to piece cache size = 0.2984 MB
0.00.047.753 I print_info: arch             = gptneox
0.00.047.753 I print_info: vocab_only       = 0
0.00.047.753 I print_info: n_ctx_train      = 2048
0.00.047.753 I print_info: n_embd           = 2048
0.00.047.753 I print_info: n_layer          = 24
0.00.047.757 I print_info: n_head           = 16
0.00.047.757 I print_info: n_head_kv        = 16
0.00.047.758 I print_info: n_rot            = 32
0.00.047.758 I print_info: n_swa            = 0
0.00.047.758 I print_info: n_embd_head_k    = 128
0.00.047.758 I print_info: n_embd_head_v    = 128
0.00.047.759 I print_info: n_gqa            = 1
0.00.047.760 I print_info: n_embd_k_gqa     = 2048
0.00.047.760 I print_info: n_embd_v_gqa     = 2048
0.00.047.761 I print_info: f_norm_eps       = 1.0e-05
0.00.047.761 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.761 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.761 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.763 I print_info: f_logit_scale    = 0.0e+00
0.00.047.764 I print_info: n_ff             = 8192
0.00.047.764 I print_info: n_expert         = 0
0.00.047.764 I print_info: n_expert_used    = 0
0.00.047.764 I print_info: causal attn      = 1
0.00.047.764 I print_info: pooling type     = 0
0.00.047.764 I print_info: rope type        = 2
0.00.047.765 I print_info: rope scaling     = linear
0.00.047.765 I print_info: freq_base_train  = 10000.0
0.00.047.765 I print_info: freq_scale_train = 1
0.00.047.767 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.768 I print_info: rope_finetuned   = unknown
0.00.047.768 I print_info: ssm_d_conv       = 0
0.00.047.768 I print_info: ssm_d_inner      = 0
0.00.047.768 I print_info: ssm_d_state      = 0
0.00.047.768 I print_info: ssm_dt_rank      = 0
0.00.047.768 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.769 I print_info: model type       = 1.4B
0.00.047.769 I print_info: model params     = 1.41 B
0.00.047.769 I print_info: general.name     = 1.4B
0.00.047.770 I print_info: vocab type       = BPE
0.00.047.770 I print_info: n_vocab          = 50304
0.00.047.770 I print_info: n_merges         = 50009
0.00.047.770 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.770 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.771 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.773 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.773 I print_info: LF token         = 128 'Ä'
0.00.047.774 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.774 I print_info: max token length = 1024
0.00.049.723 I load_tensors: offloading 24 repeating layers to GPU
0.00.049.723 I load_tensors: offloading output layer to GPU
0.00.049.724 I load_tensors: offloaded 25/25 layers to GPU
0.00.049.734 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.049.735 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.050.012 I llama_init_from_model: n_seq_max     = 1
0.00.050.013 I llama_init_from_model: n_ctx         = 128
0.00.050.013 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.013 I llama_init_from_model: n_batch       = 128
0.00.050.013 I llama_init_from_model: n_ubatch      = 128
0.00.050.014 I llama_init_from_model: flash_attn    = 0
0.00.050.014 I llama_init_from_model: freq_base     = 10000.0
0.00.050.014 I llama_init_from_model: freq_scale    = 1
0.00.050.014 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.015 I ggml_metal_init: allocating
0.00.050.017 I ggml_metal_init: found device: Apple M4
0.00.050.019 I ggml_metal_init: picking default device: Apple M4
0.00.050.571 I ggml_metal_init: using embedded metal library
0.00.052.932 I ggml_metal_init: GPU name:   Apple M4
0.00.052.933 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.052.934 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.052.934 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.052.934 I ggml_metal_init: simdgroup reduction   = true
0.00.052.934 I ggml_metal_init: simdgroup matrix mul. = true
0.00.052.935 I ggml_metal_init: has bfloat            = true
0.00.052.935 I ggml_metal_init: use bfloat            = true
0.00.052.935 I ggml_metal_init: hasUnifiedMemory      = true
0.00.052.935 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.365 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.062.607 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.609 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.642 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.063.548 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.063.549 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.063.549 I llama_init_from_model: graph nodes  = 967
0.00.063.550 I llama_init_from_model: graph splits = 2
0.00.063.551 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.063.551 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.417.045 I 
0.00.417.086 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.417.098 I perplexity: tokenizing the input ..
0.00.425.342 I perplexity: tokenization took 8.243 ms
0.00.425.346 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.565.704 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.566.947 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.566.973 I llama_perf_context_print:        load time =     408.43 ms
0.00.566.974 I llama_perf_context_print: prompt eval time =     140.13 ms /   128 tokens (    1.09 ms per token,   913.45 tokens per second)
0.00.566.974 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.566.975 I llama_perf_context_print:       total time =     149.93 ms /   129 tokens
0.00.567.482 I ggml_metal_free: deallocating

real	0m0.581s
user	0m0.075s
sys	0m0.087s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.242 I build: 4520 (2139667e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.865 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.880 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.888 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.889 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.890 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.890 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.891 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.891 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.893 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.893 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.893 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.894 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.895 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.896 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.896 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.899 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.899 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.899 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.632 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.657 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.362 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.364 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.365 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.365 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.366 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.367 I llama_model_loader: - type  f32:  194 tensors
0.00.056.367 I llama_model_loader: - type  f16:   98 tensors
0.00.056.368 I print_info: file format = GGUF V3 (latest)
0.00.056.369 I print_info: file type   = all F32 (guessed)
0.00.056.371 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.083.295 I load: special tokens cache size = 25
0.00.089.852 I load: token to piece cache size = 0.2984 MB
0.00.089.855 I print_info: arch             = gptneox
0.00.089.855 I print_info: vocab_only       = 0
0.00.089.855 I print_info: n_ctx_train      = 2048
0.00.089.855 I print_info: n_embd           = 2048
0.00.089.855 I print_info: n_layer          = 24
0.00.089.858 I print_info: n_head           = 16
0.00.089.859 I print_info: n_head_kv        = 16
0.00.089.859 I print_info: n_rot            = 32
0.00.089.859 I print_info: n_swa            = 0
0.00.089.860 I print_info: n_embd_head_k    = 128
0.00.089.862 I print_info: n_embd_head_v    = 128
0.00.089.863 I print_info: n_gqa            = 1
0.00.089.863 I print_info: n_embd_k_gqa     = 2048
0.00.089.864 I print_info: n_embd_v_gqa     = 2048
0.00.089.865 I print_info: f_norm_eps       = 1.0e-05
0.00.089.865 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.089.865 I print_info: f_clamp_kqv      = 0.0e+00
0.00.089.865 I print_info: f_max_alibi_bias = 0.0e+00
0.00.089.867 I print_info: f_logit_scale    = 0.0e+00
0.00.089.867 I print_info: n_ff             = 8192
0.00.089.867 I print_info: n_expert         = 0
0.00.089.867 I print_info: n_expert_used    = 0
0.00.089.868 I print_info: causal attn      = 1
0.00.089.868 I print_info: pooling type     = 0
0.00.089.868 I print_info: rope type        = 2
0.00.089.868 I print_info: rope scaling     = linear
0.00.089.868 I print_info: freq_base_train  = 10000.0
0.00.089.869 I print_info: freq_scale_train = 1
0.00.089.869 I print_info: n_ctx_orig_yarn  = 2048
0.00.089.869 I print_info: rope_finetuned   = unknown
0.00.089.869 I print_info: ssm_d_conv       = 0
0.00.089.869 I print_info: ssm_d_inner      = 0
0.00.089.869 I print_info: ssm_d_state      = 0
0.00.089.869 I print_info: ssm_dt_rank      = 0
0.00.089.870 I print_info: ssm_dt_b_c_rms   = 0
0.00.089.870 I print_info: model type       = 1.4B
0.00.089.870 I print_info: model params     = 1.41 B
0.00.089.870 I print_info: general.name     = 1.4B
0.00.089.871 I print_info: vocab type       = BPE
0.00.089.871 I print_info: n_vocab          = 50304
0.00.089.871 I print_info: n_merges         = 50009
0.00.089.875 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.089.875 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.089.876 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.089.876 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.089.876 I print_info: LF token         = 128 'Ä'
0.00.089.876 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.089.876 I print_info: max token length = 1024
0.00.092.646 I load_tensors: offloading 24 repeating layers to GPU
0.00.092.646 I load_tensors: offloading output layer to GPU
0.00.092.647 I load_tensors: offloaded 25/25 layers to GPU
0.00.092.658 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.659 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.092.921 I llama_init_from_model: n_seq_max     = 1
0.00.092.922 I llama_init_from_model: n_ctx         = 128
0.00.092.922 I llama_init_from_model: n_ctx_per_seq = 128
0.00.092.922 I llama_init_from_model: n_batch       = 128
0.00.092.922 I llama_init_from_model: n_ubatch      = 128
0.00.092.923 I llama_init_from_model: flash_attn    = 0
0.00.092.923 I llama_init_from_model: freq_base     = 10000.0
0.00.092.923 I llama_init_from_model: freq_scale    = 1
0.00.092.924 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.924 I ggml_metal_init: allocating
0.00.092.927 I ggml_metal_init: found device: Apple M4
0.00.092.929 I ggml_metal_init: picking default device: Apple M4
0.00.093.548 I ggml_metal_init: using embedded metal library
0.00.096.118 I ggml_metal_init: GPU name:   Apple M4
0.00.096.119 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.120 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.120 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.120 I ggml_metal_init: simdgroup reduction   = true
0.00.096.120 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.120 I ggml_metal_init: has bfloat            = true
0.00.096.120 I ggml_metal_init: use bfloat            = true
0.00.096.121 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.121 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.373 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.106.576 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.579 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.592 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.107.430 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.107.431 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.107.432 I llama_init_from_model: graph nodes  = 967
0.00.107.432 I llama_init_from_model: graph splits = 2
0.00.107.433 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.107.433 I 
0.00.107.469 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.107.470 I compute_imatrix: tokenizing the input ..
0.00.114.366 I compute_imatrix: tokenization took 6.895 ms
0.00.114.367 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.526.034 I compute_imatrix: 1.41 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.529.387 I llama_perf_context_print:        load time =    1502.17 ms
0.01.529.389 I llama_perf_context_print: prompt eval time =    1411.05 ms /   128 tokens (   11.02 ms per token,    90.71 tokens per second)
0.01.529.390 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.529.390 I llama_perf_context_print:       total time =    1505.50 ms /   129 tokens
0.01.530.498 I ggml_metal_free: deallocating

real	0m1.718s
user	0m0.175s
sys	0m0.241s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4520 (2139667e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f60a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f60ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f60b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f60b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f60bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f60c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f60c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f60cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f60d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f60d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f60dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f60e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f60ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f60f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f60fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f610430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f610b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f611270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f611990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f612160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f612880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f612fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f6136c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f613f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f614680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f614940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f614f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f615bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f616100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f6163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f616860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f616b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f6173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f6178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f617bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f618050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f6184f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f618990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f618e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f6192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f619770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f619c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f61a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f61a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f61a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f61ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f61b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f61bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f61c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f61c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f61cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f61d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f61dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f61e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f61e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f61ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f61f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f61f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f61fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f6203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f620660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f620b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f620fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f621440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f6218e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f621d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f622220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f6226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11f622b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11f623000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f6234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11f623940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11f623de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11f624330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11f624880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11f624dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11f625320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11f625870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11f625dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11f626310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11f626860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11f626db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11f627300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11f627850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11f627da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11f6282f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11f628840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11f628d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11f6292e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11f629830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11f629d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11f62a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11f62a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11f62ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11f62b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11f62b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11f62bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11f61ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11f62c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11f62c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11f62ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11f62d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11f62d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11f62dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11f62e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11f62e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11f62eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11f62f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11f62f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11f62fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11f6303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11f630940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11f630e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11f631330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11f6317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11f631c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f632110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11f6325b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11f632a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f632ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11f633390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11f633830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11f633cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11f634170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11f634610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11f634ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11f634f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11f6353f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11f635890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11f635d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11f6361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11f636670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11f636b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11f636fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11f637450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11f6378f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11f637d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11f638230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11f6386d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11f638b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11f639010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11f6394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f639950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11f639df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11f63a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11f63a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11f63abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11f63b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11f63b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11f63b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11f63be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11f63c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11f63c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11f63cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11f63d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11f63d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11f63da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11f63deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11f63e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11f63e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11f63ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11f63f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11f63f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11f63fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11f63ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f6403b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11f640850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11f640cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11f641190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f641630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11f641ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11f641f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11f642410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11f6428b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11f642d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11f6431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11f643690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11f643b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11f643fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11f644470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11f644910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11f644db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11f645250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11f6456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11f645b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11f646030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11f6464d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11f646970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11f646e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11f6472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11f647750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11f647bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11f648090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11f6485e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11f648b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11f649080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11f6495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11f649890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11f649ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11f64a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11f64aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11f64b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11f64b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11f64ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11f64c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11f64c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11f64ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11f64d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11f64d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11f64dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11f64e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f64e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11f64ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11f64f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11f64f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11f64fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11f650390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11f6508e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11f650e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11f651380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11f6518d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11f651e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11f652370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11f6528c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11f652e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11f653360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f6538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11f653e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11f654350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11f6548a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11f654df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11f655340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11f655890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11f655de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11f656330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11f656880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11f656dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11f657320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11f657870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11f657dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11f658310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11f658860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11f658db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11f659300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11f659850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11f659da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11f65a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11f65a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11f65ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11f65b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11f65b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11f65bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11f65c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11f65c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11f65cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11f65d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11f65d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11f65dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11f65e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11f65e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f65ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11f65f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11f65f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11f65fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11f660290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11f6607e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f660d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11f6611d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11f661670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f661b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f661fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11f662450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f6628f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f662d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11f663230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11f6636d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f663b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f664010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f6644b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f664950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f664df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f665290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11f6657e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11f665f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11f666620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11f666d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11f667460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11f667720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11f667f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11f6681d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11f6687e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.143.173 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.143.176 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10c1085a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10c108a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10c108e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10c1092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10c109760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10c109bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10c10a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10c10a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10c10a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10c10ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10c10b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10c10b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10c10c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10c10cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10c10d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10c10dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10c10e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10c10e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10c10f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10c10f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10c10ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10c110630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10c110d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10c111470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10c111b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10c111e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10c112110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10c112580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10c1129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10c112e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10c113360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10c113870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10c113ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10c113fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10c114410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10c114880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10c114de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10c1152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10c1157e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10c115ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10c1161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10c1166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10c116be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10c1170e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10c1175e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10c117a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10c117ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10c118330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10c1187a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10c118c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10c119080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10c1194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10c119960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10c119dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10c11a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10c11aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10c11aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10c11b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10c11b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10c11bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10c11c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10c11c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10c11cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10c11d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10c11d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10c11db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10c11dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10c11e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10c11e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10c11edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10c11f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10c11f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10c11fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10c1200e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10c120630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10c120b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10c1210d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10c121620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10c121b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10c1220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10c122610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10c122b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10c1230b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10c123600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10c123b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10c1240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10c1245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10c124b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10c125090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10c1255e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10c125b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10c126080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10c1265d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10c126b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10c127070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10c1275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10c127b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10c128060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10c1285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10c128b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10c129050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10c1295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10c129af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10c12a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10c12a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10c12aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10c12b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10c12b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10c12bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10c12c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10c12c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10c12cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10c12d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10c12d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10c12d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10c12ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10c12e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10c12e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10c12ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10c12f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10c12f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10c12f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10c12fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10c1302f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10c130790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10c130c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10c1310d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10c131570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10c131a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10c131eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10c132350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10c1327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10c132c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10c133130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10c1335d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10c133a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10c133f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10c1343b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10c134850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10c134cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10c135190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10c135630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10c135ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10c135f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10c136410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10c1368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10c136d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10c1371f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10c137690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10c137b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10c137fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10c138470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10c138910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10c138db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10c139250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10c1396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10c139b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10c13a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10c13a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10c13a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10c13ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10c13b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10c13b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10c13bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10c13c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10c13c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10c13c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10c13ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10c13d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10c13d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10c13dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10c13e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10c13e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10c13ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10c13eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10c13f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10c13f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10c13fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10c140150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10c1405f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10c140a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10c140f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10c1413d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10c141870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10c141d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10c1421b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10c142650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10c142af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10c142f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10c143430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10c1438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10c143d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10c144210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10c144760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10c144cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10c145200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10cb04280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10cb046f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10cb04b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10cb04fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10cb05440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10cb058b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10cb05d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10cb06190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10cb06600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10cb06a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10cb06ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10cb07350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10cb077c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10cb07c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10cb087e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10cb08aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10cb08d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10cb091d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10cb09640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10cb09ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10cb09f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10cb0a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10cb0a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10cb0ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10cb0b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10cb0b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10cb0b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10cb0be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10cb0c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10cb0c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10cb0cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10cb0cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10cb0d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10cb0d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10cb0dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10cb0e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10cb0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10cb0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10cb0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10cb0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10cb0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10cb0fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10cb100c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10cb10530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10cb109a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10cb10e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10cb11280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10cb116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10cb11b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10cb11fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10cb12440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10cb128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10cb12d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10cb13190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10cb13600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10cb13a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10cb13ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10cb14350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10cb147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10cb14c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10cb150a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10cb15510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10cb15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10cb15df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10cb16260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10cb166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10cb16b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10cb16fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10cb17420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10cb17890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10cb17d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10cb18170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10cb185e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10cb18a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10cb18ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10cb19330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10cb197a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10cb19c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10cb1a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10cb1a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10cb1a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10cb1add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10cb1b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10cb1b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10cb1bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10cb1bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10cb1c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10cb1ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10cb1d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10cb1dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10cb1e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10cb1e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10cb1eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10cb1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10cb1f710 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f3044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f304950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f304dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f305230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f3056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f305b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f305f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f3063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f306860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f306db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f307220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f3078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f3083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f308b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f309380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f309aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f30a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f30a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f30b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f30b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f30bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f30c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f30cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f30d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f30db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f30de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f30e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f30e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f30e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f30ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f30f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f30f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f30fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f30ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f310380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f3107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f310c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f3110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f311540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f3119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f311e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f312290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f312700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f312b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f312fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f313450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f3138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f313d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f3141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f314610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f314a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f314ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f315360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f3157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f315c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f3160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f316620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f316b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f316f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f317400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f317870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f317ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f318150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f3185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f318a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f318ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f319310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f319780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11f319bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11f31a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f31a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11f31a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11f31adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11f31b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11f31b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11f31bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11f31bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11f31c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11f31c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11f31ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11f31d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11f31d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11f31da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11f31de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11f31e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11f31e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11f31ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11f31f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11f31f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11f31f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11f31fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11f320200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11f320670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11f320ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11f320f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11f3213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11f321830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11f321ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11f322110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11f322580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11f3229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11f322e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11f3232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11f323b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11f323e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11f324290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11f324700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11f324b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11f324fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11f325450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11f3258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11f325d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11f3261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11f326610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11f326a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11f326ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f327360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11f3277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11f327c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f3280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11f328520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11f328990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11f328e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11f329270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11f3296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11f329b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11f329fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11f32a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11f32a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11f32ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11f32b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11f32b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11f32ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11f32bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11f32c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11f32c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11f32cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11f32d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11f32d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11f32d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11f32dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11f32e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f32e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11f32eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11f32efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11f32f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11f32f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11f32fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11f330160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11f3305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11f330a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11f330eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11f331320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11f331790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11f331c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11f332070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11f3324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11f332950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11f332dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11f333230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11f3336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11f333b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11f333f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11f3343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11f334860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f334cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11f335140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11f3355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11f335a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f335e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11f336300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11f336770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11f336be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11f337050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11f3374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11f337930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11f337da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11f338210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11f338680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11f338af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11f338f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11f3393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11f339840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11f339cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11f33a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11f33a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11f33aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11f33ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11f33b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11f33b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11f33bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11f33c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11f33c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11f33c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11f33cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11f33d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11f33d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11f33dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11f33df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11f33e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11f33e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11f33ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11f33f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11f33f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11f33f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11f33fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11f3402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11f340730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11f340ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11f341010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11f341b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f341e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11f342110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11f342580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11f3429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11f342e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11f3432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11f343740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11f343bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11f344020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11f344490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11f344900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11f344d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11f3451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11f345650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11f345ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f345f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11f3463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11f346810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11f346c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11f3470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11f347560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11f3479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11f347e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11f3482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11f348720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11f348b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11f349000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11f349470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11f3498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11f349d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11f34a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11f34a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11f34aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11f34af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11f34b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11f34b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11f34bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11f34c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11f34c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11f34c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11f34ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11f34d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11f34d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11f34db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11f34dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11f34e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11f34e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11f34ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11f34f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f34f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11f34fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11f34fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11f350360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11f3507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11f350c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f3510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11f351520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11f351990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f351e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f352270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11f3526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f352b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f352fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11f353430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11f3538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f353d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f354180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f3545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f354a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f354ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f355340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11f3557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11f356220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11f356940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11f357060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11f357780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11f357a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11f357eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11f3584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11f358ac0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.806s
user	0m0.296s
sys	0m0.312s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4520 (2139667e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14960b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14960bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14960c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14960c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14960cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14960d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14960d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14960de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14960e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14960e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14960ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14960f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14960fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1496105d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x149610de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x149611500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x149611c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x149612340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x149612a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x149613230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x149613950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x149614070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x149614790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x149615030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x149615750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x149615a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x149616020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x149616c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1496171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x149617490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x149617930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x149617bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x149618480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1496189c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149618c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149619120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1496195c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149619a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149619f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14961a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14961a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14961ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14961b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14961b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14961b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14961bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14961c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14961ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14961d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14961da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14961e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14961e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14961ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14961f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14961fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14961ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1496203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x149620670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x149620c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x149621470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x149621730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x149621bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x149622070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x149622510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1496229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x149622e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1496232f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x149623790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x149623c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1496240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x149624570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x149624a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x149624eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x149625400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x149625950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x149625ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1496263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x149626940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x149626e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1496273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x149627930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x149627e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1496283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x149628920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x149628e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1496293c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x149629910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x149629e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14962a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14962a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14962ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14962b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14962b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14962be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14962c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14962c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14962ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14961cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14962d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14962da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14962dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14962e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14962ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14962ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14962f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14962fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14962ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1496304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x149630a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x149630f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1496314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x149631a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x149631f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x149632400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1496328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149632d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1496331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149633680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x149633b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x149633fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x149634460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x149634900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x149634da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x149635240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1496356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x149635b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x149636020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1496364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x149636960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x149636e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1496372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x149637740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x149637be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149638080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x149638520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1496389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x149638e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x149639300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1496397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x149639c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14963a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14963a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14963aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14963aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14963b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14963b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14963bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14963c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14963c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14963ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14963cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14963d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14963d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14963dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14963e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14963e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14963eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14963ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14963f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14963f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14963fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x149640200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1496406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149640b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x149640fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x149641480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x149641920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x149641dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x149642260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x149642700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x149642ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x149643040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1496434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x149643980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x149643e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1496442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x149644760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149644c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1496450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x149645540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1496459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x149645e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x149646320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1496467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x149646c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x149647100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1496475a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x149647a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x149647ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149648380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149648820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x149648cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x149649160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1496496b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x149649c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14964a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14964a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14964a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14964af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14964b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14964bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14964c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14964c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14964cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14964d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14964d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14964def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14964e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14964e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14964ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14964f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14964f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14964ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149650470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1496509c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x149650f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x149651460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1496519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x149651f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x149652450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1496529a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x149652ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x149653440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x149653990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x149653ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x149654430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x149654980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x149654ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x149655420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x149655970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x149655ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x149656410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x149656960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x149656eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x149657400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x149657950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x149657ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1496583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x149658940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x149658e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1496593e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x149659930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x149659e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14965a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14965a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14965ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14965b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14965b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14965be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14965c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14965c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14965ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14965d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14965d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14965de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14965e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14965e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14965ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14965f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14965f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14965fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x149660370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1496608c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x149660e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x149661360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1496618b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x149661e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1496622a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x149662740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x149662be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149663080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x149663520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1496639c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149663e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x149664300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1496647a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x149664c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1496650e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149665580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x149665a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x149665ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x149666360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1496668b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x149666fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1496676f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x149667e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x149668530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1496687f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x149668fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1496692a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1496698b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.090.902 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.913 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x149669560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14964b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14964ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14964b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14961e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14961e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x149620930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14964d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x149615cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14961c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14961d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14961d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14961bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14961dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x149614cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14960ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14961f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x149620f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14962d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x149668ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x149617eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x149618170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14964d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14964be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1496162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1496165a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x149616860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x149669d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x149669fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14966a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14966a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14966a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14966aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14966ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14966b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14966b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14966b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14966b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14966bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14966be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14966c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14966c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14966c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14966c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14966cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14966ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14966d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14966d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14966d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14966d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14966dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14966df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14966e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14966e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14966e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14966ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14966ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14966ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14966f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14966f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14966f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14966fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14966fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x149670010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1496702d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x149670590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x149670850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x149670b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x149670dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x149671090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x149671350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x149671610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1496718d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x149671b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x149671e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x149672110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1496723d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x149672690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x149672950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x149672c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x149672ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x149673190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x149673450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x149673710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1496739d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x149673c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x149673f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x149674210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1496744d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x149674790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x149674a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x149674d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x149674fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x149675290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x149675550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x149675810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x149675ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x149675d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x149676050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x149676310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1496765d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x149676890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x149676b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x149676e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1496770d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x149677390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x149677650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x149677910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x149677bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x149677e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x149678150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x149678410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1496786d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x149678990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x149678c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149678f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1496791d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149679490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x149679750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x149679a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x149679cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x149679f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14967a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14967a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14967a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14967aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14967ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14967b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14967b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14967b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14967b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14967bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14967bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14967c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14967c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14967c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14967c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14967cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14967ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14967d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14967d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14967d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14967d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14967dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14967ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14967e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14967e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14967e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14967e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14967ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14967ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14967f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14967f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14967f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14967fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14967fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14967ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x149680290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x149680550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x149680810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x149680ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x149680d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x149681050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149681310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1496815d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x149681890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x149681b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x149681e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1496820d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x149682390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x149682650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x149682910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x149682bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x149682e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x149683150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x149683410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1496836d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149683990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x149683c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x149683f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1496841d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x149684490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x149684750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x149684a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x149684cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x149684f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x149685250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x149685510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1496857d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149685a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149685d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x149686010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1496862d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x149686590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x149686850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x149686b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x149686dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x149687090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x149687350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x149687610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1496878d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x149687b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x149687e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x149688110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1496883d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x149688690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x149688950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x149688c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x149688ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x149689190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x149689760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x149689a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x149689ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149689fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14968a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14968a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14968a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14968aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14968ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14968b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14968b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14968b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14968b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14968bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14968bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14968c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14968c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14968c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14968c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14968cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14968ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14968d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14968d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14968d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14968d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14968dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14968dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14968e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14968e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14968e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14968ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14968f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14968f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14968fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1496901b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x149690700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x149690c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1496911a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1496916f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x149691c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x149692190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1496926e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x149692c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x149693180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1496936d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x149693c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x149694170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1496946c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x149694c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x149695160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1496956b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x149695c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x149696150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1496966a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x149696bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x149697140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x149697690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x149697950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x149697c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x149698110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149698610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x149698b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x149699010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149699510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x149699a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x149699f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14969a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14969a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14969ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14969b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14969b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14969bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14969c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14969cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14969d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14969da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14969e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14969e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14969ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14969eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14969f500 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14970a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14970ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14970b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14970b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14970b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14970be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14970c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14970c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14970cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14970d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14970d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14970dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14970e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14970ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14970f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14970fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1497104d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x149710bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x149711310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x149711a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x149712160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x149712880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x149712fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1497136c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x149713de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1497140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x149714360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1497147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x149714c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1497150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1497155b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x149715ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x149715f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1497161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149716660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149716ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x149717030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149717530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149717a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x149717f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x149718430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x149718930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x149718e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x149719330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x149719830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x149719ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14971a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14971a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14971a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14971ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14971b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14971b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14971bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14971c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14971c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14971cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14971d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14971d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14971d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14971e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14971e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14971eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14971efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14971f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14971f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14971fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x149720220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1497206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x149720b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x149721000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1497214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x149721940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x149721de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x149722330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x149722880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x149722dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x149723320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x149723870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x149723dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x149724310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x149724860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x149724db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x149725300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x149725850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x149725da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1497262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x149726840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x149726d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1497272e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x149727830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x149727d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1497282d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x149728820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x149728d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1497292c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x149729810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x149729d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14972a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14972a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14972ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14972b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14972b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14972bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14972c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14972c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14972cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14972d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14972d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14972dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14972e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14972e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14972ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14972f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14972f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14972fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149730040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1497304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149730980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x149730e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1497312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x149731760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x149731c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1497320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x149732540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1497329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x149732e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x149733320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1497337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x149733c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x149734100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1497345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x149734a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x149734ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149735380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x149735820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x149735cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x149736160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x149736600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x149736aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x149736f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1497373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x149737880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x149737d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1497381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x149738660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x149738b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x149738fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x149739440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1497398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x149739d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14973a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14973a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14973ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14973b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14973b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14973b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14973bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14973c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14973c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14973cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14973d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14973d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14973d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14973de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14973e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14973e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14973ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14973f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14973f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14973fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14973fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x149740340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1497407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x149740c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x149741120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1497415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x149741a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149741f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1497423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x149742840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x149742ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x149743180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x149743620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x149743ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x149743f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x149744400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1497448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x149744d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1497451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149745680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149745b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x149745fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x149746460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1497469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x149746f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x149747450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1497479a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x149747c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x149748270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x149748880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x149748e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x149749680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x149749b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x149749de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14974a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14974aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14974b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14974b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14974bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14974bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14974c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14974ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14974d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14974d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14974dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14974e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14974e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14974ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14974f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14974f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14974fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1497501f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x149750740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x149750c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1497511e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x149751730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x149751c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1497521d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x149752720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x149752c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1497531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x149753710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x149753c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1497541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x149754700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x149754c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1497551a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1497556f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x149755c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x149756190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1497566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x149756c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x149757180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1497576d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x149757c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x149758170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1497586c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x149758c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x149759160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1497596b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x149759c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14975a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14975a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14975abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14975b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14975b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14975bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14975c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14975c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14975cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14975d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14975d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14975dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14975e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14975e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14975ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14975f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14975f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14975fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14975fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149760380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x149760820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x149760cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149761160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x149761600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x149761aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x149761f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1497623e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149762880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x149762d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1497631c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x149763660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x149763bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1497642d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1497649f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x149765110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x149765830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x149765af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1497662e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1497665a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x149766bb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.915s
user	0m0.243s
sys	0m0.131s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
