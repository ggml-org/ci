### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.33 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.84 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.43 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.27 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.22 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.03 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.58 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  179.01 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.90 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.14 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 222.24 sec*proc (28 tests)

Total Test time (real) = 222.26 sec

real	3m42.257s
user	7m41.413s
sys	0m6.259s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.16 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.93 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.15 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.36 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.36 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.08 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.39 sec*proc (28 tests)

Total Test time (real) =  51.41 sec

real	0m51.417s
user	1m11.700s
sys	0m5.651s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.103 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.412 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.289 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.026.299 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.306 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.026.307 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.307 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.026.308 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.026.309 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.026.310 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.026.311 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.026.312 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.026.312 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.026.313 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.026.317 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.026.317 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.026.318 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.026.319 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.026.319 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.026.320 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.026.320 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.031.530 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.032.969 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.971 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.032.972 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.032.972 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.032.973 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.032.974 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.032.974 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.032.975 I llama_model_loader: - type  f32:  124 tensors
0.00.032.975 I llama_model_loader: - type  f16:   73 tensors
0.00.037.762 I llm_load_vocab: special tokens cache size = 5
0.00.040.237 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.040.241 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.040.241 I llm_load_print_meta: arch             = bert
0.00.040.242 I llm_load_print_meta: vocab type       = WPM
0.00.040.242 I llm_load_print_meta: n_vocab          = 30522
0.00.040.242 I llm_load_print_meta: n_merges         = 0
0.00.040.242 I llm_load_print_meta: vocab_only       = 0
0.00.040.243 I llm_load_print_meta: n_ctx_train      = 512
0.00.040.243 I llm_load_print_meta: n_embd           = 384
0.00.040.243 I llm_load_print_meta: n_layer          = 12
0.00.040.246 I llm_load_print_meta: n_head           = 12
0.00.040.248 I llm_load_print_meta: n_head_kv        = 12
0.00.040.248 I llm_load_print_meta: n_rot            = 32
0.00.040.248 I llm_load_print_meta: n_swa            = 0
0.00.040.248 I llm_load_print_meta: n_embd_head_k    = 32
0.00.040.248 I llm_load_print_meta: n_embd_head_v    = 32
0.00.040.249 I llm_load_print_meta: n_gqa            = 1
0.00.040.250 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.040.251 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.040.252 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.040.259 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.040.259 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.040.262 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.040.262 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.040.263 I llm_load_print_meta: n_ff             = 1536
0.00.040.264 I llm_load_print_meta: n_expert         = 0
0.00.040.264 I llm_load_print_meta: n_expert_used    = 0
0.00.040.264 I llm_load_print_meta: causal attn      = 0
0.00.040.265 I llm_load_print_meta: pooling type     = 2
0.00.040.265 I llm_load_print_meta: rope type        = 2
0.00.040.265 I llm_load_print_meta: rope scaling     = linear
0.00.040.266 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.040.267 I llm_load_print_meta: freq_scale_train = 1
0.00.040.268 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.040.268 I llm_load_print_meta: rope_finetuned   = unknown
0.00.040.270 I llm_load_print_meta: ssm_d_conv       = 0
0.00.040.270 I llm_load_print_meta: ssm_d_inner      = 0
0.00.040.270 I llm_load_print_meta: ssm_d_state      = 0
0.00.040.271 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.040.271 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.040.271 I llm_load_print_meta: model type       = 33M
0.00.040.271 I llm_load_print_meta: model ftype      = F16
0.00.040.276 I llm_load_print_meta: model params     = 33.21 M
0.00.040.277 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.040.277 I llm_load_print_meta: general.name     = Bge Small
0.00.040.278 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.040.278 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.040.279 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.040.279 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.040.279 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.040.280 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.040.282 I llm_load_print_meta: max token length = 21
0.00.042.417 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.042.418 I llm_load_tensors: offloading output layer to GPU
0.00.042.419 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.042.444 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.042.446 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.043.012 I llama_new_context_with_model: n_seq_max     = 1
0.00.043.014 I llama_new_context_with_model: n_ctx         = 512
0.00.043.014 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.043.014 I llama_new_context_with_model: n_batch       = 2048
0.00.043.015 I llama_new_context_with_model: n_ubatch      = 2048
0.00.043.015 I llama_new_context_with_model: flash_attn    = 0
0.00.043.016 I llama_new_context_with_model: freq_base     = 10000.0
0.00.043.016 I llama_new_context_with_model: freq_scale    = 1
0.00.043.017 I ggml_metal_init: allocating
0.00.043.021 I ggml_metal_init: found device: Apple M4
0.00.043.024 I ggml_metal_init: picking default device: Apple M4
0.00.043.944 I ggml_metal_init: using embedded metal library
0.00.048.442 I ggml_metal_init: GPU name:   Apple M4
0.00.048.445 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.048.446 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.048.446 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.048.446 I ggml_metal_init: simdgroup reduction   = true
0.00.048.447 I ggml_metal_init: simdgroup matrix mul. = true
0.00.048.447 I ggml_metal_init: has bfloat            = true
0.00.048.447 I ggml_metal_init: use bfloat            = true
0.00.048.448 I ggml_metal_init: hasUnifiedMemory      = true
0.00.048.449 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.061.557 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.062.478 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.062.481 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.062.483 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.063.574 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.063.576 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.063.576 I llama_new_context_with_model: graph nodes  = 429
0.00.063.576 I llama_new_context_with_model: graph splits = 2
0.00.063.603 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.063.603 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.070.105 I 
0.00.070.140 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.070.823 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.075.788 I llama_perf_context_print:        load time =      48.68 ms
0.00.075.789 I llama_perf_context_print: prompt eval time =       4.82 ms /     9 tokens (    0.54 ms per token,  1868.77 tokens per second)
0.00.075.790 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.075.791 I llama_perf_context_print:       total time =       5.68 ms /    10 tokens
0.00.075.939 I ggml_metal_free: deallocating

real	0m0.292s
user	0m0.053s
sys	0m0.032s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.035 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.385 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.530 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.534 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.535 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.536 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.536 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.536 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.537 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.537 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.538 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.538 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.538 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.539 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.541 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.541 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.542 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.542 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.542 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.543 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.543 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.112 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.779 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.780 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.781 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.781 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.781 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.782 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.782 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.782 I llama_model_loader: - type  f32:  124 tensors
0.00.014.782 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.496 I llm_load_vocab: special tokens cache size = 5
0.00.018.859 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.862 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.863 I llm_load_print_meta: arch             = bert
0.00.018.863 I llm_load_print_meta: vocab type       = WPM
0.00.018.863 I llm_load_print_meta: n_vocab          = 30522
0.00.018.863 I llm_load_print_meta: n_merges         = 0
0.00.018.863 I llm_load_print_meta: vocab_only       = 0
0.00.018.865 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.865 I llm_load_print_meta: n_embd           = 384
0.00.018.866 I llm_load_print_meta: n_layer          = 12
0.00.018.869 I llm_load_print_meta: n_head           = 12
0.00.018.869 I llm_load_print_meta: n_head_kv        = 12
0.00.018.869 I llm_load_print_meta: n_rot            = 32
0.00.018.869 I llm_load_print_meta: n_swa            = 0
0.00.018.873 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.874 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.875 I llm_load_print_meta: n_gqa            = 1
0.00.018.875 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.876 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.877 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.877 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.877 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.877 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.878 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.878 I llm_load_print_meta: n_ff             = 1536
0.00.018.878 I llm_load_print_meta: n_expert         = 0
0.00.018.879 I llm_load_print_meta: n_expert_used    = 0
0.00.018.879 I llm_load_print_meta: causal attn      = 0
0.00.018.879 I llm_load_print_meta: pooling type     = 2
0.00.018.879 I llm_load_print_meta: rope type        = 2
0.00.018.883 I llm_load_print_meta: rope scaling     = linear
0.00.018.883 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.883 I llm_load_print_meta: freq_scale_train = 1
0.00.018.883 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.884 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.884 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.884 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.884 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.884 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.884 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.885 I llm_load_print_meta: model type       = 33M
0.00.018.886 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.886 I llm_load_print_meta: model params     = 33.21 M
0.00.018.887 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.887 I llm_load_print_meta: general.name     = Bge Small
0.00.018.887 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.887 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.888 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.888 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.888 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.888 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.890 I llm_load_print_meta: max token length = 21
0.00.020.191 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.191 I llm_load_tensors: offloading output layer to GPU
0.00.020.192 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.197 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.197 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.571 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.572 I llama_new_context_with_model: n_ctx         = 512
0.00.020.572 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.573 I llama_new_context_with_model: n_batch       = 2048
0.00.020.573 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.573 I llama_new_context_with_model: flash_attn    = 0
0.00.020.574 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.574 I llama_new_context_with_model: freq_scale    = 1
0.00.020.574 I ggml_metal_init: allocating
0.00.020.581 I ggml_metal_init: found device: Apple M4
0.00.020.583 I ggml_metal_init: picking default device: Apple M4
0.00.021.238 I ggml_metal_init: using embedded metal library
0.00.023.757 I ggml_metal_init: GPU name:   Apple M4
0.00.023.761 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.761 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.761 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.762 I ggml_metal_init: simdgroup reduction   = true
0.00.023.762 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.762 I ggml_metal_init: has bfloat            = true
0.00.023.762 I ggml_metal_init: use bfloat            = true
0.00.023.763 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.763 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.907 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.034.385 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.386 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.388 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.994 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.995 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.996 I llama_new_context_with_model: graph nodes  = 429
0.00.034.996 I llama_new_context_with_model: graph splits = 2
0.00.035.009 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.010 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.359 I 
0.00.039.383 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.894 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.267 I llama_perf_context_print:        load time =      29.97 ms
0.00.044.268 I llama_perf_context_print: prompt eval time =       4.26 ms /     9 tokens (    0.47 ms per token,  2113.67 tokens per second)
0.00.044.269 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.269 I llama_perf_context_print:       total time =       4.91 ms /    10 tokens
0.00.044.467 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.030s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.082 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.354 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.557 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.024.562 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.563 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.024.564 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.565 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.024.565 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.024.571 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.024.572 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.024.573 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.024.573 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.024.574 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.024.574 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.024.576 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.024.577 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.024.577 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.024.577 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.578 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.028.457 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.029.668 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.366 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.032.367 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.368 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.032.368 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.032.368 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.032.369 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.032.369 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.032.369 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.032.369 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.032.370 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.032.370 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.032.370 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.032.371 I llama_model_loader: - type  f32:   40 tensors
0.00.032.371 I llama_model_loader: - type  f16:   30 tensors
0.00.045.864 W llm_load_vocab: empty token at index 5
0.00.049.734 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.050.891 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.050.918 I llm_load_vocab: special tokens cache size = 5
0.00.313.530 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.313.538 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.313.539 I llm_load_print_meta: arch             = jina-bert-v2
0.00.313.539 I llm_load_print_meta: vocab type       = BPE
0.00.313.539 I llm_load_print_meta: n_vocab          = 61056
0.00.313.554 I llm_load_print_meta: n_merges         = 39382
0.00.313.556 I llm_load_print_meta: vocab_only       = 0
0.00.313.558 I llm_load_print_meta: n_ctx_train      = 8192
0.00.313.558 I llm_load_print_meta: n_embd           = 384
0.00.313.559 I llm_load_print_meta: n_layer          = 4
0.00.313.563 I llm_load_print_meta: n_head           = 12
0.00.313.564 I llm_load_print_meta: n_head_kv        = 12
0.00.313.564 I llm_load_print_meta: n_rot            = 32
0.00.313.564 I llm_load_print_meta: n_swa            = 0
0.00.313.564 I llm_load_print_meta: n_embd_head_k    = 32
0.00.313.565 I llm_load_print_meta: n_embd_head_v    = 32
0.00.313.565 I llm_load_print_meta: n_gqa            = 1
0.00.313.566 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.313.567 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.313.568 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.313.568 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.313.568 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.313.569 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.313.569 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.313.569 I llm_load_print_meta: n_ff             = 1536
0.00.313.569 I llm_load_print_meta: n_expert         = 0
0.00.313.569 I llm_load_print_meta: n_expert_used    = 0
0.00.313.570 I llm_load_print_meta: causal attn      = 0
0.00.313.570 I llm_load_print_meta: pooling type     = -1
0.00.313.570 I llm_load_print_meta: rope type        = -1
0.00.313.570 I llm_load_print_meta: rope scaling     = linear
0.00.313.570 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.313.571 I llm_load_print_meta: freq_scale_train = 1
0.00.313.572 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.313.572 I llm_load_print_meta: rope_finetuned   = unknown
0.00.313.572 I llm_load_print_meta: ssm_d_conv       = 0
0.00.313.572 I llm_load_print_meta: ssm_d_inner      = 0
0.00.313.573 I llm_load_print_meta: ssm_d_state      = 0
0.00.313.573 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.313.573 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.313.573 I llm_load_print_meta: model type       = 33M
0.00.313.573 I llm_load_print_meta: model ftype      = F16
0.00.313.574 I llm_load_print_meta: model params     = 32.90 M
0.00.313.574 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.313.574 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.313.575 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.313.575 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.313.575 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.313.575 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.313.575 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.313.576 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.313.576 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.313.576 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.313.576 I llm_load_print_meta: max token length = 45
0.00.314.505 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.314.506 I llm_load_tensors: offloading output layer to GPU
0.00.314.507 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.314.525 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.314.526 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.315.201 I llama_new_context_with_model: n_seq_max     = 1
0.00.315.202 I llama_new_context_with_model: n_ctx         = 8192
0.00.315.202 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.315.203 I llama_new_context_with_model: n_batch       = 2048
0.00.315.203 I llama_new_context_with_model: n_ubatch      = 2048
0.00.315.203 I llama_new_context_with_model: flash_attn    = 0
0.00.315.203 I llama_new_context_with_model: freq_base     = 10000.0
0.00.315.203 I llama_new_context_with_model: freq_scale    = 1
0.00.315.204 I ggml_metal_init: allocating
0.00.315.208 I ggml_metal_init: found device: Apple M4
0.00.315.210 I ggml_metal_init: picking default device: Apple M4
0.00.315.892 I ggml_metal_init: using embedded metal library
0.00.318.392 I ggml_metal_init: GPU name:   Apple M4
0.00.318.393 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.318.394 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.318.394 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.318.394 I ggml_metal_init: simdgroup reduction   = true
0.00.318.395 I ggml_metal_init: simdgroup matrix mul. = true
0.00.318.395 I ggml_metal_init: has bfloat            = true
0.00.318.395 I ggml_metal_init: use bfloat            = true
0.00.318.396 I ggml_metal_init: hasUnifiedMemory      = true
0.00.318.397 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.328.432 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4
0.00.331.095 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.331.097 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.331.098 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.331.710 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.331.711 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.331.711 I llama_new_context_with_model: graph nodes  = 154
0.00.331.711 I llama_new_context_with_model: graph splits = 2
0.00.331.728 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.331.729 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.344.747 I 
0.00.344.785 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.344.949 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.344.949 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.344.958 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.344.958 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.344.962 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.344.962 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.345.450 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.348.903 I llama_perf_context_print:        load time =     328.38 ms
0.00.348.904 I llama_perf_context_print: prompt eval time =       3.45 ms /    62 tokens (    0.06 ms per token, 17997.10 tokens per second)
0.00.348.905 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.348.905 I llama_perf_context_print:       total time =       4.16 ms /    63 tokens
0.00.349.115 I ggml_metal_free: deallocating

real	0m1.069s
user	0m0.324s
sys	0m0.038s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.107 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.219 I main: llama backend init
0.00.000.226 I main: load the model and apply lora adapter, if any
0.00.056.766 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.068.198 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.068.215 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.068.232 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.068.233 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.068.234 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.068.234 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.068.235 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.068.237 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.068.237 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.068.237 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.068.239 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.068.239 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.068.240 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.068.240 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.068.246 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.068.246 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.068.247 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.075.348 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.077.637 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.086.170 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.086.180 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.086.181 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.086.181 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.086.182 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.086.183 I llama_model_loader: - type  f32:  194 tensors
0.00.086.184 I llama_model_loader: - type  f16:   98 tensors
0.00.124.206 I llm_load_vocab: special tokens cache size = 25
0.00.131.639 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.131.642 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.131.643 I llm_load_print_meta: arch             = gptneox
0.00.131.643 I llm_load_print_meta: vocab type       = BPE
0.00.131.643 I llm_load_print_meta: n_vocab          = 50304
0.00.131.644 I llm_load_print_meta: n_merges         = 50009
0.00.131.644 I llm_load_print_meta: vocab_only       = 0
0.00.131.644 I llm_load_print_meta: n_ctx_train      = 2048
0.00.131.644 I llm_load_print_meta: n_embd           = 2048
0.00.131.644 I llm_load_print_meta: n_layer          = 24
0.00.131.648 I llm_load_print_meta: n_head           = 16
0.00.131.649 I llm_load_print_meta: n_head_kv        = 16
0.00.131.649 I llm_load_print_meta: n_rot            = 32
0.00.131.649 I llm_load_print_meta: n_swa            = 0
0.00.131.650 I llm_load_print_meta: n_embd_head_k    = 128
0.00.131.651 I llm_load_print_meta: n_embd_head_v    = 128
0.00.131.653 I llm_load_print_meta: n_gqa            = 1
0.00.131.654 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.131.654 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.131.662 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.131.664 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.131.664 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.131.666 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.131.666 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.131.675 I llm_load_print_meta: n_ff             = 8192
0.00.131.675 I llm_load_print_meta: n_expert         = 0
0.00.131.675 I llm_load_print_meta: n_expert_used    = 0
0.00.131.675 I llm_load_print_meta: causal attn      = 1
0.00.131.675 I llm_load_print_meta: pooling type     = 0
0.00.131.676 I llm_load_print_meta: rope type        = 2
0.00.131.676 I llm_load_print_meta: rope scaling     = linear
0.00.131.676 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.131.677 I llm_load_print_meta: freq_scale_train = 1
0.00.131.678 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.131.678 I llm_load_print_meta: rope_finetuned   = unknown
0.00.131.679 I llm_load_print_meta: ssm_d_conv       = 0
0.00.131.679 I llm_load_print_meta: ssm_d_inner      = 0
0.00.131.679 I llm_load_print_meta: ssm_d_state      = 0
0.00.131.679 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.131.679 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.131.679 I llm_load_print_meta: model type       = 1.4B
0.00.131.680 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.131.681 I llm_load_print_meta: model params     = 1.41 B
0.00.131.681 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.131.682 I llm_load_print_meta: general.name     = 1.4B
0.00.131.682 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.131.682 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.131.682 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.131.683 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.131.683 I llm_load_print_meta: LF token         = 128 ''
0.00.131.683 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.131.683 I llm_load_print_meta: max token length = 1024
0.00.134.410 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.134.411 I llm_load_tensors: offloading output layer to GPU
0.00.134.411 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.134.430 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.134.431 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.135.458 I llama_new_context_with_model: n_seq_max     = 1
0.00.135.459 I llama_new_context_with_model: n_ctx         = 2048
0.00.135.459 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.135.460 I llama_new_context_with_model: n_batch       = 2048
0.00.135.460 I llama_new_context_with_model: n_ubatch      = 512
0.00.135.460 I llama_new_context_with_model: flash_attn    = 0
0.00.135.460 I llama_new_context_with_model: freq_base     = 10000.0
0.00.135.461 I llama_new_context_with_model: freq_scale    = 1
0.00.135.461 I ggml_metal_init: allocating
0.00.135.465 I ggml_metal_init: found device: Apple M4
0.00.135.467 I ggml_metal_init: picking default device: Apple M4
0.00.136.182 I ggml_metal_init: using embedded metal library
0.00.146.053 I ggml_metal_init: GPU name:   Apple M4
0.00.146.055 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.146.055 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.146.056 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.146.056 I ggml_metal_init: simdgroup reduction   = true
0.00.146.056 I ggml_metal_init: simdgroup matrix mul. = true
0.00.146.056 I ggml_metal_init: has bfloat            = true
0.00.146.056 I ggml_metal_init: use bfloat            = true
0.00.146.057 I ggml_metal_init: hasUnifiedMemory      = true
0.00.146.057 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.171.657 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.193.480 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.193.487 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.193.508 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.194.427 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.194.429 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.194.430 I llama_new_context_with_model: graph nodes  = 967
0.00.194.430 I llama_new_context_with_model: graph splits = 2
0.00.194.455 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.194.587 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.194.587 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.275.577 I main: llama threadpool init, n_threads = 4
0.00.275.614 I 
0.00.275.651 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.275.652 I 
0.00.275.728 I sampler seed: 1234
0.00.275.733 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.275.767 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.275.769 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.275.769 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.123.208 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56215.36 tokens per second)
0.02.123.209 I llama_perf_context_print:        load time =     218.80 ms
0.02.123.210 I llama_perf_context_print: prompt eval time =      43.65 ms /     7 tokens (    6.24 ms per token,   160.38 tokens per second)
0.02.123.211 I llama_perf_context_print:        eval time =    1800.81 ms /    63 runs   (   28.58 ms per token,    34.98 tokens per second)
0.02.123.211 I llama_perf_context_print:       total time =    1847.63 ms /    70 tokens
0.02.123.403 I ggml_metal_free: deallocating

real	0m2.433s
user	0m0.151s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.908 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.195 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.544 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.550 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.552 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.553 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.553 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.554 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.554 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.556 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.558 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.559 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.559 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.560 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.560 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.561 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.566 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.566 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.567 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.813 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.577 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.575 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.049.577 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.578 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.578 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.578 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.579 I llama_model_loader: - type  f32:  194 tensors
0.00.049.579 I llama_model_loader: - type  f16:   98 tensors
0.00.077.294 I llm_load_vocab: special tokens cache size = 25
0.00.083.570 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.083.573 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.083.573 I llm_load_print_meta: arch             = gptneox
0.00.083.573 I llm_load_print_meta: vocab type       = BPE
0.00.083.574 I llm_load_print_meta: n_vocab          = 50304
0.00.083.574 I llm_load_print_meta: n_merges         = 50009
0.00.083.574 I llm_load_print_meta: vocab_only       = 0
0.00.083.574 I llm_load_print_meta: n_ctx_train      = 2048
0.00.083.574 I llm_load_print_meta: n_embd           = 2048
0.00.083.574 I llm_load_print_meta: n_layer          = 24
0.00.083.578 I llm_load_print_meta: n_head           = 16
0.00.083.578 I llm_load_print_meta: n_head_kv        = 16
0.00.083.579 I llm_load_print_meta: n_rot            = 32
0.00.083.579 I llm_load_print_meta: n_swa            = 0
0.00.083.579 I llm_load_print_meta: n_embd_head_k    = 128
0.00.083.579 I llm_load_print_meta: n_embd_head_v    = 128
0.00.083.580 I llm_load_print_meta: n_gqa            = 1
0.00.083.581 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.083.581 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.083.582 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.083.582 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.083.582 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.083.583 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.083.583 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.083.583 I llm_load_print_meta: n_ff             = 8192
0.00.083.584 I llm_load_print_meta: n_expert         = 0
0.00.083.584 I llm_load_print_meta: n_expert_used    = 0
0.00.083.584 I llm_load_print_meta: causal attn      = 1
0.00.083.584 I llm_load_print_meta: pooling type     = 0
0.00.083.584 I llm_load_print_meta: rope type        = 2
0.00.083.585 I llm_load_print_meta: rope scaling     = linear
0.00.083.585 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.083.585 I llm_load_print_meta: freq_scale_train = 1
0.00.083.586 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.083.586 I llm_load_print_meta: rope_finetuned   = unknown
0.00.083.586 I llm_load_print_meta: ssm_d_conv       = 0
0.00.083.586 I llm_load_print_meta: ssm_d_inner      = 0
0.00.083.586 I llm_load_print_meta: ssm_d_state      = 0
0.00.083.587 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.083.587 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.083.587 I llm_load_print_meta: model type       = 1.4B
0.00.083.588 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.083.588 I llm_load_print_meta: model params     = 1.41 B
0.00.083.589 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.083.590 I llm_load_print_meta: general.name     = 1.4B
0.00.083.590 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.083.590 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.083.591 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.083.591 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.083.591 I llm_load_print_meta: LF token         = 128 ''
0.00.083.591 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.083.591 I llm_load_print_meta: max token length = 1024
0.00.086.901 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.086.902 I llm_load_tensors: offloading output layer to GPU
0.00.086.902 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.086.912 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.086.914 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.087.938 I llama_new_context_with_model: n_seq_max     = 1
0.00.087.939 I llama_new_context_with_model: n_ctx         = 128
0.00.087.939 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.087.940 I llama_new_context_with_model: n_batch       = 128
0.00.087.940 I llama_new_context_with_model: n_ubatch      = 128
0.00.087.940 I llama_new_context_with_model: flash_attn    = 0
0.00.087.940 I llama_new_context_with_model: freq_base     = 10000.0
0.00.087.941 I llama_new_context_with_model: freq_scale    = 1
0.00.087.941 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.087.941 I ggml_metal_init: allocating
0.00.087.944 I ggml_metal_init: found device: Apple M4
0.00.087.946 I ggml_metal_init: picking default device: Apple M4
0.00.088.537 I ggml_metal_init: using embedded metal library
0.00.091.096 I ggml_metal_init: GPU name:   Apple M4
0.00.091.098 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.098 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.099 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.099 I ggml_metal_init: simdgroup reduction   = true
0.00.091.099 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.099 I ggml_metal_init: has bfloat            = true
0.00.091.099 I ggml_metal_init: use bfloat            = true
0.00.091.100 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.100 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.607 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.101.904 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.101.908 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.101.922 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.838 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.102.839 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.102.840 I llama_new_context_with_model: graph nodes  = 967
0.00.102.840 I llama_new_context_with_model: graph splits = 2
0.00.102.852 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.102.853 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.281.659 I 
0.01.281.758 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.281.814 I perplexity: tokenizing the input ..
0.01.294.396 I perplexity: tokenization took 12.578 ms
0.01.294.404 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.413.783 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.415.656 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.415.673 I llama_perf_context_print:        load time =    1259.45 ms
0.01.415.676 I llama_perf_context_print: prompt eval time =     119.08 ms /   128 tokens (    0.93 ms per token,  1074.95 tokens per second)
0.01.415.678 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.415.678 I llama_perf_context_print:       total time =     134.02 ms /   129 tokens
0.01.416.215 I ggml_metal_free: deallocating

real	0m1.604s
user	0m0.120s
sys	0m0.217s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.740 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.034 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.039 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.041 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.047 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.047 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.048 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.048 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.049 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.050 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.050 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.050 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.052 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.053 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.053 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.055 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.055 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.055 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.904 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.988 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.897 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.898 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.898 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.899 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.899 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.899 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.900 I llama_model_loader: - type  f32:  194 tensors
0.00.032.900 I llama_model_loader: - type q8_0:   98 tensors
0.00.054.579 I llm_load_vocab: special tokens cache size = 25
0.00.060.460 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.465 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.465 I llm_load_print_meta: arch             = gptneox
0.00.060.466 I llm_load_print_meta: vocab type       = BPE
0.00.060.466 I llm_load_print_meta: n_vocab          = 50304
0.00.060.466 I llm_load_print_meta: n_merges         = 50009
0.00.060.466 I llm_load_print_meta: vocab_only       = 0
0.00.060.466 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.469 I llm_load_print_meta: n_embd           = 2048
0.00.060.469 I llm_load_print_meta: n_layer          = 24
0.00.060.475 I llm_load_print_meta: n_head           = 16
0.00.060.477 I llm_load_print_meta: n_head_kv        = 16
0.00.060.477 I llm_load_print_meta: n_rot            = 32
0.00.060.477 I llm_load_print_meta: n_swa            = 0
0.00.060.477 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.477 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.478 I llm_load_print_meta: n_gqa            = 1
0.00.060.479 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.480 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.480 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.481 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.481 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.481 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.481 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.482 I llm_load_print_meta: n_ff             = 8192
0.00.060.482 I llm_load_print_meta: n_expert         = 0
0.00.060.482 I llm_load_print_meta: n_expert_used    = 0
0.00.060.482 I llm_load_print_meta: causal attn      = 1
0.00.060.482 I llm_load_print_meta: pooling type     = 0
0.00.060.483 I llm_load_print_meta: rope type        = 2
0.00.060.484 I llm_load_print_meta: rope scaling     = linear
0.00.060.485 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.485 I llm_load_print_meta: freq_scale_train = 1
0.00.060.485 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.486 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.486 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.486 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.486 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.486 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.486 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.486 I llm_load_print_meta: model type       = 1.4B
0.00.060.487 I llm_load_print_meta: model ftype      = Q8_0
0.00.060.487 I llm_load_print_meta: model params     = 1.41 B
0.00.060.487 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.060.488 I llm_load_print_meta: general.name     = 1.4B
0.00.060.488 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.488 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.488 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.488 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.492 I llm_load_print_meta: LF token         = 128 ''
0.00.060.492 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.492 I llm_load_print_meta: max token length = 1024
0.00.062.928 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.929 I llm_load_tensors: offloading output layer to GPU
0.00.062.929 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.940 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.941 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.063.916 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.917 I llama_new_context_with_model: n_ctx         = 2048
0.00.063.917 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.063.917 I llama_new_context_with_model: n_batch       = 2048
0.00.063.917 I llama_new_context_with_model: n_ubatch      = 512
0.00.063.917 I llama_new_context_with_model: flash_attn    = 0
0.00.063.918 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.918 I llama_new_context_with_model: freq_scale    = 1
0.00.063.919 I ggml_metal_init: allocating
0.00.063.922 I ggml_metal_init: found device: Apple M4
0.00.063.924 I ggml_metal_init: picking default device: Apple M4
0.00.064.652 I ggml_metal_init: using embedded metal library
0.00.067.172 I ggml_metal_init: GPU name:   Apple M4
0.00.067.173 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.174 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.174 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.174 I ggml_metal_init: simdgroup reduction   = true
0.00.067.174 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.175 I ggml_metal_init: has bfloat            = true
0.00.067.175 I ggml_metal_init: use bfloat            = true
0.00.067.175 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.176 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.515 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.103.359 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.103.366 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.103.390 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.641 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.104.643 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.104.643 I llama_new_context_with_model: graph nodes  = 967
0.00.104.643 I llama_new_context_with_model: graph splits = 2
0.00.104.660 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.104.795 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.796 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.218.545 I main: llama threadpool init, n_threads = 4
0.01.218.580 I 
0.01.218.606 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.218.607 I 
0.01.218.833 I sampler seed: 1234
0.01.218.838 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.218.853 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.218.854 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.218.854 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.314.881 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53584.91 tokens per second)
0.02.314.881 I llama_perf_context_print:        load time =    1208.80 ms
0.02.314.882 I llama_perf_context_print: prompt eval time =      39.75 ms /     7 tokens (    5.68 ms per token,   176.09 tokens per second)
0.02.314.883 I llama_perf_context_print:        eval time =    1053.50 ms /    63 runs   (   16.72 ms per token,    59.80 tokens per second)
0.02.314.883 I llama_perf_context_print:       total time =    1096.34 ms /    70 tokens
0.02.315.108 I ggml_metal_free: deallocating

real	0m2.335s
user	0m0.114s
sys	0m0.229s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.332 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.383 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.836 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.844 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.853 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.853 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.854 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.854 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.855 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.858 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.858 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.858 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.859 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.859 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.860 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.860 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.863 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.863 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.864 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.904 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.591 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.460 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.462 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.462 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.462 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.463 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.463 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.464 I llama_model_loader: - type  f32:  194 tensors
0.00.035.464 I llama_model_loader: - type q8_0:   98 tensors
0.00.062.967 I llm_load_vocab: special tokens cache size = 25
0.00.069.283 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.286 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.286 I llm_load_print_meta: arch             = gptneox
0.00.069.286 I llm_load_print_meta: vocab type       = BPE
0.00.069.286 I llm_load_print_meta: n_vocab          = 50304
0.00.069.287 I llm_load_print_meta: n_merges         = 50009
0.00.069.287 I llm_load_print_meta: vocab_only       = 0
0.00.069.287 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.287 I llm_load_print_meta: n_embd           = 2048
0.00.069.287 I llm_load_print_meta: n_layer          = 24
0.00.069.290 I llm_load_print_meta: n_head           = 16
0.00.069.293 I llm_load_print_meta: n_head_kv        = 16
0.00.069.293 I llm_load_print_meta: n_rot            = 32
0.00.069.293 I llm_load_print_meta: n_swa            = 0
0.00.069.294 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.294 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.294 I llm_load_print_meta: n_gqa            = 1
0.00.069.295 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.296 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.296 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.297 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.297 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.297 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.297 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.299 I llm_load_print_meta: n_ff             = 8192
0.00.069.299 I llm_load_print_meta: n_expert         = 0
0.00.069.299 I llm_load_print_meta: n_expert_used    = 0
0.00.069.299 I llm_load_print_meta: causal attn      = 1
0.00.069.300 I llm_load_print_meta: pooling type     = 0
0.00.069.300 I llm_load_print_meta: rope type        = 2
0.00.069.300 I llm_load_print_meta: rope scaling     = linear
0.00.069.300 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.300 I llm_load_print_meta: freq_scale_train = 1
0.00.069.301 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.301 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.301 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.301 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.301 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.301 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.301 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.302 I llm_load_print_meta: model type       = 1.4B
0.00.069.305 I llm_load_print_meta: model ftype      = Q8_0
0.00.069.306 I llm_load_print_meta: model params     = 1.41 B
0.00.069.306 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.069.307 I llm_load_print_meta: general.name     = 1.4B
0.00.069.308 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.308 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.308 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.309 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.309 I llm_load_print_meta: LF token         = 128 ''
0.00.069.309 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.309 I llm_load_print_meta: max token length = 1024
0.00.071.302 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.303 I llm_load_tensors: offloading output layer to GPU
0.00.071.303 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.308 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.310 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.072.235 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.236 I llama_new_context_with_model: n_ctx         = 128
0.00.072.236 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.072.236 I llama_new_context_with_model: n_batch       = 128
0.00.072.237 I llama_new_context_with_model: n_ubatch      = 128
0.00.072.237 I llama_new_context_with_model: flash_attn    = 0
0.00.072.237 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.237 I llama_new_context_with_model: freq_scale    = 1
0.00.072.238 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.072.238 I ggml_metal_init: allocating
0.00.072.244 I ggml_metal_init: found device: Apple M4
0.00.072.246 I ggml_metal_init: picking default device: Apple M4
0.00.072.851 I ggml_metal_init: using embedded metal library
0.00.075.440 I ggml_metal_init: GPU name:   Apple M4
0.00.075.442 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.442 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.442 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.443 I ggml_metal_init: simdgroup reduction   = true
0.00.075.443 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.443 I ggml_metal_init: has bfloat            = true
0.00.075.443 I ggml_metal_init: use bfloat            = true
0.00.075.443 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.444 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.855 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.176 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.087.179 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.087.195 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.084 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.088.086 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.088.086 I llama_new_context_with_model: graph nodes  = 967
0.00.088.086 I llama_new_context_with_model: graph splits = 2
0.00.088.100 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.088.101 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.929.423 I 
0.00.929.460 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.929.471 I perplexity: tokenizing the input ..
0.00.937.797 I perplexity: tokenization took 8.324 ms
0.00.937.801 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.062.076 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.063.238 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.063.252 I llama_perf_context_print:        load time =     917.03 ms
0.01.063.253 I llama_perf_context_print: prompt eval time =     124.05 ms /   128 tokens (    0.97 ms per token,  1031.83 tokens per second)
0.01.063.254 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.063.254 I llama_perf_context_print:       total time =     133.83 ms /   129 tokens
0.01.063.709 I ggml_metal_free: deallocating

real	0m1.085s
user	0m0.098s
sys	0m0.157s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.015.701 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.346 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.037.358 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.362 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.362 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.363 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.363 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.363 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.365 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.365 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.366 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.366 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.366 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.368 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.369 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.371 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.372 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.372 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.719 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.902 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.324 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.047.326 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.326 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.326 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.327 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.327 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.047.328 I llama_model_loader: - type  f32:  194 tensors
0.00.047.328 I llama_model_loader: - type q4_0:   97 tensors
0.00.047.328 I llama_model_loader: - type q6_K:    1 tensors
0.00.075.415 I llm_load_vocab: special tokens cache size = 25
0.00.085.559 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.564 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.564 I llm_load_print_meta: arch             = gptneox
0.00.085.565 I llm_load_print_meta: vocab type       = BPE
0.00.085.565 I llm_load_print_meta: n_vocab          = 50304
0.00.085.565 I llm_load_print_meta: n_merges         = 50009
0.00.085.566 I llm_load_print_meta: vocab_only       = 0
0.00.085.566 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.568 I llm_load_print_meta: n_embd           = 2048
0.00.085.568 I llm_load_print_meta: n_layer          = 24
0.00.085.573 I llm_load_print_meta: n_head           = 16
0.00.085.574 I llm_load_print_meta: n_head_kv        = 16
0.00.085.574 I llm_load_print_meta: n_rot            = 32
0.00.085.575 I llm_load_print_meta: n_swa            = 0
0.00.085.575 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.575 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.576 I llm_load_print_meta: n_gqa            = 1
0.00.085.577 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.578 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.579 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.580 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.586 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.587 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.587 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.591 I llm_load_print_meta: n_ff             = 8192
0.00.085.591 I llm_load_print_meta: n_expert         = 0
0.00.085.591 I llm_load_print_meta: n_expert_used    = 0
0.00.085.591 I llm_load_print_meta: causal attn      = 1
0.00.085.592 I llm_load_print_meta: pooling type     = 0
0.00.085.592 I llm_load_print_meta: rope type        = 2
0.00.085.592 I llm_load_print_meta: rope scaling     = linear
0.00.085.593 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.593 I llm_load_print_meta: freq_scale_train = 1
0.00.085.593 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.594 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.602 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.603 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.603 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.603 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.604 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.604 I llm_load_print_meta: model type       = 1.4B
0.00.085.605 I llm_load_print_meta: model ftype      = Q4_0
0.00.085.606 I llm_load_print_meta: model params     = 1.41 B
0.00.085.612 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.085.613 I llm_load_print_meta: general.name     = 1.4B
0.00.085.613 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.614 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.614 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.614 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.615 I llm_load_print_meta: LF token         = 128 ''
0.00.085.615 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.616 I llm_load_print_meta: max token length = 1024
0.00.088.879 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.880 I llm_load_tensors: offloading output layer to GPU
0.00.088.881 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.893 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.088.894 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.090.493 I llama_new_context_with_model: n_seq_max     = 1
0.00.090.495 I llama_new_context_with_model: n_ctx         = 2048
0.00.090.495 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.090.495 I llama_new_context_with_model: n_batch       = 2048
0.00.090.496 I llama_new_context_with_model: n_ubatch      = 512
0.00.090.496 I llama_new_context_with_model: flash_attn    = 0
0.00.090.497 I llama_new_context_with_model: freq_base     = 10000.0
0.00.090.497 I llama_new_context_with_model: freq_scale    = 1
0.00.090.498 I ggml_metal_init: allocating
0.00.090.502 I ggml_metal_init: found device: Apple M4
0.00.090.505 I ggml_metal_init: picking default device: Apple M4
0.00.092.527 I ggml_metal_init: using embedded metal library
0.00.096.360 I ggml_metal_init: GPU name:   Apple M4
0.00.096.363 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.363 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.364 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.364 I ggml_metal_init: simdgroup reduction   = true
0.00.096.364 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.364 I ggml_metal_init: has bfloat            = true
0.00.096.364 I ggml_metal_init: use bfloat            = true
0.00.096.365 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.366 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.180 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.134.168 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.134.175 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.134.197 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.135.175 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.135.176 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.135.176 I llama_new_context_with_model: graph nodes  = 967
0.00.135.176 I llama_new_context_with_model: graph splits = 2
0.00.135.189 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.135.330 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.135.331 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.474 I main: llama threadpool init, n_threads = 4
0.00.750.561 I 
0.00.750.645 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.646 I 
0.00.751.140 I sampler seed: 1234
0.00.751.151 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.751.237 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.751.243 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.751.243 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.440.329 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 59966.22 tokens per second)
0.01.440.329 I llama_perf_context_print:        load time =     734.76 ms
0.01.440.330 I llama_perf_context_print: prompt eval time =      46.39 ms /     7 tokens (    6.63 ms per token,   150.90 tokens per second)
0.01.440.331 I llama_perf_context_print:        eval time =     639.77 ms /    63 runs   (   10.16 ms per token,    98.47 tokens per second)
0.01.440.331 I llama_perf_context_print:       total time =     689.86 ms /    70 tokens
0.01.440.528 I ggml_metal_free: deallocating

real	0m1.460s
user	0m0.136s
sys	0m0.177s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.263 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.150 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.868 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.871 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.874 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.874 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.874 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.875 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.875 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.876 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.876 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.876 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.879 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.879 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.880 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.880 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.881 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.882 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.882 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.760 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.819 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.744 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.745 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.745 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.746 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.746 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.746 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.747 I llama_model_loader: - type  f32:  194 tensors
0.00.024.747 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.747 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.109 I llm_load_vocab: special tokens cache size = 25
0.00.050.901 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.904 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.904 I llm_load_print_meta: arch             = gptneox
0.00.050.904 I llm_load_print_meta: vocab type       = BPE
0.00.050.905 I llm_load_print_meta: n_vocab          = 50304
0.00.050.905 I llm_load_print_meta: n_merges         = 50009
0.00.050.905 I llm_load_print_meta: vocab_only       = 0
0.00.050.905 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.905 I llm_load_print_meta: n_embd           = 2048
0.00.050.905 I llm_load_print_meta: n_layer          = 24
0.00.050.908 I llm_load_print_meta: n_head           = 16
0.00.050.909 I llm_load_print_meta: n_head_kv        = 16
0.00.050.909 I llm_load_print_meta: n_rot            = 32
0.00.050.911 I llm_load_print_meta: n_swa            = 0
0.00.050.911 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.911 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.912 I llm_load_print_meta: n_gqa            = 1
0.00.050.913 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.919 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.920 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.921 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.921 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.923 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.923 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.929 I llm_load_print_meta: n_ff             = 8192
0.00.050.929 I llm_load_print_meta: n_expert         = 0
0.00.050.929 I llm_load_print_meta: n_expert_used    = 0
0.00.050.929 I llm_load_print_meta: causal attn      = 1
0.00.050.929 I llm_load_print_meta: pooling type     = 0
0.00.050.930 I llm_load_print_meta: rope type        = 2
0.00.050.930 I llm_load_print_meta: rope scaling     = linear
0.00.050.930 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.931 I llm_load_print_meta: freq_scale_train = 1
0.00.050.931 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.931 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.931 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.931 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.931 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.931 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.932 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.932 I llm_load_print_meta: model type       = 1.4B
0.00.050.932 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.933 I llm_load_print_meta: model params     = 1.41 B
0.00.050.933 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.933 I llm_load_print_meta: general.name     = 1.4B
0.00.050.933 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.934 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.934 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.934 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.934 I llm_load_print_meta: LF token         = 128 ''
0.00.050.935 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.935 I llm_load_print_meta: max token length = 1024
0.00.052.826 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.826 I llm_load_tensors: offloading output layer to GPU
0.00.052.826 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.837 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.837 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.695 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.696 I llama_new_context_with_model: n_ctx         = 128
0.00.053.696 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.696 I llama_new_context_with_model: n_batch       = 128
0.00.053.697 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.697 I llama_new_context_with_model: flash_attn    = 0
0.00.053.697 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.698 I llama_new_context_with_model: freq_scale    = 1
0.00.053.698 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.698 I ggml_metal_init: allocating
0.00.053.705 I ggml_metal_init: found device: Apple M4
0.00.053.707 I ggml_metal_init: picking default device: Apple M4
0.00.054.277 I ggml_metal_init: using embedded metal library
0.00.056.642 I ggml_metal_init: GPU name:   Apple M4
0.00.056.643 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.643 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.644 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.644 I ggml_metal_init: simdgroup reduction   = true
0.00.056.644 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.644 I ggml_metal_init: has bfloat            = true
0.00.056.644 I ggml_metal_init: use bfloat            = true
0.00.056.645 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.645 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.517 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.735 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.736 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.749 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.647 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.648 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.648 I llama_new_context_with_model: graph nodes  = 967
0.00.068.649 I llama_new_context_with_model: graph splits = 2
0.00.068.661 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.662 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.623.599 I 
0.00.623.638 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.623.651 I perplexity: tokenizing the input ..
0.00.631.428 I perplexity: tokenization took 7.776 ms
0.00.631.436 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.754.099 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.755.262 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.755.285 I llama_perf_context_print:        load time =     613.44 ms
0.00.755.286 I llama_perf_context_print: prompt eval time =     122.44 ms /   128 tokens (    0.96 ms per token,  1045.43 tokens per second)
0.00.755.286 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.755.287 I llama_perf_context_print:       total time =     131.69 ms /   129 tokens
0.00.755.666 I ggml_metal_free: deallocating

real	0m0.771s
user	0m0.078s
sys	0m0.111s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.014.292 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.595 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.020.599 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.600 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.601 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.601 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.602 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.602 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.603 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.603 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.604 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.604 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.605 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.605 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.605 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.608 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.609 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.609 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.513 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.617 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.596 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.598 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.598 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.598 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.598 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.599 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.029.599 I llama_model_loader: - type  f32:  194 tensors
0.00.029.599 I llama_model_loader: - type q4_1:   97 tensors
0.00.029.600 I llama_model_loader: - type q6_K:    1 tensors
0.00.050.950 I llm_load_vocab: special tokens cache size = 25
0.00.056.936 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.056.939 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.056.939 I llm_load_print_meta: arch             = gptneox
0.00.056.940 I llm_load_print_meta: vocab type       = BPE
0.00.056.940 I llm_load_print_meta: n_vocab          = 50304
0.00.056.940 I llm_load_print_meta: n_merges         = 50009
0.00.056.940 I llm_load_print_meta: vocab_only       = 0
0.00.056.940 I llm_load_print_meta: n_ctx_train      = 2048
0.00.056.941 I llm_load_print_meta: n_embd           = 2048
0.00.056.941 I llm_load_print_meta: n_layer          = 24
0.00.056.943 I llm_load_print_meta: n_head           = 16
0.00.056.944 I llm_load_print_meta: n_head_kv        = 16
0.00.056.944 I llm_load_print_meta: n_rot            = 32
0.00.056.944 I llm_load_print_meta: n_swa            = 0
0.00.056.945 I llm_load_print_meta: n_embd_head_k    = 128
0.00.056.945 I llm_load_print_meta: n_embd_head_v    = 128
0.00.056.945 I llm_load_print_meta: n_gqa            = 1
0.00.056.946 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.056.947 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.056.948 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.056.948 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.948 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.948 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.948 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.056.949 I llm_load_print_meta: n_ff             = 8192
0.00.056.949 I llm_load_print_meta: n_expert         = 0
0.00.056.950 I llm_load_print_meta: n_expert_used    = 0
0.00.056.950 I llm_load_print_meta: causal attn      = 1
0.00.056.950 I llm_load_print_meta: pooling type     = 0
0.00.056.953 I llm_load_print_meta: rope type        = 2
0.00.056.953 I llm_load_print_meta: rope scaling     = linear
0.00.056.953 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.056.953 I llm_load_print_meta: freq_scale_train = 1
0.00.056.954 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.056.954 I llm_load_print_meta: rope_finetuned   = unknown
0.00.056.954 I llm_load_print_meta: ssm_d_conv       = 0
0.00.056.954 I llm_load_print_meta: ssm_d_inner      = 0
0.00.056.954 I llm_load_print_meta: ssm_d_state      = 0
0.00.056.954 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.056.955 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.056.955 I llm_load_print_meta: model type       = 1.4B
0.00.056.955 I llm_load_print_meta: model ftype      = Q4_1
0.00.056.955 I llm_load_print_meta: model params     = 1.41 B
0.00.056.956 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.056.956 I llm_load_print_meta: general.name     = 1.4B
0.00.056.956 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.957 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.957 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.957 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.957 I llm_load_print_meta: LF token         = 128 ''
0.00.056.957 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.958 I llm_load_print_meta: max token length = 1024
0.00.058.757 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.758 I llm_load_tensors: offloading output layer to GPU
0.00.058.758 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.769 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.058.770 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.059.665 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.666 I llama_new_context_with_model: n_ctx         = 2048
0.00.059.666 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.059.666 I llama_new_context_with_model: n_batch       = 2048
0.00.059.666 I llama_new_context_with_model: n_ubatch      = 512
0.00.059.667 I llama_new_context_with_model: flash_attn    = 0
0.00.059.667 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.667 I llama_new_context_with_model: freq_scale    = 1
0.00.059.668 I ggml_metal_init: allocating
0.00.059.672 I ggml_metal_init: found device: Apple M4
0.00.059.675 I ggml_metal_init: picking default device: Apple M4
0.00.060.306 I ggml_metal_init: using embedded metal library
0.00.062.660 I ggml_metal_init: GPU name:   Apple M4
0.00.062.661 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.661 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.662 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.662 I ggml_metal_init: simdgroup reduction   = true
0.00.062.662 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.662 I ggml_metal_init: has bfloat            = true
0.00.062.663 I ggml_metal_init: use bfloat            = true
0.00.062.663 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.663 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.518 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.091.265 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.091.273 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.291 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.092.284 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.092.285 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.092.286 I llama_new_context_with_model: graph nodes  = 967
0.00.092.286 I llama_new_context_with_model: graph splits = 2
0.00.092.302 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.092.442 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.092.442 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.714.542 I main: llama threadpool init, n_threads = 4
0.00.714.584 I 
0.00.714.617 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.714.619 I 
0.00.714.867 I sampler seed: 1234
0.00.714.872 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.714.913 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.714.928 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.714.928 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.436.083 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62500.00 tokens per second)
0.01.436.084 I llama_perf_context_print:        load time =     700.24 ms
0.01.436.085 I llama_perf_context_print: prompt eval time =      39.63 ms /     7 tokens (    5.66 ms per token,   176.61 tokens per second)
0.01.436.086 I llama_perf_context_print:        eval time =     678.56 ms /    63 runs   (   10.77 ms per token,    92.84 tokens per second)
0.01.436.086 I llama_perf_context_print:       total time =     721.55 ms /    70 tokens
0.01.436.254 I ggml_metal_free: deallocating

real	0m1.454s
user	0m0.110s
sys	0m0.133s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.912 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.536 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.540 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.541 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.542 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.542 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.543 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.543 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.544 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.544 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.545 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.545 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.545 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.546 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.546 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.547 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.548 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.548 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.397 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.483 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.292 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.293 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.293 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.294 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.294 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.294 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.295 I llama_model_loader: - type  f32:  194 tensors
0.00.023.295 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.296 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.240 I llm_load_vocab: special tokens cache size = 25
0.00.050.043 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.045 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.046 I llm_load_print_meta: arch             = gptneox
0.00.050.046 I llm_load_print_meta: vocab type       = BPE
0.00.050.046 I llm_load_print_meta: n_vocab          = 50304
0.00.050.047 I llm_load_print_meta: n_merges         = 50009
0.00.050.047 I llm_load_print_meta: vocab_only       = 0
0.00.050.047 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.047 I llm_load_print_meta: n_embd           = 2048
0.00.050.047 I llm_load_print_meta: n_layer          = 24
0.00.050.050 I llm_load_print_meta: n_head           = 16
0.00.050.051 I llm_load_print_meta: n_head_kv        = 16
0.00.050.051 I llm_load_print_meta: n_rot            = 32
0.00.050.051 I llm_load_print_meta: n_swa            = 0
0.00.050.052 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.052 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.053 I llm_load_print_meta: n_gqa            = 1
0.00.050.054 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.054 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.055 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.055 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.058 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.058 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.058 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.059 I llm_load_print_meta: n_ff             = 8192
0.00.050.059 I llm_load_print_meta: n_expert         = 0
0.00.050.059 I llm_load_print_meta: n_expert_used    = 0
0.00.050.059 I llm_load_print_meta: causal attn      = 1
0.00.050.059 I llm_load_print_meta: pooling type     = 0
0.00.050.060 I llm_load_print_meta: rope type        = 2
0.00.050.060 I llm_load_print_meta: rope scaling     = linear
0.00.050.060 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.061 I llm_load_print_meta: freq_scale_train = 1
0.00.050.061 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.061 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.062 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.063 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.063 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.063 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.063 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.063 I llm_load_print_meta: model type       = 1.4B
0.00.050.063 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.064 I llm_load_print_meta: model params     = 1.41 B
0.00.050.066 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.066 I llm_load_print_meta: general.name     = 1.4B
0.00.050.066 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.067 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.067 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.067 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.067 I llm_load_print_meta: LF token         = 128 ''
0.00.050.067 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.068 I llm_load_print_meta: max token length = 1024
0.00.052.051 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.051 I llm_load_tensors: offloading output layer to GPU
0.00.052.051 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.061 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.062 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.965 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.965 I llama_new_context_with_model: n_ctx         = 128
0.00.052.966 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.966 I llama_new_context_with_model: n_batch       = 128
0.00.052.966 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.966 I llama_new_context_with_model: flash_attn    = 0
0.00.052.966 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.967 I llama_new_context_with_model: freq_scale    = 1
0.00.052.967 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.967 I ggml_metal_init: allocating
0.00.052.970 I ggml_metal_init: found device: Apple M4
0.00.052.972 I ggml_metal_init: picking default device: Apple M4
0.00.053.555 I ggml_metal_init: using embedded metal library
0.00.055.885 I ggml_metal_init: GPU name:   Apple M4
0.00.055.886 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.886 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.887 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.887 I ggml_metal_init: simdgroup reduction   = true
0.00.055.887 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.887 I ggml_metal_init: has bfloat            = true
0.00.055.887 I ggml_metal_init: use bfloat            = true
0.00.055.888 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.888 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.668 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.955 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.957 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.971 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.941 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.942 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.943 I llama_new_context_with_model: graph nodes  = 967
0.00.067.943 I llama_new_context_with_model: graph splits = 2
0.00.067.956 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.956 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.674.883 I 
0.00.674.952 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.674.972 I perplexity: tokenizing the input ..
0.00.682.584 I perplexity: tokenization took 7.611 ms
0.00.682.587 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.805.670 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.806.912 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.806.930 I llama_perf_context_print:        load time =     665.96 ms
0.00.806.933 I llama_perf_context_print: prompt eval time =     122.86 ms /   128 tokens (    0.96 ms per token,  1041.88 tokens per second)
0.00.806.933 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.806.934 I llama_perf_context_print:       total time =     132.05 ms /   129 tokens
0.00.807.379 I ggml_metal_free: deallocating

real	0m0.820s
user	0m0.078s
sys	0m0.098s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.208 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.665 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.670 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.671 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.672 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.672 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.676 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.676 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.677 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.678 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.678 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.679 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.679 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.679 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.680 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.682 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.682 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.683 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.609 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.665 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.616 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.618 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.618 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.618 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.618 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.619 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.619 I llama_model_loader: - type  f32:  194 tensors
0.00.024.620 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.620 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.092 I llm_load_vocab: special tokens cache size = 25
0.00.050.924 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.927 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.927 I llm_load_print_meta: arch             = gptneox
0.00.050.928 I llm_load_print_meta: vocab type       = BPE
0.00.050.928 I llm_load_print_meta: n_vocab          = 50304
0.00.050.928 I llm_load_print_meta: n_merges         = 50009
0.00.050.928 I llm_load_print_meta: vocab_only       = 0
0.00.050.928 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.929 I llm_load_print_meta: n_embd           = 2048
0.00.050.929 I llm_load_print_meta: n_layer          = 24
0.00.050.932 I llm_load_print_meta: n_head           = 16
0.00.050.933 I llm_load_print_meta: n_head_kv        = 16
0.00.050.933 I llm_load_print_meta: n_rot            = 32
0.00.050.933 I llm_load_print_meta: n_swa            = 0
0.00.050.933 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.933 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.934 I llm_load_print_meta: n_gqa            = 1
0.00.050.937 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.938 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.939 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.940 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.940 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.940 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.942 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.942 I llm_load_print_meta: n_ff             = 8192
0.00.050.943 I llm_load_print_meta: n_expert         = 0
0.00.050.943 I llm_load_print_meta: n_expert_used    = 0
0.00.050.944 I llm_load_print_meta: causal attn      = 1
0.00.050.946 I llm_load_print_meta: pooling type     = 0
0.00.050.946 I llm_load_print_meta: rope type        = 2
0.00.050.946 I llm_load_print_meta: rope scaling     = linear
0.00.050.946 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.947 I llm_load_print_meta: freq_scale_train = 1
0.00.050.947 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.950 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.950 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.950 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.951 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.952 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.953 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.953 I llm_load_print_meta: model type       = 1.4B
0.00.050.953 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.953 I llm_load_print_meta: model params     = 1.41 B
0.00.050.954 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.954 I llm_load_print_meta: general.name     = 1.4B
0.00.050.954 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.954 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.955 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.955 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.955 I llm_load_print_meta: LF token         = 128 ''
0.00.050.957 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.957 I llm_load_print_meta: max token length = 1024
0.00.053.006 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.006 I llm_load_tensors: offloading output layer to GPU
0.00.053.006 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.017 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.018 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.997 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.998 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.998 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.998 I llama_new_context_with_model: n_batch       = 2048
0.00.053.998 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.999 I llama_new_context_with_model: flash_attn    = 0
0.00.053.999 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.999 I llama_new_context_with_model: freq_scale    = 1
0.00.054.000 I ggml_metal_init: allocating
0.00.054.003 I ggml_metal_init: found device: Apple M4
0.00.054.005 I ggml_metal_init: picking default device: Apple M4
0.00.054.613 I ggml_metal_init: using embedded metal library
0.00.056.910 I ggml_metal_init: GPU name:   Apple M4
0.00.056.911 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.912 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.912 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.912 I ggml_metal_init: simdgroup reduction   = true
0.00.056.912 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.912 I ggml_metal_init: has bfloat            = true
0.00.056.913 I ggml_metal_init: use bfloat            = true
0.00.056.913 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.915 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.667 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.860 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.867 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.888 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.037 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.038 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.039 I llama_new_context_with_model: graph nodes  = 967
0.00.087.039 I llama_new_context_with_model: graph splits = 2
0.00.087.054 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.194 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.195 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.262 I main: llama threadpool init, n_threads = 4
0.00.734.295 I 
0.00.734.324 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.734.326 I 
0.00.734.555 I sampler seed: 1234
0.00.734.561 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.734.576 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.734.577 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.734.577 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.524.195 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56754.60 tokens per second)
0.01.524.195 I llama_perf_context_print:        load time =     725.05 ms
0.01.524.196 I llama_perf_context_print: prompt eval time =      43.07 ms /     7 tokens (    6.15 ms per token,   162.51 tokens per second)
0.01.524.197 I llama_perf_context_print:        eval time =     743.51 ms /    63 runs   (   11.80 ms per token,    84.73 tokens per second)
0.01.524.197 I llama_perf_context_print:       total time =     789.94 ms /    70 tokens
0.01.524.356 I ggml_metal_free: deallocating

real	0m1.542s
user	0m0.109s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.853 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.864 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.868 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.870 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.870 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.871 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.871 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.871 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.872 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.872 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.872 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.873 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.873 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.874 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.876 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.877 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.877 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.878 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.786 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.853 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.700 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.701 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.702 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.702 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.702 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.703 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.703 I llama_model_loader: - type  f32:  194 tensors
0.00.024.703 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.703 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.985 I llm_load_vocab: special tokens cache size = 25
0.00.050.876 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.879 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.879 I llm_load_print_meta: arch             = gptneox
0.00.050.880 I llm_load_print_meta: vocab type       = BPE
0.00.050.880 I llm_load_print_meta: n_vocab          = 50304
0.00.050.880 I llm_load_print_meta: n_merges         = 50009
0.00.050.880 I llm_load_print_meta: vocab_only       = 0
0.00.050.881 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.881 I llm_load_print_meta: n_embd           = 2048
0.00.050.881 I llm_load_print_meta: n_layer          = 24
0.00.050.884 I llm_load_print_meta: n_head           = 16
0.00.050.886 I llm_load_print_meta: n_head_kv        = 16
0.00.050.887 I llm_load_print_meta: n_rot            = 32
0.00.050.887 I llm_load_print_meta: n_swa            = 0
0.00.050.887 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.887 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.888 I llm_load_print_meta: n_gqa            = 1
0.00.050.889 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.889 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.890 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.890 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.890 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.890 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.891 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.891 I llm_load_print_meta: n_ff             = 8192
0.00.050.891 I llm_load_print_meta: n_expert         = 0
0.00.050.892 I llm_load_print_meta: n_expert_used    = 0
0.00.050.894 I llm_load_print_meta: causal attn      = 1
0.00.050.894 I llm_load_print_meta: pooling type     = 0
0.00.050.894 I llm_load_print_meta: rope type        = 2
0.00.050.894 I llm_load_print_meta: rope scaling     = linear
0.00.050.895 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.895 I llm_load_print_meta: freq_scale_train = 1
0.00.050.895 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.895 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.896 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.896 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.896 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.898 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.898 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.898 I llm_load_print_meta: model type       = 1.4B
0.00.050.898 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.899 I llm_load_print_meta: model params     = 1.41 B
0.00.050.900 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.901 I llm_load_print_meta: general.name     = 1.4B
0.00.050.901 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.901 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.901 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.901 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.902 I llm_load_print_meta: LF token         = 128 ''
0.00.050.902 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.902 I llm_load_print_meta: max token length = 1024
0.00.052.845 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.845 I llm_load_tensors: offloading output layer to GPU
0.00.052.846 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.856 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.857 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.787 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.788 I llama_new_context_with_model: n_ctx         = 128
0.00.053.788 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.788 I llama_new_context_with_model: n_batch       = 128
0.00.053.788 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.788 I llama_new_context_with_model: flash_attn    = 0
0.00.053.789 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.789 I llama_new_context_with_model: freq_scale    = 1
0.00.053.789 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.790 I ggml_metal_init: allocating
0.00.053.796 I ggml_metal_init: found device: Apple M4
0.00.053.798 I ggml_metal_init: picking default device: Apple M4
0.00.054.346 I ggml_metal_init: using embedded metal library
0.00.056.677 I ggml_metal_init: GPU name:   Apple M4
0.00.056.678 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.679 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.679 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.679 I ggml_metal_init: simdgroup reduction   = true
0.00.056.679 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.679 I ggml_metal_init: has bfloat            = true
0.00.056.680 I ggml_metal_init: use bfloat            = true
0.00.056.680 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.681 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.269 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.732 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.734 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.750 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.690 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.691 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.692 I llama_new_context_with_model: graph nodes  = 967
0.00.068.692 I llama_new_context_with_model: graph splits = 2
0.00.068.704 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.705 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.305 I 
0.00.707.354 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.367 I perplexity: tokenizing the input ..
0.00.714.564 I perplexity: tokenization took 7.195 ms
0.00.714.568 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.848.193 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.849.594 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.849.606 I llama_perf_context_print:        load time =     697.45 ms
0.00.849.607 I llama_perf_context_print: prompt eval time =     133.40 ms /   128 tokens (    1.04 ms per token,   959.54 tokens per second)
0.00.849.608 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.849.608 I llama_perf_context_print:       total time =     142.30 ms /   129 tokens
0.00.849.957 I ggml_metal_free: deallocating

real	0m0.863s
user	0m0.077s
sys	0m0.097s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.640 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.378 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.382 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.384 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.384 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.384 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.385 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.385 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.386 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.386 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.387 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.387 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.387 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.388 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.388 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.392 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.392 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.393 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.251 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.327 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.191 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.192 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.192 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.193 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.193 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.193 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.194 I llama_model_loader: - type  f32:  194 tensors
0.00.024.194 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.194 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.656 I llm_load_vocab: special tokens cache size = 25
0.00.050.637 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.640 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.640 I llm_load_print_meta: arch             = gptneox
0.00.050.641 I llm_load_print_meta: vocab type       = BPE
0.00.050.641 I llm_load_print_meta: n_vocab          = 50304
0.00.050.641 I llm_load_print_meta: n_merges         = 50009
0.00.050.641 I llm_load_print_meta: vocab_only       = 0
0.00.050.641 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.642 I llm_load_print_meta: n_embd           = 2048
0.00.050.642 I llm_load_print_meta: n_layer          = 24
0.00.050.644 I llm_load_print_meta: n_head           = 16
0.00.050.645 I llm_load_print_meta: n_head_kv        = 16
0.00.050.645 I llm_load_print_meta: n_rot            = 32
0.00.050.646 I llm_load_print_meta: n_swa            = 0
0.00.050.646 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.647 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.648 I llm_load_print_meta: n_gqa            = 1
0.00.050.649 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.649 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.650 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.650 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.650 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.651 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.651 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.651 I llm_load_print_meta: n_ff             = 8192
0.00.050.652 I llm_load_print_meta: n_expert         = 0
0.00.050.652 I llm_load_print_meta: n_expert_used    = 0
0.00.050.654 I llm_load_print_meta: causal attn      = 1
0.00.050.655 I llm_load_print_meta: pooling type     = 0
0.00.050.655 I llm_load_print_meta: rope type        = 2
0.00.050.655 I llm_load_print_meta: rope scaling     = linear
0.00.050.656 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.656 I llm_load_print_meta: freq_scale_train = 1
0.00.050.656 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.656 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.657 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.657 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.657 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.657 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.657 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.657 I llm_load_print_meta: model type       = 1.4B
0.00.050.658 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.658 I llm_load_print_meta: model params     = 1.41 B
0.00.050.663 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.663 I llm_load_print_meta: general.name     = 1.4B
0.00.050.663 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.663 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.663 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.664 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.664 I llm_load_print_meta: LF token         = 128 ''
0.00.050.664 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.664 I llm_load_print_meta: max token length = 1024
0.00.052.690 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.690 I llm_load_tensors: offloading output layer to GPU
0.00.052.691 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.701 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.702 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.624 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.625 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.625 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.625 I llama_new_context_with_model: n_batch       = 2048
0.00.053.625 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.626 I llama_new_context_with_model: flash_attn    = 0
0.00.053.626 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.626 I llama_new_context_with_model: freq_scale    = 1
0.00.053.627 I ggml_metal_init: allocating
0.00.053.635 I ggml_metal_init: found device: Apple M4
0.00.053.637 I ggml_metal_init: picking default device: Apple M4
0.00.054.237 I ggml_metal_init: using embedded metal library
0.00.056.564 I ggml_metal_init: GPU name:   Apple M4
0.00.056.566 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.566 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.566 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.567 I ggml_metal_init: simdgroup reduction   = true
0.00.056.568 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.568 I ggml_metal_init: has bfloat            = true
0.00.056.568 I ggml_metal_init: use bfloat            = true
0.00.056.569 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.569 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.458 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.246 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.255 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.274 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.369 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.370 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.370 I llama_new_context_with_model: graph nodes  = 967
0.00.087.370 I llama_new_context_with_model: graph splits = 2
0.00.087.386 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.521 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.521 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.740.572 I main: llama threadpool init, n_threads = 4
0.00.740.627 I 
0.00.740.656 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.740.657 I 
0.00.740.913 I sampler seed: 1234
0.00.740.918 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.740.962 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.740.965 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.740.966 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.580.458 I llama_perf_sampler_print:    sampling time =       1.54 ms /    71 runs   (    0.02 ms per token, 45984.46 tokens per second)
0.01.580.459 I llama_perf_context_print:        load time =     731.93 ms
0.01.580.460 I llama_perf_context_print: prompt eval time =      42.15 ms /     7 tokens (    6.02 ms per token,   166.09 tokens per second)
0.01.580.463 I llama_perf_context_print:        eval time =     794.80 ms /    63 runs   (   12.62 ms per token,    79.27 tokens per second)
0.01.580.465 I llama_perf_context_print:       total time =     839.89 ms /    70 tokens
0.01.580.671 I ggml_metal_free: deallocating

real	0m1.599s
user	0m0.110s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.117 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.102 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.086 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.090 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.092 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.092 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.093 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.093 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.093 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.094 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.097 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.097 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.097 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.098 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.098 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.098 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.100 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.100 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.100 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.033 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.143 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.042 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.043 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.043 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.044 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.044 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.044 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.045 I llama_model_loader: - type  f32:  194 tensors
0.00.024.045 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.046 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.817 I llm_load_vocab: special tokens cache size = 25
0.00.051.784 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.789 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.789 I llm_load_print_meta: arch             = gptneox
0.00.051.789 I llm_load_print_meta: vocab type       = BPE
0.00.051.789 I llm_load_print_meta: n_vocab          = 50304
0.00.051.790 I llm_load_print_meta: n_merges         = 50009
0.00.051.790 I llm_load_print_meta: vocab_only       = 0
0.00.051.790 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.790 I llm_load_print_meta: n_embd           = 2048
0.00.051.790 I llm_load_print_meta: n_layer          = 24
0.00.051.795 I llm_load_print_meta: n_head           = 16
0.00.051.795 I llm_load_print_meta: n_head_kv        = 16
0.00.051.795 I llm_load_print_meta: n_rot            = 32
0.00.051.798 I llm_load_print_meta: n_swa            = 0
0.00.051.798 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.798 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.799 I llm_load_print_meta: n_gqa            = 1
0.00.051.800 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.800 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.801 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.801 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.805 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.805 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.805 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.806 I llm_load_print_meta: n_ff             = 8192
0.00.051.806 I llm_load_print_meta: n_expert         = 0
0.00.051.807 I llm_load_print_meta: n_expert_used    = 0
0.00.051.807 I llm_load_print_meta: causal attn      = 1
0.00.051.807 I llm_load_print_meta: pooling type     = 0
0.00.051.807 I llm_load_print_meta: rope type        = 2
0.00.051.807 I llm_load_print_meta: rope scaling     = linear
0.00.051.807 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.808 I llm_load_print_meta: freq_scale_train = 1
0.00.051.808 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.808 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.808 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.808 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.808 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.809 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.809 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.809 I llm_load_print_meta: model type       = 1.4B
0.00.051.809 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.810 I llm_load_print_meta: model params     = 1.41 B
0.00.051.810 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.810 I llm_load_print_meta: general.name     = 1.4B
0.00.051.811 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.811 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.811 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.811 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.811 I llm_load_print_meta: LF token         = 128 ''
0.00.051.811 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.812 I llm_load_print_meta: max token length = 1024
0.00.053.584 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.585 I llm_load_tensors: offloading output layer to GPU
0.00.053.585 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.590 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.591 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.516 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.517 I llama_new_context_with_model: n_ctx         = 128
0.00.054.517 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.517 I llama_new_context_with_model: n_batch       = 128
0.00.054.517 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.517 I llama_new_context_with_model: flash_attn    = 0
0.00.054.518 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.518 I llama_new_context_with_model: freq_scale    = 1
0.00.054.519 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.519 I ggml_metal_init: allocating
0.00.054.523 I ggml_metal_init: found device: Apple M4
0.00.054.525 I ggml_metal_init: picking default device: Apple M4
0.00.055.164 I ggml_metal_init: using embedded metal library
0.00.057.781 I ggml_metal_init: GPU name:   Apple M4
0.00.057.783 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.783 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.784 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.784 I ggml_metal_init: simdgroup reduction   = true
0.00.057.785 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.785 I ggml_metal_init: has bfloat            = true
0.00.057.785 I ggml_metal_init: use bfloat            = true
0.00.057.786 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.788 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.221 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.570 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.572 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.587 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.591 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.593 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.593 I llama_new_context_with_model: graph nodes  = 967
0.00.069.593 I llama_new_context_with_model: graph splits = 2
0.00.069.600 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.601 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.753 I 
0.00.658.786 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.797 I perplexity: tokenizing the input ..
0.00.666.596 I perplexity: tokenization took 7.798 ms
0.00.666.599 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.668 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.801.996 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.802.016 I llama_perf_context_print:        load time =     649.64 ms
0.00.802.017 I llama_perf_context_print: prompt eval time =     133.84 ms /   128 tokens (    1.05 ms per token,   956.38 tokens per second)
0.00.802.017 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.018 I llama_perf_context_print:       total time =     143.26 ms /   129 tokens
0.00.802.305 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.079s
sys	0m0.112s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.012.722 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.820 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.021.833 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.837 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.838 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.839 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.839 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.840 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.842 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.843 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.844 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.845 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.845 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.846 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.847 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.850 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.850 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.851 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.673 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.768 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.820 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.821 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.821 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.822 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.822 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.822 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.030.823 I llama_model_loader: - type  f32:  194 tensors
0.00.030.823 I llama_model_loader: - type q2_K:   49 tensors
0.00.030.823 I llama_model_loader: - type q3_K:   48 tensors
0.00.030.823 I llama_model_loader: - type q6_K:    1 tensors
0.00.054.371 I llm_load_vocab: special tokens cache size = 25
0.00.060.737 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.740 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.741 I llm_load_print_meta: arch             = gptneox
0.00.060.741 I llm_load_print_meta: vocab type       = BPE
0.00.060.741 I llm_load_print_meta: n_vocab          = 50304
0.00.060.741 I llm_load_print_meta: n_merges         = 50009
0.00.060.742 I llm_load_print_meta: vocab_only       = 0
0.00.060.742 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.742 I llm_load_print_meta: n_embd           = 2048
0.00.060.742 I llm_load_print_meta: n_layer          = 24
0.00.060.746 I llm_load_print_meta: n_head           = 16
0.00.060.747 I llm_load_print_meta: n_head_kv        = 16
0.00.060.749 I llm_load_print_meta: n_rot            = 32
0.00.060.749 I llm_load_print_meta: n_swa            = 0
0.00.060.749 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.750 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.750 I llm_load_print_meta: n_gqa            = 1
0.00.060.752 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.753 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.754 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.754 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.754 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.754 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.754 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.755 I llm_load_print_meta: n_ff             = 8192
0.00.060.755 I llm_load_print_meta: n_expert         = 0
0.00.060.755 I llm_load_print_meta: n_expert_used    = 0
0.00.060.756 I llm_load_print_meta: causal attn      = 1
0.00.060.757 I llm_load_print_meta: pooling type     = 0
0.00.060.757 I llm_load_print_meta: rope type        = 2
0.00.060.757 I llm_load_print_meta: rope scaling     = linear
0.00.060.757 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.758 I llm_load_print_meta: freq_scale_train = 1
0.00.060.759 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.760 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.760 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.760 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.760 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.760 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.760 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.760 I llm_load_print_meta: model type       = 1.4B
0.00.060.761 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.060.761 I llm_load_print_meta: model params     = 1.41 B
0.00.060.762 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.060.762 I llm_load_print_meta: general.name     = 1.4B
0.00.060.762 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.762 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.762 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.763 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.763 I llm_load_print_meta: LF token         = 128 ''
0.00.060.763 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.763 I llm_load_print_meta: max token length = 1024
0.00.062.774 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.775 I llm_load_tensors: offloading output layer to GPU
0.00.062.775 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.785 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.062.786 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.063.692 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.692 I llama_new_context_with_model: n_ctx         = 2048
0.00.063.693 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.063.693 I llama_new_context_with_model: n_batch       = 2048
0.00.063.693 I llama_new_context_with_model: n_ubatch      = 512
0.00.063.693 I llama_new_context_with_model: flash_attn    = 0
0.00.063.694 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.694 I llama_new_context_with_model: freq_scale    = 1
0.00.063.695 I ggml_metal_init: allocating
0.00.063.699 I ggml_metal_init: found device: Apple M4
0.00.063.701 I ggml_metal_init: picking default device: Apple M4
0.00.064.293 I ggml_metal_init: using embedded metal library
0.00.066.763 I ggml_metal_init: GPU name:   Apple M4
0.00.066.765 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.765 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.765 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.766 I ggml_metal_init: simdgroup reduction   = true
0.00.066.766 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.766 I ggml_metal_init: has bfloat            = true
0.00.066.766 I ggml_metal_init: use bfloat            = true
0.00.066.767 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.767 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.861 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.099.205 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.099.210 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.099.232 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.100.360 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.100.361 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.100.362 I llama_new_context_with_model: graph nodes  = 967
0.00.100.362 I llama_new_context_with_model: graph splits = 2
0.00.100.377 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.100.523 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.524 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.461.218 I main: llama threadpool init, n_threads = 4
0.00.461.265 I 
0.00.461.300 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.461.300 I 
0.00.461.524 I sampler seed: 1234
0.00.461.529 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.461.545 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.461.545 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.461.545 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.142.958 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62226.12 tokens per second)
0.01.142.958 I llama_perf_context_print:        load time =     448.49 ms
0.01.142.959 I llama_perf_context_print: prompt eval time =      35.83 ms /     7 tokens (    5.12 ms per token,   195.35 tokens per second)
0.01.142.960 I llama_perf_context_print:        eval time =     642.68 ms /    63 runs   (   10.20 ms per token,    98.03 tokens per second)
0.01.142.960 I llama_perf_context_print:       total time =     681.74 ms /    70 tokens
0.01.143.203 I ggml_metal_free: deallocating

real	0m1.163s
user	0m0.114s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.291 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.775 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.780 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.781 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.782 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.782 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.782 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.783 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.784 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.784 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.784 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.784 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.785 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.785 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.785 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.787 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.787 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.787 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.673 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.739 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.636 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.637 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.638 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.638 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.638 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.639 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.639 I llama_model_loader: - type  f32:  194 tensors
0.00.026.640 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.640 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.640 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.117 I llm_load_vocab: special tokens cache size = 25
0.00.054.068 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.072 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.072 I llm_load_print_meta: arch             = gptneox
0.00.054.073 I llm_load_print_meta: vocab type       = BPE
0.00.054.073 I llm_load_print_meta: n_vocab          = 50304
0.00.054.073 I llm_load_print_meta: n_merges         = 50009
0.00.054.074 I llm_load_print_meta: vocab_only       = 0
0.00.054.074 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.074 I llm_load_print_meta: n_embd           = 2048
0.00.054.074 I llm_load_print_meta: n_layer          = 24
0.00.054.078 I llm_load_print_meta: n_head           = 16
0.00.054.079 I llm_load_print_meta: n_head_kv        = 16
0.00.054.079 I llm_load_print_meta: n_rot            = 32
0.00.054.079 I llm_load_print_meta: n_swa            = 0
0.00.054.080 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.080 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.081 I llm_load_print_meta: n_gqa            = 1
0.00.054.081 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.082 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.083 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.083 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.083 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.084 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.084 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.084 I llm_load_print_meta: n_ff             = 8192
0.00.054.085 I llm_load_print_meta: n_expert         = 0
0.00.054.085 I llm_load_print_meta: n_expert_used    = 0
0.00.054.085 I llm_load_print_meta: causal attn      = 1
0.00.054.085 I llm_load_print_meta: pooling type     = 0
0.00.054.085 I llm_load_print_meta: rope type        = 2
0.00.054.088 I llm_load_print_meta: rope scaling     = linear
0.00.054.089 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.089 I llm_load_print_meta: freq_scale_train = 1
0.00.054.089 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.089 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.089 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.090 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.090 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.090 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.090 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.090 I llm_load_print_meta: model type       = 1.4B
0.00.054.091 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.054.091 I llm_load_print_meta: model params     = 1.41 B
0.00.054.091 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.054.092 I llm_load_print_meta: general.name     = 1.4B
0.00.054.092 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.092 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.092 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.092 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.093 I llm_load_print_meta: LF token         = 128 ''
0.00.054.093 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.093 I llm_load_print_meta: max token length = 1024
0.00.055.775 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.776 I llm_load_tensors: offloading output layer to GPU
0.00.055.777 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.787 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.055.787 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.056.608 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.609 I llama_new_context_with_model: n_ctx         = 128
0.00.056.609 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.609 I llama_new_context_with_model: n_batch       = 128
0.00.056.610 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.610 I llama_new_context_with_model: flash_attn    = 0
0.00.056.610 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.611 I llama_new_context_with_model: freq_scale    = 1
0.00.056.611 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.611 I ggml_metal_init: allocating
0.00.056.614 I ggml_metal_init: found device: Apple M4
0.00.056.617 I ggml_metal_init: picking default device: Apple M4
0.00.057.183 I ggml_metal_init: using embedded metal library
0.00.059.537 I ggml_metal_init: GPU name:   Apple M4
0.00.059.539 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.539 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.540 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.540 I ggml_metal_init: simdgroup reduction   = true
0.00.059.540 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.540 I ggml_metal_init: has bfloat            = true
0.00.059.540 I ggml_metal_init: use bfloat            = true
0.00.059.541 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.542 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.756 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.071.076 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.071.080 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.071.095 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.955 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.957 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.957 I llama_new_context_with_model: graph nodes  = 967
0.00.071.957 I llama_new_context_with_model: graph splits = 2
0.00.071.970 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.971 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.386.201 I 
0.00.386.235 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.386.246 I perplexity: tokenizing the input ..
0.00.394.345 I perplexity: tokenization took 8.098 ms
0.00.394.348 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.526.969 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.528.142 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.528.161 I llama_perf_context_print:        load time =     376.91 ms
0.00.528.162 I llama_perf_context_print: prompt eval time =     132.39 ms /   128 tokens (    1.03 ms per token,   966.87 tokens per second)
0.00.528.163 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.528.164 I llama_perf_context_print:       total time =     141.96 ms /   129 tokens
0.00.528.649 I ggml_metal_free: deallocating

real	0m0.544s
user	0m0.083s
sys	0m0.072s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.184 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.089 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.024.094 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.096 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.096 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.097 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.097 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.097 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.098 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.098 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.099 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.099 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.099 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.100 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.100 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.103 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.103 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.104 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.112 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.168 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.444 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.445 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.446 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.446 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.446 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.446 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.033.447 I llama_model_loader: - type  f32:  194 tensors
0.00.033.447 I llama_model_loader: - type q3_K:   25 tensors
0.00.033.447 I llama_model_loader: - type q4_K:   71 tensors
0.00.033.448 I llama_model_loader: - type q5_K:    1 tensors
0.00.033.448 I llama_model_loader: - type q6_K:    1 tensors
0.00.054.253 I llm_load_vocab: special tokens cache size = 25
0.00.060.058 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.061 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.061 I llm_load_print_meta: arch             = gptneox
0.00.060.062 I llm_load_print_meta: vocab type       = BPE
0.00.060.062 I llm_load_print_meta: n_vocab          = 50304
0.00.060.062 I llm_load_print_meta: n_merges         = 50009
0.00.060.062 I llm_load_print_meta: vocab_only       = 0
0.00.060.062 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.062 I llm_load_print_meta: n_embd           = 2048
0.00.060.063 I llm_load_print_meta: n_layer          = 24
0.00.060.066 I llm_load_print_meta: n_head           = 16
0.00.060.066 I llm_load_print_meta: n_head_kv        = 16
0.00.060.067 I llm_load_print_meta: n_rot            = 32
0.00.060.067 I llm_load_print_meta: n_swa            = 0
0.00.060.067 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.068 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.069 I llm_load_print_meta: n_gqa            = 1
0.00.060.070 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.070 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.071 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.071 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.072 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.072 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.072 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.073 I llm_load_print_meta: n_ff             = 8192
0.00.060.073 I llm_load_print_meta: n_expert         = 0
0.00.060.073 I llm_load_print_meta: n_expert_used    = 0
0.00.060.073 I llm_load_print_meta: causal attn      = 1
0.00.060.073 I llm_load_print_meta: pooling type     = 0
0.00.060.073 I llm_load_print_meta: rope type        = 2
0.00.060.074 I llm_load_print_meta: rope scaling     = linear
0.00.060.074 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.074 I llm_load_print_meta: freq_scale_train = 1
0.00.060.074 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.075 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.075 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.075 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.075 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.077 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.077 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.077 I llm_load_print_meta: model type       = 1.4B
0.00.060.078 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.060.078 I llm_load_print_meta: model params     = 1.41 B
0.00.060.079 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.060.079 I llm_load_print_meta: general.name     = 1.4B
0.00.060.079 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.079 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.079 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.080 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.084 I llm_load_print_meta: LF token         = 128 ''
0.00.060.084 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.084 I llm_load_print_meta: max token length = 1024
0.00.061.993 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.994 I llm_load_tensors: offloading output layer to GPU
0.00.061.994 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.004 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.062.005 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.062.875 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.876 I llama_new_context_with_model: n_ctx         = 2048
0.00.062.876 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.062.877 I llama_new_context_with_model: n_batch       = 2048
0.00.062.877 I llama_new_context_with_model: n_ubatch      = 512
0.00.062.877 I llama_new_context_with_model: flash_attn    = 0
0.00.062.877 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.878 I llama_new_context_with_model: freq_scale    = 1
0.00.062.878 I ggml_metal_init: allocating
0.00.062.881 I ggml_metal_init: found device: Apple M4
0.00.062.885 I ggml_metal_init: picking default device: Apple M4
0.00.063.471 I ggml_metal_init: using embedded metal library
0.00.065.778 I ggml_metal_init: GPU name:   Apple M4
0.00.065.780 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.780 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.781 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.781 I ggml_metal_init: simdgroup reduction   = true
0.00.065.781 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.781 I ggml_metal_init: has bfloat            = true
0.00.065.781 I ggml_metal_init: use bfloat            = true
0.00.065.782 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.782 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.571 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.096.218 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.224 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.241 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.097.301 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.097.302 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.097.302 I llama_new_context_with_model: graph nodes  = 967
0.00.097.303 I llama_new_context_with_model: graph splits = 2
0.00.097.318 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.097.446 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.447 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.648.031 I main: llama threadpool init, n_threads = 4
0.00.648.077 I 
0.00.648.116 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.648.118 I 
0.00.648.356 I sampler seed: 1234
0.00.648.361 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.648.377 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.648.379 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.648.379 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.390.184 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54364.47 tokens per second)
0.01.390.184 I llama_perf_context_print:        load time =     638.84 ms
0.01.390.185 I llama_perf_context_print: prompt eval time =      40.56 ms /     7 tokens (    5.79 ms per token,   172.57 tokens per second)
0.01.390.186 I llama_perf_context_print:        eval time =     698.05 ms /    63 runs   (   11.08 ms per token,    90.25 tokens per second)
0.01.390.186 I llama_perf_context_print:       total time =     742.16 ms /    70 tokens
0.01.390.365 I ggml_metal_free: deallocating

real	0m1.408s
user	0m0.110s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.776 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.634 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.639 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.641 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.641 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.641 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.642 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.642 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.643 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.643 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.644 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.644 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.644 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.645 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.645 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.647 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.647 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.647 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.616 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.721 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.636 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.637 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.637 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.638 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.638 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.638 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.639 I llama_model_loader: - type  f32:  194 tensors
0.00.023.639 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.639 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.640 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.640 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.590 I llm_load_vocab: special tokens cache size = 25
0.00.050.489 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.492 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.492 I llm_load_print_meta: arch             = gptneox
0.00.050.492 I llm_load_print_meta: vocab type       = BPE
0.00.050.492 I llm_load_print_meta: n_vocab          = 50304
0.00.050.493 I llm_load_print_meta: n_merges         = 50009
0.00.050.493 I llm_load_print_meta: vocab_only       = 0
0.00.050.493 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.493 I llm_load_print_meta: n_embd           = 2048
0.00.050.493 I llm_load_print_meta: n_layer          = 24
0.00.050.496 I llm_load_print_meta: n_head           = 16
0.00.050.497 I llm_load_print_meta: n_head_kv        = 16
0.00.050.497 I llm_load_print_meta: n_rot            = 32
0.00.050.497 I llm_load_print_meta: n_swa            = 0
0.00.050.497 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.497 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.498 I llm_load_print_meta: n_gqa            = 1
0.00.050.499 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.499 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.500 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.500 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.500 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.501 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.501 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.501 I llm_load_print_meta: n_ff             = 8192
0.00.050.502 I llm_load_print_meta: n_expert         = 0
0.00.050.502 I llm_load_print_meta: n_expert_used    = 0
0.00.050.502 I llm_load_print_meta: causal attn      = 1
0.00.050.502 I llm_load_print_meta: pooling type     = 0
0.00.050.502 I llm_load_print_meta: rope type        = 2
0.00.050.503 I llm_load_print_meta: rope scaling     = linear
0.00.050.503 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.503 I llm_load_print_meta: freq_scale_train = 1
0.00.050.503 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.505 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.505 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.505 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.506 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.506 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.506 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.506 I llm_load_print_meta: model type       = 1.4B
0.00.050.506 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.507 I llm_load_print_meta: model params     = 1.41 B
0.00.050.507 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.507 I llm_load_print_meta: general.name     = 1.4B
0.00.050.508 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.510 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.510 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.510 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.510 I llm_load_print_meta: LF token         = 128 ''
0.00.050.511 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.511 I llm_load_print_meta: max token length = 1024
0.00.052.458 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.459 I llm_load_tensors: offloading output layer to GPU
0.00.052.459 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.469 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.470 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.376 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.377 I llama_new_context_with_model: n_ctx         = 128
0.00.053.377 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.377 I llama_new_context_with_model: n_batch       = 128
0.00.053.378 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.378 I llama_new_context_with_model: flash_attn    = 0
0.00.053.378 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.378 I llama_new_context_with_model: freq_scale    = 1
0.00.053.379 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.379 I ggml_metal_init: allocating
0.00.053.382 I ggml_metal_init: found device: Apple M4
0.00.053.384 I ggml_metal_init: picking default device: Apple M4
0.00.053.962 I ggml_metal_init: using embedded metal library
0.00.056.292 I ggml_metal_init: GPU name:   Apple M4
0.00.056.293 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.293 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.294 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.294 I ggml_metal_init: simdgroup reduction   = true
0.00.056.294 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.294 I ggml_metal_init: has bfloat            = true
0.00.056.295 I ggml_metal_init: use bfloat            = true
0.00.056.295 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.296 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.097 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.403 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.407 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.422 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.311 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.312 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.312 I llama_new_context_with_model: graph nodes  = 967
0.00.068.312 I llama_new_context_with_model: graph splits = 2
0.00.068.324 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.325 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.463.128 I 
0.00.463.166 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.463.177 I perplexity: tokenizing the input ..
0.00.471.088 I perplexity: tokenization took 7.909 ms
0.00.471.091 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.603.506 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.604.770 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.604.788 I llama_perf_context_print:        load time =     454.35 ms
0.00.604.789 I llama_perf_context_print: prompt eval time =     132.19 ms /   128 tokens (    1.03 ms per token,   968.33 tokens per second)
0.00.604.790 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.604.791 I llama_perf_context_print:       total time =     141.66 ms /   129 tokens
0.00.605.293 I ggml_metal_free: deallocating

real	0m0.618s
user	0m0.079s
sys	0m0.082s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.010.988 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.459 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.464 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.466 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.466 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.467 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.467 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.468 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.468 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.469 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.469 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.469 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.470 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.470 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.472 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.475 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.476 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.476 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.382 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.511 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.274 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.275 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.275 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.276 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.276 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.276 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.277 I llama_model_loader: - type  f32:  194 tensors
0.00.026.277 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.277 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.277 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.866 I llm_load_vocab: special tokens cache size = 25
0.00.052.940 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.943 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.943 I llm_load_print_meta: arch             = gptneox
0.00.052.943 I llm_load_print_meta: vocab type       = BPE
0.00.052.944 I llm_load_print_meta: n_vocab          = 50304
0.00.052.944 I llm_load_print_meta: n_merges         = 50009
0.00.052.944 I llm_load_print_meta: vocab_only       = 0
0.00.052.944 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.944 I llm_load_print_meta: n_embd           = 2048
0.00.052.945 I llm_load_print_meta: n_layer          = 24
0.00.052.947 I llm_load_print_meta: n_head           = 16
0.00.052.950 I llm_load_print_meta: n_head_kv        = 16
0.00.052.950 I llm_load_print_meta: n_rot            = 32
0.00.052.950 I llm_load_print_meta: n_swa            = 0
0.00.052.950 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.951 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.951 I llm_load_print_meta: n_gqa            = 1
0.00.052.952 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.953 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.953 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.954 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.954 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.954 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.956 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.957 I llm_load_print_meta: n_ff             = 8192
0.00.052.957 I llm_load_print_meta: n_expert         = 0
0.00.052.958 I llm_load_print_meta: n_expert_used    = 0
0.00.052.959 I llm_load_print_meta: causal attn      = 1
0.00.052.959 I llm_load_print_meta: pooling type     = 0
0.00.052.959 I llm_load_print_meta: rope type        = 2
0.00.052.959 I llm_load_print_meta: rope scaling     = linear
0.00.052.960 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.960 I llm_load_print_meta: freq_scale_train = 1
0.00.052.960 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.961 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.961 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.965 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.965 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.965 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.965 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.965 I llm_load_print_meta: model type       = 1.4B
0.00.052.966 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.966 I llm_load_print_meta: model params     = 1.41 B
0.00.052.967 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.967 I llm_load_print_meta: general.name     = 1.4B
0.00.052.968 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.968 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.968 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.968 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.969 I llm_load_print_meta: LF token         = 128 ''
0.00.052.969 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.969 I llm_load_print_meta: max token length = 1024
0.00.054.967 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.968 I llm_load_tensors: offloading output layer to GPU
0.00.054.968 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.978 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.979 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.055.909 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.910 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.910 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.910 I llama_new_context_with_model: n_batch       = 2048
0.00.055.911 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.911 I llama_new_context_with_model: flash_attn    = 0
0.00.055.911 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.911 I llama_new_context_with_model: freq_scale    = 1
0.00.055.912 I ggml_metal_init: allocating
0.00.055.917 I ggml_metal_init: found device: Apple M4
0.00.055.919 I ggml_metal_init: picking default device: Apple M4
0.00.056.535 I ggml_metal_init: using embedded metal library
0.00.058.846 I ggml_metal_init: GPU name:   Apple M4
0.00.058.848 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.848 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.849 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.850 I ggml_metal_init: simdgroup reduction   = true
0.00.058.851 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.851 I ggml_metal_init: has bfloat            = true
0.00.058.851 I ggml_metal_init: use bfloat            = true
0.00.058.851 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.852 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.651 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.088.425 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.430 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.453 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.491 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.492 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.492 I llama_new_context_with_model: graph nodes  = 967
0.00.089.492 I llama_new_context_with_model: graph splits = 2
0.00.089.507 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.655 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.656 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.613.504 I main: llama threadpool init, n_threads = 4
0.00.613.539 I 
0.00.613.569 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.613.570 I 
0.00.613.794 I sampler seed: 1234
0.00.613.798 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.613.832 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.613.835 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.613.835 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.378.595 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57304.28 tokens per second)
0.01.378.596 I llama_perf_context_print:        load time =     602.51 ms
0.01.378.597 I llama_perf_context_print: prompt eval time =      50.45 ms /     7 tokens (    7.21 ms per token,   138.75 tokens per second)
0.01.378.598 I llama_perf_context_print:        eval time =     711.31 ms /    63 runs   (   11.29 ms per token,    88.57 tokens per second)
0.01.378.598 I llama_perf_context_print:       total time =     765.09 ms /    70 tokens
0.01.378.780 I ggml_metal_free: deallocating

real	0m1.396s
user	0m0.110s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.823 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.699 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.704 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.706 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.706 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.707 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.707 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.707 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.708 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.709 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.709 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.709 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.710 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.710 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.710 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.716 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.716 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.716 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.548 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.633 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.694 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.696 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.696 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.696 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.697 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.697 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.697 I llama_model_loader: - type  f32:  194 tensors
0.00.023.698 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.698 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.698 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.978 I llm_load_vocab: special tokens cache size = 25
0.00.050.908 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.911 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.911 I llm_load_print_meta: arch             = gptneox
0.00.050.911 I llm_load_print_meta: vocab type       = BPE
0.00.050.912 I llm_load_print_meta: n_vocab          = 50304
0.00.050.912 I llm_load_print_meta: n_merges         = 50009
0.00.050.912 I llm_load_print_meta: vocab_only       = 0
0.00.050.912 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.912 I llm_load_print_meta: n_embd           = 2048
0.00.050.913 I llm_load_print_meta: n_layer          = 24
0.00.050.916 I llm_load_print_meta: n_head           = 16
0.00.050.916 I llm_load_print_meta: n_head_kv        = 16
0.00.050.917 I llm_load_print_meta: n_rot            = 32
0.00.050.917 I llm_load_print_meta: n_swa            = 0
0.00.050.917 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.917 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.918 I llm_load_print_meta: n_gqa            = 1
0.00.050.919 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.920 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.920 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.921 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.921 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.921 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.924 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.924 I llm_load_print_meta: n_ff             = 8192
0.00.050.925 I llm_load_print_meta: n_expert         = 0
0.00.050.925 I llm_load_print_meta: n_expert_used    = 0
0.00.050.925 I llm_load_print_meta: causal attn      = 1
0.00.050.925 I llm_load_print_meta: pooling type     = 0
0.00.050.925 I llm_load_print_meta: rope type        = 2
0.00.050.927 I llm_load_print_meta: rope scaling     = linear
0.00.050.927 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.928 I llm_load_print_meta: freq_scale_train = 1
0.00.050.928 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.928 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.928 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.928 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.929 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.929 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.930 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.930 I llm_load_print_meta: model type       = 1.4B
0.00.050.931 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.931 I llm_load_print_meta: model params     = 1.41 B
0.00.050.932 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.932 I llm_load_print_meta: general.name     = 1.4B
0.00.050.934 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.934 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.934 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.934 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.934 I llm_load_print_meta: LF token         = 128 ''
0.00.050.935 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.935 I llm_load_print_meta: max token length = 1024
0.00.053.028 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.028 I llm_load_tensors: offloading output layer to GPU
0.00.053.028 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.039 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.040 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.000 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.001 I llama_new_context_with_model: n_ctx         = 128
0.00.054.001 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.001 I llama_new_context_with_model: n_batch       = 128
0.00.054.001 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.001 I llama_new_context_with_model: flash_attn    = 0
0.00.054.002 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.002 I llama_new_context_with_model: freq_scale    = 1
0.00.054.002 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.003 I ggml_metal_init: allocating
0.00.054.006 I ggml_metal_init: found device: Apple M4
0.00.054.009 I ggml_metal_init: picking default device: Apple M4
0.00.054.593 I ggml_metal_init: using embedded metal library
0.00.056.963 I ggml_metal_init: GPU name:   Apple M4
0.00.056.964 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.964 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.965 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.965 I ggml_metal_init: simdgroup reduction   = true
0.00.056.965 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.965 I ggml_metal_init: has bfloat            = true
0.00.056.967 I ggml_metal_init: use bfloat            = true
0.00.056.967 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.968 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.953 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.279 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.281 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.295 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.219 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.220 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.220 I llama_new_context_with_model: graph nodes  = 967
0.00.069.221 I llama_new_context_with_model: graph splits = 2
0.00.069.233 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.234 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.603.905 I 
0.00.603.937 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.603.948 I perplexity: tokenizing the input ..
0.00.611.958 I perplexity: tokenization took 8.009 ms
0.00.611.961 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.746.146 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.747.323 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.747.340 I llama_perf_context_print:        load time =     595.08 ms
0.00.747.341 I llama_perf_context_print: prompt eval time =     133.95 ms /   128 tokens (    1.05 ms per token,   955.59 tokens per second)
0.00.747.342 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.747.342 I llama_perf_context_print:       total time =     143.44 ms /   129 tokens
0.00.747.843 I ggml_metal_free: deallocating

real	0m0.761s
user	0m0.079s
sys	0m0.101s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.709 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.996 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.001 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.002 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.007 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.008 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.008 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.008 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.010 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.010 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.010 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.011 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.011 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.011 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.012 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.015 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.015 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.015 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.990 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.057 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.974 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.975 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.975 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.976 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.976 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.976 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.977 I llama_model_loader: - type  f32:  194 tensors
0.00.024.977 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.978 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.343 I llm_load_vocab: special tokens cache size = 25
0.00.052.221 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.223 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.224 I llm_load_print_meta: arch             = gptneox
0.00.052.224 I llm_load_print_meta: vocab type       = BPE
0.00.052.224 I llm_load_print_meta: n_vocab          = 50304
0.00.052.224 I llm_load_print_meta: n_merges         = 50009
0.00.052.225 I llm_load_print_meta: vocab_only       = 0
0.00.052.225 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.225 I llm_load_print_meta: n_embd           = 2048
0.00.052.225 I llm_load_print_meta: n_layer          = 24
0.00.052.228 I llm_load_print_meta: n_head           = 16
0.00.052.228 I llm_load_print_meta: n_head_kv        = 16
0.00.052.229 I llm_load_print_meta: n_rot            = 32
0.00.052.229 I llm_load_print_meta: n_swa            = 0
0.00.052.229 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.229 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.230 I llm_load_print_meta: n_gqa            = 1
0.00.052.231 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.231 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.232 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.232 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.232 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.233 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.233 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.233 I llm_load_print_meta: n_ff             = 8192
0.00.052.234 I llm_load_print_meta: n_expert         = 0
0.00.052.234 I llm_load_print_meta: n_expert_used    = 0
0.00.052.234 I llm_load_print_meta: causal attn      = 1
0.00.052.234 I llm_load_print_meta: pooling type     = 0
0.00.052.234 I llm_load_print_meta: rope type        = 2
0.00.052.237 I llm_load_print_meta: rope scaling     = linear
0.00.052.237 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.237 I llm_load_print_meta: freq_scale_train = 1
0.00.052.238 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.238 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.238 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.238 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.238 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.238 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.239 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.239 I llm_load_print_meta: model type       = 1.4B
0.00.052.239 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.240 I llm_load_print_meta: model params     = 1.41 B
0.00.052.240 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.240 I llm_load_print_meta: general.name     = 1.4B
0.00.052.240 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.241 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.241 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.241 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.241 I llm_load_print_meta: LF token         = 128 ''
0.00.052.242 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.242 I llm_load_print_meta: max token length = 1024
0.00.054.340 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.340 I llm_load_tensors: offloading output layer to GPU
0.00.054.341 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.351 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.352 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.238 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.239 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.239 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.239 I llama_new_context_with_model: n_batch       = 2048
0.00.055.239 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.239 I llama_new_context_with_model: flash_attn    = 0
0.00.055.240 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.240 I llama_new_context_with_model: freq_scale    = 1
0.00.055.241 I ggml_metal_init: allocating
0.00.055.244 I ggml_metal_init: found device: Apple M4
0.00.055.246 I ggml_metal_init: picking default device: Apple M4
0.00.055.871 I ggml_metal_init: using embedded metal library
0.00.058.274 I ggml_metal_init: GPU name:   Apple M4
0.00.058.275 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.276 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.276 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.276 I ggml_metal_init: simdgroup reduction   = true
0.00.058.276 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.277 I ggml_metal_init: has bfloat            = true
0.00.058.277 I ggml_metal_init: use bfloat            = true
0.00.058.277 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.278 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.294 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.088.570 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.579 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.599 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.643 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.644 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.645 I llama_new_context_with_model: graph nodes  = 967
0.00.089.645 I llama_new_context_with_model: graph splits = 2
0.00.089.661 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.790 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.791 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.339 I main: llama threadpool init, n_threads = 4
0.00.739.381 I 
0.00.739.433 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.739.434 I 
0.00.739.661 I sampler seed: 1234
0.00.739.665 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.739.710 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.739.715 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.739.715 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.589.167 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51787.02 tokens per second)
0.01.589.167 I llama_perf_context_print:        load time =     730.62 ms
0.01.589.168 I llama_perf_context_print: prompt eval time =      51.62 ms /     7 tokens (    7.37 ms per token,   135.60 tokens per second)
0.01.589.172 I llama_perf_context_print:        eval time =     795.09 ms /    63 runs   (   12.62 ms per token,    79.24 tokens per second)
0.01.589.172 I llama_perf_context_print:       total time =     849.83 ms /    70 tokens
0.01.589.358 I ggml_metal_free: deallocating

real	0m1.607s
user	0m0.110s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.914 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.944 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.949 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.950 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.951 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.951 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.951 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.952 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.953 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.953 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.953 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.954 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.954 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.955 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.955 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.959 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.959 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.959 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.859 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.966 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.933 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.934 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.934 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.935 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.935 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.935 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.936 I llama_model_loader: - type  f32:  194 tensors
0.00.024.936 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.937 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.098 I llm_load_vocab: special tokens cache size = 25
0.00.051.098 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.101 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.101 I llm_load_print_meta: arch             = gptneox
0.00.051.102 I llm_load_print_meta: vocab type       = BPE
0.00.051.102 I llm_load_print_meta: n_vocab          = 50304
0.00.051.102 I llm_load_print_meta: n_merges         = 50009
0.00.051.102 I llm_load_print_meta: vocab_only       = 0
0.00.051.102 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.103 I llm_load_print_meta: n_embd           = 2048
0.00.051.103 I llm_load_print_meta: n_layer          = 24
0.00.051.105 I llm_load_print_meta: n_head           = 16
0.00.051.106 I llm_load_print_meta: n_head_kv        = 16
0.00.051.106 I llm_load_print_meta: n_rot            = 32
0.00.051.106 I llm_load_print_meta: n_swa            = 0
0.00.051.106 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.107 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.107 I llm_load_print_meta: n_gqa            = 1
0.00.051.108 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.109 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.109 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.110 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.110 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.110 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.111 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.111 I llm_load_print_meta: n_ff             = 8192
0.00.051.111 I llm_load_print_meta: n_expert         = 0
0.00.051.112 I llm_load_print_meta: n_expert_used    = 0
0.00.051.112 I llm_load_print_meta: causal attn      = 1
0.00.051.112 I llm_load_print_meta: pooling type     = 0
0.00.051.112 I llm_load_print_meta: rope type        = 2
0.00.051.112 I llm_load_print_meta: rope scaling     = linear
0.00.051.113 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.113 I llm_load_print_meta: freq_scale_train = 1
0.00.051.113 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.113 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.113 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.114 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.114 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.114 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.114 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.114 I llm_load_print_meta: model type       = 1.4B
0.00.051.115 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.115 I llm_load_print_meta: model params     = 1.41 B
0.00.051.116 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.116 I llm_load_print_meta: general.name     = 1.4B
0.00.051.116 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.117 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.117 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.117 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.117 I llm_load_print_meta: LF token         = 128 ''
0.00.051.118 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.120 I llm_load_print_meta: max token length = 1024
0.00.053.060 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.060 I llm_load_tensors: offloading output layer to GPU
0.00.053.061 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.071 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.072 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.951 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.952 I llama_new_context_with_model: n_ctx         = 128
0.00.053.952 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.952 I llama_new_context_with_model: n_batch       = 128
0.00.053.952 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.952 I llama_new_context_with_model: flash_attn    = 0
0.00.053.953 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.953 I llama_new_context_with_model: freq_scale    = 1
0.00.053.953 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.954 I ggml_metal_init: allocating
0.00.053.957 I ggml_metal_init: found device: Apple M4
0.00.053.959 I ggml_metal_init: picking default device: Apple M4
0.00.054.551 I ggml_metal_init: using embedded metal library
0.00.056.865 I ggml_metal_init: GPU name:   Apple M4
0.00.056.866 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.867 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.867 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.868 I ggml_metal_init: simdgroup reduction   = true
0.00.056.868 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.868 I ggml_metal_init: has bfloat            = true
0.00.056.868 I ggml_metal_init: use bfloat            = true
0.00.056.868 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.869 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.544 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.868 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.871 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.884 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.760 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.761 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.761 I llama_new_context_with_model: graph nodes  = 967
0.00.068.761 I llama_new_context_with_model: graph splits = 2
0.00.068.774 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.774 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.603 I 
0.00.686.644 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.656 I perplexity: tokenizing the input ..
0.00.694.418 I perplexity: tokenization took 7.761 ms
0.00.694.423 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.834.919 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.836.083 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.836.096 I llama_perf_context_print:        load time =     676.69 ms
0.00.836.097 I llama_perf_context_print: prompt eval time =     140.24 ms /   128 tokens (    1.10 ms per token,   912.70 tokens per second)
0.00.836.098 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.836.099 I llama_perf_context_print:       total time =     149.49 ms /   129 tokens
0.00.836.512 I ggml_metal_free: deallocating

real	0m0.850s
user	0m0.078s
sys	0m0.128s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.010.308 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.058 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.062 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.063 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.064 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.064 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.064 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.065 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.065 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.066 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.066 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.066 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.066 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.067 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.067 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.070 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.070 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.070 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.038 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.117 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.032 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.033 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.033 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.033 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.034 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.034 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.034 I llama_model_loader: - type  f32:  194 tensors
0.00.026.035 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.521 I llm_load_vocab: special tokens cache size = 25
0.00.052.534 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.537 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.537 I llm_load_print_meta: arch             = gptneox
0.00.052.538 I llm_load_print_meta: vocab type       = BPE
0.00.052.538 I llm_load_print_meta: n_vocab          = 50304
0.00.052.538 I llm_load_print_meta: n_merges         = 50009
0.00.052.538 I llm_load_print_meta: vocab_only       = 0
0.00.052.539 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.539 I llm_load_print_meta: n_embd           = 2048
0.00.052.539 I llm_load_print_meta: n_layer          = 24
0.00.052.542 I llm_load_print_meta: n_head           = 16
0.00.052.544 I llm_load_print_meta: n_head_kv        = 16
0.00.052.544 I llm_load_print_meta: n_rot            = 32
0.00.052.544 I llm_load_print_meta: n_swa            = 0
0.00.052.544 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.544 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.545 I llm_load_print_meta: n_gqa            = 1
0.00.052.546 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.547 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.547 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.547 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.548 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.548 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.548 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.549 I llm_load_print_meta: n_ff             = 8192
0.00.052.549 I llm_load_print_meta: n_expert         = 0
0.00.052.549 I llm_load_print_meta: n_expert_used    = 0
0.00.052.549 I llm_load_print_meta: causal attn      = 1
0.00.052.551 I llm_load_print_meta: pooling type     = 0
0.00.052.552 I llm_load_print_meta: rope type        = 2
0.00.052.553 I llm_load_print_meta: rope scaling     = linear
0.00.052.553 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.553 I llm_load_print_meta: freq_scale_train = 1
0.00.052.554 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.554 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.554 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.554 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.554 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.554 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.554 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.555 I llm_load_print_meta: model type       = 1.4B
0.00.052.555 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.555 I llm_load_print_meta: model params     = 1.41 B
0.00.052.556 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.556 I llm_load_print_meta: general.name     = 1.4B
0.00.052.556 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.556 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.557 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.557 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.557 I llm_load_print_meta: LF token         = 128 ''
0.00.052.561 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.561 I llm_load_print_meta: max token length = 1024
0.00.054.582 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.583 I llm_load_tensors: offloading output layer to GPU
0.00.054.583 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.594 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.595 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.535 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.536 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.536 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.537 I llama_new_context_with_model: n_batch       = 2048
0.00.055.537 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.537 I llama_new_context_with_model: flash_attn    = 0
0.00.055.537 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.538 I llama_new_context_with_model: freq_scale    = 1
0.00.055.538 I ggml_metal_init: allocating
0.00.055.543 I ggml_metal_init: found device: Apple M4
0.00.055.545 I ggml_metal_init: picking default device: Apple M4
0.00.056.130 I ggml_metal_init: using embedded metal library
0.00.058.433 I ggml_metal_init: GPU name:   Apple M4
0.00.058.435 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.435 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.435 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.435 I ggml_metal_init: simdgroup reduction   = true
0.00.058.437 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.437 I ggml_metal_init: has bfloat            = true
0.00.058.437 I ggml_metal_init: use bfloat            = true
0.00.058.438 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.438 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.278 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.088.026 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.030 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.049 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.078 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.079 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.080 I llama_new_context_with_model: graph nodes  = 967
0.00.089.080 I llama_new_context_with_model: graph splits = 2
0.00.089.095 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.235 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.235 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.361 I main: llama threadpool init, n_threads = 4
0.00.754.403 I 
0.00.754.441 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.754.442 I 
0.00.754.668 I sampler seed: 1234
0.00.754.674 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.754.719 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.754.721 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.754.721 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.636.871 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 63055.06 tokens per second)
0.01.636.872 I llama_perf_context_print:        load time =     744.05 ms
0.01.636.872 I llama_perf_context_print: prompt eval time =      54.44 ms /     7 tokens (    7.78 ms per token,   128.58 tokens per second)
0.01.636.873 I llama_perf_context_print:        eval time =     824.86 ms /    63 runs   (   13.09 ms per token,    76.38 tokens per second)
0.01.636.873 I llama_perf_context_print:       total time =     882.51 ms /    70 tokens
0.01.637.062 I ggml_metal_free: deallocating

real	0m1.654s
user	0m0.110s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4369 (21ae3b9b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.240 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.991 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.995 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.997 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.001 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.001 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.002 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.003 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.005 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.005 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.006 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.006 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.006 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.007 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.010 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.012 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.012 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.012 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.885 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.926 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.865 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.866 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.866 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.867 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.867 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.867 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.868 I llama_model_loader: - type  f32:  194 tensors
0.00.023.868 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.895 I llm_load_vocab: special tokens cache size = 25
0.00.050.802 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.805 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.806 I llm_load_print_meta: arch             = gptneox
0.00.050.806 I llm_load_print_meta: vocab type       = BPE
0.00.050.806 I llm_load_print_meta: n_vocab          = 50304
0.00.050.806 I llm_load_print_meta: n_merges         = 50009
0.00.050.807 I llm_load_print_meta: vocab_only       = 0
0.00.050.807 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.807 I llm_load_print_meta: n_embd           = 2048
0.00.050.807 I llm_load_print_meta: n_layer          = 24
0.00.050.810 I llm_load_print_meta: n_head           = 16
0.00.050.811 I llm_load_print_meta: n_head_kv        = 16
0.00.050.811 I llm_load_print_meta: n_rot            = 32
0.00.050.811 I llm_load_print_meta: n_swa            = 0
0.00.050.812 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.812 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.812 I llm_load_print_meta: n_gqa            = 1
0.00.050.813 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.816 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.817 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.817 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.818 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.818 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.818 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.819 I llm_load_print_meta: n_ff             = 8192
0.00.050.819 I llm_load_print_meta: n_expert         = 0
0.00.050.819 I llm_load_print_meta: n_expert_used    = 0
0.00.050.819 I llm_load_print_meta: causal attn      = 1
0.00.050.819 I llm_load_print_meta: pooling type     = 0
0.00.050.819 I llm_load_print_meta: rope type        = 2
0.00.050.820 I llm_load_print_meta: rope scaling     = linear
0.00.050.820 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.820 I llm_load_print_meta: freq_scale_train = 1
0.00.050.821 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.821 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.821 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.821 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.821 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.821 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.821 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.822 I llm_load_print_meta: model type       = 1.4B
0.00.050.822 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.822 I llm_load_print_meta: model params     = 1.41 B
0.00.050.823 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.823 I llm_load_print_meta: general.name     = 1.4B
0.00.050.823 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.823 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.824 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.824 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.824 I llm_load_print_meta: LF token         = 128 ''
0.00.050.824 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.825 I llm_load_print_meta: max token length = 1024
0.00.052.493 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.494 I llm_load_tensors: offloading output layer to GPU
0.00.052.494 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.504 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.505 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.381 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.382 I llama_new_context_with_model: n_ctx         = 128
0.00.053.382 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.382 I llama_new_context_with_model: n_batch       = 128
0.00.053.382 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.383 I llama_new_context_with_model: flash_attn    = 0
0.00.053.383 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.383 I llama_new_context_with_model: freq_scale    = 1
0.00.053.384 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.384 I ggml_metal_init: allocating
0.00.053.387 I ggml_metal_init: found device: Apple M4
0.00.053.390 I ggml_metal_init: picking default device: Apple M4
0.00.053.978 I ggml_metal_init: using embedded metal library
0.00.056.390 I ggml_metal_init: GPU name:   Apple M4
0.00.056.392 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.392 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.392 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.393 I ggml_metal_init: simdgroup reduction   = true
0.00.056.393 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.393 I ggml_metal_init: has bfloat            = true
0.00.056.393 I ggml_metal_init: use bfloat            = true
0.00.056.393 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.394 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.247 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.645 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.650 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.665 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.612 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.613 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.613 I llama_new_context_with_model: graph nodes  = 967
0.00.068.613 I llama_new_context_with_model: graph splits = 2
0.00.068.626 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.626 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.528.155 I 
0.00.528.187 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.528.197 I perplexity: tokenizing the input ..
0.00.535.952 I perplexity: tokenization took 7.753 ms
0.00.535.956 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.676.054 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.677.197 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.677.221 I llama_perf_context_print:        load time =     518.91 ms
0.00.677.222 I llama_perf_context_print: prompt eval time =     139.84 ms /   128 tokens (    1.09 ms per token,   915.33 tokens per second)
0.00.677.223 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.677.223 I llama_perf_context_print:       total time =     149.07 ms /   129 tokens
0.00.677.650 I ggml_metal_free: deallocating

real	0m0.691s
user	0m0.080s
sys	0m0.102s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4369 (21ae3b9b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14e30a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14e30a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14e30aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14e30b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14e30ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14e30bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14e30c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14e30cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14e30d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14e30d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14e30daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14e30dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14e30eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14e30f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14e30fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14e3101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14e310910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14e311030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14e311750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14e311f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14e312640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14e312d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14e313480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14e313d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14e314440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14e314700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14e314d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14e315980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14e315ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14e316180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14e316620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14e3168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14e317170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14e3176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14e317970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14e317e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14e3182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14e318750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14e318bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14e319090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14e319530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14e3199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14e319e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14e31a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14e31a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14e31abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14e31b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14e31bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14e31c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14e31c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14e31cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14e31d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14e31d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14e31df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14e31e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14e31ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14e31f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14e31f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14e31f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14e320160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14e320420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14e3208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14e320d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14e321200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14e3216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14e321b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14e321fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14e322480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14e322920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14e322dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14e323260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14e323700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14e323ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14e3240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14e324640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14e324b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14e3250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14e325630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14e325b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14e3260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14e326620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14e326b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14e3270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14e327610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14e327b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14e3280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14e328600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14e328b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14e3290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14e3295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14e329b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14e32a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14e32a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14e32ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14e32b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14e32b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14e32bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14e31b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14e32bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14e32c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14e32cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14e32d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14e32d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14e32dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14e32e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14e32e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14e32ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14e32f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14e32f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14e32fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14e3301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14e330700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14e330c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14e3310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14e331590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14e331a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14e331ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14e332370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14e332810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14e332cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14e333150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14e3335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14e333a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14e333f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14e3343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14e334870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14e334d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14e3351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14e335650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14e335af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14e335f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14e336430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14e3368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14e336d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14e337210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14e3376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14e337b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14e337ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14e338490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14e338930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14e338dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14e339270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14e339710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14e339bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14e33a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14e33a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14e33a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14e33ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14e33b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14e33b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14e33bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14e33c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14e33c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14e33c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14e33ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14e33d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14e33d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14e33dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14e33e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14e33e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14e33ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14e33eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14e33f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14e33f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14e33fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14e340170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14e340610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14e340ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14e340f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14e3413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14e341890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14e341d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14e3421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14e342670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14e342b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14e342fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14e343450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14e3438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14e343d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14e344230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14e3446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14e344b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14e345010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14e3454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14e345950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14e345df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14e346290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14e346730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14e346bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14e347070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14e347510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14e3479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14e347e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14e3483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14e3488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14e348e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14e349390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14e349650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14e349c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14e34a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14e34a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14e34b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14e34b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14e34b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14e34bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14e34c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14e34cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14e34d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14e34d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14e34d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14e34e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14e34e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14e34ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14e34f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14e34f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14e34fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14e350150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14e3506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14e350bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14e351140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14e351690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14e351be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14e352130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14e352680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14e352bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14e353120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14e353670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14e353bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14e354110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14e354660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14e354bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14e355100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14e355650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14e355ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14e3560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14e356640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14e356b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14e3570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14e357630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14e357b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14e3580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14e358620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14e358b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14e3590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14e359610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14e359b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14e35a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14e35a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14e35ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14e35b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14e35b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14e35bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14e35c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14e35c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14e35cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14e35d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14e35d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14e35db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14e35e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14e35e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14e35eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14e35f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14e35f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14e35fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14e360050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14e3605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14e360af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14e360f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14e361430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14e3618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14e361d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14e362210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14e3626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14e362b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14e362ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14e363490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14e363930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14e363dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14e364270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14e364710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14e364bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14e365050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14e3655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14e365cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14e3663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14e366b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14e367220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14e3674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14e367cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14e367f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14e3685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.141.372 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.141.376 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x109004dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x109005230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1090056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x109005b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x109005f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1090063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x109006860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x109006cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x109007140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1090075b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x109007a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1090080e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x109008c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1090093b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x109009bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10900a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10900aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10900b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10900b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10900c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10900c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10900ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10900d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10900dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10900e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10900e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10900e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10900eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10900f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10900f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10900faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x109010020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x109010490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x109010750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x109010bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x109011030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1090114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x109011910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x109011d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1090121f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x109012660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x109012ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x109012f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1090133b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x109013820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x109013c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x109014100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x109014570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1090149e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x109014e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1090152c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x109015730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x109015ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x109016010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x109016480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1090168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x109016e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x109017360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1090177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x109017c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1090180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x109018520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x109018990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x109018e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x109019270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1090196e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x109019b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x109019fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10901a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10901a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10901ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10901b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10901b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10901ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10901bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10901c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10901c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10901cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10901d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10901d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10901d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10901dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10901e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10901e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10901eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10901efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10901f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10901f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10901fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x109020160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1090205d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x109020a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x109020eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x109021320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x109021790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x109021c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x109022070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1090224e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x109022950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x109022dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x109023230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1090236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x109023b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x109023f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1090243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x109024860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x109024cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x109025140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1090255b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x109025a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x109025e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x109026300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x109026770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x109026be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x109027050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1090274c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x109027930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x109027da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x109028210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x109028680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x109028af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x109028f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1090293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x109029840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x109029cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10902a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10902a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10902aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10902ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10902b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10902b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10902bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10902c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10902c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10902c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10902cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10902d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10902d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10902dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10902df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10902e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10902e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10902ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10902f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10902f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10902f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10902fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1090302c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x109030730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x109030ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x109031010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x109031480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1090318f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x109031d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1090321d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x109032640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x109032ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x109032f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x109033390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x109033800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x109033c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1090340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x109034550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1090349c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x109034e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1090352a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x109035710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x109035b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x109035ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x109036460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1090368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x109036d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1090371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x109037620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x109037a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x109037f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x109038370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1090387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x109038c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1090390c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x109039530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1090399a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x109039e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10903a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10903a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10903ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10903afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10903b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10903b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10903bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10903c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10903c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10903ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10903cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10903d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10903d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10903dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10903e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10903e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10903e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10903edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10903f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10903f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10903fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10903ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x109040420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x109040890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x109040e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x109041290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x109041700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x109042250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x109042510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1090427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x109042c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1090430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x109043520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x109043990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x109043e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x109044270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1090446e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x109044b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x109044fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x109045430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1090458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x109045d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x109046180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1090465f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x109046a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x109046ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x109047340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1090477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x109047c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x109048090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x109048500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x109048970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x109048de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x109049250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1090496c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x109049b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x109049fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10904a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10904a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10904acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10904b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10904b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10904ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10904beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10904c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10904c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10904cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10904d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10904d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10904d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10904ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10904e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10904e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10904eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10904ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10904f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10904f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10904fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x109050140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1090505b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x109050a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x109050e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x109051300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x109051770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x109051be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x109052050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1090524c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x109052930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x109052da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x109053210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x109053680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x109053af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x109053f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1090543d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x109054840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x109054cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x109055120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x109055590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x109055a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x109055e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1090568e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x109057000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x109057720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x109057e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x109058100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x109058570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x109058b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x109059180 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14e2059d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14e206110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14e206580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14e2069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14e206e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14e2072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14e207740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14e207bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14e204900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14e204d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14e204230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14e208020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14e208b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14e2092b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14e209ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14e20a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14e20a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14e20b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14e20b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14e20bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14e20c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14e20cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14e20d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14e20db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14e20e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14e20e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14e20e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14e20ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14e20f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14e20f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14e20fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14e210100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14e210570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14e2109e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14e210ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14e2111b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14e211680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14e211b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14e212020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14e2124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14e2129c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14e212e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14e213360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14e213830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14e213d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14e214170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14e2145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14e214a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14e214ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14e215330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14e2157a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14e215c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14e216080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14e2164f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14e216960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14e217010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14e2174b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14e217770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14e217be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14e218050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14e2185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14e218aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14e218fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14e2194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14e2199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14e219ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14e21a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14e21a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14e21ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14e21b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14e21b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14e21bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14e21c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14e21c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14e21cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14e21d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14e21d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14e21dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14e21e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14e21e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14e21ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14e21f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14e21f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14e21ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14e220530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14e220ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14e221090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14e221640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14e221bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14e2221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14e222750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14e222d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14e2232b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14e223860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14e223e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14e2243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14e224970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14e224f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14e2254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14e225a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14e226030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14e2265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14e226b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14e227140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14e2276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14e227ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14e228250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14e228800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14e228db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14e229360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14e229910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14e229ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14e22a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14e22aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14e22af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14e22b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14e22b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14e22be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14e22c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14e22c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14e22cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14e22d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14e22d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14e22dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14e22e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14e22e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14e22eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14e22f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14e22f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14e22fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14e22ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14e230420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14e230920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14e230e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14e231320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14e231820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14e231d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14e232220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14e232720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14e232c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14e233120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14e233620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14e233b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14e234020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14e234520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14e234a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14e234f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14e235420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14e235920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14e235e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14e236320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14e236820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14e236d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14e237220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14e237720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14e237c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14e238120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14e238620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14e238b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14e239020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14e239520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14e239a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14e239f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14e23a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14e23a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14e23ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14e23b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14e23b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14e23bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14e23c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14e23c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14e23cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14e23d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14e23d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14e23db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14e23e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14e23e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14e23ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14e23ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14e23f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14e23f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14e23fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14e240320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14e240820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14e240d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14e241220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14e241720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14e241c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14e242120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14e242620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14e242b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14e243020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14e243520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14e243a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14e243fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14e244580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14e244b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14e2450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14e2456f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14e245d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14e246310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14e246b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14e246fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14e247260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14e247870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14e247e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14e248670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14e248b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14e248fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14e249450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14e249c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14e24a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14e24a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14e24abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14e24b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14e24b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14e24bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14e24c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14e24c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14e24cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14e24d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14e24d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14e24dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14e24e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14e24e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14e24ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14e24f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14e24f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14e24fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14e2500f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14e250640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14e250b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14e2510e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14e251630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14e251b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14e2520d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14e252620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14e252b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14e2530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14e253610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14e253b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14e2540b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14e254600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14e254b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14e2550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14e2555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14e255b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14e256090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14e2565e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14e256b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14e257080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14e2575d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14e257b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14e258070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14e2585c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14e258b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14e259060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14e2595b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14e259b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14e25a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14e25a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14e25aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14e25b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14e25b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14e25bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14e25c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14e25c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14e25ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14e25cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14e25d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14e25d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14e25dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14e25e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14e25e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14e25ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14e25ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14e25f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14e25f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14e25fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14e2601a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14e260640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14e260ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14e261030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14e261750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14e261e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14e262590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14e262cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14e262f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14e263760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14e263a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14e264030 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.779s
user	0m0.292s
sys	0m0.298s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4369 (21ae3b9b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14000a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14000a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14000ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14000b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14000b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14000bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14000c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14000cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14000d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14000d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14000da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14000df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14000eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14000f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14000fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x140010190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1400108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x140010fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1400116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x140011ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1400125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x140012d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x140013420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x140013cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1400143e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1400146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x140014cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x140015920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x140015e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x140016120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1400165c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x140016880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x140017110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x140017650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x140017910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x140017db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x140018250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1400186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x140018b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x140019030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1400194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x140019970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x140019e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14001a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14001a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14001ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14001b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14001bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14001c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14001c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14001cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14001d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14001d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14001df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14001e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14001eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14001f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14001f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14001f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x140020100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1400203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x140020860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x140020d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1400211a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x140021640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x140021ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x140021f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x140022420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1400228c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x140022d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x140023200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1400236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x140023b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x140024090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1400245e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x140024b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x140025080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1400255d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x140025b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x140026070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1400265c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x140026b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x140027060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1400275b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x140027b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x140028050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1400285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x140028af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x140029040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x140029590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x140029ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14002a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14002a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14002aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14002b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14002b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14002bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14001b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14002bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14002c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14002cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14002d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14002d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14002dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14002e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14002e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14002ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14002f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14002f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14002fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x140030150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1400306a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x140030bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x140031090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x140031530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1400319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x140031e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x140032310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1400327b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x140032c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1400330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x140033590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x140033a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x140033ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x140034370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x140034810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x140034cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x140035150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1400355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x140035a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x140035f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1400363d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x140036870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x140036d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1400371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x140037650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x140037af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x140037f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x140038430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1400388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x140038d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x140039210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1400396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x140039b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x140039ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14003a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14003a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14003add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14003b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14003b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14003bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14003c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14003c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14003c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14003ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14003d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14003d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14003dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14003e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14003e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14003e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14003ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14003f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14003f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14003fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x140040110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1400405b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x140040a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x140040ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x140041390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x140041830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x140041cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x140042170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x140042610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x140042ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x140042f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1400433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x140043890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x140043d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1400441d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x140044670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x140044b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x140044fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x140045450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1400458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x140045d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x140046230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1400466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x140046b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x140047010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1400474b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x140047950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x140047df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x140048340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x140048890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x140048de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x140049330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1400495f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x140049c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14004a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14004a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14004b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14004b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14004b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14004bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14004c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14004cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14004d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14004d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14004d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14004e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14004e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14004ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14004f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14004f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14004fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1400500f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x140050640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x140050b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1400510e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x140051630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x140051b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1400520d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x140052620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x140052b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1400530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x140053610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x140053b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1400540b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x140054600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x140054b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1400550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1400555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x140055b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x140056090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1400565e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x140056b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x140057080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1400575d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x140057b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x140058070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1400585c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x140058b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x140059060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1400595b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x140059b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14005a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14005a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14005aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14005b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14005b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14005bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14005c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14005c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14005cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14005d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14005d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14005dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14005e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14005e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14005eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14005f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14005f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14005faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14005fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x140060540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x140060a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x140060f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1400613d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x140061870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x140061d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1400621b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x140062650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x140062af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x140062f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x140063430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1400638d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x140063d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x140064210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1400646b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x140064b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x140064ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x140065540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x140065c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x140066380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x140066aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1400671c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x140067480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x140067c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x140067f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x140068540 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.104.130 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.135 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f004b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f004fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f005430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f0058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f005d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f006180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f0065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f006a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f006ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f007340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f0077b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f007ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f0089c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f009170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f009980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f00a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f00a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f00aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f00b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f00bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f00c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f00cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f00d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f00d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f00e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f00e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f00e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f00eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f00ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f00f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f00f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f00fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f0101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f010470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f0108e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f010d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f0111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f011630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f011aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f011f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f012380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f0127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f012c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f0130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f013540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f0139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f013e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f014290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f014700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f014b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f014fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f015450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f0158c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f015d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f0161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f016610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f016b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f017080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f0174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f017960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f017dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f018240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f0186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f018b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f018f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f019400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f019870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f019ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11f01a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11f01a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f01aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11f01aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11f01b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11f01b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11f01bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11f01c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11f01c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11f01c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11f01cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11f01d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11f01d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11f01db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11f01df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11f01e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11f01e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11f01ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11f01f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11f01f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11f01fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11f01fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11f0202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11f020760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11f020bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11f021040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11f0214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11f021920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11f021d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11f022200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11f022670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11f022ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11f022f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11f0233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11f023830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11f023ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11f024110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11f024580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11f0249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11f024e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11f0252d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11f025740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11f025bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11f026020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11f026490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11f026900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11f026d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11f0271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f027650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11f027ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11f027f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f0283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11f028810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11f028c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11f0290f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11f029560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11f0299d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11f029e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11f02a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11f02a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11f02ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11f02b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11f02b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11f02b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11f02bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11f02c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11f02c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11f02caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11f02cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11f02d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11f02d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11f02dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11f02e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11f02e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f02e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11f02ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11f02f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11f02f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11f02fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11f02ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11f030450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11f0308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11f030d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11f0311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11f031610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11f031a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11f031ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11f032360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11f0327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11f032c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11f0330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11f033520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11f033990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11f033e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11f034270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11f0346e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11f034b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f034fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11f035430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11f0358a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11f035d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f036180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11f0365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11f036a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11f036ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11f037340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11f0377b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11f037c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11f038090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11f038500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11f038970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11f038de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11f039250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11f0396c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11f039b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11f039fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11f03a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11f03a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11f03acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11f03b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11f03b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11f03ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11f03beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11f03c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11f03c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11f03cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11f03d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11f03d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11f03d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11f03ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11f03e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11f03e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11f03eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11f03ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11f03f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11f03f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11f03fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11f040140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11f0405b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11f040b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11f040fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11f041420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11f041f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f042230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11f0424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11f042960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11f042dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11f043240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11f0436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11f043b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11f043f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11f044400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11f044870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11f044ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11f045150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11f0455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11f045a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11f045ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f046310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11f046780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11f046bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11f047060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11f0474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11f047940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11f047db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11f048220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11f048690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11f048b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11f048f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11f0493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11f049850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11f049cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11f04a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11f04a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11f04aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11f04ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11f04b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11f04b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11f04bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11f04c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11f04c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11f04c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11f04cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11f04d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11f04d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11f04dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11f04df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11f04e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11f04e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11f04eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11f04f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11f04f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f04f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11f04fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11f0502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11f050740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11f050bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11f051020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f051490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11f051900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11f051d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f0521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f052650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11f052ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f052f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f0533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11f053810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11f053c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f0540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f054560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f0549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f054e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f0552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f055720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11f055b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11f056600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11f056d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11f057440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11f057b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11f057e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11f058290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11f058890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11f058ea0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x140024c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1400250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x140025530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1400259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x140025e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x140026280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1400266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x140026b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x140026fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x140027440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1400278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x140027e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x140028780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x140028f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1400296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x140029dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14002a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14002abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14002b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14002bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14002c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14002ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14002d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14002d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14002ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14002e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14002e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14002ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14002f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14002f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14002f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14002fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x140030250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x140030510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x140030980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x140030df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x140031260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1400316d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x140031b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x140031fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x140032420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x140032890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x140032d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x140033170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1400335e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x140033a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x140033ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x140034330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1400347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x140034c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x140035080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1400354f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x140035960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x140035dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x140036240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1400366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x140036b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x140036f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x140037400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x140037870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x140037ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x140038150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1400385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x140038a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x140038ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x140039310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x140039780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x140039bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14003a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14003a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14003a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14003adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14003b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14003b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14003bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14003bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14003c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14003c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14003ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14003d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14003d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14003da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14003de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14003e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14003e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14003ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14003f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14003f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14003f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14003fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x140040200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x140040670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x140040ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x140040f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1400413c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x140041830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x140041ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x140042110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x140042580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1400429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x140042e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1400432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x140043740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x140043bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x140044020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x140044490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x140044900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x140044d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1400451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x140045650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x140045ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x140045f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1400463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x140046810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x140046c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1400470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x140047560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1400479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x140047e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1400482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x140048720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x140048b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x140049000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x140049470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1400498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x140049d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14004a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14004a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14004aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14004af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14004b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14004b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14004bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14004c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14004c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14004c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14004ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14004d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14004d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14004db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14004dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14004e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14004e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14004ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14004f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14004f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14004fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14004fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x140050360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1400507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x140050c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1400510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x140051520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x140051990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x140051e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x140052270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1400526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x140052b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x140052fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x140053430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1400538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x140053d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x140054180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1400545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x140054a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x140054ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x140055340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1400557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x140055c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x140056090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x140056500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x140056970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x140056de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x140057250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1400576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x140057b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x140057fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x140058410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x140058880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x140058cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x140059160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1400595d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x140059a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x140059eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14005a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14005a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14005ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14005b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14005b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14005b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14005bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14005c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14005c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14005cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14005cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14005d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14005d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14005dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14005e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14005e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14005ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14005ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14005f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14005f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14005fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x140060050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1400604c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x140060930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x140060da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x140061210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x140061990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x140061e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x140062270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1400626e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x140062b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x140062fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x140063430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1400638a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x140063d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x140064180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1400645f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x140064a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x140064ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x140065340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1400657b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x140065c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x140066090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x140066500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x140066970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x140066de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x140067250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1400676c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x140067b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x140067fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x140068410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14000b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14000acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1400097e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14000a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1400177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x140017c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1400180d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x140018540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1400189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x140018e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x140019290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x140019700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x140019b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x140019fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14001a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14001a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14001ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14001b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14001b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14001ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14001bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14001c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14001c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14001cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14001d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14001d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14001d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14001de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14001e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14001e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14001eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14001efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14001f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14001f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14001fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x140020180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1400205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x140020a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x140020ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x140021340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1400217b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x140021c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x140022090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x140022500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x140022970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x140022de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x140023250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1400236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x140023db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1400244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x140016280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x140016970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x140016de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14000d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14000d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14000de30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.967s
user	0m0.246s
sys	0m0.150s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.54 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.13 sec*proc (2 tests)

Total Test time (real) =   1.14 sec
        1.16 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.14 user         0.04 sys
```
