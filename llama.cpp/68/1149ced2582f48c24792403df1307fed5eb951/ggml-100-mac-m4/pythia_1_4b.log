Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.553s
user	0m0.886s
sys	0m1.228s
++ nproc
+ make -j10
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Built target sha1
[  5%] Built target build_info
[  5%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 32%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Linking CXX executable ../../bin/llama-simple
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target llama-quantize-stats
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 36%] Built target llama-simple-chat
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target common
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Built target llava_static
[ 42%] Built target llava_shared
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-sampling
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-json-schema-to-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-backend-ops
[ 55%] Built target test-log
[ 55%] Linking CXX executable ../bin/test-gguf
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Built target test-arg-parser
[ 57%] Linking CXX executable ../bin/test-barrier
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-chat-template
[ 62%] Built target test-gguf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-backend-ops
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-barrier
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-autorelease
[ 63%] Built target test-model-load-cancel
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 63%] Built target test-quantize-fns
[ 63%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Linking CXX executable ../../bin/llama-batched
[ 64%] Built target test-quantize-perf
[ 64%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched-bench
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Built target llama-batched
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-batched-bench
[ 71%] Built target llama-embedding
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target llama-gbnf-validator
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-gguf-split
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Built target llama-imatrix
[ 76%] Linking CXX executable ../../bin/llama-bench
[ 76%] Built target llama-gritlm
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookahead
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Built target llama-infill
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-cli
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Built target llama-bench
[ 81%] Built target llama-lookahead
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-lookup
[ 81%] Built target llama-lookup-merge
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Built target llama-lookup-stats
[ 82%] Generating loading.html.hpp
[ 83%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 84%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Generating index.html.gz.hpp
[ 85%] Built target llama-cli
[ 85%] Built target llama-passkey
[ 85%] Built target llama-parallel
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-quantize
[ 86%] Built target llama-perplexity
[ 87%] Linking CXX executable ../../bin/llama-retrieval
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-run
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-tts
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Built target llama-quantize
[ 90%] Built target llama-save-load-state
[ 90%] Built target llama-retrieval
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-speculative
[ 91%] Built target llama-run
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 92%] Built target llama-tokenize
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-tts
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 93%] Built target llama-gen-docs
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-convert-llama2c-to-ggml
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.070s
user	0m6.124s
sys	0m9.759s

main: quantize time =  2951.40 ms
main:    total time =  2951.40 ms

main: quantize time =  1348.44 ms
main:    total time =  1348.44 ms

main: quantize time =  1337.52 ms
main:    total time =  1337.52 ms

main: quantize time =  1525.34 ms
main:    total time =  1525.34 ms

main: quantize time =  2016.12 ms
main:    total time =  2016.12 ms

main: quantize time =  4905.90 ms
main:    total time =  4905.90 ms

main: quantize time =  5580.25 ms
main:    total time =  5580.25 ms

main: quantize time =  6651.42 ms
main:    total time =  6651.42 ms

main: quantize time =  5705.14 ms
main:    total time =  5705.14 ms

main: quantize time =  4535.82 ms
main:    total time =  4535.82 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.193 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.322 I main: llama backend init
0.00.000.328 I main: load the model and apply lora adapter, if any
0.00.052.511 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.065.934 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.065.968 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.065.973 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.065.974 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.065.974 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.065.974 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.065.975 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.065.978 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.065.979 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.065.979 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.065.980 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.065.981 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.065.982 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.065.983 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.065.988 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.065.988 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.065.989 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.074.847 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.077.080 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.084.784 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.084.789 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.084.789 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.084.790 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.084.790 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.084.791 I llama_model_loader: - type  f32:  194 tensors
0.00.084.792 I llama_model_loader: - type  f16:   98 tensors
0.00.084.793 I print_info: file format = GGUF V3 (latest)
0.00.084.794 I print_info: file type   = all F32 (guessed)
0.00.084.797 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.114.197 I load: special tokens cache size = 25
0.00.121.586 I load: token to piece cache size = 0.2984 MB
0.00.121.589 I print_info: arch             = gptneox
0.00.121.589 I print_info: vocab_only       = 0
0.00.121.589 I print_info: n_ctx_train      = 2048
0.00.121.590 I print_info: n_embd           = 2048
0.00.121.590 I print_info: n_layer          = 24
0.00.121.593 I print_info: n_head           = 16
0.00.121.594 I print_info: n_head_kv        = 16
0.00.121.594 I print_info: n_rot            = 32
0.00.121.594 I print_info: n_swa            = 0
0.00.121.594 I print_info: n_embd_head_k    = 128
0.00.121.594 I print_info: n_embd_head_v    = 128
0.00.121.595 I print_info: n_gqa            = 1
0.00.121.596 I print_info: n_embd_k_gqa     = 2048
0.00.121.596 I print_info: n_embd_v_gqa     = 2048
0.00.121.597 I print_info: f_norm_eps       = 1.0e-05
0.00.121.597 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.121.598 I print_info: f_clamp_kqv      = 0.0e+00
0.00.121.598 I print_info: f_max_alibi_bias = 0.0e+00
0.00.121.599 I print_info: f_logit_scale    = 0.0e+00
0.00.121.600 I print_info: n_ff             = 8192
0.00.121.600 I print_info: n_expert         = 0
0.00.121.600 I print_info: n_expert_used    = 0
0.00.121.600 I print_info: causal attn      = 1
0.00.121.600 I print_info: pooling type     = 0
0.00.121.602 I print_info: rope type        = 2
0.00.121.602 I print_info: rope scaling     = linear
0.00.121.602 I print_info: freq_base_train  = 10000.0
0.00.121.603 I print_info: freq_scale_train = 1
0.00.121.603 I print_info: n_ctx_orig_yarn  = 2048
0.00.121.603 I print_info: rope_finetuned   = unknown
0.00.121.603 I print_info: ssm_d_conv       = 0
0.00.121.603 I print_info: ssm_d_inner      = 0
0.00.121.603 I print_info: ssm_d_state      = 0
0.00.121.603 I print_info: ssm_dt_rank      = 0
0.00.121.604 I print_info: ssm_dt_b_c_rms   = 0
0.00.121.604 I print_info: model type       = 1.4B
0.00.121.604 I print_info: model params     = 1.41 B
0.00.121.604 I print_info: general.name     = 1.4B
0.00.121.605 I print_info: vocab type       = BPE
0.00.121.605 I print_info: n_vocab          = 50304
0.00.121.605 I print_info: n_merges         = 50009
0.00.121.605 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.121.606 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.121.606 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.121.606 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.121.606 I print_info: LF token         = 128 'Ä'
0.00.121.611 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.121.611 I print_info: max token length = 1024
0.00.124.180 I load_tensors: offloading 24 repeating layers to GPU
0.00.124.180 I load_tensors: offloading output layer to GPU
0.00.124.180 I load_tensors: offloaded 25/25 layers to GPU
0.00.124.199 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.124.200 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.124.493 I llama_init_from_model: n_seq_max     = 1
0.00.124.494 I llama_init_from_model: n_ctx         = 2048
0.00.124.494 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.124.494 I llama_init_from_model: n_batch       = 2048
0.00.124.495 I llama_init_from_model: n_ubatch      = 512
0.00.124.495 I llama_init_from_model: flash_attn    = 0
0.00.124.495 I llama_init_from_model: freq_base     = 10000.0
0.00.124.495 I llama_init_from_model: freq_scale    = 1
0.00.124.496 I ggml_metal_init: allocating
0.00.124.499 I ggml_metal_init: found device: Apple M4
0.00.124.501 I ggml_metal_init: picking default device: Apple M4
0.00.125.196 I ggml_metal_init: using embedded metal library
0.00.145.550 I ggml_metal_init: GPU name:   Apple M4
0.00.145.552 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.145.553 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.145.553 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.145.553 I ggml_metal_init: simdgroup reduction   = true
0.00.145.553 I ggml_metal_init: simdgroup matrix mul. = true
0.00.145.553 I ggml_metal_init: has bfloat            = true
0.00.145.554 I ggml_metal_init: use bfloat            = true
0.00.145.554 I ggml_metal_init: hasUnifiedMemory      = true
0.00.145.555 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.169.676 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.192.260 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.192.268 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.192.289 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.193.404 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.193.406 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.193.407 I llama_init_from_model: graph nodes  = 967
0.00.193.407 I llama_init_from_model: graph splits = 2
0.00.193.411 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.193.535 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.193.536 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.274.713 I main: llama threadpool init, n_threads = 4
0.00.274.754 I 
0.00.274.775 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.274.775 I 
0.00.274.846 I sampler seed: 1234
0.00.274.850 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.274.874 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.274.876 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.274.876 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.126.161 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57817.59 tokens per second)
0.02.126.162 I llama_perf_context_print:        load time =     222.19 ms
0.02.126.163 I llama_perf_context_print: prompt eval time =      53.62 ms /     7 tokens (    7.66 ms per token,   130.55 tokens per second)
0.02.126.163 I llama_perf_context_print:        eval time =    1794.73 ms /    63 runs   (   28.49 ms per token,    35.10 tokens per second)
0.02.126.164 I llama_perf_context_print:       total time =    1851.45 ms /    70 tokens
0.02.126.372 I ggml_metal_free: deallocating

real	0m2.459s
user	0m0.145s
sys	0m0.105s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.747 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.470 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.475 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.482 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.482 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.483 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.483 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.483 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.485 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.485 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.485 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.485 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.486 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.486 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.486 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.488 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.489 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.489 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.431 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.481 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.364 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.366 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.366 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.367 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.367 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.367 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.368 I llama_model_loader: - type  f32:  194 tensors
0.00.033.368 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.369 I print_info: file format = GGUF V3 (latest)
0.00.033.370 I print_info: file type   = Q8_0
0.00.033.371 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.053.651 I load: special tokens cache size = 25
0.00.059.675 I load: token to piece cache size = 0.2984 MB
0.00.059.680 I print_info: arch             = gptneox
0.00.059.680 I print_info: vocab_only       = 0
0.00.059.680 I print_info: n_ctx_train      = 2048
0.00.059.681 I print_info: n_embd           = 2048
0.00.059.681 I print_info: n_layer          = 24
0.00.059.688 I print_info: n_head           = 16
0.00.059.689 I print_info: n_head_kv        = 16
0.00.059.689 I print_info: n_rot            = 32
0.00.059.689 I print_info: n_swa            = 0
0.00.059.689 I print_info: n_embd_head_k    = 128
0.00.059.689 I print_info: n_embd_head_v    = 128
0.00.059.690 I print_info: n_gqa            = 1
0.00.059.691 I print_info: n_embd_k_gqa     = 2048
0.00.059.692 I print_info: n_embd_v_gqa     = 2048
0.00.059.693 I print_info: f_norm_eps       = 1.0e-05
0.00.059.693 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.693 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.693 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.695 I print_info: f_logit_scale    = 0.0e+00
0.00.059.696 I print_info: n_ff             = 8192
0.00.059.696 I print_info: n_expert         = 0
0.00.059.696 I print_info: n_expert_used    = 0
0.00.059.696 I print_info: causal attn      = 1
0.00.059.697 I print_info: pooling type     = 0
0.00.059.697 I print_info: rope type        = 2
0.00.059.697 I print_info: rope scaling     = linear
0.00.059.697 I print_info: freq_base_train  = 10000.0
0.00.059.697 I print_info: freq_scale_train = 1
0.00.059.698 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.698 I print_info: rope_finetuned   = unknown
0.00.059.698 I print_info: ssm_d_conv       = 0
0.00.059.698 I print_info: ssm_d_inner      = 0
0.00.059.702 I print_info: ssm_d_state      = 0
0.00.059.702 I print_info: ssm_dt_rank      = 0
0.00.059.704 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.704 I print_info: model type       = 1.4B
0.00.059.704 I print_info: model params     = 1.41 B
0.00.059.704 I print_info: general.name     = 1.4B
0.00.059.705 I print_info: vocab type       = BPE
0.00.059.705 I print_info: n_vocab          = 50304
0.00.059.705 I print_info: n_merges         = 50009
0.00.059.706 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.706 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.706 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.706 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.707 I print_info: LF token         = 128 'Ä'
0.00.059.707 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.707 I print_info: max token length = 1024
0.00.062.097 I load_tensors: offloading 24 repeating layers to GPU
0.00.062.098 I load_tensors: offloading output layer to GPU
0.00.062.098 I load_tensors: offloaded 25/25 layers to GPU
0.00.062.109 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.111 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.062.403 I llama_init_from_model: n_seq_max     = 1
0.00.062.404 I llama_init_from_model: n_ctx         = 2048
0.00.062.404 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.062.404 I llama_init_from_model: n_batch       = 2048
0.00.062.404 I llama_init_from_model: n_ubatch      = 512
0.00.062.404 I llama_init_from_model: flash_attn    = 0
0.00.062.405 I llama_init_from_model: freq_base     = 10000.0
0.00.062.405 I llama_init_from_model: freq_scale    = 1
0.00.062.406 I ggml_metal_init: allocating
0.00.062.409 I ggml_metal_init: found device: Apple M4
0.00.062.411 I ggml_metal_init: picking default device: Apple M4
0.00.063.167 I ggml_metal_init: using embedded metal library
0.00.065.738 I ggml_metal_init: GPU name:   Apple M4
0.00.065.739 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.740 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.740 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.741 I ggml_metal_init: simdgroup reduction   = true
0.00.065.741 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.741 I ggml_metal_init: has bfloat            = true
0.00.065.741 I ggml_metal_init: use bfloat            = true
0.00.065.742 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.742 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.173 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.101.083 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.101.097 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.101.123 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.102.389 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.102.390 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.102.391 I llama_init_from_model: graph nodes  = 967
0.00.102.391 I llama_init_from_model: graph splits = 2
0.00.102.396 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.102.525 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.525 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.398.011 I main: llama threadpool init, n_threads = 4
0.01.398.050 I 
0.01.398.077 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.398.077 I 
0.01.398.309 I sampler seed: 1234
0.01.398.313 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.398.351 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.398.355 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.398.355 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.484.999 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60528.56 tokens per second)
0.02.485.000 I llama_perf_context_print:        load time =    1388.26 ms
0.02.485.001 I llama_perf_context_print: prompt eval time =      46.13 ms /     7 tokens (    6.59 ms per token,   151.75 tokens per second)
0.02.485.001 I llama_perf_context_print:        eval time =    1037.59 ms /    63 runs   (   16.47 ms per token,    60.72 tokens per second)
0.02.485.002 I llama_perf_context_print:       total time =    1086.99 ms /    70 tokens
0.02.485.208 I ggml_metal_free: deallocating

real	0m2.503s
user	0m0.112s
sys	0m0.232s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.013.773 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.033.431 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.033.437 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.440 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.440 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.441 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.441 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.441 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.442 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.443 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.443 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.444 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.444 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.446 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.446 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.448 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.448 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.448 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.058 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.324 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.887 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.888 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.889 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.889 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.889 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.890 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.043.890 I llama_model_loader: - type  f32:  194 tensors
0.00.043.891 I llama_model_loader: - type q4_0:   97 tensors
0.00.043.891 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.892 I print_info: file format = GGUF V3 (latest)
0.00.043.892 I print_info: file type   = Q4_0
0.00.043.893 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.072.040 I load: special tokens cache size = 25
0.00.082.598 I load: token to piece cache size = 0.2984 MB
0.00.082.603 I print_info: arch             = gptneox
0.00.082.604 I print_info: vocab_only       = 0
0.00.082.604 I print_info: n_ctx_train      = 2048
0.00.082.604 I print_info: n_embd           = 2048
0.00.082.605 I print_info: n_layer          = 24
0.00.082.609 I print_info: n_head           = 16
0.00.082.610 I print_info: n_head_kv        = 16
0.00.082.610 I print_info: n_rot            = 32
0.00.082.610 I print_info: n_swa            = 0
0.00.082.611 I print_info: n_embd_head_k    = 128
0.00.082.611 I print_info: n_embd_head_v    = 128
0.00.082.612 I print_info: n_gqa            = 1
0.00.082.613 I print_info: n_embd_k_gqa     = 2048
0.00.082.614 I print_info: n_embd_v_gqa     = 2048
0.00.082.615 I print_info: f_norm_eps       = 1.0e-05
0.00.082.615 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.082.615 I print_info: f_clamp_kqv      = 0.0e+00
0.00.082.618 I print_info: f_max_alibi_bias = 0.0e+00
0.00.082.618 I print_info: f_logit_scale    = 0.0e+00
0.00.082.620 I print_info: n_ff             = 8192
0.00.082.620 I print_info: n_expert         = 0
0.00.082.620 I print_info: n_expert_used    = 0
0.00.082.620 I print_info: causal attn      = 1
0.00.082.620 I print_info: pooling type     = 0
0.00.082.621 I print_info: rope type        = 2
0.00.082.621 I print_info: rope scaling     = linear
0.00.082.621 I print_info: freq_base_train  = 10000.0
0.00.082.622 I print_info: freq_scale_train = 1
0.00.082.622 I print_info: n_ctx_orig_yarn  = 2048
0.00.082.624 I print_info: rope_finetuned   = unknown
0.00.082.624 I print_info: ssm_d_conv       = 0
0.00.082.624 I print_info: ssm_d_inner      = 0
0.00.082.625 I print_info: ssm_d_state      = 0
0.00.082.625 I print_info: ssm_dt_rank      = 0
0.00.082.625 I print_info: ssm_dt_b_c_rms   = 0
0.00.082.626 I print_info: model type       = 1.4B
0.00.082.626 I print_info: model params     = 1.41 B
0.00.082.627 I print_info: general.name     = 1.4B
0.00.082.627 I print_info: vocab type       = BPE
0.00.082.629 I print_info: n_vocab          = 50304
0.00.082.630 I print_info: n_merges         = 50009
0.00.082.630 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.082.630 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.082.630 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.082.631 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.082.633 I print_info: LF token         = 128 'Ä'
0.00.082.633 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.082.634 I print_info: max token length = 1024
0.00.085.764 I load_tensors: offloading 24 repeating layers to GPU
0.00.085.764 I load_tensors: offloading output layer to GPU
0.00.085.764 I load_tensors: offloaded 25/25 layers to GPU
0.00.085.776 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.085.778 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.086.195 I llama_init_from_model: n_seq_max     = 1
0.00.086.197 I llama_init_from_model: n_ctx         = 2048
0.00.086.197 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.086.197 I llama_init_from_model: n_batch       = 2048
0.00.086.198 I llama_init_from_model: n_ubatch      = 512
0.00.086.198 I llama_init_from_model: flash_attn    = 0
0.00.086.198 I llama_init_from_model: freq_base     = 10000.0
0.00.086.199 I llama_init_from_model: freq_scale    = 1
0.00.086.199 I ggml_metal_init: allocating
0.00.086.204 I ggml_metal_init: found device: Apple M4
0.00.086.206 I ggml_metal_init: picking default device: Apple M4
0.00.087.137 I ggml_metal_init: using embedded metal library
0.00.090.908 I ggml_metal_init: GPU name:   Apple M4
0.00.090.911 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.090.911 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.090.912 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.090.912 I ggml_metal_init: simdgroup reduction   = true
0.00.090.912 I ggml_metal_init: simdgroup matrix mul. = true
0.00.090.912 I ggml_metal_init: has bfloat            = true
0.00.090.912 I ggml_metal_init: use bfloat            = true
0.00.090.913 I ggml_metal_init: hasUnifiedMemory      = true
0.00.090.915 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.042 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.130.316 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.130.328 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.130.361 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.131.447 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.131.449 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.131.449 I llama_init_from_model: graph nodes  = 967
0.00.131.449 I llama_init_from_model: graph splits = 2
0.00.131.454 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.131.586 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.131.587 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.783.995 I main: llama threadpool init, n_threads = 4
0.00.784.047 I 
0.00.784.077 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.077 I 
0.00.784.345 I sampler seed: 1234
0.00.784.351 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.784.399 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.784.400 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.784.400 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.471.746 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60067.68 tokens per second)
0.01.471.746 I llama_perf_context_print:        load time =     770.21 ms
0.01.471.747 I llama_perf_context_print: prompt eval time =      46.74 ms /     7 tokens (    6.68 ms per token,   149.77 tokens per second)
0.01.471.748 I llama_perf_context_print:        eval time =     637.68 ms /    63 runs   (   10.12 ms per token,    98.80 tokens per second)
0.01.471.748 I llama_perf_context_print:       total time =     687.76 ms /    70 tokens
0.01.471.995 I ggml_metal_free: deallocating

real	0m1.496s
user	0m0.132s
sys	0m0.169s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.755 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.921 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.925 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.932 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.932 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.932 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.933 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.933 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.934 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.934 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.935 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.935 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.936 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.936 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.936 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.938 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.938 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.938 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.821 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.865 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.731 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.732 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.733 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.733 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.733 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.734 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.027.734 I llama_model_loader: - type  f32:  194 tensors
0.00.027.735 I llama_model_loader: - type q4_1:   97 tensors
0.00.027.735 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.736 I print_info: file format = GGUF V3 (latest)
0.00.027.736 I print_info: file type   = Q4_1
0.00.027.737 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.047.357 I load: special tokens cache size = 25
0.00.053.624 I load: token to piece cache size = 0.2984 MB
0.00.053.627 I print_info: arch             = gptneox
0.00.053.627 I print_info: vocab_only       = 0
0.00.053.627 I print_info: n_ctx_train      = 2048
0.00.053.627 I print_info: n_embd           = 2048
0.00.053.628 I print_info: n_layer          = 24
0.00.053.630 I print_info: n_head           = 16
0.00.053.631 I print_info: n_head_kv        = 16
0.00.053.631 I print_info: n_rot            = 32
0.00.053.634 I print_info: n_swa            = 0
0.00.053.634 I print_info: n_embd_head_k    = 128
0.00.053.634 I print_info: n_embd_head_v    = 128
0.00.053.635 I print_info: n_gqa            = 1
0.00.053.636 I print_info: n_embd_k_gqa     = 2048
0.00.053.636 I print_info: n_embd_v_gqa     = 2048
0.00.053.637 I print_info: f_norm_eps       = 1.0e-05
0.00.053.637 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.637 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.637 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.638 I print_info: f_logit_scale    = 0.0e+00
0.00.053.638 I print_info: n_ff             = 8192
0.00.053.638 I print_info: n_expert         = 0
0.00.053.639 I print_info: n_expert_used    = 0
0.00.053.639 I print_info: causal attn      = 1
0.00.053.639 I print_info: pooling type     = 0
0.00.053.639 I print_info: rope type        = 2
0.00.053.639 I print_info: rope scaling     = linear
0.00.053.640 I print_info: freq_base_train  = 10000.0
0.00.053.642 I print_info: freq_scale_train = 1
0.00.053.642 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.642 I print_info: rope_finetuned   = unknown
0.00.053.643 I print_info: ssm_d_conv       = 0
0.00.053.643 I print_info: ssm_d_inner      = 0
0.00.053.643 I print_info: ssm_d_state      = 0
0.00.053.643 I print_info: ssm_dt_rank      = 0
0.00.053.643 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.643 I print_info: model type       = 1.4B
0.00.053.644 I print_info: model params     = 1.41 B
0.00.053.644 I print_info: general.name     = 1.4B
0.00.053.645 I print_info: vocab type       = BPE
0.00.053.645 I print_info: n_vocab          = 50304
0.00.053.645 I print_info: n_merges         = 50009
0.00.053.645 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.645 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.645 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.646 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.646 I print_info: LF token         = 128 'Ä'
0.00.053.646 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.646 I print_info: max token length = 1024
0.00.055.654 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.654 I load_tensors: offloading output layer to GPU
0.00.055.655 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.665 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.055.666 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.055.992 I llama_init_from_model: n_seq_max     = 1
0.00.055.993 I llama_init_from_model: n_ctx         = 2048
0.00.055.993 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.993 I llama_init_from_model: n_batch       = 2048
0.00.055.993 I llama_init_from_model: n_ubatch      = 512
0.00.055.993 I llama_init_from_model: flash_attn    = 0
0.00.055.994 I llama_init_from_model: freq_base     = 10000.0
0.00.055.994 I llama_init_from_model: freq_scale    = 1
0.00.055.994 I ggml_metal_init: allocating
0.00.055.997 I ggml_metal_init: found device: Apple M4
0.00.055.999 I ggml_metal_init: picking default device: Apple M4
0.00.056.598 I ggml_metal_init: using embedded metal library
0.00.058.963 I ggml_metal_init: GPU name:   Apple M4
0.00.058.965 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.965 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.965 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.966 I ggml_metal_init: simdgroup reduction   = true
0.00.058.966 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.966 I ggml_metal_init: has bfloat            = true
0.00.058.966 I ggml_metal_init: use bfloat            = true
0.00.058.967 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.967 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.870 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.885 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.893 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.915 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.964 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.965 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.966 I llama_init_from_model: graph nodes  = 967
0.00.088.966 I llama_init_from_model: graph splits = 2
0.00.088.971 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.088 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.089 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.909.654 I main: llama threadpool init, n_threads = 4
0.00.909.694 I 
0.00.909.718 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.909.718 I 
0.00.909.938 I sampler seed: 1234
0.00.909.943 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.909.983 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.909.983 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.909.983 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.641.026 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62887.51 tokens per second)
0.01.641.027 I llama_perf_context_print:        load time =     900.89 ms
0.01.641.028 I llama_perf_context_print: prompt eval time =      39.57 ms /     7 tokens (    5.65 ms per token,   176.89 tokens per second)
0.01.641.028 I llama_perf_context_print:        eval time =     688.58 ms /    63 runs   (   10.93 ms per token,    91.49 tokens per second)
0.01.641.029 I llama_perf_context_print:       total time =     731.38 ms /    70 tokens
0.01.641.285 I ggml_metal_free: deallocating

real	0m1.657s
user	0m0.109s
sys	0m0.141s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.014.377 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.752 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.032.757 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.758 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.759 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.759 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.760 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.760 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.761 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.761 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.762 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.762 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.762 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.763 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.763 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.764 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.765 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.765 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.108 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.269 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.562 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.564 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.564 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.564 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.565 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.565 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.042.565 I llama_model_loader: - type  f32:  194 tensors
0.00.042.566 I llama_model_loader: - type q5_0:   97 tensors
0.00.042.566 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.566 I print_info: file format = GGUF V3 (latest)
0.00.042.567 I print_info: file type   = Q5_0
0.00.042.568 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.067.506 I load: special tokens cache size = 25
0.00.076.381 I load: token to piece cache size = 0.2984 MB
0.00.076.385 I print_info: arch             = gptneox
0.00.076.385 I print_info: vocab_only       = 0
0.00.076.386 I print_info: n_ctx_train      = 2048
0.00.076.386 I print_info: n_embd           = 2048
0.00.076.386 I print_info: n_layer          = 24
0.00.076.390 I print_info: n_head           = 16
0.00.076.391 I print_info: n_head_kv        = 16
0.00.076.392 I print_info: n_rot            = 32
0.00.076.392 I print_info: n_swa            = 0
0.00.076.392 I print_info: n_embd_head_k    = 128
0.00.076.392 I print_info: n_embd_head_v    = 128
0.00.076.393 I print_info: n_gqa            = 1
0.00.076.394 I print_info: n_embd_k_gqa     = 2048
0.00.076.395 I print_info: n_embd_v_gqa     = 2048
0.00.076.396 I print_info: f_norm_eps       = 1.0e-05
0.00.076.397 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.397 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.397 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.399 I print_info: f_logit_scale    = 0.0e+00
0.00.076.402 I print_info: n_ff             = 8192
0.00.076.402 I print_info: n_expert         = 0
0.00.076.403 I print_info: n_expert_used    = 0
0.00.076.403 I print_info: causal attn      = 1
0.00.076.403 I print_info: pooling type     = 0
0.00.076.403 I print_info: rope type        = 2
0.00.076.404 I print_info: rope scaling     = linear
0.00.076.404 I print_info: freq_base_train  = 10000.0
0.00.076.404 I print_info: freq_scale_train = 1
0.00.076.405 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.405 I print_info: rope_finetuned   = unknown
0.00.076.405 I print_info: ssm_d_conv       = 0
0.00.076.405 I print_info: ssm_d_inner      = 0
0.00.076.405 I print_info: ssm_d_state      = 0
0.00.076.406 I print_info: ssm_dt_rank      = 0
0.00.076.406 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.406 I print_info: model type       = 1.4B
0.00.076.406 I print_info: model params     = 1.41 B
0.00.076.407 I print_info: general.name     = 1.4B
0.00.076.407 I print_info: vocab type       = BPE
0.00.076.413 I print_info: n_vocab          = 50304
0.00.076.415 I print_info: n_merges         = 50009
0.00.076.416 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.416 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.416 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.417 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.417 I print_info: LF token         = 128 'Ä'
0.00.076.417 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.418 I print_info: max token length = 1024
0.00.079.261 I load_tensors: offloading 24 repeating layers to GPU
0.00.079.261 I load_tensors: offloading output layer to GPU
0.00.079.262 I load_tensors: offloaded 25/25 layers to GPU
0.00.079.273 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.079.275 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.079.707 I llama_init_from_model: n_seq_max     = 1
0.00.079.708 I llama_init_from_model: n_ctx         = 2048
0.00.079.708 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.079.709 I llama_init_from_model: n_batch       = 2048
0.00.079.709 I llama_init_from_model: n_ubatch      = 512
0.00.079.709 I llama_init_from_model: flash_attn    = 0
0.00.079.710 I llama_init_from_model: freq_base     = 10000.0
0.00.079.710 I llama_init_from_model: freq_scale    = 1
0.00.079.711 I ggml_metal_init: allocating
0.00.079.715 I ggml_metal_init: found device: Apple M4
0.00.079.718 I ggml_metal_init: picking default device: Apple M4
0.00.080.606 I ggml_metal_init: using embedded metal library
0.00.084.659 I ggml_metal_init: GPU name:   Apple M4
0.00.084.662 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.084.662 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.084.663 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.084.663 I ggml_metal_init: simdgroup reduction   = true
0.00.084.663 I ggml_metal_init: simdgroup matrix mul. = true
0.00.084.664 I ggml_metal_init: has bfloat            = true
0.00.084.664 I ggml_metal_init: use bfloat            = true
0.00.084.664 I ggml_metal_init: hasUnifiedMemory      = true
0.00.084.667 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.023 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.121.079 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.121.091 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.121.125 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.122.197 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.122.198 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.122.198 I llama_init_from_model: graph nodes  = 967
0.00.122.199 I llama_init_from_model: graph splits = 2
0.00.122.202 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.122.332 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.122.333 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.870.527 I main: llama threadpool init, n_threads = 4
0.00.870.568 I 
0.00.870.591 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.870.592 I 
0.00.870.815 I sampler seed: 1234
0.00.870.819 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.870.829 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.870.829 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.870.829 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.663.318 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54239.88 tokens per second)
0.01.663.318 I llama_perf_context_print:        load time =     856.15 ms
0.01.663.319 I llama_perf_context_print: prompt eval time =      42.84 ms /     7 tokens (    6.12 ms per token,   163.40 tokens per second)
0.01.663.321 I llama_perf_context_print:        eval time =     746.97 ms /    63 runs   (   11.86 ms per token,    84.34 tokens per second)
0.01.663.321 I llama_perf_context_print:       total time =     792.79 ms /    70 tokens
0.01.663.586 I ggml_metal_free: deallocating

real	0m1.685s
user	0m0.128s
sys	0m0.178s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.888 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.521 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.527 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.529 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.529 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.530 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.530 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.530 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.531 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.531 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.532 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.532 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.533 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.533 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.534 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.535 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.536 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.536 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.570 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.691 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.543 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.546 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.546 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.547 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.547 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.547 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.548 I llama_model_loader: - type  f32:  194 tensors
0.00.025.548 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.549 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.549 I print_info: file format = GGUF V3 (latest)
0.00.025.556 I print_info: file type   = Q5_1
0.00.025.558 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.374 I load: special tokens cache size = 25
0.00.051.615 I load: token to piece cache size = 0.2984 MB
0.00.051.620 I print_info: arch             = gptneox
0.00.051.620 I print_info: vocab_only       = 0
0.00.051.620 I print_info: n_ctx_train      = 2048
0.00.051.621 I print_info: n_embd           = 2048
0.00.051.621 I print_info: n_layer          = 24
0.00.051.625 I print_info: n_head           = 16
0.00.051.626 I print_info: n_head_kv        = 16
0.00.051.626 I print_info: n_rot            = 32
0.00.051.626 I print_info: n_swa            = 0
0.00.051.627 I print_info: n_embd_head_k    = 128
0.00.051.627 I print_info: n_embd_head_v    = 128
0.00.051.628 I print_info: n_gqa            = 1
0.00.051.628 I print_info: n_embd_k_gqa     = 2048
0.00.051.629 I print_info: n_embd_v_gqa     = 2048
0.00.051.631 I print_info: f_norm_eps       = 1.0e-05
0.00.051.631 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.631 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.631 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.632 I print_info: f_logit_scale    = 0.0e+00
0.00.051.632 I print_info: n_ff             = 8192
0.00.051.632 I print_info: n_expert         = 0
0.00.051.633 I print_info: n_expert_used    = 0
0.00.051.633 I print_info: causal attn      = 1
0.00.051.633 I print_info: pooling type     = 0
0.00.051.633 I print_info: rope type        = 2
0.00.051.636 I print_info: rope scaling     = linear
0.00.051.637 I print_info: freq_base_train  = 10000.0
0.00.051.638 I print_info: freq_scale_train = 1
0.00.051.638 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.638 I print_info: rope_finetuned   = unknown
0.00.051.638 I print_info: ssm_d_conv       = 0
0.00.051.639 I print_info: ssm_d_inner      = 0
0.00.051.639 I print_info: ssm_d_state      = 0
0.00.051.639 I print_info: ssm_dt_rank      = 0
0.00.051.639 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.639 I print_info: model type       = 1.4B
0.00.051.640 I print_info: model params     = 1.41 B
0.00.051.640 I print_info: general.name     = 1.4B
0.00.051.640 I print_info: vocab type       = BPE
0.00.051.640 I print_info: n_vocab          = 50304
0.00.051.640 I print_info: n_merges         = 50009
0.00.051.641 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.641 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.642 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.642 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.643 I print_info: LF token         = 128 'Ä'
0.00.051.643 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.643 I print_info: max token length = 1024
0.00.053.701 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.701 I load_tensors: offloading output layer to GPU
0.00.053.701 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.712 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.713 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.054.030 I llama_init_from_model: n_seq_max     = 1
0.00.054.031 I llama_init_from_model: n_ctx         = 2048
0.00.054.031 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.031 I llama_init_from_model: n_batch       = 2048
0.00.054.031 I llama_init_from_model: n_ubatch      = 512
0.00.054.032 I llama_init_from_model: flash_attn    = 0
0.00.054.032 I llama_init_from_model: freq_base     = 10000.0
0.00.054.032 I llama_init_from_model: freq_scale    = 1
0.00.054.033 I ggml_metal_init: allocating
0.00.054.036 I ggml_metal_init: found device: Apple M4
0.00.054.039 I ggml_metal_init: picking default device: Apple M4
0.00.054.661 I ggml_metal_init: using embedded metal library
0.00.057.032 I ggml_metal_init: GPU name:   Apple M4
0.00.057.033 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.034 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.034 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.034 I ggml_metal_init: simdgroup reduction   = true
0.00.057.035 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.035 I ggml_metal_init: has bfloat            = true
0.00.057.035 I ggml_metal_init: use bfloat            = true
0.00.057.035 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.036 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.329 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.541 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.550 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.569 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.532 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.533 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.533 I llama_init_from_model: graph nodes  = 967
0.00.088.533 I llama_init_from_model: graph splits = 2
0.00.088.537 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.665 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.666 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.779.613 I main: llama threadpool init, n_threads = 4
0.00.779.665 I 
0.00.779.685 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.779.686 I 
0.00.779.940 I sampler seed: 1234
0.00.779.945 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.779.984 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.779.985 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.779.985 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.617.269 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49824.56 tokens per second)
0.01.617.270 I llama_perf_context_print:        load time =     770.72 ms
0.01.617.271 I llama_perf_context_print: prompt eval time =      45.60 ms /     7 tokens (    6.51 ms per token,   153.50 tokens per second)
0.01.617.271 I llama_perf_context_print:        eval time =     788.89 ms /    63 runs   (   12.52 ms per token,    79.86 tokens per second)
0.01.617.272 I llama_perf_context_print:       total time =     837.66 ms /    70 tokens
0.01.617.519 I ggml_metal_free: deallocating

real	0m1.636s
user	0m0.111s
sys	0m0.147s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.556 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.266 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.271 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.273 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.273 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.274 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.274 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.274 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.276 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.276 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.277 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.277 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.278 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.278 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.278 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.280 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.280 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.280 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.103 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.097 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.896 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.898 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.898 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.898 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.898 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.899 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.899 I llama_model_loader: - type  f32:  194 tensors
0.00.024.900 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.900 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.900 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.901 I print_info: file format = GGUF V3 (latest)
0.00.024.901 I print_info: file type   = Q2_K - Medium
0.00.024.902 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.784 I load: special tokens cache size = 25
0.00.049.714 I load: token to piece cache size = 0.2984 MB
0.00.049.717 I print_info: arch             = gptneox
0.00.049.717 I print_info: vocab_only       = 0
0.00.049.718 I print_info: n_ctx_train      = 2048
0.00.049.718 I print_info: n_embd           = 2048
0.00.049.718 I print_info: n_layer          = 24
0.00.049.721 I print_info: n_head           = 16
0.00.049.722 I print_info: n_head_kv        = 16
0.00.049.722 I print_info: n_rot            = 32
0.00.049.722 I print_info: n_swa            = 0
0.00.049.722 I print_info: n_embd_head_k    = 128
0.00.049.722 I print_info: n_embd_head_v    = 128
0.00.049.723 I print_info: n_gqa            = 1
0.00.049.724 I print_info: n_embd_k_gqa     = 2048
0.00.049.726 I print_info: n_embd_v_gqa     = 2048
0.00.049.726 I print_info: f_norm_eps       = 1.0e-05
0.00.049.727 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.727 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.728 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.728 I print_info: f_logit_scale    = 0.0e+00
0.00.049.728 I print_info: n_ff             = 8192
0.00.049.729 I print_info: n_expert         = 0
0.00.049.731 I print_info: n_expert_used    = 0
0.00.049.731 I print_info: causal attn      = 1
0.00.049.731 I print_info: pooling type     = 0
0.00.049.731 I print_info: rope type        = 2
0.00.049.731 I print_info: rope scaling     = linear
0.00.049.732 I print_info: freq_base_train  = 10000.0
0.00.049.732 I print_info: freq_scale_train = 1
0.00.049.732 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.732 I print_info: rope_finetuned   = unknown
0.00.049.733 I print_info: ssm_d_conv       = 0
0.00.049.733 I print_info: ssm_d_inner      = 0
0.00.049.733 I print_info: ssm_d_state      = 0
0.00.049.733 I print_info: ssm_dt_rank      = 0
0.00.049.733 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.733 I print_info: model type       = 1.4B
0.00.049.734 I print_info: model params     = 1.41 B
0.00.049.734 I print_info: general.name     = 1.4B
0.00.049.734 I print_info: vocab type       = BPE
0.00.049.735 I print_info: n_vocab          = 50304
0.00.049.735 I print_info: n_merges         = 50009
0.00.049.735 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.735 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.735 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.735 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.736 I print_info: LF token         = 128 'Ä'
0.00.049.740 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.740 I print_info: max token length = 1024
0.00.051.638 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.638 I load_tensors: offloading output layer to GPU
0.00.051.638 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.648 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.650 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.913 I llama_init_from_model: n_seq_max     = 1
0.00.051.914 I llama_init_from_model: n_ctx         = 2048
0.00.051.914 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.914 I llama_init_from_model: n_batch       = 2048
0.00.051.914 I llama_init_from_model: n_ubatch      = 512
0.00.051.914 I llama_init_from_model: flash_attn    = 0
0.00.051.915 I llama_init_from_model: freq_base     = 10000.0
0.00.051.915 I llama_init_from_model: freq_scale    = 1
0.00.051.916 I ggml_metal_init: allocating
0.00.051.919 I ggml_metal_init: found device: Apple M4
0.00.051.921 I ggml_metal_init: picking default device: Apple M4
0.00.052.519 I ggml_metal_init: using embedded metal library
0.00.054.885 I ggml_metal_init: GPU name:   Apple M4
0.00.054.886 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.886 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.887 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.887 I ggml_metal_init: simdgroup reduction   = true
0.00.054.887 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.887 I ggml_metal_init: has bfloat            = true
0.00.054.887 I ggml_metal_init: use bfloat            = true
0.00.054.888 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.888 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.776 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.756 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.761 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.781 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.911 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.912 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.913 I llama_init_from_model: graph nodes  = 967
0.00.085.913 I llama_init_from_model: graph splits = 2
0.00.085.916 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.046 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.047 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.429.123 I main: llama threadpool init, n_threads = 4
0.00.429.170 I 
0.00.429.192 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.429.192 I 
0.00.429.454 I sampler seed: 1234
0.00.429.459 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.429.497 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.429.501 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.429.501 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.112.522 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 63848.92 tokens per second)
0.01.112.523 I llama_perf_context_print:        load time =     419.56 ms
0.01.112.524 I llama_perf_context_print: prompt eval time =      42.66 ms /     7 tokens (    6.09 ms per token,   164.10 tokens per second)
0.01.112.524 I llama_perf_context_print:        eval time =     637.50 ms /    63 runs   (   10.12 ms per token,    98.82 tokens per second)
0.01.112.525 I llama_perf_context_print:       total time =     683.41 ms /    70 tokens
0.01.112.759 I ggml_metal_free: deallocating

real	0m1.129s
user	0m0.108s
sys	0m0.101s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.737 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.556 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.562 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.564 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.565 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.565 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.565 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.566 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.566 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.567 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.569 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.570 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.570 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.570 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.571 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.572 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.573 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.573 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.440 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.445 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.272 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.273 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.273 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.273 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.274 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.274 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.275 I llama_model_loader: - type  f32:  194 tensors
0.00.025.275 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.275 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.275 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.276 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.276 I print_info: file format = GGUF V3 (latest)
0.00.025.277 I print_info: file type   = Q3_K - Medium
0.00.025.277 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.839 I load: special tokens cache size = 25
0.00.050.865 I load: token to piece cache size = 0.2984 MB
0.00.050.868 I print_info: arch             = gptneox
0.00.050.868 I print_info: vocab_only       = 0
0.00.050.868 I print_info: n_ctx_train      = 2048
0.00.050.869 I print_info: n_embd           = 2048
0.00.050.869 I print_info: n_layer          = 24
0.00.050.871 I print_info: n_head           = 16
0.00.050.872 I print_info: n_head_kv        = 16
0.00.050.872 I print_info: n_rot            = 32
0.00.050.872 I print_info: n_swa            = 0
0.00.050.872 I print_info: n_embd_head_k    = 128
0.00.050.873 I print_info: n_embd_head_v    = 128
0.00.050.875 I print_info: n_gqa            = 1
0.00.050.876 I print_info: n_embd_k_gqa     = 2048
0.00.050.876 I print_info: n_embd_v_gqa     = 2048
0.00.050.882 I print_info: f_norm_eps       = 1.0e-05
0.00.050.882 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.882 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.883 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.883 I print_info: f_logit_scale    = 0.0e+00
0.00.050.884 I print_info: n_ff             = 8192
0.00.050.884 I print_info: n_expert         = 0
0.00.050.884 I print_info: n_expert_used    = 0
0.00.050.884 I print_info: causal attn      = 1
0.00.050.884 I print_info: pooling type     = 0
0.00.050.885 I print_info: rope type        = 2
0.00.050.885 I print_info: rope scaling     = linear
0.00.050.886 I print_info: freq_base_train  = 10000.0
0.00.050.886 I print_info: freq_scale_train = 1
0.00.050.886 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.888 I print_info: rope_finetuned   = unknown
0.00.050.888 I print_info: ssm_d_conv       = 0
0.00.050.888 I print_info: ssm_d_inner      = 0
0.00.050.888 I print_info: ssm_d_state      = 0
0.00.050.889 I print_info: ssm_dt_rank      = 0
0.00.050.889 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.889 I print_info: model type       = 1.4B
0.00.050.889 I print_info: model params     = 1.41 B
0.00.050.890 I print_info: general.name     = 1.4B
0.00.050.890 I print_info: vocab type       = BPE
0.00.050.891 I print_info: n_vocab          = 50304
0.00.050.891 I print_info: n_merges         = 50009
0.00.050.892 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.892 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.892 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.895 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.895 I print_info: LF token         = 128 'Ä'
0.00.050.895 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.895 I print_info: max token length = 1024
0.00.052.896 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.897 I load_tensors: offloading output layer to GPU
0.00.052.897 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.907 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.909 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.053.213 I llama_init_from_model: n_seq_max     = 1
0.00.053.214 I llama_init_from_model: n_ctx         = 2048
0.00.053.214 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.214 I llama_init_from_model: n_batch       = 2048
0.00.053.215 I llama_init_from_model: n_ubatch      = 512
0.00.053.215 I llama_init_from_model: flash_attn    = 0
0.00.053.215 I llama_init_from_model: freq_base     = 10000.0
0.00.053.215 I llama_init_from_model: freq_scale    = 1
0.00.053.216 I ggml_metal_init: allocating
0.00.053.219 I ggml_metal_init: found device: Apple M4
0.00.053.221 I ggml_metal_init: picking default device: Apple M4
0.00.053.826 I ggml_metal_init: using embedded metal library
0.00.056.161 I ggml_metal_init: GPU name:   Apple M4
0.00.056.162 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.162 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.163 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.163 I ggml_metal_init: simdgroup reduction   = true
0.00.056.163 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.163 I ggml_metal_init: has bfloat            = true
0.00.056.163 I ggml_metal_init: use bfloat            = true
0.00.056.164 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.164 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.957 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.587 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.592 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.614 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.606 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.608 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.608 I llama_init_from_model: graph nodes  = 967
0.00.086.608 I llama_init_from_model: graph splits = 2
0.00.086.612 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.748 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.749 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.549.138 I main: llama threadpool init, n_threads = 4
0.00.549.184 I 
0.00.549.213 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.549.214 I 
0.00.549.443 I sampler seed: 1234
0.00.549.448 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.549.459 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.549.459 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.549.461 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.293.130 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54868.62 tokens per second)
0.01.293.131 I llama_perf_context_print:        load time =     539.39 ms
0.01.293.132 I llama_perf_context_print: prompt eval time =      44.14 ms /     7 tokens (    6.31 ms per token,   158.58 tokens per second)
0.01.293.133 I llama_perf_context_print:        eval time =     696.32 ms /    63 runs   (   11.05 ms per token,    90.48 tokens per second)
0.01.293.133 I llama_perf_context_print:       total time =     744.00 ms /    70 tokens
0.01.293.367 I ggml_metal_free: deallocating

real	0m1.310s
user	0m0.111s
sys	0m0.122s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.860 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.581 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.586 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.588 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.588 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.588 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.589 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.589 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.590 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.590 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.591 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.591 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.592 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.592 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.592 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.594 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.594 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.594 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.374 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.421 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.160 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.161 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.162 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.162 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.162 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.163 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.163 I llama_model_loader: - type  f32:  194 tensors
0.00.024.163 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.164 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.164 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.165 I print_info: file format = GGUF V3 (latest)
0.00.024.165 I print_info: file type   = Q4_K - Medium
0.00.024.170 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.169 I load: special tokens cache size = 25
0.00.049.414 I load: token to piece cache size = 0.2984 MB
0.00.049.417 I print_info: arch             = gptneox
0.00.049.418 I print_info: vocab_only       = 0
0.00.049.418 I print_info: n_ctx_train      = 2048
0.00.049.418 I print_info: n_embd           = 2048
0.00.049.418 I print_info: n_layer          = 24
0.00.049.421 I print_info: n_head           = 16
0.00.049.422 I print_info: n_head_kv        = 16
0.00.049.422 I print_info: n_rot            = 32
0.00.049.422 I print_info: n_swa            = 0
0.00.049.422 I print_info: n_embd_head_k    = 128
0.00.049.422 I print_info: n_embd_head_v    = 128
0.00.049.423 I print_info: n_gqa            = 1
0.00.049.424 I print_info: n_embd_k_gqa     = 2048
0.00.049.424 I print_info: n_embd_v_gqa     = 2048
0.00.049.425 I print_info: f_norm_eps       = 1.0e-05
0.00.049.425 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.426 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.426 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.426 I print_info: f_logit_scale    = 0.0e+00
0.00.049.427 I print_info: n_ff             = 8192
0.00.049.427 I print_info: n_expert         = 0
0.00.049.427 I print_info: n_expert_used    = 0
0.00.049.427 I print_info: causal attn      = 1
0.00.049.429 I print_info: pooling type     = 0
0.00.049.431 I print_info: rope type        = 2
0.00.049.431 I print_info: rope scaling     = linear
0.00.049.431 I print_info: freq_base_train  = 10000.0
0.00.049.431 I print_info: freq_scale_train = 1
0.00.049.432 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.432 I print_info: rope_finetuned   = unknown
0.00.049.433 I print_info: ssm_d_conv       = 0
0.00.049.434 I print_info: ssm_d_inner      = 0
0.00.049.434 I print_info: ssm_d_state      = 0
0.00.049.434 I print_info: ssm_dt_rank      = 0
0.00.049.434 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.434 I print_info: model type       = 1.4B
0.00.049.434 I print_info: model params     = 1.41 B
0.00.049.436 I print_info: general.name     = 1.4B
0.00.049.436 I print_info: vocab type       = BPE
0.00.049.437 I print_info: n_vocab          = 50304
0.00.049.437 I print_info: n_merges         = 50009
0.00.049.437 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.437 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.437 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.438 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.438 I print_info: LF token         = 128 'Ä'
0.00.049.438 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.438 I print_info: max token length = 1024
0.00.051.447 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.447 I load_tensors: offloading output layer to GPU
0.00.051.447 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.457 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.459 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.820 I llama_init_from_model: n_seq_max     = 1
0.00.051.821 I llama_init_from_model: n_ctx         = 2048
0.00.051.821 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.821 I llama_init_from_model: n_batch       = 2048
0.00.051.822 I llama_init_from_model: n_ubatch      = 512
0.00.051.822 I llama_init_from_model: flash_attn    = 0
0.00.051.822 I llama_init_from_model: freq_base     = 10000.0
0.00.051.822 I llama_init_from_model: freq_scale    = 1
0.00.051.823 I ggml_metal_init: allocating
0.00.051.826 I ggml_metal_init: found device: Apple M4
0.00.051.827 I ggml_metal_init: picking default device: Apple M4
0.00.052.440 I ggml_metal_init: using embedded metal library
0.00.054.812 I ggml_metal_init: GPU name:   Apple M4
0.00.054.813 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.814 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.814 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.814 I ggml_metal_init: simdgroup reduction   = true
0.00.054.814 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.814 I ggml_metal_init: has bfloat            = true
0.00.054.815 I ggml_metal_init: use bfloat            = true
0.00.054.815 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.816 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.515 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.392 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.401 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.435 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.461 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.463 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.463 I llama_init_from_model: graph nodes  = 967
0.00.085.463 I llama_init_from_model: graph splits = 2
0.00.085.467 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.598 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.599 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.170 I main: llama threadpool init, n_threads = 4
0.00.629.211 I 
0.00.629.243 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.244 I 
0.00.629.477 I sampler seed: 1234
0.00.629.481 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.629.502 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.629.503 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.629.503 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.385.360 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57959.18 tokens per second)
0.01.385.360 I llama_perf_context_print:        load time =     620.31 ms
0.01.385.361 I llama_perf_context_print: prompt eval time =      47.04 ms /     7 tokens (    6.72 ms per token,   148.81 tokens per second)
0.01.385.362 I llama_perf_context_print:        eval time =     705.88 ms /    63 runs   (   11.20 ms per token,    89.25 tokens per second)
0.01.385.362 I llama_perf_context_print:       total time =     756.19 ms /    70 tokens
0.01.385.573 I ggml_metal_free: deallocating

real	0m1.401s
user	0m0.108s
sys	0m0.133s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.012.162 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.838 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.019.843 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.844 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.845 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.845 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.846 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.847 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.849 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.849 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.850 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.850 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.852 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.852 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.853 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.857 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.857 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.857 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.717 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.782 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.561 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.562 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.562 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.563 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.563 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.563 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.028.564 I llama_model_loader: - type  f32:  194 tensors
0.00.028.564 I llama_model_loader: - type q5_K:   61 tensors
0.00.028.564 I llama_model_loader: - type q6_K:   37 tensors
0.00.028.565 I print_info: file format = GGUF V3 (latest)
0.00.028.566 I print_info: file type   = Q5_K - Medium
0.00.028.566 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.047.531 I load: special tokens cache size = 25
0.00.053.423 I load: token to piece cache size = 0.2984 MB
0.00.053.426 I print_info: arch             = gptneox
0.00.053.426 I print_info: vocab_only       = 0
0.00.053.427 I print_info: n_ctx_train      = 2048
0.00.053.427 I print_info: n_embd           = 2048
0.00.053.427 I print_info: n_layer          = 24
0.00.053.430 I print_info: n_head           = 16
0.00.053.430 I print_info: n_head_kv        = 16
0.00.053.431 I print_info: n_rot            = 32
0.00.053.431 I print_info: n_swa            = 0
0.00.053.431 I print_info: n_embd_head_k    = 128
0.00.053.431 I print_info: n_embd_head_v    = 128
0.00.053.432 I print_info: n_gqa            = 1
0.00.053.433 I print_info: n_embd_k_gqa     = 2048
0.00.053.434 I print_info: n_embd_v_gqa     = 2048
0.00.053.434 I print_info: f_norm_eps       = 1.0e-05
0.00.053.436 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.437 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.437 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.437 I print_info: f_logit_scale    = 0.0e+00
0.00.053.438 I print_info: n_ff             = 8192
0.00.053.438 I print_info: n_expert         = 0
0.00.053.439 I print_info: n_expert_used    = 0
0.00.053.439 I print_info: causal attn      = 1
0.00.053.440 I print_info: pooling type     = 0
0.00.053.441 I print_info: rope type        = 2
0.00.053.442 I print_info: rope scaling     = linear
0.00.053.442 I print_info: freq_base_train  = 10000.0
0.00.053.443 I print_info: freq_scale_train = 1
0.00.053.443 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.443 I print_info: rope_finetuned   = unknown
0.00.053.443 I print_info: ssm_d_conv       = 0
0.00.053.443 I print_info: ssm_d_inner      = 0
0.00.053.443 I print_info: ssm_d_state      = 0
0.00.053.444 I print_info: ssm_dt_rank      = 0
0.00.053.444 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.444 I print_info: model type       = 1.4B
0.00.053.445 I print_info: model params     = 1.41 B
0.00.053.449 I print_info: general.name     = 1.4B
0.00.053.449 I print_info: vocab type       = BPE
0.00.053.449 I print_info: n_vocab          = 50304
0.00.053.449 I print_info: n_merges         = 50009
0.00.053.450 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.450 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.450 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.450 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.450 I print_info: LF token         = 128 'Ä'
0.00.053.451 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.451 I print_info: max token length = 1024
0.00.055.474 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.474 I load_tensors: offloading output layer to GPU
0.00.055.474 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.485 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.486 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.055.745 I llama_init_from_model: n_seq_max     = 1
0.00.055.746 I llama_init_from_model: n_ctx         = 2048
0.00.055.746 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.746 I llama_init_from_model: n_batch       = 2048
0.00.055.746 I llama_init_from_model: n_ubatch      = 512
0.00.055.746 I llama_init_from_model: flash_attn    = 0
0.00.055.747 I llama_init_from_model: freq_base     = 10000.0
0.00.055.747 I llama_init_from_model: freq_scale    = 1
0.00.055.748 I ggml_metal_init: allocating
0.00.055.750 I ggml_metal_init: found device: Apple M4
0.00.055.752 I ggml_metal_init: picking default device: Apple M4
0.00.056.362 I ggml_metal_init: using embedded metal library
0.00.058.688 I ggml_metal_init: GPU name:   Apple M4
0.00.058.689 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.690 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.690 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.690 I ggml_metal_init: simdgroup reduction   = true
0.00.058.690 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.691 I ggml_metal_init: has bfloat            = true
0.00.058.691 I ggml_metal_init: use bfloat            = true
0.00.058.691 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.692 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.408 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.969 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.974 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.992 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.145 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.146 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.146 I llama_init_from_model: graph nodes  = 967
0.00.089.147 I llama_init_from_model: graph splits = 2
0.00.089.149 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.289 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.290 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.695.129 I main: llama threadpool init, n_threads = 4
0.00.695.166 I 
0.00.695.187 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.695.187 I 
0.00.695.417 I sampler seed: 1234
0.00.695.422 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.695.467 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.695.467 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.695.468 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.545.323 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57073.95 tokens per second)
0.01.545.324 I llama_perf_context_print:        load time =     682.96 ms
0.01.545.324 I llama_perf_context_print: prompt eval time =      51.65 ms /     7 tokens (    7.38 ms per token,   135.53 tokens per second)
0.01.545.325 I llama_perf_context_print:        eval time =     795.13 ms /    63 runs   (   12.62 ms per token,    79.23 tokens per second)
0.01.545.325 I llama_perf_context_print:       total time =     850.20 ms /    70 tokens
0.01.545.553 I ggml_metal_free: deallocating

real	0m1.565s
user	0m0.109s
sys	0m0.151s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.635 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.296 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.300 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.302 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.303 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.303 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.303 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.304 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.306 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.307 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.307 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.308 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.308 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.308 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.313 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.315 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.315 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.317 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.153 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.189 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.003 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.004 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.005 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.005 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.005 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.006 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.006 I llama_model_loader: - type  f32:  194 tensors
0.00.024.006 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.007 I print_info: file format = GGUF V3 (latest)
0.00.024.008 I print_info: file type   = Q6_K
0.00.024.008 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.042.893 I load: special tokens cache size = 25
0.00.048.620 I load: token to piece cache size = 0.2984 MB
0.00.048.623 I print_info: arch             = gptneox
0.00.048.624 I print_info: vocab_only       = 0
0.00.048.624 I print_info: n_ctx_train      = 2048
0.00.048.624 I print_info: n_embd           = 2048
0.00.048.624 I print_info: n_layer          = 24
0.00.048.627 I print_info: n_head           = 16
0.00.048.628 I print_info: n_head_kv        = 16
0.00.048.628 I print_info: n_rot            = 32
0.00.048.628 I print_info: n_swa            = 0
0.00.048.628 I print_info: n_embd_head_k    = 128
0.00.048.628 I print_info: n_embd_head_v    = 128
0.00.048.629 I print_info: n_gqa            = 1
0.00.048.630 I print_info: n_embd_k_gqa     = 2048
0.00.048.630 I print_info: n_embd_v_gqa     = 2048
0.00.048.631 I print_info: f_norm_eps       = 1.0e-05
0.00.048.633 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.633 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.633 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.634 I print_info: f_logit_scale    = 0.0e+00
0.00.048.635 I print_info: n_ff             = 8192
0.00.048.635 I print_info: n_expert         = 0
0.00.048.635 I print_info: n_expert_used    = 0
0.00.048.635 I print_info: causal attn      = 1
0.00.048.635 I print_info: pooling type     = 0
0.00.048.635 I print_info: rope type        = 2
0.00.048.635 I print_info: rope scaling     = linear
0.00.048.637 I print_info: freq_base_train  = 10000.0
0.00.048.637 I print_info: freq_scale_train = 1
0.00.048.638 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.638 I print_info: rope_finetuned   = unknown
0.00.048.638 I print_info: ssm_d_conv       = 0
0.00.048.638 I print_info: ssm_d_inner      = 0
0.00.048.638 I print_info: ssm_d_state      = 0
0.00.048.638 I print_info: ssm_dt_rank      = 0
0.00.048.639 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.639 I print_info: model type       = 1.4B
0.00.048.639 I print_info: model params     = 1.41 B
0.00.048.639 I print_info: general.name     = 1.4B
0.00.048.640 I print_info: vocab type       = BPE
0.00.048.640 I print_info: n_vocab          = 50304
0.00.048.640 I print_info: n_merges         = 50009
0.00.048.640 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.644 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.644 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.645 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.645 I print_info: LF token         = 128 'Ä'
0.00.048.645 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.645 I print_info: max token length = 1024
0.00.050.706 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.706 I load_tensors: offloading output layer to GPU
0.00.050.706 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.717 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.718 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.050.984 I llama_init_from_model: n_seq_max     = 1
0.00.050.985 I llama_init_from_model: n_ctx         = 2048
0.00.050.985 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.050.985 I llama_init_from_model: n_batch       = 2048
0.00.050.985 I llama_init_from_model: n_ubatch      = 512
0.00.050.985 I llama_init_from_model: flash_attn    = 0
0.00.050.986 I llama_init_from_model: freq_base     = 10000.0
0.00.050.986 I llama_init_from_model: freq_scale    = 1
0.00.050.986 I ggml_metal_init: allocating
0.00.050.989 I ggml_metal_init: found device: Apple M4
0.00.050.991 I ggml_metal_init: picking default device: Apple M4
0.00.051.582 I ggml_metal_init: using embedded metal library
0.00.053.920 I ggml_metal_init: GPU name:   Apple M4
0.00.053.922 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.922 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.922 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.923 I ggml_metal_init: simdgroup reduction   = true
0.00.053.923 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.923 I ggml_metal_init: has bfloat            = true
0.00.053.923 I ggml_metal_init: use bfloat            = true
0.00.053.924 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.924 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.697 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.494 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.502 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.522 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.511 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.513 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.513 I llama_init_from_model: graph nodes  = 967
0.00.084.513 I llama_init_from_model: graph splits = 2
0.00.084.516 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.648 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.649 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.532 I main: llama threadpool init, n_threads = 4
0.00.743.610 I 
0.00.743.634 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.636 I 
0.00.743.875 I sampler seed: 1234
0.00.743.880 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.743.943 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.743.955 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.743.955 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.628.815 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54447.85 tokens per second)
0.01.628.815 I llama_perf_context_print:        load time =     734.89 ms
0.01.628.818 I llama_perf_context_print: prompt eval time =      54.37 ms /     7 tokens (    7.77 ms per token,   128.76 tokens per second)
0.01.628.819 I llama_perf_context_print:        eval time =     827.70 ms /    63 runs   (   13.14 ms per token,    76.11 tokens per second)
0.01.628.819 I llama_perf_context_print:       total time =     885.29 ms /    70 tokens
0.01.629.075 I ggml_metal_free: deallocating

real	0m1.645s
user	0m0.108s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.735 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.955 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.042 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.049 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.051 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.052 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.053 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.053 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.054 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.055 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.056 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.056 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.057 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.057 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.058 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.059 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.062 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.062 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.063 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.037 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.778 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.079 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.081 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.082 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.082 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.083 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.083 I llama_model_loader: - type  f32:  194 tensors
0.00.054.084 I llama_model_loader: - type  f16:   98 tensors
0.00.054.084 I print_info: file format = GGUF V3 (latest)
0.00.054.085 I print_info: file type   = all F32 (guessed)
0.00.054.086 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.079.254 I load: special tokens cache size = 25
0.00.085.424 I load: token to piece cache size = 0.2984 MB
0.00.085.427 I print_info: arch             = gptneox
0.00.085.428 I print_info: vocab_only       = 0
0.00.085.428 I print_info: n_ctx_train      = 2048
0.00.085.428 I print_info: n_embd           = 2048
0.00.085.428 I print_info: n_layer          = 24
0.00.085.431 I print_info: n_head           = 16
0.00.085.432 I print_info: n_head_kv        = 16
0.00.085.432 I print_info: n_rot            = 32
0.00.085.432 I print_info: n_swa            = 0
0.00.085.432 I print_info: n_embd_head_k    = 128
0.00.085.432 I print_info: n_embd_head_v    = 128
0.00.085.433 I print_info: n_gqa            = 1
0.00.085.434 I print_info: n_embd_k_gqa     = 2048
0.00.085.434 I print_info: n_embd_v_gqa     = 2048
0.00.085.435 I print_info: f_norm_eps       = 1.0e-05
0.00.085.435 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.435 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.436 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.436 I print_info: f_logit_scale    = 0.0e+00
0.00.085.436 I print_info: n_ff             = 8192
0.00.085.437 I print_info: n_expert         = 0
0.00.085.439 I print_info: n_expert_used    = 0
0.00.085.439 I print_info: causal attn      = 1
0.00.085.439 I print_info: pooling type     = 0
0.00.085.439 I print_info: rope type        = 2
0.00.085.439 I print_info: rope scaling     = linear
0.00.085.440 I print_info: freq_base_train  = 10000.0
0.00.085.440 I print_info: freq_scale_train = 1
0.00.085.440 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.440 I print_info: rope_finetuned   = unknown
0.00.085.440 I print_info: ssm_d_conv       = 0
0.00.085.440 I print_info: ssm_d_inner      = 0
0.00.085.441 I print_info: ssm_d_state      = 0
0.00.085.441 I print_info: ssm_dt_rank      = 0
0.00.085.441 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.441 I print_info: model type       = 1.4B
0.00.085.441 I print_info: model params     = 1.41 B
0.00.085.441 I print_info: general.name     = 1.4B
0.00.085.442 I print_info: vocab type       = BPE
0.00.085.442 I print_info: n_vocab          = 50304
0.00.085.443 I print_info: n_merges         = 50009
0.00.085.446 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.446 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.447 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.448 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.448 I print_info: LF token         = 128 'Ä'
0.00.085.448 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.448 I print_info: max token length = 1024
0.00.087.884 I load_tensors: offloading 24 repeating layers to GPU
0.00.087.884 I load_tensors: offloading output layer to GPU
0.00.087.884 I load_tensors: offloaded 25/25 layers to GPU
0.00.087.895 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.896 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.088.219 I llama_init_from_model: n_seq_max     = 1
0.00.088.220 I llama_init_from_model: n_ctx         = 128
0.00.088.220 I llama_init_from_model: n_ctx_per_seq = 128
0.00.088.220 I llama_init_from_model: n_batch       = 128
0.00.088.221 I llama_init_from_model: n_ubatch      = 128
0.00.088.221 I llama_init_from_model: flash_attn    = 0
0.00.088.221 I llama_init_from_model: freq_base     = 10000.0
0.00.088.221 I llama_init_from_model: freq_scale    = 1
0.00.088.222 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.222 I ggml_metal_init: allocating
0.00.088.225 I ggml_metal_init: found device: Apple M4
0.00.088.227 I ggml_metal_init: picking default device: Apple M4
0.00.088.825 I ggml_metal_init: using embedded metal library
0.00.091.331 I ggml_metal_init: GPU name:   Apple M4
0.00.091.333 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.333 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.334 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.334 I ggml_metal_init: simdgroup reduction   = true
0.00.091.334 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.334 I ggml_metal_init: has bfloat            = true
0.00.091.334 I ggml_metal_init: use bfloat            = true
0.00.091.335 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.335 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.462 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.101.725 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.101.731 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.101.744 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.102.747 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.102.748 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.102.748 I llama_init_from_model: graph nodes  = 967
0.00.102.749 I llama_init_from_model: graph splits = 2
0.00.102.750 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.102.750 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.396.746 I 
0.01.396.776 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.396.780 I perplexity: tokenizing the input ..
0.01.410.041 I perplexity: tokenization took 13.257 ms
0.01.410.046 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.532.174 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.533.876 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.533.948 I llama_perf_context_print:        load time =    1372.79 ms
0.01.533.949 I llama_perf_context_print: prompt eval time =     121.17 ms /   128 tokens (    0.95 ms per token,  1056.38 tokens per second)
0.01.533.951 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.533.952 I llama_perf_context_print:       total time =     137.20 ms /   129 tokens
0.01.534.631 I ggml_metal_free: deallocating

real	0m1.730s
user	0m0.119s
sys	0m0.253s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.132 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.377 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.177 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.184 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.186 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.187 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.188 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.188 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.189 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.193 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.193 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.194 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.194 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.195 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.195 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.200 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.202 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.203 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.203 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.228 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.602 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.040 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.042 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.042 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.043 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.043 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.044 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.045 I llama_model_loader: - type  f32:  194 tensors
0.00.036.045 I llama_model_loader: - type q8_0:   98 tensors
0.00.036.046 I print_info: file format = GGUF V3 (latest)
0.00.036.047 I print_info: file type   = Q8_0
0.00.036.049 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.061.205 I load: special tokens cache size = 25
0.00.067.377 I load: token to piece cache size = 0.2984 MB
0.00.067.380 I print_info: arch             = gptneox
0.00.067.381 I print_info: vocab_only       = 0
0.00.067.381 I print_info: n_ctx_train      = 2048
0.00.067.381 I print_info: n_embd           = 2048
0.00.067.381 I print_info: n_layer          = 24
0.00.067.386 I print_info: n_head           = 16
0.00.067.387 I print_info: n_head_kv        = 16
0.00.067.387 I print_info: n_rot            = 32
0.00.067.388 I print_info: n_swa            = 0
0.00.067.389 I print_info: n_embd_head_k    = 128
0.00.067.389 I print_info: n_embd_head_v    = 128
0.00.067.391 I print_info: n_gqa            = 1
0.00.067.392 I print_info: n_embd_k_gqa     = 2048
0.00.067.392 I print_info: n_embd_v_gqa     = 2048
0.00.067.393 I print_info: f_norm_eps       = 1.0e-05
0.00.067.398 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.067.399 I print_info: f_clamp_kqv      = 0.0e+00
0.00.067.399 I print_info: f_max_alibi_bias = 0.0e+00
0.00.067.399 I print_info: f_logit_scale    = 0.0e+00
0.00.067.403 I print_info: n_ff             = 8192
0.00.067.403 I print_info: n_expert         = 0
0.00.067.403 I print_info: n_expert_used    = 0
0.00.067.423 I print_info: causal attn      = 1
0.00.067.424 I print_info: pooling type     = 0
0.00.067.424 I print_info: rope type        = 2
0.00.067.425 I print_info: rope scaling     = linear
0.00.067.425 I print_info: freq_base_train  = 10000.0
0.00.067.426 I print_info: freq_scale_train = 1
0.00.067.426 I print_info: n_ctx_orig_yarn  = 2048
0.00.067.426 I print_info: rope_finetuned   = unknown
0.00.067.426 I print_info: ssm_d_conv       = 0
0.00.067.427 I print_info: ssm_d_inner      = 0
0.00.067.427 I print_info: ssm_d_state      = 0
0.00.067.427 I print_info: ssm_dt_rank      = 0
0.00.067.427 I print_info: ssm_dt_b_c_rms   = 0
0.00.067.427 I print_info: model type       = 1.4B
0.00.067.428 I print_info: model params     = 1.41 B
0.00.067.428 I print_info: general.name     = 1.4B
0.00.067.429 I print_info: vocab type       = BPE
0.00.067.429 I print_info: n_vocab          = 50304
0.00.067.429 I print_info: n_merges         = 50009
0.00.067.430 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.067.430 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.067.430 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.067.430 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.067.431 I print_info: LF token         = 128 'Ä'
0.00.067.431 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.067.431 I print_info: max token length = 1024
0.00.070.005 I load_tensors: offloading 24 repeating layers to GPU
0.00.070.005 I load_tensors: offloading output layer to GPU
0.00.070.005 I load_tensors: offloaded 25/25 layers to GPU
0.00.070.017 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.070.018 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.070.334 I llama_init_from_model: n_seq_max     = 1
0.00.070.334 I llama_init_from_model: n_ctx         = 128
0.00.070.335 I llama_init_from_model: n_ctx_per_seq = 128
0.00.070.335 I llama_init_from_model: n_batch       = 128
0.00.070.335 I llama_init_from_model: n_ubatch      = 128
0.00.070.335 I llama_init_from_model: flash_attn    = 0
0.00.070.336 I llama_init_from_model: freq_base     = 10000.0
0.00.070.336 I llama_init_from_model: freq_scale    = 1
0.00.070.336 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.070.337 I ggml_metal_init: allocating
0.00.070.340 I ggml_metal_init: found device: Apple M4
0.00.070.342 I ggml_metal_init: picking default device: Apple M4
0.00.071.093 I ggml_metal_init: using embedded metal library
0.00.073.889 I ggml_metal_init: GPU name:   Apple M4
0.00.073.891 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.891 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.892 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.892 I ggml_metal_init: simdgroup reduction   = true
0.00.073.892 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.892 I ggml_metal_init: has bfloat            = true
0.00.073.892 I ggml_metal_init: use bfloat            = true
0.00.073.893 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.893 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.158 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.640 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.084.643 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.084.660 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.636 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.085.637 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.085.638 I llama_init_from_model: graph nodes  = 967
0.00.085.638 I llama_init_from_model: graph splits = 2
0.00.085.639 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.085.640 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.004.690 I 
0.01.004.726 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.004.730 I perplexity: tokenizing the input ..
0.01.012.547 I perplexity: tokenization took 7.816 ms
0.01.012.558 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.137.371 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.138.628 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.138.652 I llama_perf_context_print:        load time =     992.31 ms
0.01.138.654 I llama_perf_context_print: prompt eval time =     124.52 ms /   128 tokens (    0.97 ms per token,  1027.99 tokens per second)
0.01.138.655 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.138.655 I llama_perf_context_print:       total time =     133.96 ms /   129 tokens
0.01.139.086 I ggml_metal_free: deallocating

real	0m1.157s
user	0m0.094s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.144 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.093 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.097 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.099 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.099 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.100 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.100 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.100 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.101 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.102 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.102 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.103 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.103 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.103 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.104 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.105 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.105 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.106 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.895 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.891 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.702 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.704 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.704 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.704 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.705 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.705 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.706 I llama_model_loader: - type  f32:  194 tensors
0.00.025.706 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.706 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.707 I print_info: file format = GGUF V3 (latest)
0.00.025.707 I print_info: file type   = Q4_0
0.00.025.708 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.353 I load: special tokens cache size = 25
0.00.050.496 I load: token to piece cache size = 0.2984 MB
0.00.050.500 I print_info: arch             = gptneox
0.00.050.500 I print_info: vocab_only       = 0
0.00.050.500 I print_info: n_ctx_train      = 2048
0.00.050.500 I print_info: n_embd           = 2048
0.00.050.501 I print_info: n_layer          = 24
0.00.050.504 I print_info: n_head           = 16
0.00.050.504 I print_info: n_head_kv        = 16
0.00.050.505 I print_info: n_rot            = 32
0.00.050.505 I print_info: n_swa            = 0
0.00.050.505 I print_info: n_embd_head_k    = 128
0.00.050.505 I print_info: n_embd_head_v    = 128
0.00.050.506 I print_info: n_gqa            = 1
0.00.050.506 I print_info: n_embd_k_gqa     = 2048
0.00.050.509 I print_info: n_embd_v_gqa     = 2048
0.00.050.510 I print_info: f_norm_eps       = 1.0e-05
0.00.050.510 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.511 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.511 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.511 I print_info: f_logit_scale    = 0.0e+00
0.00.050.512 I print_info: n_ff             = 8192
0.00.050.512 I print_info: n_expert         = 0
0.00.050.513 I print_info: n_expert_used    = 0
0.00.050.514 I print_info: causal attn      = 1
0.00.050.514 I print_info: pooling type     = 0
0.00.050.514 I print_info: rope type        = 2
0.00.050.514 I print_info: rope scaling     = linear
0.00.050.514 I print_info: freq_base_train  = 10000.0
0.00.050.515 I print_info: freq_scale_train = 1
0.00.050.515 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.515 I print_info: rope_finetuned   = unknown
0.00.050.515 I print_info: ssm_d_conv       = 0
0.00.050.515 I print_info: ssm_d_inner      = 0
0.00.050.516 I print_info: ssm_d_state      = 0
0.00.050.516 I print_info: ssm_dt_rank      = 0
0.00.050.516 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.516 I print_info: model type       = 1.4B
0.00.050.516 I print_info: model params     = 1.41 B
0.00.050.517 I print_info: general.name     = 1.4B
0.00.050.517 I print_info: vocab type       = BPE
0.00.050.518 I print_info: n_vocab          = 50304
0.00.050.518 I print_info: n_merges         = 50009
0.00.050.522 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.522 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.522 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.527 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.527 I print_info: LF token         = 128 'Ä'
0.00.050.528 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.528 I print_info: max token length = 1024
0.00.052.312 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.313 I load_tensors: offloading output layer to GPU
0.00.052.313 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.318 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.319 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.577 I llama_init_from_model: n_seq_max     = 1
0.00.052.578 I llama_init_from_model: n_ctx         = 128
0.00.052.578 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.579 I llama_init_from_model: n_batch       = 128
0.00.052.579 I llama_init_from_model: n_ubatch      = 128
0.00.052.579 I llama_init_from_model: flash_attn    = 0
0.00.052.579 I llama_init_from_model: freq_base     = 10000.0
0.00.052.579 I llama_init_from_model: freq_scale    = 1
0.00.052.580 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.580 I ggml_metal_init: allocating
0.00.052.583 I ggml_metal_init: found device: Apple M4
0.00.052.585 I ggml_metal_init: picking default device: Apple M4
0.00.053.136 I ggml_metal_init: using embedded metal library
0.00.055.496 I ggml_metal_init: GPU name:   Apple M4
0.00.055.497 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.498 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.498 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.498 I ggml_metal_init: simdgroup reduction   = true
0.00.055.498 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.499 I ggml_metal_init: has bfloat            = true
0.00.055.499 I ggml_metal_init: use bfloat            = true
0.00.055.499 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.500 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.138 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.414 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.416 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.440 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.323 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.324 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.325 I llama_init_from_model: graph nodes  = 967
0.00.068.325 I llama_init_from_model: graph splits = 2
0.00.068.326 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.326 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.628.759 I 
0.00.628.834 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.628.845 I perplexity: tokenizing the input ..
0.00.636.585 I perplexity: tokenization took 7.737 ms
0.00.636.588 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.759.260 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.760.438 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.760.460 I llama_perf_context_print:        load time =     618.61 ms
0.00.760.461 I llama_perf_context_print: prompt eval time =     122.45 ms /   128 tokens (    0.96 ms per token,  1045.36 tokens per second)
0.00.760.462 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.760.463 I llama_perf_context_print:       total time =     131.71 ms /   129 tokens
0.00.760.920 I ggml_metal_free: deallocating

real	0m0.777s
user	0m0.077s
sys	0m0.094s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.774 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.502 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.506 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.508 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.509 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.509 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.509 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.509 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.510 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.511 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.511 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.511 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.512 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.512 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.513 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.516 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.516 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.516 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.380 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.385 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.203 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.204 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.205 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.205 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.205 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.206 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.206 I llama_model_loader: - type  f32:  194 tensors
0.00.024.206 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.207 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.207 I print_info: file format = GGUF V3 (latest)
0.00.024.208 I print_info: file type   = Q4_1
0.00.024.209 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.042.851 I load: special tokens cache size = 25
0.00.048.939 I load: token to piece cache size = 0.2984 MB
0.00.048.942 I print_info: arch             = gptneox
0.00.048.943 I print_info: vocab_only       = 0
0.00.048.943 I print_info: n_ctx_train      = 2048
0.00.048.943 I print_info: n_embd           = 2048
0.00.048.943 I print_info: n_layer          = 24
0.00.048.946 I print_info: n_head           = 16
0.00.048.947 I print_info: n_head_kv        = 16
0.00.048.947 I print_info: n_rot            = 32
0.00.048.947 I print_info: n_swa            = 0
0.00.048.947 I print_info: n_embd_head_k    = 128
0.00.048.948 I print_info: n_embd_head_v    = 128
0.00.048.948 I print_info: n_gqa            = 1
0.00.048.949 I print_info: n_embd_k_gqa     = 2048
0.00.048.950 I print_info: n_embd_v_gqa     = 2048
0.00.048.950 I print_info: f_norm_eps       = 1.0e-05
0.00.048.951 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.951 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.951 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.951 I print_info: f_logit_scale    = 0.0e+00
0.00.048.952 I print_info: n_ff             = 8192
0.00.048.952 I print_info: n_expert         = 0
0.00.048.952 I print_info: n_expert_used    = 0
0.00.048.952 I print_info: causal attn      = 1
0.00.048.952 I print_info: pooling type     = 0
0.00.048.953 I print_info: rope type        = 2
0.00.048.956 I print_info: rope scaling     = linear
0.00.048.956 I print_info: freq_base_train  = 10000.0
0.00.048.956 I print_info: freq_scale_train = 1
0.00.048.957 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.957 I print_info: rope_finetuned   = unknown
0.00.048.957 I print_info: ssm_d_conv       = 0
0.00.048.957 I print_info: ssm_d_inner      = 0
0.00.048.957 I print_info: ssm_d_state      = 0
0.00.048.957 I print_info: ssm_dt_rank      = 0
0.00.048.957 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.958 I print_info: model type       = 1.4B
0.00.048.958 I print_info: model params     = 1.41 B
0.00.048.958 I print_info: general.name     = 1.4B
0.00.048.965 I print_info: vocab type       = BPE
0.00.048.967 I print_info: n_vocab          = 50304
0.00.048.967 I print_info: n_merges         = 50009
0.00.048.967 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.967 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.968 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.968 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.968 I print_info: LF token         = 128 'Ä'
0.00.048.968 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.968 I print_info: max token length = 1024
0.00.050.950 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.950 I load_tensors: offloading output layer to GPU
0.00.050.950 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.960 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.962 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.222 I llama_init_from_model: n_seq_max     = 1
0.00.051.222 I llama_init_from_model: n_ctx         = 128
0.00.051.223 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.223 I llama_init_from_model: n_batch       = 128
0.00.051.223 I llama_init_from_model: n_ubatch      = 128
0.00.051.223 I llama_init_from_model: flash_attn    = 0
0.00.051.223 I llama_init_from_model: freq_base     = 10000.0
0.00.051.224 I llama_init_from_model: freq_scale    = 1
0.00.051.224 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.224 I ggml_metal_init: allocating
0.00.051.227 I ggml_metal_init: found device: Apple M4
0.00.051.229 I ggml_metal_init: picking default device: Apple M4
0.00.051.813 I ggml_metal_init: using embedded metal library
0.00.054.152 I ggml_metal_init: GPU name:   Apple M4
0.00.054.153 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.153 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.154 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.154 I ggml_metal_init: simdgroup reduction   = true
0.00.054.154 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.155 I ggml_metal_init: has bfloat            = true
0.00.054.155 I ggml_metal_init: use bfloat            = true
0.00.054.155 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.156 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.666 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.133 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.138 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.155 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.022 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.023 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.024 I llama_init_from_model: graph nodes  = 967
0.00.066.024 I llama_init_from_model: graph splits = 2
0.00.066.025 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.025 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.688.643 I 
0.00.688.678 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.688.682 I perplexity: tokenizing the input ..
0.00.696.502 I perplexity: tokenization took 7.818 ms
0.00.696.508 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.819.302 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.820.465 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.820.485 I llama_perf_context_print:        load time =     679.86 ms
0.00.820.486 I llama_perf_context_print: prompt eval time =     122.57 ms /   128 tokens (    0.96 ms per token,  1044.34 tokens per second)
0.00.820.487 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.820.488 I llama_perf_context_print:       total time =     131.85 ms /   129 tokens
0.00.820.973 I ggml_metal_free: deallocating

real	0m0.834s
user	0m0.077s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.855 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.809 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.813 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.815 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.816 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.816 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.816 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.817 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.817 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.818 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.820 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.820 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.820 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.821 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.821 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.825 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.825 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.826 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.644 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.697 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.520 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.521 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.521 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.522 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.522 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.522 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.523 I llama_model_loader: - type  f32:  194 tensors
0.00.024.523 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.523 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.524 I print_info: file format = GGUF V3 (latest)
0.00.024.524 I print_info: file type   = Q5_0
0.00.024.525 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.043.911 I load: special tokens cache size = 25
0.00.049.887 I load: token to piece cache size = 0.2984 MB
0.00.049.889 I print_info: arch             = gptneox
0.00.049.890 I print_info: vocab_only       = 0
0.00.049.890 I print_info: n_ctx_train      = 2048
0.00.049.890 I print_info: n_embd           = 2048
0.00.049.890 I print_info: n_layer          = 24
0.00.049.893 I print_info: n_head           = 16
0.00.049.894 I print_info: n_head_kv        = 16
0.00.049.894 I print_info: n_rot            = 32
0.00.049.894 I print_info: n_swa            = 0
0.00.049.894 I print_info: n_embd_head_k    = 128
0.00.049.895 I print_info: n_embd_head_v    = 128
0.00.049.895 I print_info: n_gqa            = 1
0.00.049.896 I print_info: n_embd_k_gqa     = 2048
0.00.049.897 I print_info: n_embd_v_gqa     = 2048
0.00.049.897 I print_info: f_norm_eps       = 1.0e-05
0.00.049.898 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.900 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.900 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.900 I print_info: f_logit_scale    = 0.0e+00
0.00.049.901 I print_info: n_ff             = 8192
0.00.049.901 I print_info: n_expert         = 0
0.00.049.901 I print_info: n_expert_used    = 0
0.00.049.901 I print_info: causal attn      = 1
0.00.049.903 I print_info: pooling type     = 0
0.00.049.903 I print_info: rope type        = 2
0.00.049.903 I print_info: rope scaling     = linear
0.00.049.905 I print_info: freq_base_train  = 10000.0
0.00.049.905 I print_info: freq_scale_train = 1
0.00.049.906 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.906 I print_info: rope_finetuned   = unknown
0.00.049.911 I print_info: ssm_d_conv       = 0
0.00.049.911 I print_info: ssm_d_inner      = 0
0.00.049.911 I print_info: ssm_d_state      = 0
0.00.049.911 I print_info: ssm_dt_rank      = 0
0.00.049.911 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.911 I print_info: model type       = 1.4B
0.00.049.912 I print_info: model params     = 1.41 B
0.00.049.912 I print_info: general.name     = 1.4B
0.00.049.912 I print_info: vocab type       = BPE
0.00.049.912 I print_info: n_vocab          = 50304
0.00.049.913 I print_info: n_merges         = 50009
0.00.049.913 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.913 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.913 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.913 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.913 I print_info: LF token         = 128 'Ä'
0.00.049.914 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.914 I print_info: max token length = 1024
0.00.051.928 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.928 I load_tensors: offloading output layer to GPU
0.00.051.929 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.939 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.940 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.208 I llama_init_from_model: n_seq_max     = 1
0.00.052.209 I llama_init_from_model: n_ctx         = 128
0.00.052.209 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.209 I llama_init_from_model: n_batch       = 128
0.00.052.210 I llama_init_from_model: n_ubatch      = 128
0.00.052.210 I llama_init_from_model: flash_attn    = 0
0.00.052.210 I llama_init_from_model: freq_base     = 10000.0
0.00.052.211 I llama_init_from_model: freq_scale    = 1
0.00.052.211 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.211 I ggml_metal_init: allocating
0.00.052.215 I ggml_metal_init: found device: Apple M4
0.00.052.217 I ggml_metal_init: picking default device: Apple M4
0.00.052.785 I ggml_metal_init: using embedded metal library
0.00.055.118 I ggml_metal_init: GPU name:   Apple M4
0.00.055.120 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.120 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.120 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.121 I ggml_metal_init: simdgroup reduction   = true
0.00.055.121 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.121 I ggml_metal_init: has bfloat            = true
0.00.055.121 I ggml_metal_init: use bfloat            = true
0.00.055.122 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.122 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.874 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.092 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.094 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.108 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.083 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.085 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.085 I llama_init_from_model: graph nodes  = 967
0.00.067.085 I llama_init_from_model: graph splits = 2
0.00.067.086 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.086 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.678 I 
0.00.686.707 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.710 I perplexity: tokenizing the input ..
0.00.694.682 I perplexity: tokenization took 7.971 ms
0.00.694.689 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.829.893 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.831.052 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.831.149 I llama_perf_context_print:        load time =     677.82 ms
0.00.831.150 I llama_perf_context_print: prompt eval time =     134.98 ms /   128 tokens (    1.05 ms per token,   948.30 tokens per second)
0.00.831.150 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.831.151 I llama_perf_context_print:       total time =     144.47 ms /   129 tokens
0.00.831.550 I ggml_metal_free: deallocating

real	0m0.845s
user	0m0.078s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.805 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.345 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.350 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.357 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.358 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.358 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.358 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.359 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.359 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.360 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.360 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.360 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.361 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.362 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.362 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.364 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.364 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.364 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.145 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.139 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.933 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.934 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.935 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.935 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.935 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.935 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.936 I llama_model_loader: - type  f32:  194 tensors
0.00.024.936 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.937 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.937 I print_info: file format = GGUF V3 (latest)
0.00.024.938 I print_info: file type   = Q5_1
0.00.024.938 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.621 I load: special tokens cache size = 25
0.00.049.653 I load: token to piece cache size = 0.2984 MB
0.00.049.656 I print_info: arch             = gptneox
0.00.049.656 I print_info: vocab_only       = 0
0.00.049.656 I print_info: n_ctx_train      = 2048
0.00.049.656 I print_info: n_embd           = 2048
0.00.049.656 I print_info: n_layer          = 24
0.00.049.660 I print_info: n_head           = 16
0.00.049.660 I print_info: n_head_kv        = 16
0.00.049.661 I print_info: n_rot            = 32
0.00.049.661 I print_info: n_swa            = 0
0.00.049.661 I print_info: n_embd_head_k    = 128
0.00.049.661 I print_info: n_embd_head_v    = 128
0.00.049.662 I print_info: n_gqa            = 1
0.00.049.663 I print_info: n_embd_k_gqa     = 2048
0.00.049.666 I print_info: n_embd_v_gqa     = 2048
0.00.049.666 I print_info: f_norm_eps       = 1.0e-05
0.00.049.667 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.667 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.667 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.667 I print_info: f_logit_scale    = 0.0e+00
0.00.049.668 I print_info: n_ff             = 8192
0.00.049.670 I print_info: n_expert         = 0
0.00.049.670 I print_info: n_expert_used    = 0
0.00.049.670 I print_info: causal attn      = 1
0.00.049.670 I print_info: pooling type     = 0
0.00.049.670 I print_info: rope type        = 2
0.00.049.670 I print_info: rope scaling     = linear
0.00.049.671 I print_info: freq_base_train  = 10000.0
0.00.049.671 I print_info: freq_scale_train = 1
0.00.049.671 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.671 I print_info: rope_finetuned   = unknown
0.00.049.672 I print_info: ssm_d_conv       = 0
0.00.049.672 I print_info: ssm_d_inner      = 0
0.00.049.672 I print_info: ssm_d_state      = 0
0.00.049.672 I print_info: ssm_dt_rank      = 0
0.00.049.672 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.672 I print_info: model type       = 1.4B
0.00.049.673 I print_info: model params     = 1.41 B
0.00.049.673 I print_info: general.name     = 1.4B
0.00.049.674 I print_info: vocab type       = BPE
0.00.049.674 I print_info: n_vocab          = 50304
0.00.049.674 I print_info: n_merges         = 50009
0.00.049.674 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.675 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.675 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.675 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.675 I print_info: LF token         = 128 'Ä'
0.00.049.675 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.676 I print_info: max token length = 1024
0.00.051.644 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.645 I load_tensors: offloading output layer to GPU
0.00.051.645 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.655 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.656 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.975 I llama_init_from_model: n_seq_max     = 1
0.00.051.976 I llama_init_from_model: n_ctx         = 128
0.00.051.976 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.976 I llama_init_from_model: n_batch       = 128
0.00.051.976 I llama_init_from_model: n_ubatch      = 128
0.00.051.976 I llama_init_from_model: flash_attn    = 0
0.00.051.977 I llama_init_from_model: freq_base     = 10000.0
0.00.051.977 I llama_init_from_model: freq_scale    = 1
0.00.051.977 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.978 I ggml_metal_init: allocating
0.00.051.981 I ggml_metal_init: found device: Apple M4
0.00.051.982 I ggml_metal_init: picking default device: Apple M4
0.00.052.548 I ggml_metal_init: using embedded metal library
0.00.054.873 I ggml_metal_init: GPU name:   Apple M4
0.00.054.874 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.874 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.875 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.875 I ggml_metal_init: simdgroup reduction   = true
0.00.054.875 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.875 I ggml_metal_init: has bfloat            = true
0.00.054.876 I ggml_metal_init: use bfloat            = true
0.00.054.876 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.877 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.405 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.610 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.612 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.625 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.562 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.563 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.563 I llama_init_from_model: graph nodes  = 967
0.00.066.564 I llama_init_from_model: graph splits = 2
0.00.066.565 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.565 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.654.640 I 
0.00.654.681 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.654.685 I perplexity: tokenizing the input ..
0.00.662.670 I perplexity: tokenization took 7.982 ms
0.00.662.674 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.732 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.798.873 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.798.904 I llama_perf_context_print:        load time =     644.83 ms
0.00.798.905 I llama_perf_context_print: prompt eval time =     134.83 ms /   128 tokens (    1.05 ms per token,   949.33 tokens per second)
0.00.798.906 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.906 I llama_perf_context_print:       total time =     144.27 ms /   129 tokens
0.00.799.361 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.077s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.983 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.798 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.805 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.807 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.807 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.808 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.808 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.808 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.811 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.812 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.812 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.813 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.814 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.814 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.815 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.816 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.818 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.819 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.590 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.578 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.332 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.333 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.333 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.334 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.334 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.334 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.335 I llama_model_loader: - type  f32:  194 tensors
0.00.024.335 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.336 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.336 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.336 I print_info: file format = GGUF V3 (latest)
0.00.024.337 I print_info: file type   = Q2_K - Medium
0.00.024.337 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.042.945 I load: special tokens cache size = 25
0.00.049.179 I load: token to piece cache size = 0.2984 MB
0.00.049.182 I print_info: arch             = gptneox
0.00.049.182 I print_info: vocab_only       = 0
0.00.049.182 I print_info: n_ctx_train      = 2048
0.00.049.182 I print_info: n_embd           = 2048
0.00.049.183 I print_info: n_layer          = 24
0.00.049.185 I print_info: n_head           = 16
0.00.049.186 I print_info: n_head_kv        = 16
0.00.049.186 I print_info: n_rot            = 32
0.00.049.186 I print_info: n_swa            = 0
0.00.049.186 I print_info: n_embd_head_k    = 128
0.00.049.187 I print_info: n_embd_head_v    = 128
0.00.049.187 I print_info: n_gqa            = 1
0.00.049.189 I print_info: n_embd_k_gqa     = 2048
0.00.049.190 I print_info: n_embd_v_gqa     = 2048
0.00.049.190 I print_info: f_norm_eps       = 1.0e-05
0.00.049.190 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.191 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.191 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.191 I print_info: f_logit_scale    = 0.0e+00
0.00.049.193 I print_info: n_ff             = 8192
0.00.049.193 I print_info: n_expert         = 0
0.00.049.193 I print_info: n_expert_used    = 0
0.00.049.194 I print_info: causal attn      = 1
0.00.049.194 I print_info: pooling type     = 0
0.00.049.194 I print_info: rope type        = 2
0.00.049.194 I print_info: rope scaling     = linear
0.00.049.196 I print_info: freq_base_train  = 10000.0
0.00.049.196 I print_info: freq_scale_train = 1
0.00.049.196 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.197 I print_info: rope_finetuned   = unknown
0.00.049.197 I print_info: ssm_d_conv       = 0
0.00.049.197 I print_info: ssm_d_inner      = 0
0.00.049.197 I print_info: ssm_d_state      = 0
0.00.049.197 I print_info: ssm_dt_rank      = 0
0.00.049.197 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.198 I print_info: model type       = 1.4B
0.00.049.198 I print_info: model params     = 1.41 B
0.00.049.202 I print_info: general.name     = 1.4B
0.00.049.203 I print_info: vocab type       = BPE
0.00.049.203 I print_info: n_vocab          = 50304
0.00.049.203 I print_info: n_merges         = 50009
0.00.049.203 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.204 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.204 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.205 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.205 I print_info: LF token         = 128 'Ä'
0.00.049.206 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.206 I print_info: max token length = 1024
0.00.051.058 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.058 I load_tensors: offloading output layer to GPU
0.00.051.058 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.068 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.069 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.328 I llama_init_from_model: n_seq_max     = 1
0.00.051.328 I llama_init_from_model: n_ctx         = 128
0.00.051.329 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.329 I llama_init_from_model: n_batch       = 128
0.00.051.329 I llama_init_from_model: n_ubatch      = 128
0.00.051.329 I llama_init_from_model: flash_attn    = 0
0.00.051.329 I llama_init_from_model: freq_base     = 10000.0
0.00.051.330 I llama_init_from_model: freq_scale    = 1
0.00.051.330 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.330 I ggml_metal_init: allocating
0.00.051.333 I ggml_metal_init: found device: Apple M4
0.00.051.335 I ggml_metal_init: picking default device: Apple M4
0.00.051.894 I ggml_metal_init: using embedded metal library
0.00.054.219 I ggml_metal_init: GPU name:   Apple M4
0.00.054.220 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.221 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.221 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.221 I ggml_metal_init: simdgroup reduction   = true
0.00.054.222 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.222 I ggml_metal_init: has bfloat            = true
0.00.054.222 I ggml_metal_init: use bfloat            = true
0.00.054.222 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.223 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.836 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.105 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.107 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.129 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.052 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.053 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.054 I llama_init_from_model: graph nodes  = 967
0.00.066.054 I llama_init_from_model: graph splits = 2
0.00.066.055 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.055 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.370.666 I 
0.00.370.692 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.370.700 I perplexity: tokenizing the input ..
0.00.377.944 I perplexity: tokenization took 7.243 ms
0.00.377.947 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.510.267 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.511.471 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.511.506 I llama_perf_context_print:        load time =     361.68 ms
0.00.511.507 I llama_perf_context_print: prompt eval time =     132.10 ms /   128 tokens (    1.03 ms per token,   968.98 tokens per second)
0.00.511.508 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.511.508 I llama_perf_context_print:       total time =     140.84 ms /   129 tokens
0.00.512.050 I ggml_metal_free: deallocating

real	0m0.525s
user	0m0.077s
sys	0m0.062s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.765 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.530 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.535 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.537 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.537 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.537 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.538 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.538 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.539 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.539 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.542 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.543 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.543 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.543 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.544 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.547 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.547 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.547 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.218 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.267 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.961 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.962 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.962 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.963 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.963 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.963 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.964 I llama_model_loader: - type  f32:  194 tensors
0.00.023.964 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.964 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.964 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.965 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.965 I print_info: file format = GGUF V3 (latest)
0.00.023.966 I print_info: file type   = Q3_K - Medium
0.00.023.967 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.042.642 I load: special tokens cache size = 25
0.00.048.842 I load: token to piece cache size = 0.2984 MB
0.00.048.845 I print_info: arch             = gptneox
0.00.048.845 I print_info: vocab_only       = 0
0.00.048.846 I print_info: n_ctx_train      = 2048
0.00.048.846 I print_info: n_embd           = 2048
0.00.048.846 I print_info: n_layer          = 24
0.00.048.849 I print_info: n_head           = 16
0.00.048.850 I print_info: n_head_kv        = 16
0.00.048.850 I print_info: n_rot            = 32
0.00.048.852 I print_info: n_swa            = 0
0.00.048.852 I print_info: n_embd_head_k    = 128
0.00.048.852 I print_info: n_embd_head_v    = 128
0.00.048.853 I print_info: n_gqa            = 1
0.00.048.853 I print_info: n_embd_k_gqa     = 2048
0.00.048.859 I print_info: n_embd_v_gqa     = 2048
0.00.048.860 I print_info: f_norm_eps       = 1.0e-05
0.00.048.860 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.860 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.861 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.861 I print_info: f_logit_scale    = 0.0e+00
0.00.048.864 I print_info: n_ff             = 8192
0.00.048.864 I print_info: n_expert         = 0
0.00.048.864 I print_info: n_expert_used    = 0
0.00.048.864 I print_info: causal attn      = 1
0.00.048.864 I print_info: pooling type     = 0
0.00.048.866 I print_info: rope type        = 2
0.00.048.867 I print_info: rope scaling     = linear
0.00.048.868 I print_info: freq_base_train  = 10000.0
0.00.048.868 I print_info: freq_scale_train = 1
0.00.048.868 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.868 I print_info: rope_finetuned   = unknown
0.00.048.868 I print_info: ssm_d_conv       = 0
0.00.048.868 I print_info: ssm_d_inner      = 0
0.00.048.869 I print_info: ssm_d_state      = 0
0.00.048.869 I print_info: ssm_dt_rank      = 0
0.00.048.869 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.869 I print_info: model type       = 1.4B
0.00.048.869 I print_info: model params     = 1.41 B
0.00.048.869 I print_info: general.name     = 1.4B
0.00.048.870 I print_info: vocab type       = BPE
0.00.048.870 I print_info: n_vocab          = 50304
0.00.048.870 I print_info: n_merges         = 50009
0.00.048.870 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.870 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.871 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.871 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.874 I print_info: LF token         = 128 'Ä'
0.00.048.874 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.875 I print_info: max token length = 1024
0.00.050.756 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.756 I load_tensors: offloading output layer to GPU
0.00.050.756 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.767 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.768 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.056 I llama_init_from_model: n_seq_max     = 1
0.00.051.057 I llama_init_from_model: n_ctx         = 128
0.00.051.057 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.057 I llama_init_from_model: n_batch       = 128
0.00.051.057 I llama_init_from_model: n_ubatch      = 128
0.00.051.058 I llama_init_from_model: flash_attn    = 0
0.00.051.058 I llama_init_from_model: freq_base     = 10000.0
0.00.051.058 I llama_init_from_model: freq_scale    = 1
0.00.051.059 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.059 I ggml_metal_init: allocating
0.00.051.062 I ggml_metal_init: found device: Apple M4
0.00.051.064 I ggml_metal_init: picking default device: Apple M4
0.00.051.645 I ggml_metal_init: using embedded metal library
0.00.053.962 I ggml_metal_init: GPU name:   Apple M4
0.00.053.964 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.964 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.964 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.965 I ggml_metal_init: simdgroup reduction   = true
0.00.053.965 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.965 I ggml_metal_init: has bfloat            = true
0.00.053.965 I ggml_metal_init: use bfloat            = true
0.00.053.965 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.966 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.490 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.732 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.735 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.748 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.674 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.675 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.675 I llama_init_from_model: graph nodes  = 967
0.00.065.676 I llama_init_from_model: graph splits = 2
0.00.065.677 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.677 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.475.316 I 
0.00.475.340 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.475.343 I perplexity: tokenizing the input ..
0.00.483.590 I perplexity: tokenization took 8.246 ms
0.00.483.597 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.615.077 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.616.296 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.616.318 I llama_perf_context_print:        load time =     466.55 ms
0.00.616.319 I llama_perf_context_print: prompt eval time =     131.23 ms /   128 tokens (    1.03 ms per token,   975.42 tokens per second)
0.00.616.319 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.616.320 I llama_perf_context_print:       total time =     141.00 ms /   129 tokens
0.00.616.650 I ggml_metal_free: deallocating

real	0m0.630s
user	0m0.077s
sys	0m0.085s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.544 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.228 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.233 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.239 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.239 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.240 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.240 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.241 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.242 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.243 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.243 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.244 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.244 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.244 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.245 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.246 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.248 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.248 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.024 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.052 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.854 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.855 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.855 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.856 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.856 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.856 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.857 I llama_model_loader: - type  f32:  194 tensors
0.00.024.857 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.858 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.858 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.858 I print_info: file format = GGUF V3 (latest)
0.00.024.859 I print_info: file type   = Q4_K - Medium
0.00.024.860 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.500 I load: special tokens cache size = 25
0.00.050.534 I load: token to piece cache size = 0.2984 MB
0.00.050.538 I print_info: arch             = gptneox
0.00.050.538 I print_info: vocab_only       = 0
0.00.050.538 I print_info: n_ctx_train      = 2048
0.00.050.538 I print_info: n_embd           = 2048
0.00.050.538 I print_info: n_layer          = 24
0.00.050.541 I print_info: n_head           = 16
0.00.050.542 I print_info: n_head_kv        = 16
0.00.050.543 I print_info: n_rot            = 32
0.00.050.543 I print_info: n_swa            = 0
0.00.050.544 I print_info: n_embd_head_k    = 128
0.00.050.544 I print_info: n_embd_head_v    = 128
0.00.050.545 I print_info: n_gqa            = 1
0.00.050.546 I print_info: n_embd_k_gqa     = 2048
0.00.050.546 I print_info: n_embd_v_gqa     = 2048
0.00.050.547 I print_info: f_norm_eps       = 1.0e-05
0.00.050.547 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.547 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.547 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.547 I print_info: f_logit_scale    = 0.0e+00
0.00.050.548 I print_info: n_ff             = 8192
0.00.050.548 I print_info: n_expert         = 0
0.00.050.550 I print_info: n_expert_used    = 0
0.00.050.550 I print_info: causal attn      = 1
0.00.050.550 I print_info: pooling type     = 0
0.00.050.551 I print_info: rope type        = 2
0.00.050.551 I print_info: rope scaling     = linear
0.00.050.551 I print_info: freq_base_train  = 10000.0
0.00.050.552 I print_info: freq_scale_train = 1
0.00.050.552 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.552 I print_info: rope_finetuned   = unknown
0.00.050.552 I print_info: ssm_d_conv       = 0
0.00.050.552 I print_info: ssm_d_inner      = 0
0.00.050.552 I print_info: ssm_d_state      = 0
0.00.050.553 I print_info: ssm_dt_rank      = 0
0.00.050.553 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.553 I print_info: model type       = 1.4B
0.00.050.554 I print_info: model params     = 1.41 B
0.00.050.554 I print_info: general.name     = 1.4B
0.00.050.554 I print_info: vocab type       = BPE
0.00.050.555 I print_info: n_vocab          = 50304
0.00.050.555 I print_info: n_merges         = 50009
0.00.050.555 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.555 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.557 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.557 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.557 I print_info: LF token         = 128 'Ä'
0.00.050.558 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.558 I print_info: max token length = 1024
0.00.052.561 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.561 I load_tensors: offloading output layer to GPU
0.00.052.561 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.572 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.573 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.883 I llama_init_from_model: n_seq_max     = 1
0.00.052.883 I llama_init_from_model: n_ctx         = 128
0.00.052.883 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.884 I llama_init_from_model: n_batch       = 128
0.00.052.884 I llama_init_from_model: n_ubatch      = 128
0.00.052.884 I llama_init_from_model: flash_attn    = 0
0.00.052.884 I llama_init_from_model: freq_base     = 10000.0
0.00.052.885 I llama_init_from_model: freq_scale    = 1
0.00.052.885 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.886 I ggml_metal_init: allocating
0.00.052.889 I ggml_metal_init: found device: Apple M4
0.00.052.891 I ggml_metal_init: picking default device: Apple M4
0.00.053.468 I ggml_metal_init: using embedded metal library
0.00.055.873 I ggml_metal_init: GPU name:   Apple M4
0.00.055.875 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.875 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.876 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.876 I ggml_metal_init: simdgroup reduction   = true
0.00.055.876 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.876 I ggml_metal_init: has bfloat            = true
0.00.055.877 I ggml_metal_init: use bfloat            = true
0.00.055.877 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.878 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.061 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.336 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.339 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.361 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.345 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.346 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.346 I llama_init_from_model: graph nodes  = 967
0.00.068.346 I llama_init_from_model: graph splits = 2
0.00.068.348 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.348 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.548.875 I 
0.00.548.901 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.548.905 I perplexity: tokenizing the input ..
0.00.557.202 I perplexity: tokenization took 8.296 ms
0.00.557.210 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.691.348 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.692.523 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.692.552 I llama_perf_context_print:        load time =     539.33 ms
0.00.692.553 I llama_perf_context_print: prompt eval time =     133.91 ms /   128 tokens (    1.05 ms per token,   955.85 tokens per second)
0.00.692.553 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.692.554 I llama_perf_context_print:       total time =     143.68 ms /   129 tokens
0.00.693.018 I ggml_metal_free: deallocating

real	0m0.709s
user	0m0.079s
sys	0m0.092s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.822 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.876 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.881 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.882 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.882 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.883 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.883 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.883 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.884 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.885 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.885 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.886 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.886 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.886 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.887 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.890 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.890 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.890 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.717 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.714 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.529 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.530 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.531 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.531 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.531 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.532 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.532 I llama_model_loader: - type  f32:  194 tensors
0.00.024.533 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.533 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.533 I print_info: file format = GGUF V3 (latest)
0.00.024.534 I print_info: file type   = Q5_K - Medium
0.00.024.535 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.926 I load: special tokens cache size = 25
0.00.050.166 I load: token to piece cache size = 0.2984 MB
0.00.050.169 I print_info: arch             = gptneox
0.00.050.169 I print_info: vocab_only       = 0
0.00.050.170 I print_info: n_ctx_train      = 2048
0.00.050.170 I print_info: n_embd           = 2048
0.00.050.170 I print_info: n_layer          = 24
0.00.050.173 I print_info: n_head           = 16
0.00.050.174 I print_info: n_head_kv        = 16
0.00.050.174 I print_info: n_rot            = 32
0.00.050.174 I print_info: n_swa            = 0
0.00.050.174 I print_info: n_embd_head_k    = 128
0.00.050.174 I print_info: n_embd_head_v    = 128
0.00.050.175 I print_info: n_gqa            = 1
0.00.050.176 I print_info: n_embd_k_gqa     = 2048
0.00.050.177 I print_info: n_embd_v_gqa     = 2048
0.00.050.177 I print_info: f_norm_eps       = 1.0e-05
0.00.050.178 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.178 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.178 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.178 I print_info: f_logit_scale    = 0.0e+00
0.00.050.180 I print_info: n_ff             = 8192
0.00.050.180 I print_info: n_expert         = 0
0.00.050.180 I print_info: n_expert_used    = 0
0.00.050.180 I print_info: causal attn      = 1
0.00.050.181 I print_info: pooling type     = 0
0.00.050.181 I print_info: rope type        = 2
0.00.050.181 I print_info: rope scaling     = linear
0.00.050.181 I print_info: freq_base_train  = 10000.0
0.00.050.182 I print_info: freq_scale_train = 1
0.00.050.182 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.182 I print_info: rope_finetuned   = unknown
0.00.050.184 I print_info: ssm_d_conv       = 0
0.00.050.184 I print_info: ssm_d_inner      = 0
0.00.050.184 I print_info: ssm_d_state      = 0
0.00.050.185 I print_info: ssm_dt_rank      = 0
0.00.050.185 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.185 I print_info: model type       = 1.4B
0.00.050.185 I print_info: model params     = 1.41 B
0.00.050.185 I print_info: general.name     = 1.4B
0.00.050.186 I print_info: vocab type       = BPE
0.00.050.186 I print_info: n_vocab          = 50304
0.00.050.186 I print_info: n_merges         = 50009
0.00.050.186 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.187 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.187 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.191 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.191 I print_info: LF token         = 128 'Ä'
0.00.050.191 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.191 I print_info: max token length = 1024
0.00.052.164 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.164 I load_tensors: offloading output layer to GPU
0.00.052.165 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.175 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.176 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.438 I llama_init_from_model: n_seq_max     = 1
0.00.052.439 I llama_init_from_model: n_ctx         = 128
0.00.052.439 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.439 I llama_init_from_model: n_batch       = 128
0.00.052.439 I llama_init_from_model: n_ubatch      = 128
0.00.052.440 I llama_init_from_model: flash_attn    = 0
0.00.052.440 I llama_init_from_model: freq_base     = 10000.0
0.00.052.440 I llama_init_from_model: freq_scale    = 1
0.00.052.441 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.441 I ggml_metal_init: allocating
0.00.052.444 I ggml_metal_init: found device: Apple M4
0.00.052.446 I ggml_metal_init: picking default device: Apple M4
0.00.053.018 I ggml_metal_init: using embedded metal library
0.00.055.372 I ggml_metal_init: GPU name:   Apple M4
0.00.055.374 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.374 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.374 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.375 I ggml_metal_init: simdgroup reduction   = true
0.00.055.375 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.375 I ggml_metal_init: has bfloat            = true
0.00.055.375 I ggml_metal_init: use bfloat            = true
0.00.055.375 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.376 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.076 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.357 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.359 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.385 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.343 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.344 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.345 I llama_init_from_model: graph nodes  = 967
0.00.067.345 I llama_init_from_model: graph splits = 2
0.00.067.346 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.346 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.646.229 I 
0.00.646.272 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.646.276 I perplexity: tokenizing the input ..
0.00.654.447 I perplexity: tokenization took 8.169 ms
0.00.654.457 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.794.363 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.796.381 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.796.401 I llama_perf_context_print:        load time =     637.40 ms
0.00.796.402 I llama_perf_context_print: prompt eval time =     139.68 ms /   128 tokens (    1.09 ms per token,   916.40 tokens per second)
0.00.796.403 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.404 I llama_perf_context_print:       total time =     150.18 ms /   129 tokens
0.00.796.744 I ggml_metal_free: deallocating

real	0m0.810s
user	0m0.080s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.741 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.454 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.460 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.466 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.467 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.467 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.468 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.468 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.469 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.469 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.469 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.470 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.470 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.471 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.471 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.473 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.474 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.474 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.255 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.310 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.184 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.187 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.187 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.188 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.188 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.189 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.189 I llama_model_loader: - type  f32:  194 tensors
0.00.025.195 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.196 I print_info: file format = GGUF V3 (latest)
0.00.025.197 I print_info: file type   = Q6_K
0.00.025.198 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.936 I load: special tokens cache size = 25
0.00.050.771 I load: token to piece cache size = 0.2984 MB
0.00.050.777 I print_info: arch             = gptneox
0.00.050.777 I print_info: vocab_only       = 0
0.00.050.777 I print_info: n_ctx_train      = 2048
0.00.050.777 I print_info: n_embd           = 2048
0.00.050.782 I print_info: n_layer          = 24
0.00.050.786 I print_info: n_head           = 16
0.00.050.787 I print_info: n_head_kv        = 16
0.00.050.787 I print_info: n_rot            = 32
0.00.050.787 I print_info: n_swa            = 0
0.00.050.787 I print_info: n_embd_head_k    = 128
0.00.050.787 I print_info: n_embd_head_v    = 128
0.00.050.788 I print_info: n_gqa            = 1
0.00.050.788 I print_info: n_embd_k_gqa     = 2048
0.00.050.789 I print_info: n_embd_v_gqa     = 2048
0.00.050.789 I print_info: f_norm_eps       = 1.0e-05
0.00.050.791 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.791 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.791 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.791 I print_info: f_logit_scale    = 0.0e+00
0.00.050.792 I print_info: n_ff             = 8192
0.00.050.792 I print_info: n_expert         = 0
0.00.050.792 I print_info: n_expert_used    = 0
0.00.050.792 I print_info: causal attn      = 1
0.00.050.793 I print_info: pooling type     = 0
0.00.050.793 I print_info: rope type        = 2
0.00.050.793 I print_info: rope scaling     = linear
0.00.050.793 I print_info: freq_base_train  = 10000.0
0.00.050.794 I print_info: freq_scale_train = 1
0.00.050.794 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.794 I print_info: rope_finetuned   = unknown
0.00.050.794 I print_info: ssm_d_conv       = 0
0.00.050.794 I print_info: ssm_d_inner      = 0
0.00.050.795 I print_info: ssm_d_state      = 0
0.00.050.795 I print_info: ssm_dt_rank      = 0
0.00.050.795 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.796 I print_info: model type       = 1.4B
0.00.050.797 I print_info: model params     = 1.41 B
0.00.050.797 I print_info: general.name     = 1.4B
0.00.050.798 I print_info: vocab type       = BPE
0.00.050.798 I print_info: n_vocab          = 50304
0.00.050.798 I print_info: n_merges         = 50009
0.00.050.798 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.798 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.798 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.798 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.799 I print_info: LF token         = 128 'Ä'
0.00.050.799 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.799 I print_info: max token length = 1024
0.00.052.848 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.848 I load_tensors: offloading output layer to GPU
0.00.052.849 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.860 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.861 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.133 I llama_init_from_model: n_seq_max     = 1
0.00.053.134 I llama_init_from_model: n_ctx         = 128
0.00.053.134 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.134 I llama_init_from_model: n_batch       = 128
0.00.053.135 I llama_init_from_model: n_ubatch      = 128
0.00.053.135 I llama_init_from_model: flash_attn    = 0
0.00.053.135 I llama_init_from_model: freq_base     = 10000.0
0.00.053.136 I llama_init_from_model: freq_scale    = 1
0.00.053.136 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.137 I ggml_metal_init: allocating
0.00.053.140 I ggml_metal_init: found device: Apple M4
0.00.053.142 I ggml_metal_init: picking default device: Apple M4
0.00.053.765 I ggml_metal_init: using embedded metal library
0.00.056.327 I ggml_metal_init: GPU name:   Apple M4
0.00.056.329 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.329 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.330 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.330 I ggml_metal_init: simdgroup reduction   = true
0.00.056.330 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.330 I ggml_metal_init: has bfloat            = true
0.00.056.330 I ggml_metal_init: use bfloat            = true
0.00.056.331 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.333 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.782 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.116 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.118 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.133 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.070 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.071 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.071 I llama_init_from_model: graph nodes  = 967
0.00.068.072 I llama_init_from_model: graph splits = 2
0.00.068.073 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.073 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.604.254 I 
0.00.604.278 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.604.282 I perplexity: tokenizing the input ..
0.00.612.541 I perplexity: tokenization took 8.258 ms
0.00.612.545 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.751.444 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.752.770 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.752.793 I llama_perf_context_print:        load time =     594.51 ms
0.00.752.794 I llama_perf_context_print: prompt eval time =     138.67 ms /   128 tokens (    1.08 ms per token,   923.04 tokens per second)
0.00.752.795 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.752.795 I llama_perf_context_print:       total time =     148.54 ms /   129 tokens
0.00.753.081 I ggml_metal_free: deallocating

real	0m0.766s
user	0m0.079s
sys	0m0.101s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.172 I build: 4492 (681149ce) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.869 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.281 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.031.293 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.295 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.296 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.304 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.305 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.306 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.309 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.310 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.311 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.314 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.314 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.315 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.316 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.318 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.318 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.319 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.559 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.820 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.711 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.047.715 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.716 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.716 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.716 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.717 I llama_model_loader: - type  f32:  194 tensors
0.00.047.717 I llama_model_loader: - type  f16:   98 tensors
0.00.047.718 I print_info: file format = GGUF V3 (latest)
0.00.047.719 I print_info: file type   = all F32 (guessed)
0.00.047.722 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.003 I load: special tokens cache size = 25
0.00.073.139 I load: token to piece cache size = 0.2984 MB
0.00.073.145 I print_info: arch             = gptneox
0.00.073.145 I print_info: vocab_only       = 0
0.00.073.145 I print_info: n_ctx_train      = 2048
0.00.073.146 I print_info: n_embd           = 2048
0.00.073.146 I print_info: n_layer          = 24
0.00.073.150 I print_info: n_head           = 16
0.00.073.151 I print_info: n_head_kv        = 16
0.00.073.151 I print_info: n_rot            = 32
0.00.073.154 I print_info: n_swa            = 0
0.00.073.154 I print_info: n_embd_head_k    = 128
0.00.073.154 I print_info: n_embd_head_v    = 128
0.00.073.155 I print_info: n_gqa            = 1
0.00.073.156 I print_info: n_embd_k_gqa     = 2048
0.00.073.156 I print_info: n_embd_v_gqa     = 2048
0.00.073.157 I print_info: f_norm_eps       = 1.0e-05
0.00.073.157 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.073.163 I print_info: f_clamp_kqv      = 0.0e+00
0.00.073.164 I print_info: f_max_alibi_bias = 0.0e+00
0.00.073.164 I print_info: f_logit_scale    = 0.0e+00
0.00.073.165 I print_info: n_ff             = 8192
0.00.073.165 I print_info: n_expert         = 0
0.00.073.165 I print_info: n_expert_used    = 0
0.00.073.165 I print_info: causal attn      = 1
0.00.073.165 I print_info: pooling type     = 0
0.00.073.165 I print_info: rope type        = 2
0.00.073.166 I print_info: rope scaling     = linear
0.00.073.166 I print_info: freq_base_train  = 10000.0
0.00.073.166 I print_info: freq_scale_train = 1
0.00.073.166 I print_info: n_ctx_orig_yarn  = 2048
0.00.073.167 I print_info: rope_finetuned   = unknown
0.00.073.168 I print_info: ssm_d_conv       = 0
0.00.073.168 I print_info: ssm_d_inner      = 0
0.00.073.168 I print_info: ssm_d_state      = 0
0.00.073.169 I print_info: ssm_dt_rank      = 0
0.00.073.169 I print_info: ssm_dt_b_c_rms   = 0
0.00.073.169 I print_info: model type       = 1.4B
0.00.073.169 I print_info: model params     = 1.41 B
0.00.073.169 I print_info: general.name     = 1.4B
0.00.073.170 I print_info: vocab type       = BPE
0.00.073.170 I print_info: n_vocab          = 50304
0.00.073.170 I print_info: n_merges         = 50009
0.00.073.170 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.073.171 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.073.171 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.073.171 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.073.171 I print_info: LF token         = 128 'Ä'
0.00.073.171 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.073.172 I print_info: max token length = 1024
0.00.075.629 I load_tensors: offloading 24 repeating layers to GPU
0.00.075.629 I load_tensors: offloading output layer to GPU
0.00.075.629 I load_tensors: offloaded 25/25 layers to GPU
0.00.075.640 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.075.642 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.075.880 I llama_init_from_model: n_seq_max     = 1
0.00.075.881 I llama_init_from_model: n_ctx         = 128
0.00.075.881 I llama_init_from_model: n_ctx_per_seq = 128
0.00.075.882 I llama_init_from_model: n_batch       = 128
0.00.075.882 I llama_init_from_model: n_ubatch      = 128
0.00.075.882 I llama_init_from_model: flash_attn    = 0
0.00.075.882 I llama_init_from_model: freq_base     = 10000.0
0.00.075.883 I llama_init_from_model: freq_scale    = 1
0.00.075.883 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.075.884 I ggml_metal_init: allocating
0.00.075.887 I ggml_metal_init: found device: Apple M4
0.00.075.889 I ggml_metal_init: picking default device: Apple M4
0.00.076.507 I ggml_metal_init: using embedded metal library
0.00.078.919 I ggml_metal_init: GPU name:   Apple M4
0.00.078.921 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.078.921 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.078.921 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.078.922 I ggml_metal_init: simdgroup reduction   = true
0.00.078.922 I ggml_metal_init: simdgroup matrix mul. = true
0.00.078.922 I ggml_metal_init: has bfloat            = true
0.00.078.922 I ggml_metal_init: use bfloat            = true
0.00.078.923 I ggml_metal_init: hasUnifiedMemory      = true
0.00.078.923 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.325 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.090.584 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.090.588 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.090.607 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.091.469 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.091.470 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.091.470 I llama_init_from_model: graph nodes  = 967
0.00.091.470 I llama_init_from_model: graph splits = 2
0.00.091.472 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.091.473 I 
0.00.091.506 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.091.507 I compute_imatrix: tokenizing the input ..
0.00.099.166 I compute_imatrix: tokenization took 7.657 ms
0.00.099.168 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.643.374 I compute_imatrix: 1.54 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.645.730 I llama_perf_context_print:        load time =    1626.50 ms
0.01.645.731 I llama_perf_context_print: prompt eval time =    1543.42 ms /   128 tokens (   12.06 ms per token,    82.93 tokens per second)
0.01.645.732 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.645.732 I llama_perf_context_print:       total time =    1628.85 ms /   129 tokens
0.01.646.324 I ggml_metal_free: deallocating

real	0m1.833s
user	0m0.155s
sys	0m0.243s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4492 (681149ce)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15630b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15630bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15630c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15630c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15630cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15630d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15630d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15630def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15630e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15630e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15630eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15630f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15630fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x156310670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x156310e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1563115a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x156311cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1563123e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x156312b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1563132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1563139f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x156314110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x156314830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1563150d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1563157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x156315ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1563160c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x156316d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x156317270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x156317530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1563179d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x156317c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x156318520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x156318a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x156318d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1563191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x156319660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x156319b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x156319fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15631a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15631a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15631ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15631b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15631b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15631b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15631bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15631c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15631cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15631d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15631dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15631e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15631e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15631ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15631f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15631fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15631ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x156320450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x156320710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x156320d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x156321510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1563217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x156321c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x156322110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1563225b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x156322a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x156322ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x156323390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x156323830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x156323cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x156324170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x156324610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x156324ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x156324f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1563254a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1563259f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x156325f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x156326490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1563269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x156326f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x156327480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1563279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x156327f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x156328470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1563289c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x156328f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x156329460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1563299b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x156329f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15632a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15632a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15632aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15632b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15632b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15632bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15632c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15632c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15632ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15631cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15632d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15632daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15632e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15632e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15632eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15632f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15632f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15632fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x156330020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x156330570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x156330ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x156331010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x156331560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x156331ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x156332000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1563324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x156332940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x156332de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x156333280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x156333720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x156333bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x156334060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x156334500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1563349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x156334e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1563352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x156335780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x156335c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1563360c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x156336560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x156336a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x156336ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x156337340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1563377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x156337c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x156338120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1563385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x156338a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x156338f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1563393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x156339840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x156339ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15633a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15633a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15633aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15633af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15633b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15633b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15633bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15633c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15633c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15633cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15633cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15633d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15633d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15633dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15633e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15633e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15633eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15633f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15633f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15633f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15633fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1563402a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x156340740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x156340be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x156341080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x156341520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1563419c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x156341e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x156342300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1563427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x156342c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1563430e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x156343580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x156343a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x156343ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x156344360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156344800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x156344ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x156345140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1563455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x156345a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x156345f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1563463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x156346860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x156346d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1563471a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x156347640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x156347ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x156347f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x156348420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1563488c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x156348d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x156349200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x156349750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x156349ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15634a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15634a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15634aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15634b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15634b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15634bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15634c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15634c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15634cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15634d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15634d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15634df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15634e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15634e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15634ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15634f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15634fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15634ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x156350510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x156350a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x156350fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x156351500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x156351a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x156351fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1563524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x156352a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x156352f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1563534e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x156353a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x156353f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1563544d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x156354a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x156354f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1563554c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x156355a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x156355f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1563564b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x156356a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x156356f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1563574a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1563579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x156357f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x156358490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1563589e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x156358f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x156359480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1563599d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x156359f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15635a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15635a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15635af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15635b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15635b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15635bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15635c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15635c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15635cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15635d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15635d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15635dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15635e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15635e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15635eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15635f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15635f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15635fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x156360410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x156360960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x156360eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x156361400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x156361950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x156361ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x156362340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1563627e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x156362c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x156363120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1563635c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x156363a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x156363f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1563643a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x156364840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x156364ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x156365180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x156365620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x156365ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x156365f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x156366400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x156366950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x156367070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x156367790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x156367eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1563685d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x156368890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x156369080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x156369340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x156369950 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.155.656 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.155.659 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x106f04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x106f04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x106f05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x106f05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x106f05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x106f06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x106f065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x106f06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x106f06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x106f07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x106f07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x106f07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x106f08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x106f09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x106f09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x106f0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x106f0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x106f0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x106f0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x106f0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x106f0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x106f0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x106f0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x106f0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x106f0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x106f0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x106f0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x106f0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x106f0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x106f0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x106f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x106f0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x106f10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x106f10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x106f108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x106f10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x106f11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x106f11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x106f11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x106f11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x106f12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x106f127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x106f12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x106f130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x106f13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x106f13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x106f13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x106f14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x106f146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x106f14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x106f14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x106f15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x106f15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x106f15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x106f16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x106f165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x106f16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x106f17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x106f174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x106f17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x106f17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x106f18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x106f18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x106f18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x106f18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x106f193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x106f19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x106f19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x106f1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x106f1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x106f1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x106f1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x106f1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x106f1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x106f1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x106f1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x106f1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x106f1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x106f1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x106f1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x106f1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x106f1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x106f1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x106f1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x106f1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x106f1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x106f1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x106f1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x106f1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x106f1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x106f202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x106f20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x106f20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x106f21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x106f21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x106f218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x106f21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x106f221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x106f22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x106f22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x106f22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x106f23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x106f23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x106f23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x106f240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x106f24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x106f249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x106f24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x106f252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x106f25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x106f25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x106f25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x106f26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x106f268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x106f26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x106f271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x106f27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x106f27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x106f27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x106f28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x106f287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x106f28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x106f290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x106f29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x106f299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x106f29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x106f2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x106f2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x106f2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x106f2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x106f2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x106f2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x106f2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x106f2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x106f2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x106f2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x106f2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x106f2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x106f2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x106f2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x106f2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x106f2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x106f2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x106f2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x106f2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x106f2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x106f2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x106f2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x106f30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x106f30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x106f30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x106f31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x106f315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x106f31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x106f31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x106f32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x106f327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x106f32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x106f33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x106f334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x106f33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x106f33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x106f34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x106f346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x106f34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x106f34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x106f35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x106f35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x106f36140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x106f365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x106f36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x106f36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x106f37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x106f37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x106f37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x106f38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x106f384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x106f38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x106f38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x106f39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x106f39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x106f39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x106f39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x106f3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x106f3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x106f3acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x106f3b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x106f3b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x106f3ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x106f3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x106f3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x106f3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x106f3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x106f3d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x106f3d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x106f3d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x106f3dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x106f3e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x106f3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x106f3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x106f3ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x106f3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x106f3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x106f3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x106f40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x106f40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x106f40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x106f40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x106f41500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x106f41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x106f42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x106f42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x106f42e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x106f433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x106f43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x106f43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x106f44500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x106f44ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x106f45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x106f45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x106f45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x106f461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x106f46780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x106f46d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x106f47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x106f478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x106f47e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x106f48440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x106f48a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x106f48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x106f49580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x106f49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x106f4a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x106f4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x106f4ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x106f4b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x106f4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x106f4bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x106f4c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x106f4c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x106f4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x106f4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x106f4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x106f4e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x106f4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x106f4ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x106f4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x106f4f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x106f4fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x106f502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x106f50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x106f50e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x106f51400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x106f519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x106f51f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x106f52540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x106f52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x106f530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x106f53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x106f53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x106f54200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x106f547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x106f54d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x106f55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x106f55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x106f55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x106f56480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x106f56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x106f56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x106f57440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x106f57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x106f57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x106f58340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x106f58840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x106f58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x106f59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x106f59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x106f59c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x106f5a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x106f5a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x106f5ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x106f5b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x106f5b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x106f5bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x106f5c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x106f5cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x106f5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x106f5d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x106f5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x106f5e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x106f5e830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x156409430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x156407240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x156409a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x156409ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15640a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15640a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15640ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15640b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15640b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15640bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15640c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15640c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15640d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15640dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15640e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15640eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15640f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15640f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x156410050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x156410820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x156410f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x156411660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x156411d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1564124a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x156412bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x156412e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x156413490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x156413aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1564140b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1564148a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x156414d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x156415000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x156415890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x156415dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x156416090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x156416530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1564169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x156416e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x156417310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1564177b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x156417c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1564180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x156418590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x156418a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x156418cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x156419300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x156419910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x156419f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15641a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15641ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15641b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15641b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15641bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15641c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15641cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15641d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15641d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15641d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15641dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15641e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15641ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15641eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15641f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15641f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15641fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x156420130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1564205d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x156420a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x156420f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1564213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x156421850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x156421cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x156422190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1564226e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x156422c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x156423180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1564236d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x156423c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x156424170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1564246c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x156424c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x156425160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1564256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x156425c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x156426150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1564266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x156426bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x156427140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x156427690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x156427be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x156428130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x156428680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x156428bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x156429120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x156429670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x156429bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15642a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15642a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15642abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15642b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15642b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15642bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15642c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15642c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15642cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15642d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15642d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15642db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15642e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15642e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15642eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15642f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15642f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15642fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15642ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1564303f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x156430890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x156430d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1564311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x156431670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x156431b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x156431fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x156432450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1564328f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x156432d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x156433230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1564336d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x156433b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x156434010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1564344b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x156434950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x156434df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x156435290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x156435730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x156435bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x156436070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x156436510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1564369b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x156436e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1564372f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x156437790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x156437c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1564380d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x156438570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x156438a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x156438eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x156439350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1564397f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x156439c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15643a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15643a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15643aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15643af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15643b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15643b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15643bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15643c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15643c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15643cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15643cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15643d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15643d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15643dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15643e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15643e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15643eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15643efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15643f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15643f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15643fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x156440250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1564406f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x156440b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x156441030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1564414d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x156441970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156441e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1564422b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x156442750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x156442bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x156443090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x156443530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1564439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x156443e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x156444310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1564447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x156444c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1564450f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x156445590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x156445a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x156445ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x156446370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x156446810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x156446d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1564472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x156447800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x156447d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x156448010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x156448620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x156448c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x156449240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x156449a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x156449ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15644a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15644a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15644adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15644b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15644ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15644bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15644c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15644cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15644d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15644d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15644db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15644e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15644e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15644eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15644f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15644f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15644fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x156450050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1564505a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x156450af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x156451040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x156451590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x156451ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x156452030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x156452580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x156452ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x156453020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x156453570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x156453ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x156454010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x156454560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x156454ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x156455000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x156455550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x156455aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x156455ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x156456540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x156456a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x156456fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x156457530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x156457a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x156457fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x156458520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x156458a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x156458fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x156459510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x156459a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x156459fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15645a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15645aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15645afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15645b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15645ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15645bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15645c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15645ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15645cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15645d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15645da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15645df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15645e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15645ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15645ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15645f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15645f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15645fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x156460290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x156460730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x156460bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x156461070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x156461510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1564619b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x156461e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1564622f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x156462790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x156462c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1564630d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x156463570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x156463a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x156463f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x156464680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x156464da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1564654c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x156465be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x156465ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x156466690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x156466950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x156466f60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.811s
user	0m0.288s
sys	0m0.313s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4492 (681149ce)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x129608490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x129608ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x129609150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x129609700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x129609cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12960a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12960a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12960adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12960b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12960b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12960bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12960c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12960cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12960d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12960dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12960e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12960eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12960f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12960f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1296101a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1296108c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x129610fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x129611700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x129611fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1296126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x129612980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x129612f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x129613c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x129614140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x129614400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1296148a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x129614b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1296153f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x129615930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x129615bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129616090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x129616530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1296169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x129616e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x129617310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1296177b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x129617c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1296180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x129618590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x129618850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x129618e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x129619470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x129619d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12961a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12961a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12961afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12961b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12961bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12961c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12961c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12961ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12961d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12961d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12961dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12961e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12961e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12961eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12961efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12961f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12961f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12961fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x129620260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x129620700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x129620ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x129621040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1296214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x129621980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129621e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x129622370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1296228c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x129622e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x129623360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1296238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x129623e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x129624350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1296248a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129624df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x129625340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x129625890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x129625de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x129626330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x129626880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x129626dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x129627320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x129627870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x129627dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x129628310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x129628860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x129628db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x129629300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x129629850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x129629da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x129619a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12962a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12962a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12962af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12962b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12962b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12962bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12962c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12962c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12962cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12962d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12962d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12962dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12962e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12962e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12962eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12962f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12962f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12962fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x129630150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1296305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129630a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129630f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1296313d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x129631870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129631d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1296321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x129632650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129632af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129632f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x129633430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1296338d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x129633d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x129634210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1296346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x129634b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x129634ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x129635490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x129635930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x129635dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x129636270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x129636710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x129636bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x129637050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1296374f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x129637990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x129637e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1296382d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x129638770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x129638c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1296390b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x129639550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1296399f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x129639e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12963a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12963a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12963ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12963b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12963b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12963ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12963bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12963c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12963c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12963ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12963d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12963d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12963dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12963df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12963e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12963e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12963ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12963f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12963f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12963fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12963ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x129640450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1296408f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129640d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x129641230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1296416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129641b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129642010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1296424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x129642950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129642df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129643290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x129643730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129643bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129644070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x129644510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1296449b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129644e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1296452f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x129645790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x129645c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1296460d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x129646620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129646b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1296470c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x129647610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1296478d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x129647ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1296484f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x129648b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1296492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x129649790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x129649a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12964a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12964a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12964ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12964b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12964b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12964bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12964c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12964c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12964ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12964d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12964d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12964de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12964e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12964e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12964ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12964f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12964f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12964fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1296503b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x129650900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x129650e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1296513a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1296518f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x129651e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x129652390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1296528e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x129652e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x129653380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1296538d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x129653e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x129654370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1296548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x129654e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x129655360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1296558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x129655e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x129656350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1296568a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x129656df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x129657340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x129657890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x129657de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x129658330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x129658880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x129658dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x129659320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x129659870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x129659dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12965a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12965a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12965adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12965b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12965b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12965bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12965c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12965c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12965cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12965d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12965d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12965dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12965e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12965e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12965ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12965f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12965f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12965fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12965fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x129660490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x129660930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129660dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x129661270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x129661710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x129661bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129662050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1296624f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x129662990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x129662e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1296632d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x129663820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x129663f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x129664660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x129664d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1296654a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x129665760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x129665f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x129666210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x129666820 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.087.667 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.678 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12970b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12970b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12970bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12970e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12970ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12970f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12970f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12970fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12970fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1297103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x129710810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x129710e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1297119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x129712160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x129712970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x129713090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1297137b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x129713ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1297145f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x129714dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1297154e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x129715c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x129716320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x129716a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x129717160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x129717420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1297176e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x129717b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x129717fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x129718430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1297188a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x129718dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x129719240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x129719500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x129719970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129719de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12971a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12971a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12971ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12971afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12971b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12971b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12971bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12971c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12971c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12971ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12971ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12971d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12971d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12971dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12971e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12971e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12971e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12971edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12971f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12971f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12971fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x129720110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x129720580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1297209f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x129720e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1297212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x129721740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x129721bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x129722020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x129722490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x129722900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x129722d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1297231e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x129723650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x129723ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x129723f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1297243a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x129724810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x129724c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1297250f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x129725560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1297259d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x129725e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1297262b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x129726720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129726b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x129727000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x129727470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1297278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x129727d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1297281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x129728630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x129728aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x129728f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x129729380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1297297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x129729c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12972a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12972a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12972a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12972ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12972b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12972b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12972bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12972bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12972c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12972c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12972cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12972d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12972d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12972da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12972def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12972e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12972e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12972ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12972f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12972f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12972f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12972fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x129730270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1297306e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129730b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129730fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129731430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1297318a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x129731d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129732180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1297325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x129732a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129732ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129733340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1297337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x129733c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x129734090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x129734500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x129734970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x129734de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x129735250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1297356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x129735b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x129735fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x129736410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x129736880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x129736cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x129737160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1297375d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x129737a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x129737eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x129738320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x129738790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x129738c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x129739070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1297394e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x129739950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x129739dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12973a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12973a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12973ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12973af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12973b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12973b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12973bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12973c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12973c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12973ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12973ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12973d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12973d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12973dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12973e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12973ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12973ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12973f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12973f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12973fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12973ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1297403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129740830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129740ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x129741110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129741580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1297419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129741e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1297422d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x129742740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129742bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129743020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x129743490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129743900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129743d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1297441e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129744650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129744ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x129744f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1297453a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x129745810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x129745c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1297460f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129746560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1297469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x129746e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1297472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x129747720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x129747b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x129748000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x129748470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1297489d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x129748ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x129749350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1297497c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x129749c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12974a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12974a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12974aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12974b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12974b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12974bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12974c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12974ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12974d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12974d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12974db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12974e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12974e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12974ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12974f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12974f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12974fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1297503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x129750980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x129750f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x129751500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x129751ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x129752080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x129752640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x129752c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1297531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x129753780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x129753d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x129754300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1297548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x129754e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x129755440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x129755a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x129755fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x129756580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x129756b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x129757100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1297576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x129757c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x129758240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x129758800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x129758dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x129759380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x129759940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x129759f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12975a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12975aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12975b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12975b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12975bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12975c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12975c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12975cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12975d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12975d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12975de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12975e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12975e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12975ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12975f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12975fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x129760000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x129760500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x129760a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x129760f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x129761400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129761900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x129761e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x129762300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x129762800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129762d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x129763200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x129763700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x129763c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x129764100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x129764600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x129765010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x129765730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x129765e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x129766570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x129766830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x129767020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1297672e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1297678f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12a8044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12a804950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12a804dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12a805230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12a8056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12a805b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12a805f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12a8063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12a806860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12a806cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12a807140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12a807810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12a808330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12a808ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12a8092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12a809a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12a80a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12a80a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12a80af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12a80b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12a80be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12a80c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12a80cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12a80d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12a80dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12a80dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12a80e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12a80e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12a80e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12a80edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12a80f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12a80f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12a80fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12a80fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12a8102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12a810760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12a810bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12a811040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12a8114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12a811920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12a811d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12a812200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12a812670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12a812ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12a812f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12a8133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12a813830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12a813ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12a814110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12a814580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12a8149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12a814e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12a8152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12a815740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12a815bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12a816020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12a816590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12a816a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12a816f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12a817370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12a8177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12a817c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12a8180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12a818530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12a8189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12a818e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12a819280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12a8196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12a819b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12a819fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12a81a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12a81a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12a81ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12a81b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12a81b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12a81ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12a81bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12a81c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12a81c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12a81cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12a81d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12a81d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12a81d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12a81ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12a81e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12a81e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12a81eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12a81efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12a81f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12a81f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12a81fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12a820170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12a8205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12a820a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12a820ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12a821330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12a8217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12a821c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12a822080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12a8224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12a822960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12a822dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12a823240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12a823ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12a823d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12a824200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12a824670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12a824ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12a824f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12a8253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12a825830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12a825ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12a826110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12a826580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12a8269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12a826e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12a8272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12a827740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12a827bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12a828020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12a828490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12a828900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12a828d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12a8291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12a829650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12a829ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12a829f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12a82a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12a82a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12a82ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12a82b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12a82b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12a82b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12a82be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12a82c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12a82c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12a82cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12a82d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12a82d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12a82d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12a82dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12a82e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12a82e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12a82eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12a82ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12a82f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12a82f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12a82fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12a8300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12a830540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12a8309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12a830e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12a831290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12a831700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12a831b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12a831fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12a832450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12a8328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12a832d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12a8331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12a833610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12a833a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12a833ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12a834360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12a8347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12a834c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12a8350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12a835520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12a835990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12a835e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12a836270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12a8366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12a836b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12a836fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12a837430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12a8378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12a837d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12a838180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12a8385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12a838a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12a838ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12a839340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12a8397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12a839c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12a83a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12a83a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12a83a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12a83ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12a83b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12a83b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12a83bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12a83bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12a83c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12a83c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12a83ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12a83d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12a83d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12a83da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12a83deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12a83e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12a83e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12a83ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12a83f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12a83f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12a83f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12a83fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12a840230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12a8406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12a840b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12a840f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12a841b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12a841dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12a842080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12a8424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12a842960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12a842dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12a843240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12a8436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12a843b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12a843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12a844400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12a844870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12a844ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12a845150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12a8455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12a845a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12a845ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12a846310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12a846780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12a846bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12a847060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12a8474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12a847940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12a847db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12a848220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12a848690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12a848b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12a848f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12a8493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12a849850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12a849cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12a84a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12a84a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12a84aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12a84ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12a84b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12a84b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12a84bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12a84c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12a84c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12a84c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12a84cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12a84d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12a84d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12a84dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12a84df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12a84e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12a84e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12a84eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12a84f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12a84f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12a84f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12a84fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12a8502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12a850740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12a850bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12a851020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12a851490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12a851900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12a851d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12a8521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12a852650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12a852ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12a852f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12a8533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12a853810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12a853c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12a8540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12a854560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12a8549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12a854e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12a8552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12a855720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12a856190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12a8568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12a856fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12a8576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12a8579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12a857e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12a858420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12a858a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.914s
user	0m0.241s
sys	0m0.134s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
