Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:321 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.8s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.099s
user	0m1.027s
sys	0m1.464s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target sha256
[  5%] Built target xxhash
[  5%] Built target sha1
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple-chat
[ 31%] Linking C executable ../bin/test-c
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Built target llava
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX static library libllava_static.a
[ 35%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llama-quantize-stats
[ 36%] Built target test-c
[ 36%] Built target llama-simple
[ 36%] Built target llama-simple-chat
[ 36%] Built target llava_static
[ 36%] Built target common
[ 36%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-chat
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-chat
[ 48%] Built target test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-grammar-integration
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 51%] Built target test-log
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-backend-ops
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-arg-parser
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-chat-template
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 59%] Linking CXX executable ../bin/test-gguf
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Built target test-backend-ops
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-chat-template
[ 62%] Built target test-arg-parser
[ 62%] Built target test-gguf
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-quantize-fns
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-autorelease
[ 63%] Built target test-barrier
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Built target test-quantize-perf
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-batched-bench
[ 72%] Built target llama-embedding
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-gguf-split
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-imatrix
[ 72%] Built target llama-gritlm
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 74%] Built target llama-batched
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Built target llama-infill
[ 75%] Linking CXX executable ../../bin/llama-bench
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-bench
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookup-create
[ 80%] Built target llama-lookahead
[ 80%] Built target llama-lookup
[ 80%] Generating loading.html.hpp
[ 80%] Built target llama-lookup-stats
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-parallel
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Generating index.html.gz.hpp
[ 82%] Built target llama-cli
[ 82%] Built target llama-perplexity
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 82%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 82%] Built target llama-passkey
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Linking CXX executable ../../bin/llama-run
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Built target llama-quantize
[ 90%] Built target llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-gen-docs
[ 90%] Built target llama-save-load-state
[ 90%] Built target llama-run
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-tokenize
[ 92%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 92%] Built target llama-speculative
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-tts
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Built target llama-gen-docs
[ 94%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 98%] Built target llama-export-lora
[ 98%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.213s
user	0m6.570s
sys	0m9.921s

main: quantize time =  5116.58 ms
main:    total time =  5116.58 ms

main: quantize time =  2686.04 ms
main:    total time =  2686.04 ms

main: quantize time =  2964.11 ms
main:    total time =  2964.11 ms

main: quantize time =  3332.51 ms
main:    total time =  3332.51 ms

main: quantize time =  2958.67 ms
main:    total time =  2958.67 ms

main: quantize time =  5456.23 ms
main:    total time =  5456.23 ms

main: quantize time =  5790.32 ms
main:    total time =  5790.32 ms

main: quantize time =  6864.28 ms
main:    total time =  6864.28 ms

main: quantize time =  5915.87 ms
main:    total time =  5915.87 ms

main: quantize time =  4588.46 ms
main:    total time =  4588.46 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.171 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.334 I main: llama backend init
0.00.000.341 I main: load the model and apply lora adapter, if any
0.00.047.118 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.060.406 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.060.432 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.060.436 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.060.437 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.060.438 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.060.439 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.060.440 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.060.444 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.060.445 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.060.445 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.060.446 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.060.447 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.060.448 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.060.449 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.060.454 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.060.454 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.060.455 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.067.664 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.070.013 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.078.705 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.078.713 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.078.714 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.078.715 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.078.715 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.078.717 I llama_model_loader: - type  f32:  194 tensors
0.00.078.718 I llama_model_loader: - type  f16:   98 tensors
0.00.078.719 I print_info: file format = GGUF V3 (latest)
0.00.078.723 I print_info: file type   = all F32 (guessed)
0.00.078.725 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.096.078 I load: special tokens cache size = 25
0.00.106.666 I load: token to piece cache size = 0.2984 MB
0.00.106.693 I print_info: arch             = gptneox
0.00.106.694 I print_info: vocab_only       = 0
0.00.106.695 I print_info: n_ctx_train      = 2048
0.00.106.695 I print_info: n_embd           = 2048
0.00.106.695 I print_info: n_layer          = 24
0.00.106.700 I print_info: n_head           = 16
0.00.106.701 I print_info: n_head_kv        = 16
0.00.106.701 I print_info: n_rot            = 32
0.00.106.701 I print_info: n_swa            = 0
0.00.106.701 I print_info: n_embd_head_k    = 128
0.00.106.704 I print_info: n_embd_head_v    = 128
0.00.106.705 I print_info: n_gqa            = 1
0.00.106.706 I print_info: n_embd_k_gqa     = 2048
0.00.106.707 I print_info: n_embd_v_gqa     = 2048
0.00.106.708 I print_info: f_norm_eps       = 1.0e-05
0.00.106.708 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.106.708 I print_info: f_clamp_kqv      = 0.0e+00
0.00.106.708 I print_info: f_max_alibi_bias = 0.0e+00
0.00.106.709 I print_info: f_logit_scale    = 0.0e+00
0.00.106.710 I print_info: n_ff             = 8192
0.00.106.710 I print_info: n_expert         = 0
0.00.106.710 I print_info: n_expert_used    = 0
0.00.106.710 I print_info: causal attn      = 1
0.00.106.711 I print_info: pooling type     = 0
0.00.106.712 I print_info: rope type        = 2
0.00.106.712 I print_info: rope scaling     = linear
0.00.106.713 I print_info: freq_base_train  = 10000.0
0.00.106.713 I print_info: freq_scale_train = 1
0.00.106.715 I print_info: n_ctx_orig_yarn  = 2048
0.00.106.715 I print_info: rope_finetuned   = unknown
0.00.106.715 I print_info: ssm_d_conv       = 0
0.00.106.715 I print_info: ssm_d_inner      = 0
0.00.106.716 I print_info: ssm_d_state      = 0
0.00.106.716 I print_info: ssm_dt_rank      = 0
0.00.106.716 I print_info: ssm_dt_b_c_rms   = 0
0.00.106.716 I print_info: model type       = 1.4B
0.00.106.717 I print_info: model params     = 1.41 B
0.00.106.718 I print_info: general.name     = 1.4B
0.00.106.720 I print_info: vocab type       = BPE
0.00.106.720 I print_info: n_vocab          = 50304
0.00.106.720 I print_info: n_merges         = 50009
0.00.106.721 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.106.722 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.106.723 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.106.723 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.106.723 I print_info: LF token         = 187 'Ċ'
0.00.106.724 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.106.724 I print_info: max token length = 1024
0.00.106.726 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.185.601 I load_tensors: offloading 24 repeating layers to GPU
0.00.185.605 I load_tensors: offloading output layer to GPU
0.00.185.605 I load_tensors: offloaded 25/25 layers to GPU
0.00.185.633 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.185.634 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.186.320 I llama_init_from_model: n_seq_max     = 1
0.00.186.321 I llama_init_from_model: n_ctx         = 2048
0.00.186.322 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.186.322 I llama_init_from_model: n_batch       = 2048
0.00.186.322 I llama_init_from_model: n_ubatch      = 512
0.00.186.322 I llama_init_from_model: flash_attn    = 0
0.00.186.323 I llama_init_from_model: freq_base     = 10000.0
0.00.186.323 I llama_init_from_model: freq_scale    = 1
0.00.186.326 I ggml_metal_init: allocating
0.00.186.396 I ggml_metal_init: found device: Apple M4
0.00.186.402 I ggml_metal_init: picking default device: Apple M4
0.00.187.084 I ggml_metal_init: using embedded metal library
0.00.462.564 I ggml_metal_init: GPU name:   Apple M4
0.00.462.577 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.462.578 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.462.578 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.462.579 I ggml_metal_init: simdgroup reduction   = true
0.00.462.580 I ggml_metal_init: simdgroup matrix mul. = true
0.00.462.580 I ggml_metal_init: has residency sets    = true
0.00.462.580 I ggml_metal_init: has bfloat            = true
0.00.462.580 I ggml_metal_init: use bfloat            = true
0.00.462.582 I ggml_metal_init: hasUnifiedMemory      = true
0.00.462.588 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.504.702 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.540.732 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.540.742 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.540.766 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.544.680 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.544.682 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.544.683 I llama_init_from_model: graph nodes  = 967
0.00.544.683 I llama_init_from_model: graph splits = 2
0.00.544.689 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.544.812 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.544.813 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.612.585 I main: llama threadpool init, n_threads = 4
0.00.612.623 I 
0.00.612.656 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.612.657 I 
0.00.612.857 I sampler seed: 1234
0.00.612.861 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.612.900 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.612.902 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.612.902 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.453.267 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56126.48 tokens per second)
0.02.453.268 I llama_perf_context_print:        load time =     564.35 ms
0.02.453.268 I llama_perf_context_print: prompt eval time =      44.01 ms /     7 tokens (    6.29 ms per token,   159.04 tokens per second)
0.02.453.269 I llama_perf_context_print:        eval time =    1793.34 ms /    63 runs   (   28.47 ms per token,    35.13 tokens per second)
0.02.453.270 I llama_perf_context_print:       total time =    1841.79 ms /    70 tokens
0.02.453.491 I ggml_metal_free: deallocating

real	0m2.798s
user	0m0.152s
sys	0m0.181s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.010.034 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.725 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.733 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.735 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.741 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.741 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.742 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.742 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.745 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.746 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.746 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.747 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.747 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.747 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.750 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.755 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.756 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.756 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.698 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.710 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.576 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.579 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.579 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.579 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.579 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.580 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.580 I llama_model_loader: - type  f32:  194 tensors
0.00.038.581 I llama_model_loader: - type q8_0:   98 tensors
0.00.038.581 I print_info: file format = GGUF V3 (latest)
0.00.038.582 I print_info: file type   = Q8_0
0.00.038.583 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.668 I load: special tokens cache size = 25
0.00.054.805 I load: token to piece cache size = 0.2984 MB
0.00.054.821 I print_info: arch             = gptneox
0.00.054.822 I print_info: vocab_only       = 0
0.00.054.822 I print_info: n_ctx_train      = 2048
0.00.054.822 I print_info: n_embd           = 2048
0.00.054.823 I print_info: n_layer          = 24
0.00.054.828 I print_info: n_head           = 16
0.00.054.828 I print_info: n_head_kv        = 16
0.00.054.829 I print_info: n_rot            = 32
0.00.054.829 I print_info: n_swa            = 0
0.00.054.829 I print_info: n_embd_head_k    = 128
0.00.054.830 I print_info: n_embd_head_v    = 128
0.00.054.831 I print_info: n_gqa            = 1
0.00.054.832 I print_info: n_embd_k_gqa     = 2048
0.00.054.832 I print_info: n_embd_v_gqa     = 2048
0.00.054.833 I print_info: f_norm_eps       = 1.0e-05
0.00.054.833 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.834 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.834 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.834 I print_info: f_logit_scale    = 0.0e+00
0.00.054.835 I print_info: n_ff             = 8192
0.00.054.835 I print_info: n_expert         = 0
0.00.054.835 I print_info: n_expert_used    = 0
0.00.054.835 I print_info: causal attn      = 1
0.00.054.835 I print_info: pooling type     = 0
0.00.054.835 I print_info: rope type        = 2
0.00.054.836 I print_info: rope scaling     = linear
0.00.054.838 I print_info: freq_base_train  = 10000.0
0.00.054.838 I print_info: freq_scale_train = 1
0.00.054.838 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.838 I print_info: rope_finetuned   = unknown
0.00.054.838 I print_info: ssm_d_conv       = 0
0.00.054.839 I print_info: ssm_d_inner      = 0
0.00.054.839 I print_info: ssm_d_state      = 0
0.00.054.839 I print_info: ssm_dt_rank      = 0
0.00.054.839 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.839 I print_info: model type       = 1.4B
0.00.054.840 I print_info: model params     = 1.41 B
0.00.054.840 I print_info: general.name     = 1.4B
0.00.054.841 I print_info: vocab type       = BPE
0.00.054.841 I print_info: n_vocab          = 50304
0.00.054.841 I print_info: n_merges         = 50009
0.00.054.841 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.841 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.842 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.842 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.842 I print_info: LF token         = 187 'Ċ'
0.00.054.842 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.843 I print_info: max token length = 1024
0.00.054.843 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.114.916 I load_tensors: offloading 24 repeating layers to GPU
0.01.114.920 I load_tensors: offloading output layer to GPU
0.01.114.922 I load_tensors: offloaded 25/25 layers to GPU
0.01.114.946 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.114.947 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.116.044 I llama_init_from_model: n_seq_max     = 1
0.01.116.046 I llama_init_from_model: n_ctx         = 2048
0.01.116.046 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.116.046 I llama_init_from_model: n_batch       = 2048
0.01.116.047 I llama_init_from_model: n_ubatch      = 512
0.01.116.047 I llama_init_from_model: flash_attn    = 0
0.01.116.048 I llama_init_from_model: freq_base     = 10000.0
0.01.116.049 I llama_init_from_model: freq_scale    = 1
0.01.116.049 I ggml_metal_init: allocating
0.01.116.059 I ggml_metal_init: found device: Apple M4
0.01.116.066 I ggml_metal_init: picking default device: Apple M4
0.01.117.184 I ggml_metal_init: using embedded metal library
0.01.122.748 I ggml_metal_init: GPU name:   Apple M4
0.01.122.752 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.122.752 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.122.753 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.122.754 I ggml_metal_init: simdgroup reduction   = true
0.01.122.754 I ggml_metal_init: simdgroup matrix mul. = true
0.01.122.754 I ggml_metal_init: has residency sets    = true
0.01.122.754 I ggml_metal_init: has bfloat            = true
0.01.122.755 I ggml_metal_init: use bfloat            = true
0.01.122.755 I ggml_metal_init: hasUnifiedMemory      = true
0.01.122.756 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.139.677 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.196.346 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.196.352 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.196.378 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.200.544 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.200.546 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.200.546 I llama_init_from_model: graph nodes  = 967
0.01.200.547 I llama_init_from_model: graph splits = 2
0.01.200.553 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.200.719 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.200.719 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.256.858 I main: llama threadpool init, n_threads = 4
0.01.256.907 I 
0.01.256.929 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.256.932 I 
0.01.257.110 I sampler seed: 1234
0.01.257.114 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.257.158 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.257.158 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.257.160 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.360.621 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53103.96 tokens per second)
0.02.360.622 I llama_perf_context_print:        load time =    1246.09 ms
0.02.360.623 I llama_perf_context_print: prompt eval time =      49.01 ms /     7 tokens (    7.00 ms per token,   142.82 tokens per second)
0.02.360.624 I llama_perf_context_print:        eval time =    1051.66 ms /    63 runs   (   16.69 ms per token,    59.91 tokens per second)
0.02.360.625 I llama_perf_context_print:       total time =    1104.50 ms /    70 tokens
0.02.360.862 I ggml_metal_free: deallocating

real	0m2.380s
user	0m0.111s
sys	0m0.277s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.011.129 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.688 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.694 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.696 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.697 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.697 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.697 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.698 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.700 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.700 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.700 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.701 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.702 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.702 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.702 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.704 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.704 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.705 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.508 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.494 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.259 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.260 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.260 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.261 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.261 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.261 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.262 I llama_model_loader: - type  f32:  194 tensors
0.00.027.263 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.263 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.264 I print_info: file format = GGUF V3 (latest)
0.00.027.264 I print_info: file type   = Q4_0
0.00.027.265 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.378 I load: special tokens cache size = 25
0.00.041.674 I load: token to piece cache size = 0.2984 MB
0.00.041.689 I print_info: arch             = gptneox
0.00.041.690 I print_info: vocab_only       = 0
0.00.041.690 I print_info: n_ctx_train      = 2048
0.00.041.690 I print_info: n_embd           = 2048
0.00.041.691 I print_info: n_layer          = 24
0.00.041.695 I print_info: n_head           = 16
0.00.041.696 I print_info: n_head_kv        = 16
0.00.041.696 I print_info: n_rot            = 32
0.00.041.696 I print_info: n_swa            = 0
0.00.041.696 I print_info: n_embd_head_k    = 128
0.00.041.699 I print_info: n_embd_head_v    = 128
0.00.041.700 I print_info: n_gqa            = 1
0.00.041.700 I print_info: n_embd_k_gqa     = 2048
0.00.041.701 I print_info: n_embd_v_gqa     = 2048
0.00.041.702 I print_info: f_norm_eps       = 1.0e-05
0.00.041.702 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.702 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.702 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.702 I print_info: f_logit_scale    = 0.0e+00
0.00.041.703 I print_info: n_ff             = 8192
0.00.041.703 I print_info: n_expert         = 0
0.00.041.703 I print_info: n_expert_used    = 0
0.00.041.703 I print_info: causal attn      = 1
0.00.041.703 I print_info: pooling type     = 0
0.00.041.703 I print_info: rope type        = 2
0.00.041.704 I print_info: rope scaling     = linear
0.00.041.704 I print_info: freq_base_train  = 10000.0
0.00.041.705 I print_info: freq_scale_train = 1
0.00.041.705 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.705 I print_info: rope_finetuned   = unknown
0.00.041.705 I print_info: ssm_d_conv       = 0
0.00.041.705 I print_info: ssm_d_inner      = 0
0.00.041.705 I print_info: ssm_d_state      = 0
0.00.041.706 I print_info: ssm_dt_rank      = 0
0.00.041.706 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.706 I print_info: model type       = 1.4B
0.00.041.706 I print_info: model params     = 1.41 B
0.00.041.707 I print_info: general.name     = 1.4B
0.00.041.708 I print_info: vocab type       = BPE
0.00.041.708 I print_info: n_vocab          = 50304
0.00.041.708 I print_info: n_merges         = 50009
0.00.041.708 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.709 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.709 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.709 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.709 I print_info: LF token         = 187 'Ċ'
0.00.041.710 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.711 I print_info: max token length = 1024
0.00.041.711 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.558.930 I load_tensors: offloading 24 repeating layers to GPU
0.00.558.942 I load_tensors: offloading output layer to GPU
0.00.558.943 I load_tensors: offloaded 25/25 layers to GPU
0.00.558.977 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.558.978 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.560.701 I llama_init_from_model: n_seq_max     = 1
0.00.560.704 I llama_init_from_model: n_ctx         = 2048
0.00.560.704 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.560.705 I llama_init_from_model: n_batch       = 2048
0.00.560.705 I llama_init_from_model: n_ubatch      = 512
0.00.560.706 I llama_init_from_model: flash_attn    = 0
0.00.560.707 I llama_init_from_model: freq_base     = 10000.0
0.00.560.708 I llama_init_from_model: freq_scale    = 1
0.00.560.718 I ggml_metal_init: allocating
0.00.560.808 I ggml_metal_init: found device: Apple M4
0.00.560.822 I ggml_metal_init: picking default device: Apple M4
0.00.562.402 I ggml_metal_init: using embedded metal library
0.00.568.439 I ggml_metal_init: GPU name:   Apple M4
0.00.568.444 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.568.445 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.568.446 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.568.446 I ggml_metal_init: simdgroup reduction   = true
0.00.568.447 I ggml_metal_init: simdgroup matrix mul. = true
0.00.568.447 I ggml_metal_init: has residency sets    = true
0.00.568.447 I ggml_metal_init: has bfloat            = true
0.00.568.448 I ggml_metal_init: use bfloat            = true
0.00.568.449 I ggml_metal_init: hasUnifiedMemory      = true
0.00.568.450 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.588.024 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.645.478 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.645.485 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.645.518 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.649.931 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.649.933 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.649.934 I llama_init_from_model: graph nodes  = 967
0.00.649.934 I llama_init_from_model: graph splits = 2
0.00.649.938 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.650.061 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.650.062 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.706.363 I main: llama threadpool init, n_threads = 4
0.00.706.414 I 
0.00.706.435 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.706.435 I 
0.00.706.604 I sampler seed: 1234
0.00.706.609 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.706.654 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.706.657 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.706.657 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.383.840 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51374.82 tokens per second)
0.01.383.841 I llama_perf_context_print:        load time =     694.51 ms
0.01.383.842 I llama_perf_context_print: prompt eval time =      48.68 ms /     7 tokens (    6.95 ms per token,   143.79 tokens per second)
0.01.383.842 I llama_perf_context_print:        eval time =     625.73 ms /    63 runs   (    9.93 ms per token,   100.68 tokens per second)
0.01.383.843 I llama_perf_context_print:       total time =     678.20 ms /    70 tokens
0.01.384.076 I ggml_metal_free: deallocating

real	0m1.406s
user	0m0.109s
sys	0m0.206s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.912 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.013 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.017 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.023 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.024 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.024 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.025 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.025 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.026 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.026 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.027 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.027 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.029 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.029 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.030 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.031 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.032 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.032 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.757 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.756 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.456 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.457 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.458 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.458 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.458 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.459 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.459 I llama_model_loader: - type  f32:  194 tensors
0.00.025.459 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.460 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.460 I print_info: file format = GGUF V3 (latest)
0.00.025.461 I print_info: file type   = Q4_1
0.00.025.461 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.571 I load: special tokens cache size = 25
0.00.040.046 I load: token to piece cache size = 0.2984 MB
0.00.040.059 I print_info: arch             = gptneox
0.00.040.060 I print_info: vocab_only       = 0
0.00.040.061 I print_info: n_ctx_train      = 2048
0.00.040.061 I print_info: n_embd           = 2048
0.00.040.061 I print_info: n_layer          = 24
0.00.040.063 I print_info: n_head           = 16
0.00.040.064 I print_info: n_head_kv        = 16
0.00.040.064 I print_info: n_rot            = 32
0.00.040.064 I print_info: n_swa            = 0
0.00.040.064 I print_info: n_embd_head_k    = 128
0.00.040.065 I print_info: n_embd_head_v    = 128
0.00.040.065 I print_info: n_gqa            = 1
0.00.040.066 I print_info: n_embd_k_gqa     = 2048
0.00.040.067 I print_info: n_embd_v_gqa     = 2048
0.00.040.067 I print_info: f_norm_eps       = 1.0e-05
0.00.040.068 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.068 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.068 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.068 I print_info: f_logit_scale    = 0.0e+00
0.00.040.071 I print_info: n_ff             = 8192
0.00.040.071 I print_info: n_expert         = 0
0.00.040.071 I print_info: n_expert_used    = 0
0.00.040.071 I print_info: causal attn      = 1
0.00.040.071 I print_info: pooling type     = 0
0.00.040.071 I print_info: rope type        = 2
0.00.040.072 I print_info: rope scaling     = linear
0.00.040.072 I print_info: freq_base_train  = 10000.0
0.00.040.072 I print_info: freq_scale_train = 1
0.00.040.072 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.072 I print_info: rope_finetuned   = unknown
0.00.040.073 I print_info: ssm_d_conv       = 0
0.00.040.073 I print_info: ssm_d_inner      = 0
0.00.040.073 I print_info: ssm_d_state      = 0
0.00.040.073 I print_info: ssm_dt_rank      = 0
0.00.040.073 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.073 I print_info: model type       = 1.4B
0.00.040.073 I print_info: model params     = 1.41 B
0.00.040.074 I print_info: general.name     = 1.4B
0.00.040.074 I print_info: vocab type       = BPE
0.00.040.075 I print_info: n_vocab          = 50304
0.00.040.075 I print_info: n_merges         = 50009
0.00.040.075 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.075 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.075 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.075 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.076 I print_info: LF token         = 187 'Ċ'
0.00.040.076 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.076 I print_info: max token length = 1024
0.00.040.076 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.628.149 I load_tensors: offloading 24 repeating layers to GPU
0.00.628.162 I load_tensors: offloading output layer to GPU
0.00.628.162 I load_tensors: offloaded 25/25 layers to GPU
0.00.628.196 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.628.198 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.629.741 I llama_init_from_model: n_seq_max     = 1
0.00.629.744 I llama_init_from_model: n_ctx         = 2048
0.00.629.745 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.629.746 I llama_init_from_model: n_batch       = 2048
0.00.629.746 I llama_init_from_model: n_ubatch      = 512
0.00.629.747 I llama_init_from_model: flash_attn    = 0
0.00.629.749 I llama_init_from_model: freq_base     = 10000.0
0.00.629.750 I llama_init_from_model: freq_scale    = 1
0.00.629.752 I ggml_metal_init: allocating
0.00.629.831 I ggml_metal_init: found device: Apple M4
0.00.629.844 I ggml_metal_init: picking default device: Apple M4
0.00.631.443 I ggml_metal_init: using embedded metal library
0.00.638.161 I ggml_metal_init: GPU name:   Apple M4
0.00.638.166 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.638.166 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.638.167 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.638.168 I ggml_metal_init: simdgroup reduction   = true
0.00.638.168 I ggml_metal_init: simdgroup matrix mul. = true
0.00.638.168 I ggml_metal_init: has residency sets    = true
0.00.638.169 I ggml_metal_init: has bfloat            = true
0.00.638.169 I ggml_metal_init: use bfloat            = true
0.00.638.170 I ggml_metal_init: hasUnifiedMemory      = true
0.00.638.171 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.656.964 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.713.436 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.713.443 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.713.475 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.719.153 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.719.156 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.719.156 I llama_init_from_model: graph nodes  = 967
0.00.719.156 I llama_init_from_model: graph splits = 2
0.00.719.163 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.719.295 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.719.295 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.774.388 I main: llama threadpool init, n_threads = 4
0.00.774.438 I 
0.00.774.458 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.774.460 I 
0.00.774.616 I sampler seed: 1234
0.00.774.620 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.774.664 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.774.666 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.774.666 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.494.708 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55209.95 tokens per second)
0.01.494.709 I llama_perf_context_print:        load time =     764.75 ms
0.01.494.710 I llama_perf_context_print: prompt eval time =      44.70 ms /     7 tokens (    6.39 ms per token,   156.60 tokens per second)
0.01.494.711 I llama_perf_context_print:        eval time =     672.58 ms /    63 runs   (   10.68 ms per token,    93.67 tokens per second)
0.01.494.711 I llama_perf_context_print:       total time =     721.05 ms /    70 tokens
0.01.494.938 I ggml_metal_free: deallocating

real	0m1.512s
user	0m0.111s
sys	0m0.213s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.010.649 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.975 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.979 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.981 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.981 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.982 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.984 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.985 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.986 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.986 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.988 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.988 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.988 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.989 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.989 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.992 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.992 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.992 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.779 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.775 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.530 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.531 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.531 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.531 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.532 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.532 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.532 I llama_model_loader: - type  f32:  194 tensors
0.00.026.533 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.533 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.533 I print_info: file format = GGUF V3 (latest)
0.00.026.534 I print_info: file type   = Q5_0
0.00.026.538 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.412 I load: special tokens cache size = 25
0.00.040.904 I load: token to piece cache size = 0.2984 MB
0.00.040.918 I print_info: arch             = gptneox
0.00.040.919 I print_info: vocab_only       = 0
0.00.040.920 I print_info: n_ctx_train      = 2048
0.00.040.920 I print_info: n_embd           = 2048
0.00.040.920 I print_info: n_layer          = 24
0.00.040.927 I print_info: n_head           = 16
0.00.040.928 I print_info: n_head_kv        = 16
0.00.040.928 I print_info: n_rot            = 32
0.00.040.928 I print_info: n_swa            = 0
0.00.040.929 I print_info: n_embd_head_k    = 128
0.00.040.930 I print_info: n_embd_head_v    = 128
0.00.040.930 I print_info: n_gqa            = 1
0.00.040.931 I print_info: n_embd_k_gqa     = 2048
0.00.040.932 I print_info: n_embd_v_gqa     = 2048
0.00.040.939 I print_info: f_norm_eps       = 1.0e-05
0.00.040.940 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.940 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.942 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.942 I print_info: f_logit_scale    = 0.0e+00
0.00.040.948 I print_info: n_ff             = 8192
0.00.040.948 I print_info: n_expert         = 0
0.00.040.948 I print_info: n_expert_used    = 0
0.00.040.949 I print_info: causal attn      = 1
0.00.040.949 I print_info: pooling type     = 0
0.00.040.950 I print_info: rope type        = 2
0.00.040.952 I print_info: rope scaling     = linear
0.00.040.952 I print_info: freq_base_train  = 10000.0
0.00.040.952 I print_info: freq_scale_train = 1
0.00.040.952 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.952 I print_info: rope_finetuned   = unknown
0.00.040.953 I print_info: ssm_d_conv       = 0
0.00.040.953 I print_info: ssm_d_inner      = 0
0.00.040.957 I print_info: ssm_d_state      = 0
0.00.040.957 I print_info: ssm_dt_rank      = 0
0.00.040.957 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.958 I print_info: model type       = 1.4B
0.00.040.958 I print_info: model params     = 1.41 B
0.00.040.958 I print_info: general.name     = 1.4B
0.00.040.959 I print_info: vocab type       = BPE
0.00.040.959 I print_info: n_vocab          = 50304
0.00.040.959 I print_info: n_merges         = 50009
0.00.040.959 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.960 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.960 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.960 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.960 I print_info: LF token         = 187 'Ċ'
0.00.040.960 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.960 I print_info: max token length = 1024
0.00.040.961 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.658.719 I load_tensors: offloading 24 repeating layers to GPU
0.00.658.736 I load_tensors: offloading output layer to GPU
0.00.658.737 I load_tensors: offloaded 25/25 layers to GPU
0.00.658.771 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.658.772 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.660.256 I llama_init_from_model: n_seq_max     = 1
0.00.660.258 I llama_init_from_model: n_ctx         = 2048
0.00.660.259 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.660.259 I llama_init_from_model: n_batch       = 2048
0.00.660.260 I llama_init_from_model: n_ubatch      = 512
0.00.660.260 I llama_init_from_model: flash_attn    = 0
0.00.660.261 I llama_init_from_model: freq_base     = 10000.0
0.00.660.262 I llama_init_from_model: freq_scale    = 1
0.00.660.263 I ggml_metal_init: allocating
0.00.660.278 I ggml_metal_init: found device: Apple M4
0.00.660.288 I ggml_metal_init: picking default device: Apple M4
0.00.661.513 I ggml_metal_init: using embedded metal library
0.00.667.899 I ggml_metal_init: GPU name:   Apple M4
0.00.667.904 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.667.904 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.667.905 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.667.906 I ggml_metal_init: simdgroup reduction   = true
0.00.667.906 I ggml_metal_init: simdgroup matrix mul. = true
0.00.667.906 I ggml_metal_init: has residency sets    = true
0.00.667.907 I ggml_metal_init: has bfloat            = true
0.00.667.907 I ggml_metal_init: use bfloat            = true
0.00.667.908 I ggml_metal_init: hasUnifiedMemory      = true
0.00.667.909 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.685.667 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.746.038 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.746.045 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.746.068 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.750.165 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.750.167 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.750.167 I llama_init_from_model: graph nodes  = 967
0.00.750.167 I llama_init_from_model: graph splits = 2
0.00.750.173 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.750.295 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.750.295 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.809.057 I main: llama threadpool init, n_threads = 4
0.00.809.107 I 
0.00.809.127 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.809.129 I 
0.00.809.239 I sampler seed: 1234
0.00.809.244 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.809.286 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.809.290 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.809.290 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.615.765 I llama_perf_sampler_print:    sampling time =       1.51 ms /    71 runs   (    0.02 ms per token, 47176.08 tokens per second)
0.01.615.765 I llama_perf_context_print:        load time =     797.69 ms
0.01.615.767 I llama_perf_context_print: prompt eval time =      53.30 ms /     7 tokens (    7.61 ms per token,   131.32 tokens per second)
0.01.615.767 I llama_perf_context_print:        eval time =     750.56 ms /    63 runs   (   11.91 ms per token,    83.94 tokens per second)
0.01.615.768 I llama_perf_context_print:       total time =     807.42 ms /    70 tokens
0.01.616.005 I ggml_metal_free: deallocating

real	0m1.634s
user	0m0.110s
sys	0m0.207s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.658 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.295 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.025.302 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.308 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.309 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.309 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.310 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.310 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.311 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.312 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.312 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.312 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.312 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.313 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.313 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.315 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.315 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.315 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.128 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.089 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.811 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.813 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.813 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.814 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.814 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.814 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.033.815 I llama_model_loader: - type  f32:  194 tensors
0.00.033.815 I llama_model_loader: - type q5_1:   97 tensors
0.00.033.816 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.816 I print_info: file format = GGUF V3 (latest)
0.00.033.817 I print_info: file type   = Q5_1
0.00.033.819 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.041.918 I load: special tokens cache size = 25
0.00.048.442 I load: token to piece cache size = 0.2984 MB
0.00.048.460 I print_info: arch             = gptneox
0.00.048.461 I print_info: vocab_only       = 0
0.00.048.461 I print_info: n_ctx_train      = 2048
0.00.048.461 I print_info: n_embd           = 2048
0.00.048.461 I print_info: n_layer          = 24
0.00.048.465 I print_info: n_head           = 16
0.00.048.466 I print_info: n_head_kv        = 16
0.00.048.466 I print_info: n_rot            = 32
0.00.048.466 I print_info: n_swa            = 0
0.00.048.466 I print_info: n_embd_head_k    = 128
0.00.048.466 I print_info: n_embd_head_v    = 128
0.00.048.467 I print_info: n_gqa            = 1
0.00.048.467 I print_info: n_embd_k_gqa     = 2048
0.00.048.468 I print_info: n_embd_v_gqa     = 2048
0.00.048.468 I print_info: f_norm_eps       = 1.0e-05
0.00.048.469 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.469 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.469 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.469 I print_info: f_logit_scale    = 0.0e+00
0.00.048.470 I print_info: n_ff             = 8192
0.00.048.470 I print_info: n_expert         = 0
0.00.048.470 I print_info: n_expert_used    = 0
0.00.048.470 I print_info: causal attn      = 1
0.00.048.470 I print_info: pooling type     = 0
0.00.048.470 I print_info: rope type        = 2
0.00.048.471 I print_info: rope scaling     = linear
0.00.048.471 I print_info: freq_base_train  = 10000.0
0.00.048.473 I print_info: freq_scale_train = 1
0.00.048.473 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.473 I print_info: rope_finetuned   = unknown
0.00.048.474 I print_info: ssm_d_conv       = 0
0.00.048.475 I print_info: ssm_d_inner      = 0
0.00.048.475 I print_info: ssm_d_state      = 0
0.00.048.479 I print_info: ssm_dt_rank      = 0
0.00.048.480 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.480 I print_info: model type       = 1.4B
0.00.048.480 I print_info: model params     = 1.41 B
0.00.048.480 I print_info: general.name     = 1.4B
0.00.048.481 I print_info: vocab type       = BPE
0.00.048.481 I print_info: n_vocab          = 50304
0.00.048.481 I print_info: n_merges         = 50009
0.00.048.481 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.482 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.482 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.482 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.482 I print_info: LF token         = 187 'Ċ'
0.00.048.483 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.483 I print_info: max token length = 1024
0.00.048.483 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.615.368 I load_tensors: offloading 24 repeating layers to GPU
0.00.615.377 I load_tensors: offloading output layer to GPU
0.00.615.377 I load_tensors: offloaded 25/25 layers to GPU
0.00.615.396 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.615.397 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.616.272 I llama_init_from_model: n_seq_max     = 1
0.00.616.275 I llama_init_from_model: n_ctx         = 2048
0.00.616.276 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.616.276 I llama_init_from_model: n_batch       = 2048
0.00.616.276 I llama_init_from_model: n_ubatch      = 512
0.00.616.277 I llama_init_from_model: flash_attn    = 0
0.00.616.278 I llama_init_from_model: freq_base     = 10000.0
0.00.616.279 I llama_init_from_model: freq_scale    = 1
0.00.616.280 I ggml_metal_init: allocating
0.00.616.317 I ggml_metal_init: found device: Apple M4
0.00.616.329 I ggml_metal_init: picking default device: Apple M4
0.00.617.334 I ggml_metal_init: using embedded metal library
0.00.622.131 I ggml_metal_init: GPU name:   Apple M4
0.00.622.137 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.622.138 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.622.138 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.622.139 I ggml_metal_init: simdgroup reduction   = true
0.00.622.139 I ggml_metal_init: simdgroup matrix mul. = true
0.00.622.139 I ggml_metal_init: has residency sets    = true
0.00.622.140 I ggml_metal_init: has bfloat            = true
0.00.622.140 I ggml_metal_init: use bfloat            = true
0.00.622.141 I ggml_metal_init: hasUnifiedMemory      = true
0.00.622.144 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.049 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.665.825 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.665.831 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.665.853 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.670.937 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.670.939 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.670.939 I llama_init_from_model: graph nodes  = 967
0.00.670.939 I llama_init_from_model: graph splits = 2
0.00.670.945 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.671.068 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.671.069 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.727.107 I main: llama threadpool init, n_threads = 4
0.00.727.155 I 
0.00.727.174 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.727.176 I 
0.00.727.331 I sampler seed: 1234
0.00.727.335 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.727.363 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.727.363 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.727.363 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.581.932 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48464.16 tokens per second)
0.01.581.933 I llama_perf_context_print:        load time =     716.72 ms
0.01.581.934 I llama_perf_context_print: prompt eval time =      48.35 ms /     7 tokens (    6.91 ms per token,   144.77 tokens per second)
0.01.581.935 I llama_perf_context_print:        eval time =     803.65 ms /    63 runs   (   12.76 ms per token,    78.39 tokens per second)
0.01.581.935 I llama_perf_context_print:       total time =     855.55 ms /    70 tokens
0.01.582.190 I ggml_metal_free: deallocating

real	0m1.599s
user	0m0.103s
sys	0m0.156s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.944 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.714 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.719 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.721 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.722 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.722 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.723 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.723 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.724 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.724 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.725 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.725 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.725 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.726 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.726 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.728 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.730 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.730 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.610 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.695 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.623 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.624 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.625 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.625 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.625 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.626 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.627 I llama_model_loader: - type  f32:  194 tensors
0.00.025.627 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.627 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.628 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.628 I print_info: file format = GGUF V3 (latest)
0.00.025.629 I print_info: file type   = Q2_K - Medium
0.00.025.637 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.124 I load: special tokens cache size = 25
0.00.040.573 I load: token to piece cache size = 0.2984 MB
0.00.040.590 I print_info: arch             = gptneox
0.00.040.591 I print_info: vocab_only       = 0
0.00.040.591 I print_info: n_ctx_train      = 2048
0.00.040.591 I print_info: n_embd           = 2048
0.00.040.591 I print_info: n_layer          = 24
0.00.040.596 I print_info: n_head           = 16
0.00.040.596 I print_info: n_head_kv        = 16
0.00.040.596 I print_info: n_rot            = 32
0.00.040.597 I print_info: n_swa            = 0
0.00.040.597 I print_info: n_embd_head_k    = 128
0.00.040.597 I print_info: n_embd_head_v    = 128
0.00.040.597 I print_info: n_gqa            = 1
0.00.040.598 I print_info: n_embd_k_gqa     = 2048
0.00.040.599 I print_info: n_embd_v_gqa     = 2048
0.00.040.599 I print_info: f_norm_eps       = 1.0e-05
0.00.040.600 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.600 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.600 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.600 I print_info: f_logit_scale    = 0.0e+00
0.00.040.600 I print_info: n_ff             = 8192
0.00.040.601 I print_info: n_expert         = 0
0.00.040.601 I print_info: n_expert_used    = 0
0.00.040.601 I print_info: causal attn      = 1
0.00.040.604 I print_info: pooling type     = 0
0.00.040.604 I print_info: rope type        = 2
0.00.040.604 I print_info: rope scaling     = linear
0.00.040.605 I print_info: freq_base_train  = 10000.0
0.00.040.605 I print_info: freq_scale_train = 1
0.00.040.605 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.605 I print_info: rope_finetuned   = unknown
0.00.040.605 I print_info: ssm_d_conv       = 0
0.00.040.605 I print_info: ssm_d_inner      = 0
0.00.040.605 I print_info: ssm_d_state      = 0
0.00.040.606 I print_info: ssm_dt_rank      = 0
0.00.040.606 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.606 I print_info: model type       = 1.4B
0.00.040.606 I print_info: model params     = 1.41 B
0.00.040.606 I print_info: general.name     = 1.4B
0.00.040.607 I print_info: vocab type       = BPE
0.00.040.609 I print_info: n_vocab          = 50304
0.00.040.609 I print_info: n_merges         = 50009
0.00.040.609 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.609 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.609 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.609 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.610 I print_info: LF token         = 187 'Ċ'
0.00.040.610 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.610 I print_info: max token length = 1024
0.00.040.611 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.321.826 I load_tensors: offloading 24 repeating layers to GPU
0.00.321.843 I load_tensors: offloading output layer to GPU
0.00.321.844 I load_tensors: offloaded 25/25 layers to GPU
0.00.321.873 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.321.874 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.323.343 I llama_init_from_model: n_seq_max     = 1
0.00.323.351 I llama_init_from_model: n_ctx         = 2048
0.00.323.352 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.323.352 I llama_init_from_model: n_batch       = 2048
0.00.323.352 I llama_init_from_model: n_ubatch      = 512
0.00.323.353 I llama_init_from_model: flash_attn    = 0
0.00.323.355 I llama_init_from_model: freq_base     = 10000.0
0.00.323.355 I llama_init_from_model: freq_scale    = 1
0.00.323.357 I ggml_metal_init: allocating
0.00.323.480 I ggml_metal_init: found device: Apple M4
0.00.323.499 I ggml_metal_init: picking default device: Apple M4
0.00.325.128 I ggml_metal_init: using embedded metal library
0.00.329.947 I ggml_metal_init: GPU name:   Apple M4
0.00.329.956 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.329.956 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.329.957 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.329.958 I ggml_metal_init: simdgroup reduction   = true
0.00.329.958 I ggml_metal_init: simdgroup matrix mul. = true
0.00.329.958 I ggml_metal_init: has residency sets    = true
0.00.329.958 I ggml_metal_init: has bfloat            = true
0.00.329.959 I ggml_metal_init: use bfloat            = true
0.00.329.960 I ggml_metal_init: hasUnifiedMemory      = true
0.00.329.963 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.349.766 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.400.372 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.400.378 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.400.400 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.404.812 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.404.814 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.404.814 I llama_init_from_model: graph nodes  = 967
0.00.404.815 I llama_init_from_model: graph splits = 2
0.00.404.820 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.404.946 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.404.947 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.460.811 I main: llama threadpool init, n_threads = 4
0.00.460.861 I 
0.00.460.881 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.460.881 I 
0.00.461.040 I sampler seed: 1234
0.00.461.045 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.461.097 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.461.099 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.461.100 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.134.725 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54489.64 tokens per second)
0.01.134.726 I llama_perf_context_print:        load time =     450.14 ms
0.01.134.727 I llama_perf_context_print: prompt eval time =      35.77 ms /     7 tokens (    5.11 ms per token,   195.69 tokens per second)
0.01.134.727 I llama_perf_context_print:        eval time =     635.12 ms /    63 runs   (   10.08 ms per token,    99.19 tokens per second)
0.01.134.727 I llama_perf_context_print:       total time =     674.64 ms /    70 tokens
0.01.134.947 I ggml_metal_free: deallocating

real	0m1.157s
user	0m0.112s
sys	0m0.146s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.213 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.016 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.022 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.027 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.028 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.028 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.029 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.029 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.030 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.030 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.031 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.031 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.031 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.032 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.032 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.034 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.034 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.035 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.877 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.825 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.549 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.550 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.550 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.550 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.551 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.551 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.552 I llama_model_loader: - type  f32:  194 tensors
0.00.025.552 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.552 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.552 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.553 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.553 I print_info: file format = GGUF V3 (latest)
0.00.025.557 I print_info: file type   = Q3_K - Medium
0.00.025.558 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.762 I load: special tokens cache size = 25
0.00.040.240 I load: token to piece cache size = 0.2984 MB
0.00.040.253 I print_info: arch             = gptneox
0.00.040.254 I print_info: vocab_only       = 0
0.00.040.254 I print_info: n_ctx_train      = 2048
0.00.040.254 I print_info: n_embd           = 2048
0.00.040.255 I print_info: n_layer          = 24
0.00.040.257 I print_info: n_head           = 16
0.00.040.258 I print_info: n_head_kv        = 16
0.00.040.263 I print_info: n_rot            = 32
0.00.040.263 I print_info: n_swa            = 0
0.00.040.263 I print_info: n_embd_head_k    = 128
0.00.040.263 I print_info: n_embd_head_v    = 128
0.00.040.264 I print_info: n_gqa            = 1
0.00.040.265 I print_info: n_embd_k_gqa     = 2048
0.00.040.266 I print_info: n_embd_v_gqa     = 2048
0.00.040.266 I print_info: f_norm_eps       = 1.0e-05
0.00.040.267 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.267 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.267 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.267 I print_info: f_logit_scale    = 0.0e+00
0.00.040.268 I print_info: n_ff             = 8192
0.00.040.268 I print_info: n_expert         = 0
0.00.040.268 I print_info: n_expert_used    = 0
0.00.040.268 I print_info: causal attn      = 1
0.00.040.268 I print_info: pooling type     = 0
0.00.040.268 I print_info: rope type        = 2
0.00.040.270 I print_info: rope scaling     = linear
0.00.040.270 I print_info: freq_base_train  = 10000.0
0.00.040.270 I print_info: freq_scale_train = 1
0.00.040.270 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.271 I print_info: rope_finetuned   = unknown
0.00.040.271 I print_info: ssm_d_conv       = 0
0.00.040.271 I print_info: ssm_d_inner      = 0
0.00.040.272 I print_info: ssm_d_state      = 0
0.00.040.272 I print_info: ssm_dt_rank      = 0
0.00.040.272 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.272 I print_info: model type       = 1.4B
0.00.040.273 I print_info: model params     = 1.41 B
0.00.040.273 I print_info: general.name     = 1.4B
0.00.040.273 I print_info: vocab type       = BPE
0.00.040.274 I print_info: n_vocab          = 50304
0.00.040.274 I print_info: n_merges         = 50009
0.00.040.274 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.274 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.274 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.274 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.275 I print_info: LF token         = 187 'Ċ'
0.00.040.275 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.275 I print_info: max token length = 1024
0.00.040.276 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.447.595 I load_tensors: offloading 24 repeating layers to GPU
0.00.447.612 I load_tensors: offloading output layer to GPU
0.00.447.613 I load_tensors: offloaded 25/25 layers to GPU
0.00.447.647 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.447.648 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.449.268 I llama_init_from_model: n_seq_max     = 1
0.00.449.271 I llama_init_from_model: n_ctx         = 2048
0.00.449.272 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.449.272 I llama_init_from_model: n_batch       = 2048
0.00.449.273 I llama_init_from_model: n_ubatch      = 512
0.00.449.273 I llama_init_from_model: flash_attn    = 0
0.00.449.275 I llama_init_from_model: freq_base     = 10000.0
0.00.449.275 I llama_init_from_model: freq_scale    = 1
0.00.449.277 I ggml_metal_init: allocating
0.00.449.349 I ggml_metal_init: found device: Apple M4
0.00.449.362 I ggml_metal_init: picking default device: Apple M4
0.00.451.041 I ggml_metal_init: using embedded metal library
0.00.456.688 I ggml_metal_init: GPU name:   Apple M4
0.00.456.698 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.456.698 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.456.699 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.456.700 I ggml_metal_init: simdgroup reduction   = true
0.00.456.700 I ggml_metal_init: simdgroup matrix mul. = true
0.00.456.701 I ggml_metal_init: has residency sets    = true
0.00.456.701 I ggml_metal_init: has bfloat            = true
0.00.456.701 I ggml_metal_init: use bfloat            = true
0.00.456.705 I ggml_metal_init: hasUnifiedMemory      = true
0.00.456.710 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.477.390 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.532.324 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.532.331 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.532.365 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.536.851 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.536.853 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.536.853 I llama_init_from_model: graph nodes  = 967
0.00.536.854 I llama_init_from_model: graph splits = 2
0.00.536.860 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.536.998 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.536.998 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.590.268 I main: llama threadpool init, n_threads = 4
0.00.590.317 I 
0.00.590.337 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.590.338 I 
0.00.590.492 I sampler seed: 1234
0.00.590.497 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.590.526 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.590.527 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.590.530 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.323.251 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52945.56 tokens per second)
0.01.323.252 I llama_perf_context_print:        load time =     580.32 ms
0.01.323.253 I llama_perf_context_print: prompt eval time =      40.46 ms /     7 tokens (    5.78 ms per token,   172.99 tokens per second)
0.01.323.254 I llama_perf_context_print:        eval time =     689.41 ms /    63 runs   (   10.94 ms per token,    91.38 tokens per second)
0.01.323.255 I llama_perf_context_print:       total time =     733.72 ms /    70 tokens
0.01.323.468 I ggml_metal_free: deallocating

real	0m1.338s
user	0m0.110s
sys	0m0.187s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.977 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.931 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.937 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.938 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.941 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.941 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.942 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.942 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.943 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.943 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.944 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.944 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.944 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.945 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.945 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.949 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.950 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.950 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.803 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.775 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.582 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.583 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.583 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.583 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.584 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.584 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.585 I llama_model_loader: - type  f32:  194 tensors
0.00.025.585 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.585 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.585 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.586 I print_info: file format = GGUF V3 (latest)
0.00.025.586 I print_info: file type   = Q4_K - Medium
0.00.025.592 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.787 I load: special tokens cache size = 25
0.00.040.317 I load: token to piece cache size = 0.2984 MB
0.00.040.324 I print_info: arch             = gptneox
0.00.040.324 I print_info: vocab_only       = 0
0.00.040.325 I print_info: n_ctx_train      = 2048
0.00.040.325 I print_info: n_embd           = 2048
0.00.040.325 I print_info: n_layer          = 24
0.00.040.328 I print_info: n_head           = 16
0.00.040.328 I print_info: n_head_kv        = 16
0.00.040.329 I print_info: n_rot            = 32
0.00.040.329 I print_info: n_swa            = 0
0.00.040.329 I print_info: n_embd_head_k    = 128
0.00.040.329 I print_info: n_embd_head_v    = 128
0.00.040.330 I print_info: n_gqa            = 1
0.00.040.331 I print_info: n_embd_k_gqa     = 2048
0.00.040.331 I print_info: n_embd_v_gqa     = 2048
0.00.040.332 I print_info: f_norm_eps       = 1.0e-05
0.00.040.335 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.335 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.335 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.335 I print_info: f_logit_scale    = 0.0e+00
0.00.040.336 I print_info: n_ff             = 8192
0.00.040.336 I print_info: n_expert         = 0
0.00.040.336 I print_info: n_expert_used    = 0
0.00.040.336 I print_info: causal attn      = 1
0.00.040.338 I print_info: pooling type     = 0
0.00.040.338 I print_info: rope type        = 2
0.00.040.338 I print_info: rope scaling     = linear
0.00.040.342 I print_info: freq_base_train  = 10000.0
0.00.040.343 I print_info: freq_scale_train = 1
0.00.040.343 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.343 I print_info: rope_finetuned   = unknown
0.00.040.343 I print_info: ssm_d_conv       = 0
0.00.040.343 I print_info: ssm_d_inner      = 0
0.00.040.344 I print_info: ssm_d_state      = 0
0.00.040.344 I print_info: ssm_dt_rank      = 0
0.00.040.344 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.344 I print_info: model type       = 1.4B
0.00.040.344 I print_info: model params     = 1.41 B
0.00.040.345 I print_info: general.name     = 1.4B
0.00.040.346 I print_info: vocab type       = BPE
0.00.040.346 I print_info: n_vocab          = 50304
0.00.040.346 I print_info: n_merges         = 50009
0.00.040.347 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.347 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.347 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.347 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.348 I print_info: LF token         = 187 'Ċ'
0.00.040.349 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.349 I print_info: max token length = 1024
0.00.040.349 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.538.691 I load_tensors: offloading 24 repeating layers to GPU
0.00.538.705 I load_tensors: offloading output layer to GPU
0.00.538.705 I load_tensors: offloaded 25/25 layers to GPU
0.00.538.738 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.538.739 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.540.489 I llama_init_from_model: n_seq_max     = 1
0.00.540.492 I llama_init_from_model: n_ctx         = 2048
0.00.540.492 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.540.493 I llama_init_from_model: n_batch       = 2048
0.00.540.493 I llama_init_from_model: n_ubatch      = 512
0.00.540.494 I llama_init_from_model: flash_attn    = 0
0.00.540.497 I llama_init_from_model: freq_base     = 10000.0
0.00.540.497 I llama_init_from_model: freq_scale    = 1
0.00.540.501 I ggml_metal_init: allocating
0.00.540.580 I ggml_metal_init: found device: Apple M4
0.00.540.593 I ggml_metal_init: picking default device: Apple M4
0.00.542.198 I ggml_metal_init: using embedded metal library
0.00.549.206 I ggml_metal_init: GPU name:   Apple M4
0.00.549.211 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.549.212 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.549.213 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.549.214 I ggml_metal_init: simdgroup reduction   = true
0.00.549.214 I ggml_metal_init: simdgroup matrix mul. = true
0.00.549.215 I ggml_metal_init: has residency sets    = true
0.00.549.215 I ggml_metal_init: has bfloat            = true
0.00.549.215 I ggml_metal_init: use bfloat            = true
0.00.549.216 I ggml_metal_init: hasUnifiedMemory      = true
0.00.549.217 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.568.778 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.629.137 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.629.145 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.629.194 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.634.016 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.634.018 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.634.018 I llama_init_from_model: graph nodes  = 967
0.00.634.019 I llama_init_from_model: graph splits = 2
0.00.634.025 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.634.149 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.634.150 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.694.325 I main: llama threadpool init, n_threads = 4
0.00.694.376 I 
0.00.694.404 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.694.405 I 
0.00.694.571 I sampler seed: 1234
0.00.694.576 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.694.590 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.694.592 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.694.592 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.453.727 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49719.89 tokens per second)
0.01.453.727 I llama_perf_context_print:        load time =     684.58 ms
0.01.453.728 I llama_perf_context_print: prompt eval time =      55.63 ms /     7 tokens (    7.95 ms per token,   125.83 tokens per second)
0.01.453.729 I llama_perf_context_print:        eval time =     700.61 ms /    63 runs   (   11.12 ms per token,    89.92 tokens per second)
0.01.453.729 I llama_perf_context_print:       total time =     760.17 ms /    70 tokens
0.01.454.024 I ggml_metal_free: deallocating

real	0m1.471s
user	0m0.113s
sys	0m0.214s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.330 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.824 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.829 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.830 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.831 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.831 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.832 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.832 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.833 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.833 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.833 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.834 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.834 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.835 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.837 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.841 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.841 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.842 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.671 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.664 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.450 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.452 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.452 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.452 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.453 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.453 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.453 I llama_model_loader: - type  f32:  194 tensors
0.00.026.454 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.454 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.455 I print_info: file format = GGUF V3 (latest)
0.00.026.455 I print_info: file type   = Q5_K - Medium
0.00.026.460 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.377 I load: special tokens cache size = 25
0.00.040.759 I load: token to piece cache size = 0.2984 MB
0.00.040.773 I print_info: arch             = gptneox
0.00.040.774 I print_info: vocab_only       = 0
0.00.040.774 I print_info: n_ctx_train      = 2048
0.00.040.774 I print_info: n_embd           = 2048
0.00.040.775 I print_info: n_layer          = 24
0.00.040.777 I print_info: n_head           = 16
0.00.040.778 I print_info: n_head_kv        = 16
0.00.040.778 I print_info: n_rot            = 32
0.00.040.778 I print_info: n_swa            = 0
0.00.040.783 I print_info: n_embd_head_k    = 128
0.00.040.783 I print_info: n_embd_head_v    = 128
0.00.040.784 I print_info: n_gqa            = 1
0.00.040.785 I print_info: n_embd_k_gqa     = 2048
0.00.040.786 I print_info: n_embd_v_gqa     = 2048
0.00.040.786 I print_info: f_norm_eps       = 1.0e-05
0.00.040.788 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.788 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.788 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.788 I print_info: f_logit_scale    = 0.0e+00
0.00.040.789 I print_info: n_ff             = 8192
0.00.040.789 I print_info: n_expert         = 0
0.00.040.789 I print_info: n_expert_used    = 0
0.00.040.789 I print_info: causal attn      = 1
0.00.040.789 I print_info: pooling type     = 0
0.00.040.791 I print_info: rope type        = 2
0.00.040.792 I print_info: rope scaling     = linear
0.00.040.792 I print_info: freq_base_train  = 10000.0
0.00.040.793 I print_info: freq_scale_train = 1
0.00.040.793 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.793 I print_info: rope_finetuned   = unknown
0.00.040.793 I print_info: ssm_d_conv       = 0
0.00.040.793 I print_info: ssm_d_inner      = 0
0.00.040.793 I print_info: ssm_d_state      = 0
0.00.040.794 I print_info: ssm_dt_rank      = 0
0.00.040.794 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.794 I print_info: model type       = 1.4B
0.00.040.794 I print_info: model params     = 1.41 B
0.00.040.794 I print_info: general.name     = 1.4B
0.00.040.795 I print_info: vocab type       = BPE
0.00.040.795 I print_info: n_vocab          = 50304
0.00.040.795 I print_info: n_merges         = 50009
0.00.040.796 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.796 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.796 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.796 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.796 I print_info: LF token         = 187 'Ċ'
0.00.040.797 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.797 I print_info: max token length = 1024
0.00.040.798 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.589.919 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.931 I load_tensors: offloading output layer to GPU
0.00.589.932 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.956 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.589.957 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.591.227 I llama_init_from_model: n_seq_max     = 1
0.00.591.231 I llama_init_from_model: n_ctx         = 2048
0.00.591.231 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.591.232 I llama_init_from_model: n_batch       = 2048
0.00.591.232 I llama_init_from_model: n_ubatch      = 512
0.00.591.233 I llama_init_from_model: flash_attn    = 0
0.00.591.235 I llama_init_from_model: freq_base     = 10000.0
0.00.591.235 I llama_init_from_model: freq_scale    = 1
0.00.591.246 I ggml_metal_init: allocating
0.00.591.303 I ggml_metal_init: found device: Apple M4
0.00.591.318 I ggml_metal_init: picking default device: Apple M4
0.00.592.734 I ggml_metal_init: using embedded metal library
0.00.598.426 I ggml_metal_init: GPU name:   Apple M4
0.00.598.432 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.433 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.434 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.434 I ggml_metal_init: simdgroup reduction   = true
0.00.598.435 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.435 I ggml_metal_init: has residency sets    = true
0.00.598.435 I ggml_metal_init: has bfloat            = true
0.00.598.436 I ggml_metal_init: use bfloat            = true
0.00.598.437 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.447 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.617.819 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.678.078 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.678.084 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.678.108 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.682.238 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.682.239 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.682.240 I llama_init_from_model: graph nodes  = 967
0.00.682.240 I llama_init_from_model: graph splits = 2
0.00.682.245 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.682.372 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.682.373 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.755 I main: llama threadpool init, n_threads = 4
0.00.745.800 I 
0.00.745.821 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.822 I 
0.00.745.969 I sampler seed: 1234
0.00.745.980 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.746.021 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.746.022 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.746.023 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.596.818 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54783.95 tokens per second)
0.01.596.819 I llama_perf_context_print:        load time =     734.63 ms
0.01.596.820 I llama_perf_context_print: prompt eval time =      52.57 ms /     7 tokens (    7.51 ms per token,   133.15 tokens per second)
0.01.596.820 I llama_perf_context_print:        eval time =     795.45 ms /    63 runs   (   12.63 ms per token,    79.20 tokens per second)
0.01.596.821 I llama_perf_context_print:       total time =     851.85 ms /    70 tokens
0.01.597.052 I ggml_metal_free: deallocating

real	0m1.617s
user	0m0.110s
sys	0m0.214s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.929 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.049 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.053 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.055 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.056 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.058 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.058 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.058 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.064 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.065 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.065 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.066 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.066 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.066 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.068 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.070 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.071 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.071 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.882 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.923 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.675 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.676 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.677 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.677 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.677 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.678 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.678 I llama_model_loader: - type  f32:  194 tensors
0.00.025.679 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.679 I print_info: file format = GGUF V3 (latest)
0.00.025.680 I print_info: file type   = Q6_K
0.00.025.681 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.902 I load: special tokens cache size = 25
0.00.040.142 I load: token to piece cache size = 0.2984 MB
0.00.040.157 I print_info: arch             = gptneox
0.00.040.158 I print_info: vocab_only       = 0
0.00.040.158 I print_info: n_ctx_train      = 2048
0.00.040.158 I print_info: n_embd           = 2048
0.00.040.158 I print_info: n_layer          = 24
0.00.040.161 I print_info: n_head           = 16
0.00.040.162 I print_info: n_head_kv        = 16
0.00.040.162 I print_info: n_rot            = 32
0.00.040.162 I print_info: n_swa            = 0
0.00.040.163 I print_info: n_embd_head_k    = 128
0.00.040.163 I print_info: n_embd_head_v    = 128
0.00.040.163 I print_info: n_gqa            = 1
0.00.040.164 I print_info: n_embd_k_gqa     = 2048
0.00.040.167 I print_info: n_embd_v_gqa     = 2048
0.00.040.167 I print_info: f_norm_eps       = 1.0e-05
0.00.040.168 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.168 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.168 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.168 I print_info: f_logit_scale    = 0.0e+00
0.00.040.169 I print_info: n_ff             = 8192
0.00.040.169 I print_info: n_expert         = 0
0.00.040.170 I print_info: n_expert_used    = 0
0.00.040.171 I print_info: causal attn      = 1
0.00.040.171 I print_info: pooling type     = 0
0.00.040.171 I print_info: rope type        = 2
0.00.040.171 I print_info: rope scaling     = linear
0.00.040.171 I print_info: freq_base_train  = 10000.0
0.00.040.172 I print_info: freq_scale_train = 1
0.00.040.172 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.172 I print_info: rope_finetuned   = unknown
0.00.040.172 I print_info: ssm_d_conv       = 0
0.00.040.172 I print_info: ssm_d_inner      = 0
0.00.040.172 I print_info: ssm_d_state      = 0
0.00.040.172 I print_info: ssm_dt_rank      = 0
0.00.040.173 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.173 I print_info: model type       = 1.4B
0.00.040.173 I print_info: model params     = 1.41 B
0.00.040.174 I print_info: general.name     = 1.4B
0.00.040.174 I print_info: vocab type       = BPE
0.00.040.174 I print_info: n_vocab          = 50304
0.00.040.175 I print_info: n_merges         = 50009
0.00.040.175 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.175 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.175 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.175 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.176 I print_info: LF token         = 187 'Ċ'
0.00.040.176 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.176 I print_info: max token length = 1024
0.00.040.176 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.635.645 I load_tensors: offloading 24 repeating layers to GPU
0.00.635.650 I load_tensors: offloading output layer to GPU
0.00.635.651 I load_tensors: offloaded 25/25 layers to GPU
0.00.635.674 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.635.676 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.637.000 I llama_init_from_model: n_seq_max     = 1
0.00.637.002 I llama_init_from_model: n_ctx         = 2048
0.00.637.002 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.637.003 I llama_init_from_model: n_batch       = 2048
0.00.637.003 I llama_init_from_model: n_ubatch      = 512
0.00.637.003 I llama_init_from_model: flash_attn    = 0
0.00.637.005 I llama_init_from_model: freq_base     = 10000.0
0.00.637.005 I llama_init_from_model: freq_scale    = 1
0.00.637.007 I ggml_metal_init: allocating
0.00.637.026 I ggml_metal_init: found device: Apple M4
0.00.637.036 I ggml_metal_init: picking default device: Apple M4
0.00.638.272 I ggml_metal_init: using embedded metal library
0.00.644.094 I ggml_metal_init: GPU name:   Apple M4
0.00.644.097 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.644.098 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.644.099 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.644.099 I ggml_metal_init: simdgroup reduction   = true
0.00.644.099 I ggml_metal_init: simdgroup matrix mul. = true
0.00.644.099 I ggml_metal_init: has residency sets    = true
0.00.644.100 I ggml_metal_init: has bfloat            = true
0.00.644.100 I ggml_metal_init: use bfloat            = true
0.00.644.101 I ggml_metal_init: hasUnifiedMemory      = true
0.00.644.104 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.661.043 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.713.730 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.713.736 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.713.758 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.718.720 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.718.723 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.718.723 I llama_init_from_model: graph nodes  = 967
0.00.718.724 I llama_init_from_model: graph splits = 2
0.00.718.730 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.718.858 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.718.858 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.784.358 I main: llama threadpool init, n_threads = 4
0.00.784.408 I 
0.00.784.438 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.439 I 
0.00.784.596 I sampler seed: 1234
0.00.784.601 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.784.644 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.784.648 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.784.648 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.671.948 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51300.58 tokens per second)
0.01.671.950 I llama_perf_context_print:        load time =     774.69 ms
0.01.671.951 I llama_perf_context_print: prompt eval time =      57.81 ms /     7 tokens (    8.26 ms per token,   121.08 tokens per second)
0.01.671.952 I llama_perf_context_print:        eval time =     826.51 ms /    63 runs   (   13.12 ms per token,    76.22 tokens per second)
0.01.671.952 I llama_perf_context_print:       total time =     888.33 ms /    70 tokens
0.01.672.179 I ggml_metal_free: deallocating

real	0m1.689s
user	0m0.109s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.550 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.902 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.921 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.929 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.939 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.940 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.941 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.942 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.943 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.945 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.945 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.948 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.949 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.950 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.950 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.951 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.954 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.954 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.955 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.863 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.787 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.658 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.660 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.660 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.661 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.661 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.662 I llama_model_loader: - type  f32:  194 tensors
0.00.056.662 I llama_model_loader: - type  f16:   98 tensors
0.00.056.663 I print_info: file format = GGUF V3 (latest)
0.00.056.664 I print_info: file type   = all F32 (guessed)
0.00.056.665 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.623 I load: special tokens cache size = 25
0.00.077.221 I load: token to piece cache size = 0.2984 MB
0.00.077.236 I print_info: arch             = gptneox
0.00.077.237 I print_info: vocab_only       = 0
0.00.077.238 I print_info: n_ctx_train      = 2048
0.00.077.238 I print_info: n_embd           = 2048
0.00.077.238 I print_info: n_layer          = 24
0.00.077.241 I print_info: n_head           = 16
0.00.077.242 I print_info: n_head_kv        = 16
0.00.077.244 I print_info: n_rot            = 32
0.00.077.244 I print_info: n_swa            = 0
0.00.077.244 I print_info: n_embd_head_k    = 128
0.00.077.245 I print_info: n_embd_head_v    = 128
0.00.077.245 I print_info: n_gqa            = 1
0.00.077.246 I print_info: n_embd_k_gqa     = 2048
0.00.077.247 I print_info: n_embd_v_gqa     = 2048
0.00.077.248 I print_info: f_norm_eps       = 1.0e-05
0.00.077.248 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.248 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.248 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.249 I print_info: f_logit_scale    = 0.0e+00
0.00.077.250 I print_info: n_ff             = 8192
0.00.077.250 I print_info: n_expert         = 0
0.00.077.250 I print_info: n_expert_used    = 0
0.00.077.252 I print_info: causal attn      = 1
0.00.077.252 I print_info: pooling type     = 0
0.00.077.252 I print_info: rope type        = 2
0.00.077.253 I print_info: rope scaling     = linear
0.00.077.253 I print_info: freq_base_train  = 10000.0
0.00.077.253 I print_info: freq_scale_train = 1
0.00.077.254 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.254 I print_info: rope_finetuned   = unknown
0.00.077.254 I print_info: ssm_d_conv       = 0
0.00.077.254 I print_info: ssm_d_inner      = 0
0.00.077.256 I print_info: ssm_d_state      = 0
0.00.077.256 I print_info: ssm_dt_rank      = 0
0.00.077.256 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.256 I print_info: model type       = 1.4B
0.00.077.257 I print_info: model params     = 1.41 B
0.00.077.257 I print_info: general.name     = 1.4B
0.00.077.257 I print_info: vocab type       = BPE
0.00.077.259 I print_info: n_vocab          = 50304
0.00.077.259 I print_info: n_merges         = 50009
0.00.077.259 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.259 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.260 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.260 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.260 I print_info: LF token         = 187 'Ċ'
0.00.077.260 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.261 I print_info: max token length = 1024
0.00.077.261 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.429.946 I load_tensors: offloading 24 repeating layers to GPU
0.01.429.951 I load_tensors: offloading output layer to GPU
0.01.429.951 I load_tensors: offloaded 25/25 layers to GPU
0.01.429.980 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.429.984 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.431.000 I llama_init_from_model: n_seq_max     = 1
0.01.431.001 I llama_init_from_model: n_ctx         = 128
0.01.431.001 I llama_init_from_model: n_ctx_per_seq = 128
0.01.431.001 I llama_init_from_model: n_batch       = 128
0.01.431.001 I llama_init_from_model: n_ubatch      = 128
0.01.431.002 I llama_init_from_model: flash_attn    = 0
0.01.431.002 I llama_init_from_model: freq_base     = 10000.0
0.01.431.002 I llama_init_from_model: freq_scale    = 1
0.01.431.003 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.431.007 I ggml_metal_init: allocating
0.01.431.098 I ggml_metal_init: found device: Apple M4
0.01.431.105 I ggml_metal_init: picking default device: Apple M4
0.01.432.536 I ggml_metal_init: using embedded metal library
0.01.436.481 I ggml_metal_init: GPU name:   Apple M4
0.01.436.483 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.436.484 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.436.484 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.436.484 I ggml_metal_init: simdgroup reduction   = true
0.01.436.484 I ggml_metal_init: simdgroup matrix mul. = true
0.01.436.485 I ggml_metal_init: has residency sets    = true
0.01.436.485 I ggml_metal_init: has bfloat            = true
0.01.436.485 I ggml_metal_init: use bfloat            = true
0.01.436.485 I ggml_metal_init: hasUnifiedMemory      = true
0.01.436.486 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.448.409 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.450.197 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.450.202 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.450.215 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.451.909 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.451.910 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.451.911 I llama_init_from_model: graph nodes  = 967
0.01.451.911 I llama_init_from_model: graph splits = 2
0.01.451.913 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.451.913 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.485.679 I 
0.01.485.723 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.485.727 I perplexity: tokenizing the input ..
0.01.491.142 I perplexity: tokenization took 5.412 ms
0.01.491.150 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.610.414 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.611.821 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.611.852 I llama_perf_context_print:        load time =    1460.77 ms
0.01.611.853 I llama_perf_context_print: prompt eval time =     118.99 ms /   128 tokens (    0.93 ms per token,  1075.68 tokens per second)
0.01.611.854 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.611.854 I llama_perf_context_print:       total time =     126.17 ms /   129 tokens
0.01.612.217 I ggml_metal_free: deallocating

real	0m1.843s
user	0m0.099s
sys	0m0.257s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.262 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.901 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.981 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.986 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.993 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.993 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.994 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.994 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.994 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.997 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.997 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.997 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.998 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.998 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.998 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.999 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.003 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.003 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.003 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.733 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.759 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.500 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.501 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.502 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.502 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.502 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.503 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.503 I llama_model_loader: - type  f32:  194 tensors
0.00.026.504 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.504 I print_info: file format = GGUF V3 (latest)
0.00.026.505 I print_info: file type   = Q8_0
0.00.026.510 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.455 I load: special tokens cache size = 25
0.00.041.691 I load: token to piece cache size = 0.2984 MB
0.00.041.705 I print_info: arch             = gptneox
0.00.041.706 I print_info: vocab_only       = 0
0.00.041.706 I print_info: n_ctx_train      = 2048
0.00.041.706 I print_info: n_embd           = 2048
0.00.041.706 I print_info: n_layer          = 24
0.00.041.711 I print_info: n_head           = 16
0.00.041.711 I print_info: n_head_kv        = 16
0.00.041.711 I print_info: n_rot            = 32
0.00.041.712 I print_info: n_swa            = 0
0.00.041.712 I print_info: n_embd_head_k    = 128
0.00.041.712 I print_info: n_embd_head_v    = 128
0.00.041.712 I print_info: n_gqa            = 1
0.00.041.713 I print_info: n_embd_k_gqa     = 2048
0.00.041.714 I print_info: n_embd_v_gqa     = 2048
0.00.041.714 I print_info: f_norm_eps       = 1.0e-05
0.00.041.715 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.715 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.715 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.715 I print_info: f_logit_scale    = 0.0e+00
0.00.041.716 I print_info: n_ff             = 8192
0.00.041.717 I print_info: n_expert         = 0
0.00.041.719 I print_info: n_expert_used    = 0
0.00.041.719 I print_info: causal attn      = 1
0.00.041.719 I print_info: pooling type     = 0
0.00.041.719 I print_info: rope type        = 2
0.00.041.719 I print_info: rope scaling     = linear
0.00.041.720 I print_info: freq_base_train  = 10000.0
0.00.041.720 I print_info: freq_scale_train = 1
0.00.041.720 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.720 I print_info: rope_finetuned   = unknown
0.00.041.720 I print_info: ssm_d_conv       = 0
0.00.041.720 I print_info: ssm_d_inner      = 0
0.00.041.721 I print_info: ssm_d_state      = 0
0.00.041.721 I print_info: ssm_dt_rank      = 0
0.00.041.721 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.721 I print_info: model type       = 1.4B
0.00.041.721 I print_info: model params     = 1.41 B
0.00.041.721 I print_info: general.name     = 1.4B
0.00.041.722 I print_info: vocab type       = BPE
0.00.041.722 I print_info: n_vocab          = 50304
0.00.041.722 I print_info: n_merges         = 50009
0.00.041.723 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.723 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.723 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.723 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.723 I print_info: LF token         = 187 'Ċ'
0.00.041.725 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.725 I print_info: max token length = 1024
0.00.041.725 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.846.833 I load_tensors: offloading 24 repeating layers to GPU
0.00.846.840 I load_tensors: offloading output layer to GPU
0.00.846.841 I load_tensors: offloaded 25/25 layers to GPU
0.00.846.868 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.846.871 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.848.147 I llama_init_from_model: n_seq_max     = 1
0.00.848.149 I llama_init_from_model: n_ctx         = 128
0.00.848.149 I llama_init_from_model: n_ctx_per_seq = 128
0.00.848.150 I llama_init_from_model: n_batch       = 128
0.00.848.150 I llama_init_from_model: n_ubatch      = 128
0.00.848.150 I llama_init_from_model: flash_attn    = 0
0.00.848.151 I llama_init_from_model: freq_base     = 10000.0
0.00.848.152 I llama_init_from_model: freq_scale    = 1
0.00.848.152 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.848.154 I ggml_metal_init: allocating
0.00.848.194 I ggml_metal_init: found device: Apple M4
0.00.848.203 I ggml_metal_init: picking default device: Apple M4
0.00.849.385 I ggml_metal_init: using embedded metal library
0.00.854.626 I ggml_metal_init: GPU name:   Apple M4
0.00.854.629 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.854.630 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.854.631 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.854.631 I ggml_metal_init: simdgroup reduction   = true
0.00.854.632 I ggml_metal_init: simdgroup matrix mul. = true
0.00.854.632 I ggml_metal_init: has residency sets    = true
0.00.854.632 I ggml_metal_init: has bfloat            = true
0.00.854.632 I ggml_metal_init: use bfloat            = true
0.00.854.633 I ggml_metal_init: hasUnifiedMemory      = true
0.00.854.635 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.869.822 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.873.175 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.873.178 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.873.201 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.876.264 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.876.266 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.876.266 I llama_init_from_model: graph nodes  = 967
0.00.876.267 I llama_init_from_model: graph splits = 2
0.00.876.270 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.876.270 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.904.909 I 
0.00.904.987 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.904.995 I perplexity: tokenizing the input ..
0.00.911.854 I perplexity: tokenization took 6.856 ms
0.00.911.868 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.050.918 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.052.273 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.052.297 I llama_perf_context_print:        load time =     894.00 ms
0.01.052.298 I llama_perf_context_print: prompt eval time =     138.08 ms /   128 tokens (    1.08 ms per token,   926.97 tokens per second)
0.01.052.299 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.052.299 I llama_perf_context_print:       total time =     147.39 ms /   129 tokens
0.01.052.667 I ggml_metal_free: deallocating

real	0m1.069s
user	0m0.077s
sys	0m0.181s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.303 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.482 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.014 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.020 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.022 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.022 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.028 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.028 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.029 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.029 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.030 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.030 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.030 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.031 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.031 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.031 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.033 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.033 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.033 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.710 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.802 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.635 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.637 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.637 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.637 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.638 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.638 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.639 I llama_model_loader: - type  f32:  194 tensors
0.00.026.639 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.639 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.640 I print_info: file format = GGUF V3 (latest)
0.00.026.640 I print_info: file type   = Q4_0
0.00.026.641 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.694 I load: special tokens cache size = 25
0.00.040.938 I load: token to piece cache size = 0.2984 MB
0.00.040.956 I print_info: arch             = gptneox
0.00.040.957 I print_info: vocab_only       = 0
0.00.040.957 I print_info: n_ctx_train      = 2048
0.00.040.958 I print_info: n_embd           = 2048
0.00.040.958 I print_info: n_layer          = 24
0.00.040.961 I print_info: n_head           = 16
0.00.040.962 I print_info: n_head_kv        = 16
0.00.040.962 I print_info: n_rot            = 32
0.00.040.964 I print_info: n_swa            = 0
0.00.040.964 I print_info: n_embd_head_k    = 128
0.00.040.964 I print_info: n_embd_head_v    = 128
0.00.040.965 I print_info: n_gqa            = 1
0.00.040.966 I print_info: n_embd_k_gqa     = 2048
0.00.040.966 I print_info: n_embd_v_gqa     = 2048
0.00.040.967 I print_info: f_norm_eps       = 1.0e-05
0.00.040.967 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.968 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.968 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.968 I print_info: f_logit_scale    = 0.0e+00
0.00.040.968 I print_info: n_ff             = 8192
0.00.040.969 I print_info: n_expert         = 0
0.00.040.969 I print_info: n_expert_used    = 0
0.00.040.969 I print_info: causal attn      = 1
0.00.040.969 I print_info: pooling type     = 0
0.00.040.969 I print_info: rope type        = 2
0.00.040.969 I print_info: rope scaling     = linear
0.00.040.970 I print_info: freq_base_train  = 10000.0
0.00.040.970 I print_info: freq_scale_train = 1
0.00.040.970 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.973 I print_info: rope_finetuned   = unknown
0.00.040.973 I print_info: ssm_d_conv       = 0
0.00.040.973 I print_info: ssm_d_inner      = 0
0.00.040.973 I print_info: ssm_d_state      = 0
0.00.040.973 I print_info: ssm_dt_rank      = 0
0.00.040.973 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.974 I print_info: model type       = 1.4B
0.00.040.974 I print_info: model params     = 1.41 B
0.00.040.974 I print_info: general.name     = 1.4B
0.00.040.974 I print_info: vocab type       = BPE
0.00.040.975 I print_info: n_vocab          = 50304
0.00.040.975 I print_info: n_merges         = 50009
0.00.040.975 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.975 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.975 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.975 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.976 I print_info: LF token         = 187 'Ċ'
0.00.040.976 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.976 I print_info: max token length = 1024
0.00.040.977 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.547.766 I load_tensors: offloading 24 repeating layers to GPU
0.00.547.783 I load_tensors: offloading output layer to GPU
0.00.547.783 I load_tensors: offloaded 25/25 layers to GPU
0.00.547.823 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.547.824 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.549.563 I llama_init_from_model: n_seq_max     = 1
0.00.549.566 I llama_init_from_model: n_ctx         = 128
0.00.549.566 I llama_init_from_model: n_ctx_per_seq = 128
0.00.549.567 I llama_init_from_model: n_batch       = 128
0.00.549.567 I llama_init_from_model: n_ubatch      = 128
0.00.549.568 I llama_init_from_model: flash_attn    = 0
0.00.549.570 I llama_init_from_model: freq_base     = 10000.0
0.00.549.570 I llama_init_from_model: freq_scale    = 1
0.00.549.571 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.549.574 I ggml_metal_init: allocating
0.00.549.703 I ggml_metal_init: found device: Apple M4
0.00.549.716 I ggml_metal_init: picking default device: Apple M4
0.00.551.392 I ggml_metal_init: using embedded metal library
0.00.558.326 I ggml_metal_init: GPU name:   Apple M4
0.00.558.335 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.558.336 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.558.337 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.558.337 I ggml_metal_init: simdgroup reduction   = true
0.00.558.338 I ggml_metal_init: simdgroup matrix mul. = true
0.00.558.338 I ggml_metal_init: has residency sets    = true
0.00.558.338 I ggml_metal_init: has bfloat            = true
0.00.558.338 I ggml_metal_init: use bfloat            = true
0.00.558.340 I ggml_metal_init: hasUnifiedMemory      = true
0.00.558.344 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.576.745 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.580.278 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.580.282 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.580.309 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.583.745 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.583.747 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.583.748 I llama_init_from_model: graph nodes  = 967
0.00.583.748 I llama_init_from_model: graph splits = 2
0.00.583.751 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.583.752 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.607.539 I 
0.00.607.628 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.607.637 I perplexity: tokenizing the input ..
0.00.615.041 I perplexity: tokenization took 7.401 ms
0.00.615.059 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.738.480 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.739.806 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.739.831 I llama_perf_context_print:        load time =     597.05 ms
0.00.739.832 I llama_perf_context_print: prompt eval time =     122.50 ms /   128 tokens (    0.96 ms per token,  1044.92 tokens per second)
0.00.739.832 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.739.833 I llama_perf_context_print:       total time =     132.30 ms /   129 tokens
0.00.740.149 I ggml_metal_free: deallocating

real	0m0.756s
user	0m0.081s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.901 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.343 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.349 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.351 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.357 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.357 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.358 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.358 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.359 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.359 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.359 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.360 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.360 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.360 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.361 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.363 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.363 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.364 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.169 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.181 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.958 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.960 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.960 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.960 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.960 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.961 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.961 I llama_model_loader: - type  f32:  194 tensors
0.00.025.962 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.962 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.963 I print_info: file format = GGUF V3 (latest)
0.00.025.963 I print_info: file type   = Q4_1
0.00.025.965 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.351 I load: special tokens cache size = 25
0.00.040.687 I load: token to piece cache size = 0.2984 MB
0.00.040.704 I print_info: arch             = gptneox
0.00.040.705 I print_info: vocab_only       = 0
0.00.040.705 I print_info: n_ctx_train      = 2048
0.00.040.705 I print_info: n_embd           = 2048
0.00.040.705 I print_info: n_layer          = 24
0.00.040.709 I print_info: n_head           = 16
0.00.040.710 I print_info: n_head_kv        = 16
0.00.040.710 I print_info: n_rot            = 32
0.00.040.710 I print_info: n_swa            = 0
0.00.040.710 I print_info: n_embd_head_k    = 128
0.00.040.710 I print_info: n_embd_head_v    = 128
0.00.040.711 I print_info: n_gqa            = 1
0.00.040.712 I print_info: n_embd_k_gqa     = 2048
0.00.040.712 I print_info: n_embd_v_gqa     = 2048
0.00.040.713 I print_info: f_norm_eps       = 1.0e-05
0.00.040.713 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.713 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.713 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.715 I print_info: f_logit_scale    = 0.0e+00
0.00.040.715 I print_info: n_ff             = 8192
0.00.040.715 I print_info: n_expert         = 0
0.00.040.715 I print_info: n_expert_used    = 0
0.00.040.716 I print_info: causal attn      = 1
0.00.040.716 I print_info: pooling type     = 0
0.00.040.716 I print_info: rope type        = 2
0.00.040.716 I print_info: rope scaling     = linear
0.00.040.716 I print_info: freq_base_train  = 10000.0
0.00.040.717 I print_info: freq_scale_train = 1
0.00.040.717 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.717 I print_info: rope_finetuned   = unknown
0.00.040.717 I print_info: ssm_d_conv       = 0
0.00.040.717 I print_info: ssm_d_inner      = 0
0.00.040.717 I print_info: ssm_d_state      = 0
0.00.040.718 I print_info: ssm_dt_rank      = 0
0.00.040.720 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.720 I print_info: model type       = 1.4B
0.00.040.720 I print_info: model params     = 1.41 B
0.00.040.720 I print_info: general.name     = 1.4B
0.00.040.721 I print_info: vocab type       = BPE
0.00.040.721 I print_info: n_vocab          = 50304
0.00.040.721 I print_info: n_merges         = 50009
0.00.040.722 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.722 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.722 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.722 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.722 I print_info: LF token         = 187 'Ċ'
0.00.040.723 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.723 I print_info: max token length = 1024
0.00.040.723 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.596.240 I load_tensors: offloading 24 repeating layers to GPU
0.00.596.248 I load_tensors: offloading output layer to GPU
0.00.596.249 I load_tensors: offloaded 25/25 layers to GPU
0.00.596.278 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.596.280 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.597.673 I llama_init_from_model: n_seq_max     = 1
0.00.597.676 I llama_init_from_model: n_ctx         = 128
0.00.597.677 I llama_init_from_model: n_ctx_per_seq = 128
0.00.597.677 I llama_init_from_model: n_batch       = 128
0.00.597.677 I llama_init_from_model: n_ubatch      = 128
0.00.597.678 I llama_init_from_model: flash_attn    = 0
0.00.597.679 I llama_init_from_model: freq_base     = 10000.0
0.00.597.680 I llama_init_from_model: freq_scale    = 1
0.00.597.680 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.597.682 I ggml_metal_init: allocating
0.00.597.757 I ggml_metal_init: found device: Apple M4
0.00.597.804 I ggml_metal_init: picking default device: Apple M4
0.00.599.831 I ggml_metal_init: using embedded metal library
0.00.605.981 I ggml_metal_init: GPU name:   Apple M4
0.00.605.990 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.605.991 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.605.992 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.605.993 I ggml_metal_init: simdgroup reduction   = true
0.00.605.993 I ggml_metal_init: simdgroup matrix mul. = true
0.00.605.994 I ggml_metal_init: has residency sets    = true
0.00.605.994 I ggml_metal_init: has bfloat            = true
0.00.605.994 I ggml_metal_init: use bfloat            = true
0.00.605.995 I ggml_metal_init: hasUnifiedMemory      = true
0.00.606.000 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.625.115 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.628.619 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.628.624 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.628.651 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.631.688 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.631.690 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.631.691 I llama_init_from_model: graph nodes  = 967
0.00.631.691 I llama_init_from_model: graph splits = 2
0.00.631.694 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.631.694 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.660.611 I 
0.00.660.708 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.660.716 I perplexity: tokenizing the input ..
0.00.667.893 I perplexity: tokenization took 7.172 ms
0.00.667.900 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.804.481 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.805.880 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.805.908 I llama_perf_context_print:        load time =     650.70 ms
0.00.805.909 I llama_perf_context_print: prompt eval time =     135.72 ms /   128 tokens (    1.06 ms per token,   943.14 tokens per second)
0.00.805.909 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.805.910 I llama_perf_context_print:       total time =     145.30 ms /   129 tokens
0.00.806.340 I ggml_metal_free: deallocating

real	0m0.820s
user	0m0.080s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.453 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.854 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.860 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.862 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.863 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.863 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.863 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.864 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.864 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.865 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.865 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.865 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.866 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.866 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.867 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.868 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.869 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.869 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.711 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.739 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.548 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.549 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.549 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.550 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.550 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.551 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.551 I llama_model_loader: - type  f32:  194 tensors
0.00.026.552 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.552 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.552 I print_info: file format = GGUF V3 (latest)
0.00.026.553 I print_info: file type   = Q5_0
0.00.026.554 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.994 I load: special tokens cache size = 25
0.00.041.379 I load: token to piece cache size = 0.2984 MB
0.00.041.397 I print_info: arch             = gptneox
0.00.041.397 I print_info: vocab_only       = 0
0.00.041.398 I print_info: n_ctx_train      = 2048
0.00.041.398 I print_info: n_embd           = 2048
0.00.041.398 I print_info: n_layer          = 24
0.00.041.402 I print_info: n_head           = 16
0.00.041.403 I print_info: n_head_kv        = 16
0.00.041.403 I print_info: n_rot            = 32
0.00.041.403 I print_info: n_swa            = 0
0.00.041.403 I print_info: n_embd_head_k    = 128
0.00.041.404 I print_info: n_embd_head_v    = 128
0.00.041.404 I print_info: n_gqa            = 1
0.00.041.405 I print_info: n_embd_k_gqa     = 2048
0.00.041.405 I print_info: n_embd_v_gqa     = 2048
0.00.041.406 I print_info: f_norm_eps       = 1.0e-05
0.00.041.408 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.408 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.408 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.408 I print_info: f_logit_scale    = 0.0e+00
0.00.041.409 I print_info: n_ff             = 8192
0.00.041.409 I print_info: n_expert         = 0
0.00.041.409 I print_info: n_expert_used    = 0
0.00.041.409 I print_info: causal attn      = 1
0.00.041.409 I print_info: pooling type     = 0
0.00.041.409 I print_info: rope type        = 2
0.00.041.409 I print_info: rope scaling     = linear
0.00.041.410 I print_info: freq_base_train  = 10000.0
0.00.041.410 I print_info: freq_scale_train = 1
0.00.041.410 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.410 I print_info: rope_finetuned   = unknown
0.00.041.410 I print_info: ssm_d_conv       = 0
0.00.041.411 I print_info: ssm_d_inner      = 0
0.00.041.411 I print_info: ssm_d_state      = 0
0.00.041.411 I print_info: ssm_dt_rank      = 0
0.00.041.411 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.411 I print_info: model type       = 1.4B
0.00.041.411 I print_info: model params     = 1.41 B
0.00.041.411 I print_info: general.name     = 1.4B
0.00.041.412 I print_info: vocab type       = BPE
0.00.041.412 I print_info: n_vocab          = 50304
0.00.041.412 I print_info: n_merges         = 50009
0.00.041.413 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.413 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.413 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.413 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.413 I print_info: LF token         = 187 'Ċ'
0.00.041.414 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.415 I print_info: max token length = 1024
0.00.041.416 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.657.081 I load_tensors: offloading 24 repeating layers to GPU
0.00.657.087 I load_tensors: offloading output layer to GPU
0.00.657.087 I load_tensors: offloaded 25/25 layers to GPU
0.00.657.108 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.657.109 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.658.099 I llama_init_from_model: n_seq_max     = 1
0.00.658.102 I llama_init_from_model: n_ctx         = 128
0.00.658.102 I llama_init_from_model: n_ctx_per_seq = 128
0.00.658.103 I llama_init_from_model: n_batch       = 128
0.00.658.103 I llama_init_from_model: n_ubatch      = 128
0.00.658.103 I llama_init_from_model: flash_attn    = 0
0.00.658.105 I llama_init_from_model: freq_base     = 10000.0
0.00.658.105 I llama_init_from_model: freq_scale    = 1
0.00.658.106 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.658.107 I ggml_metal_init: allocating
0.00.658.157 I ggml_metal_init: found device: Apple M4
0.00.658.168 I ggml_metal_init: picking default device: Apple M4
0.00.659.106 I ggml_metal_init: using embedded metal library
0.00.663.665 I ggml_metal_init: GPU name:   Apple M4
0.00.663.670 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.663.671 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.663.671 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.663.672 I ggml_metal_init: simdgroup reduction   = true
0.00.663.672 I ggml_metal_init: simdgroup matrix mul. = true
0.00.663.673 I ggml_metal_init: has residency sets    = true
0.00.663.673 I ggml_metal_init: has bfloat            = true
0.00.663.673 I ggml_metal_init: use bfloat            = true
0.00.663.674 I ggml_metal_init: hasUnifiedMemory      = true
0.00.663.677 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.676.704 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.678.300 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.678.302 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.678.317 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.679.852 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.679.854 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.679.854 I llama_init_from_model: graph nodes  = 967
0.00.679.855 I llama_init_from_model: graph splits = 2
0.00.679.856 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.679.856 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.708.090 I 
0.00.708.129 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.708.133 I perplexity: tokenizing the input ..
0.00.712.142 I perplexity: tokenization took 4.007 ms
0.00.712.146 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.859.333 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.860.745 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.860.767 I llama_perf_context_print:        load time =     697.63 ms
0.00.860.768 I llama_perf_context_print: prompt eval time =     146.95 ms /   128 tokens (    1.15 ms per token,   871.04 tokens per second)
0.00.860.768 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.860.769 I llama_perf_context_print:       total time =     152.68 ms /   129 tokens
0.00.861.140 I ggml_metal_free: deallocating

real	0m0.877s
user	0m0.068s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.824 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.284 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.290 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.291 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.292 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.292 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.293 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.293 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.294 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.294 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.295 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.295 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.296 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.297 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.298 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.300 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.300 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.300 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.112 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.127 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.935 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.937 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.937 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.938 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.938 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.938 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.939 I llama_model_loader: - type  f32:  194 tensors
0.00.024.939 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.939 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.940 I print_info: file format = GGUF V3 (latest)
0.00.024.941 I print_info: file type   = Q5_1
0.00.024.942 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.669 I load: special tokens cache size = 25
0.00.040.353 I load: token to piece cache size = 0.2984 MB
0.00.040.371 I print_info: arch             = gptneox
0.00.040.371 I print_info: vocab_only       = 0
0.00.040.372 I print_info: n_ctx_train      = 2048
0.00.040.372 I print_info: n_embd           = 2048
0.00.040.372 I print_info: n_layer          = 24
0.00.040.378 I print_info: n_head           = 16
0.00.040.379 I print_info: n_head_kv        = 16
0.00.040.379 I print_info: n_rot            = 32
0.00.040.379 I print_info: n_swa            = 0
0.00.040.379 I print_info: n_embd_head_k    = 128
0.00.040.379 I print_info: n_embd_head_v    = 128
0.00.040.380 I print_info: n_gqa            = 1
0.00.040.380 I print_info: n_embd_k_gqa     = 2048
0.00.040.381 I print_info: n_embd_v_gqa     = 2048
0.00.040.382 I print_info: f_norm_eps       = 1.0e-05
0.00.040.382 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.382 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.382 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.382 I print_info: f_logit_scale    = 0.0e+00
0.00.040.383 I print_info: n_ff             = 8192
0.00.040.383 I print_info: n_expert         = 0
0.00.040.384 I print_info: n_expert_used    = 0
0.00.040.386 I print_info: causal attn      = 1
0.00.040.386 I print_info: pooling type     = 0
0.00.040.386 I print_info: rope type        = 2
0.00.040.387 I print_info: rope scaling     = linear
0.00.040.387 I print_info: freq_base_train  = 10000.0
0.00.040.387 I print_info: freq_scale_train = 1
0.00.040.387 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.387 I print_info: rope_finetuned   = unknown
0.00.040.388 I print_info: ssm_d_conv       = 0
0.00.040.388 I print_info: ssm_d_inner      = 0
0.00.040.388 I print_info: ssm_d_state      = 0
0.00.040.388 I print_info: ssm_dt_rank      = 0
0.00.040.388 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.388 I print_info: model type       = 1.4B
0.00.040.389 I print_info: model params     = 1.41 B
0.00.040.389 I print_info: general.name     = 1.4B
0.00.040.389 I print_info: vocab type       = BPE
0.00.040.389 I print_info: n_vocab          = 50304
0.00.040.390 I print_info: n_merges         = 50009
0.00.040.390 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.390 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.390 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.390 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.391 I print_info: LF token         = 187 'Ċ'
0.00.040.391 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.392 I print_info: max token length = 1024
0.00.040.392 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.600.443 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.461 I load_tensors: offloading output layer to GPU
0.00.600.462 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.497 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.600.499 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.602.220 I llama_init_from_model: n_seq_max     = 1
0.00.602.225 I llama_init_from_model: n_ctx         = 128
0.00.602.226 I llama_init_from_model: n_ctx_per_seq = 128
0.00.602.226 I llama_init_from_model: n_batch       = 128
0.00.602.227 I llama_init_from_model: n_ubatch      = 128
0.00.602.227 I llama_init_from_model: flash_attn    = 0
0.00.602.230 I llama_init_from_model: freq_base     = 10000.0
0.00.602.230 I llama_init_from_model: freq_scale    = 1
0.00.602.231 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.602.234 I ggml_metal_init: allocating
0.00.602.316 I ggml_metal_init: found device: Apple M4
0.00.602.331 I ggml_metal_init: picking default device: Apple M4
0.00.603.707 I ggml_metal_init: using embedded metal library
0.00.610.168 I ggml_metal_init: GPU name:   Apple M4
0.00.610.173 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.174 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.174 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.175 I ggml_metal_init: simdgroup reduction   = true
0.00.610.175 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.175 I ggml_metal_init: has residency sets    = true
0.00.610.175 I ggml_metal_init: has bfloat            = true
0.00.610.176 I ggml_metal_init: use bfloat            = true
0.00.610.177 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.179 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.627.217 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.630.816 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.630.819 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.630.848 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.634.111 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.634.113 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.634.113 I llama_init_from_model: graph nodes  = 967
0.00.634.114 I llama_init_from_model: graph splits = 2
0.00.634.117 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.634.117 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.408 I 
0.00.665.514 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.524 I perplexity: tokenizing the input ..
0.00.672.345 I perplexity: tokenization took 6.819 ms
0.00.672.353 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.811.903 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.813.266 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.813.289 I llama_perf_context_print:        load time =     656.58 ms
0.00.813.291 I llama_perf_context_print: prompt eval time =     139.32 ms /   128 tokens (    1.09 ms per token,   918.73 tokens per second)
0.00.813.292 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.813.293 I llama_perf_context_print:       total time =     147.88 ms /   129 tokens
0.00.813.677 I ggml_metal_free: deallocating

real	0m0.828s
user	0m0.079s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.001 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.295 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.300 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.302 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.303 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.303 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.303 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.304 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.305 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.305 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.306 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.306 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.306 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.307 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.307 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.309 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.309 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.309 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.119 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.136 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.962 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.964 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.964 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.964 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.965 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.965 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.966 I llama_model_loader: - type  f32:  194 tensors
0.00.025.966 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.967 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.967 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.968 I print_info: file format = GGUF V3 (latest)
0.00.025.968 I print_info: file type   = Q2_K - Medium
0.00.025.972 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.360 I load: special tokens cache size = 25
0.00.040.613 I load: token to piece cache size = 0.2984 MB
0.00.040.630 I print_info: arch             = gptneox
0.00.040.630 I print_info: vocab_only       = 0
0.00.040.631 I print_info: n_ctx_train      = 2048
0.00.040.631 I print_info: n_embd           = 2048
0.00.040.631 I print_info: n_layer          = 24
0.00.040.635 I print_info: n_head           = 16
0.00.040.636 I print_info: n_head_kv        = 16
0.00.040.636 I print_info: n_rot            = 32
0.00.040.636 I print_info: n_swa            = 0
0.00.040.636 I print_info: n_embd_head_k    = 128
0.00.040.637 I print_info: n_embd_head_v    = 128
0.00.040.637 I print_info: n_gqa            = 1
0.00.040.638 I print_info: n_embd_k_gqa     = 2048
0.00.040.638 I print_info: n_embd_v_gqa     = 2048
0.00.040.639 I print_info: f_norm_eps       = 1.0e-05
0.00.040.639 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.639 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.639 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.640 I print_info: f_logit_scale    = 0.0e+00
0.00.040.640 I print_info: n_ff             = 8192
0.00.040.640 I print_info: n_expert         = 0
0.00.040.640 I print_info: n_expert_used    = 0
0.00.040.641 I print_info: causal attn      = 1
0.00.040.641 I print_info: pooling type     = 0
0.00.040.641 I print_info: rope type        = 2
0.00.040.641 I print_info: rope scaling     = linear
0.00.040.641 I print_info: freq_base_train  = 10000.0
0.00.040.642 I print_info: freq_scale_train = 1
0.00.040.642 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.642 I print_info: rope_finetuned   = unknown
0.00.040.642 I print_info: ssm_d_conv       = 0
0.00.040.642 I print_info: ssm_d_inner      = 0
0.00.040.642 I print_info: ssm_d_state      = 0
0.00.040.642 I print_info: ssm_dt_rank      = 0
0.00.040.643 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.643 I print_info: model type       = 1.4B
0.00.040.643 I print_info: model params     = 1.41 B
0.00.040.643 I print_info: general.name     = 1.4B
0.00.040.644 I print_info: vocab type       = BPE
0.00.040.644 I print_info: n_vocab          = 50304
0.00.040.644 I print_info: n_merges         = 50009
0.00.040.644 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.644 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.645 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.645 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.645 I print_info: LF token         = 187 'Ċ'
0.00.040.645 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.645 I print_info: max token length = 1024
0.00.040.646 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.336.074 I load_tensors: offloading 24 repeating layers to GPU
0.00.336.089 I load_tensors: offloading output layer to GPU
0.00.336.090 I load_tensors: offloaded 25/25 layers to GPU
0.00.336.120 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.336.121 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.337.678 I llama_init_from_model: n_seq_max     = 1
0.00.337.683 I llama_init_from_model: n_ctx         = 128
0.00.337.684 I llama_init_from_model: n_ctx_per_seq = 128
0.00.337.684 I llama_init_from_model: n_batch       = 128
0.00.337.685 I llama_init_from_model: n_ubatch      = 128
0.00.337.685 I llama_init_from_model: flash_attn    = 0
0.00.337.686 I llama_init_from_model: freq_base     = 10000.0
0.00.337.687 I llama_init_from_model: freq_scale    = 1
0.00.337.687 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.337.689 I ggml_metal_init: allocating
0.00.337.743 I ggml_metal_init: found device: Apple M4
0.00.337.756 I ggml_metal_init: picking default device: Apple M4
0.00.339.179 I ggml_metal_init: using embedded metal library
0.00.344.755 I ggml_metal_init: GPU name:   Apple M4
0.00.344.769 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.344.770 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.344.770 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.344.771 I ggml_metal_init: simdgroup reduction   = true
0.00.344.772 I ggml_metal_init: simdgroup matrix mul. = true
0.00.344.772 I ggml_metal_init: has residency sets    = true
0.00.344.772 I ggml_metal_init: has bfloat            = true
0.00.344.772 I ggml_metal_init: use bfloat            = true
0.00.344.774 I ggml_metal_init: hasUnifiedMemory      = true
0.00.344.777 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.366.937 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.370.746 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.370.756 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.370.817 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.374.533 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.374.535 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.374.535 I llama_init_from_model: graph nodes  = 967
0.00.374.536 I llama_init_from_model: graph splits = 2
0.00.374.539 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.374.541 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.405.278 I 
0.00.405.359 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.405.366 I perplexity: tokenizing the input ..
0.00.411.449 I perplexity: tokenization took 6.082 ms
0.00.411.453 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.542.846 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.544.176 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.544.197 I llama_perf_context_print:        load time =     395.27 ms
0.00.544.198 I llama_perf_context_print: prompt eval time =     131.15 ms /   128 tokens (    1.02 ms per token,   975.97 tokens per second)
0.00.544.201 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.544.203 I llama_perf_context_print:       total time =     138.92 ms /   129 tokens
0.00.544.609 I ggml_metal_free: deallocating

real	0m0.561s
user	0m0.080s
sys	0m0.086s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.931 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.284 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.290 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.292 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.293 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.293 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.293 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.294 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.295 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.295 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.296 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.299 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.299 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.299 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.300 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.302 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.302 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.302 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.105 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.115 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.966 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.968 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.968 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.969 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.969 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.969 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.970 I llama_model_loader: - type  f32:  194 tensors
0.00.026.970 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.970 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.971 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.971 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.972 I print_info: file format = GGUF V3 (latest)
0.00.026.972 I print_info: file type   = Q3_K - Medium
0.00.026.973 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.035.129 I load: special tokens cache size = 25
0.00.041.564 I load: token to piece cache size = 0.2984 MB
0.00.041.582 I print_info: arch             = gptneox
0.00.041.582 I print_info: vocab_only       = 0
0.00.041.583 I print_info: n_ctx_train      = 2048
0.00.041.583 I print_info: n_embd           = 2048
0.00.041.583 I print_info: n_layer          = 24
0.00.041.587 I print_info: n_head           = 16
0.00.041.589 I print_info: n_head_kv        = 16
0.00.041.589 I print_info: n_rot            = 32
0.00.041.590 I print_info: n_swa            = 0
0.00.041.590 I print_info: n_embd_head_k    = 128
0.00.041.590 I print_info: n_embd_head_v    = 128
0.00.041.590 I print_info: n_gqa            = 1
0.00.041.591 I print_info: n_embd_k_gqa     = 2048
0.00.041.592 I print_info: n_embd_v_gqa     = 2048
0.00.041.592 I print_info: f_norm_eps       = 1.0e-05
0.00.041.593 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.593 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.593 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.593 I print_info: f_logit_scale    = 0.0e+00
0.00.041.597 I print_info: n_ff             = 8192
0.00.041.597 I print_info: n_expert         = 0
0.00.041.597 I print_info: n_expert_used    = 0
0.00.041.597 I print_info: causal attn      = 1
0.00.041.597 I print_info: pooling type     = 0
0.00.041.598 I print_info: rope type        = 2
0.00.041.598 I print_info: rope scaling     = linear
0.00.041.598 I print_info: freq_base_train  = 10000.0
0.00.041.598 I print_info: freq_scale_train = 1
0.00.041.599 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.599 I print_info: rope_finetuned   = unknown
0.00.041.599 I print_info: ssm_d_conv       = 0
0.00.041.599 I print_info: ssm_d_inner      = 0
0.00.041.599 I print_info: ssm_d_state      = 0
0.00.041.599 I print_info: ssm_dt_rank      = 0
0.00.041.599 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.600 I print_info: model type       = 1.4B
0.00.041.600 I print_info: model params     = 1.41 B
0.00.041.600 I print_info: general.name     = 1.4B
0.00.041.601 I print_info: vocab type       = BPE
0.00.041.601 I print_info: n_vocab          = 50304
0.00.041.601 I print_info: n_merges         = 50009
0.00.041.601 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.601 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.601 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.602 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.602 I print_info: LF token         = 187 'Ċ'
0.00.041.602 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.602 I print_info: max token length = 1024
0.00.041.603 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.437.074 I load_tensors: offloading 24 repeating layers to GPU
0.00.437.086 I load_tensors: offloading output layer to GPU
0.00.437.087 I load_tensors: offloaded 25/25 layers to GPU
0.00.437.119 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.437.121 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.438.740 I llama_init_from_model: n_seq_max     = 1
0.00.438.742 I llama_init_from_model: n_ctx         = 128
0.00.438.743 I llama_init_from_model: n_ctx_per_seq = 128
0.00.438.743 I llama_init_from_model: n_batch       = 128
0.00.438.744 I llama_init_from_model: n_ubatch      = 128
0.00.438.744 I llama_init_from_model: flash_attn    = 0
0.00.438.746 I llama_init_from_model: freq_base     = 10000.0
0.00.438.746 I llama_init_from_model: freq_scale    = 1
0.00.438.747 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.438.749 I ggml_metal_init: allocating
0.00.438.812 I ggml_metal_init: found device: Apple M4
0.00.438.826 I ggml_metal_init: picking default device: Apple M4
0.00.440.332 I ggml_metal_init: using embedded metal library
0.00.445.858 I ggml_metal_init: GPU name:   Apple M4
0.00.445.870 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.445.870 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.445.871 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.445.872 I ggml_metal_init: simdgroup reduction   = true
0.00.445.873 I ggml_metal_init: simdgroup matrix mul. = true
0.00.445.873 I ggml_metal_init: has residency sets    = true
0.00.445.873 I ggml_metal_init: has bfloat            = true
0.00.445.874 I ggml_metal_init: use bfloat            = true
0.00.445.876 I ggml_metal_init: hasUnifiedMemory      = true
0.00.445.880 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.466.806 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.470.472 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.470.479 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.470.524 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.473.818 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.473.820 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.473.821 I llama_init_from_model: graph nodes  = 967
0.00.473.821 I llama_init_from_model: graph splits = 2
0.00.473.824 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.473.824 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.498.398 I 
0.00.498.478 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.498.487 I perplexity: tokenizing the input ..
0.00.504.774 I perplexity: tokenization took 6.285 ms
0.00.504.778 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.635.895 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.637.249 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.637.269 I llama_perf_context_print:        load time =     489.46 ms
0.00.637.270 I llama_perf_context_print: prompt eval time =     130.89 ms /   128 tokens (    1.02 ms per token,   977.92 tokens per second)
0.00.637.270 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.637.271 I llama_perf_context_print:       total time =     138.88 ms /   129 tokens
0.00.637.646 I ggml_metal_free: deallocating

real	0m0.651s
user	0m0.082s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.833 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.273 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.280 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.282 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.282 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.282 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.283 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.283 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.284 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.284 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.285 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.285 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.285 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.286 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.286 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.288 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.288 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.289 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.063 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.074 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.848 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.850 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.850 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.850 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.851 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.851 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.852 I llama_model_loader: - type  f32:  194 tensors
0.00.024.852 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.852 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.852 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.853 I print_info: file format = GGUF V3 (latest)
0.00.024.854 I print_info: file type   = Q4_K - Medium
0.00.024.855 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.937 I load: special tokens cache size = 25
0.00.039.543 I load: token to piece cache size = 0.2984 MB
0.00.039.561 I print_info: arch             = gptneox
0.00.039.562 I print_info: vocab_only       = 0
0.00.039.562 I print_info: n_ctx_train      = 2048
0.00.039.562 I print_info: n_embd           = 2048
0.00.039.562 I print_info: n_layer          = 24
0.00.039.566 I print_info: n_head           = 16
0.00.039.567 I print_info: n_head_kv        = 16
0.00.039.567 I print_info: n_rot            = 32
0.00.039.568 I print_info: n_swa            = 0
0.00.039.569 I print_info: n_embd_head_k    = 128
0.00.039.569 I print_info: n_embd_head_v    = 128
0.00.039.569 I print_info: n_gqa            = 1
0.00.039.570 I print_info: n_embd_k_gqa     = 2048
0.00.039.571 I print_info: n_embd_v_gqa     = 2048
0.00.039.573 I print_info: f_norm_eps       = 1.0e-05
0.00.039.574 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.574 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.574 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.574 I print_info: f_logit_scale    = 0.0e+00
0.00.039.575 I print_info: n_ff             = 8192
0.00.039.577 I print_info: n_expert         = 0
0.00.039.577 I print_info: n_expert_used    = 0
0.00.039.577 I print_info: causal attn      = 1
0.00.039.577 I print_info: pooling type     = 0
0.00.039.577 I print_info: rope type        = 2
0.00.039.577 I print_info: rope scaling     = linear
0.00.039.578 I print_info: freq_base_train  = 10000.0
0.00.039.578 I print_info: freq_scale_train = 1
0.00.039.578 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.578 I print_info: rope_finetuned   = unknown
0.00.039.578 I print_info: ssm_d_conv       = 0
0.00.039.578 I print_info: ssm_d_inner      = 0
0.00.039.578 I print_info: ssm_d_state      = 0
0.00.039.579 I print_info: ssm_dt_rank      = 0
0.00.039.579 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.579 I print_info: model type       = 1.4B
0.00.039.579 I print_info: model params     = 1.41 B
0.00.039.579 I print_info: general.name     = 1.4B
0.00.039.589 I print_info: vocab type       = BPE
0.00.039.590 I print_info: n_vocab          = 50304
0.00.039.590 I print_info: n_merges         = 50009
0.00.039.591 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.591 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.591 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.591 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.591 I print_info: LF token         = 187 'Ċ'
0.00.039.592 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.592 I print_info: max token length = 1024
0.00.039.592 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.507.613 I load_tensors: offloading 24 repeating layers to GPU
0.00.507.631 I load_tensors: offloading output layer to GPU
0.00.507.632 I load_tensors: offloaded 25/25 layers to GPU
0.00.507.666 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.507.667 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.509.260 I llama_init_from_model: n_seq_max     = 1
0.00.509.263 I llama_init_from_model: n_ctx         = 128
0.00.509.263 I llama_init_from_model: n_ctx_per_seq = 128
0.00.509.264 I llama_init_from_model: n_batch       = 128
0.00.509.264 I llama_init_from_model: n_ubatch      = 128
0.00.509.264 I llama_init_from_model: flash_attn    = 0
0.00.509.267 I llama_init_from_model: freq_base     = 10000.0
0.00.509.267 I llama_init_from_model: freq_scale    = 1
0.00.509.268 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.509.272 I ggml_metal_init: allocating
0.00.509.366 I ggml_metal_init: found device: Apple M4
0.00.509.380 I ggml_metal_init: picking default device: Apple M4
0.00.511.113 I ggml_metal_init: using embedded metal library
0.00.517.745 I ggml_metal_init: GPU name:   Apple M4
0.00.517.750 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.517.751 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.517.751 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.517.752 I ggml_metal_init: simdgroup reduction   = true
0.00.517.752 I ggml_metal_init: simdgroup matrix mul. = true
0.00.517.753 I ggml_metal_init: has residency sets    = true
0.00.517.753 I ggml_metal_init: has bfloat            = true
0.00.517.753 I ggml_metal_init: use bfloat            = true
0.00.517.754 I ggml_metal_init: hasUnifiedMemory      = true
0.00.517.758 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.535.248 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.538.773 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.538.776 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.538.807 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.542.105 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.542.106 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.542.107 I llama_init_from_model: graph nodes  = 967
0.00.542.107 I llama_init_from_model: graph splits = 2
0.00.542.110 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.542.111 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.572.563 I 
0.00.572.660 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.572.668 I perplexity: tokenizing the input ..
0.00.579.862 I perplexity: tokenization took 7.191 ms
0.00.579.876 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.725.638 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.726.990 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.727.014 I llama_perf_context_print:        load time =     563.72 ms
0.00.727.014 I llama_perf_context_print: prompt eval time =     144.81 ms /   128 tokens (    1.13 ms per token,   883.91 tokens per second)
0.00.727.015 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.727.015 I llama_perf_context_print:       total time =     154.45 ms /   129 tokens
0.00.727.359 I ggml_metal_free: deallocating

real	0m0.741s
user	0m0.080s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.081 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.289 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.295 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.299 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.300 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.300 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.301 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.301 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.302 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.302 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.303 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.303 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.304 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.304 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.304 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.306 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.306 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.307 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.120 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.112 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.887 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.889 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.890 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.890 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.890 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.891 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.891 I llama_model_loader: - type  f32:  194 tensors
0.00.025.892 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.892 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.892 I print_info: file format = GGUF V3 (latest)
0.00.025.893 I print_info: file type   = Q5_K - Medium
0.00.025.894 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.512 I load: special tokens cache size = 25
0.00.040.608 I load: token to piece cache size = 0.2984 MB
0.00.040.627 I print_info: arch             = gptneox
0.00.040.627 I print_info: vocab_only       = 0
0.00.040.628 I print_info: n_ctx_train      = 2048
0.00.040.628 I print_info: n_embd           = 2048
0.00.040.628 I print_info: n_layer          = 24
0.00.040.632 I print_info: n_head           = 16
0.00.040.633 I print_info: n_head_kv        = 16
0.00.040.633 I print_info: n_rot            = 32
0.00.040.633 I print_info: n_swa            = 0
0.00.040.633 I print_info: n_embd_head_k    = 128
0.00.040.633 I print_info: n_embd_head_v    = 128
0.00.040.634 I print_info: n_gqa            = 1
0.00.040.635 I print_info: n_embd_k_gqa     = 2048
0.00.040.635 I print_info: n_embd_v_gqa     = 2048
0.00.040.636 I print_info: f_norm_eps       = 1.0e-05
0.00.040.636 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.636 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.636 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.636 I print_info: f_logit_scale    = 0.0e+00
0.00.040.637 I print_info: n_ff             = 8192
0.00.040.637 I print_info: n_expert         = 0
0.00.040.637 I print_info: n_expert_used    = 0
0.00.040.637 I print_info: causal attn      = 1
0.00.040.638 I print_info: pooling type     = 0
0.00.040.638 I print_info: rope type        = 2
0.00.040.638 I print_info: rope scaling     = linear
0.00.040.638 I print_info: freq_base_train  = 10000.0
0.00.040.639 I print_info: freq_scale_train = 1
0.00.040.640 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.640 I print_info: rope_finetuned   = unknown
0.00.040.640 I print_info: ssm_d_conv       = 0
0.00.040.643 I print_info: ssm_d_inner      = 0
0.00.040.643 I print_info: ssm_d_state      = 0
0.00.040.643 I print_info: ssm_dt_rank      = 0
0.00.040.643 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.643 I print_info: model type       = 1.4B
0.00.040.643 I print_info: model params     = 1.41 B
0.00.040.644 I print_info: general.name     = 1.4B
0.00.040.644 I print_info: vocab type       = BPE
0.00.040.644 I print_info: n_vocab          = 50304
0.00.040.644 I print_info: n_merges         = 50009
0.00.040.645 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.645 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.645 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.645 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.645 I print_info: LF token         = 187 'Ċ'
0.00.040.646 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.646 I print_info: max token length = 1024
0.00.040.646 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.599.922 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.934 I load_tensors: offloading output layer to GPU
0.00.599.935 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.969 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.599.970 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.601.378 I llama_init_from_model: n_seq_max     = 1
0.00.601.384 I llama_init_from_model: n_ctx         = 128
0.00.601.385 I llama_init_from_model: n_ctx_per_seq = 128
0.00.601.385 I llama_init_from_model: n_batch       = 128
0.00.601.386 I llama_init_from_model: n_ubatch      = 128
0.00.601.386 I llama_init_from_model: flash_attn    = 0
0.00.601.388 I llama_init_from_model: freq_base     = 10000.0
0.00.601.389 I llama_init_from_model: freq_scale    = 1
0.00.601.389 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.601.392 I ggml_metal_init: allocating
0.00.601.481 I ggml_metal_init: found device: Apple M4
0.00.601.503 I ggml_metal_init: picking default device: Apple M4
0.00.603.109 I ggml_metal_init: using embedded metal library
0.00.610.104 I ggml_metal_init: GPU name:   Apple M4
0.00.610.114 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.115 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.115 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.116 I ggml_metal_init: simdgroup reduction   = true
0.00.610.116 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.117 I ggml_metal_init: has residency sets    = true
0.00.610.117 I ggml_metal_init: has bfloat            = true
0.00.610.117 I ggml_metal_init: use bfloat            = true
0.00.610.118 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.121 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.614 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.632.347 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.632.352 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.632.404 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.635.657 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.635.659 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.635.660 I llama_init_from_model: graph nodes  = 967
0.00.635.660 I llama_init_from_model: graph splits = 2
0.00.635.664 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.635.665 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.036 I 
0.00.668.105 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.668.111 I perplexity: tokenizing the input ..
0.00.675.135 I perplexity: tokenization took 7.02 ms
0.00.675.152 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.813.673 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.814.967 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.814.988 I llama_perf_context_print:        load time =     657.95 ms
0.00.814.989 I llama_perf_context_print: prompt eval time =     137.55 ms /   128 tokens (    1.07 ms per token,   930.59 tokens per second)
0.00.814.989 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.814.990 I llama_perf_context_print:       total time =     146.96 ms /   129 tokens
0.00.815.397 I ggml_metal_free: deallocating

real	0m0.831s
user	0m0.082s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.541 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.623 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.629 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.631 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.632 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.632 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.632 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.633 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.633 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.634 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.634 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.635 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.635 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.635 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.636 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.638 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.638 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.638 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.439 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.449 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.275 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.276 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.277 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.277 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.277 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.278 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.278 I llama_model_loader: - type  f32:  194 tensors
0.00.025.279 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.280 I print_info: file format = GGUF V3 (latest)
0.00.025.280 I print_info: file type   = Q6_K
0.00.025.281 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.349 I load: special tokens cache size = 25
0.00.039.702 I load: token to piece cache size = 0.2984 MB
0.00.039.719 I print_info: arch             = gptneox
0.00.039.720 I print_info: vocab_only       = 0
0.00.039.720 I print_info: n_ctx_train      = 2048
0.00.039.720 I print_info: n_embd           = 2048
0.00.039.721 I print_info: n_layer          = 24
0.00.039.725 I print_info: n_head           = 16
0.00.039.726 I print_info: n_head_kv        = 16
0.00.039.726 I print_info: n_rot            = 32
0.00.039.726 I print_info: n_swa            = 0
0.00.039.726 I print_info: n_embd_head_k    = 128
0.00.039.727 I print_info: n_embd_head_v    = 128
0.00.039.727 I print_info: n_gqa            = 1
0.00.039.728 I print_info: n_embd_k_gqa     = 2048
0.00.039.728 I print_info: n_embd_v_gqa     = 2048
0.00.039.729 I print_info: f_norm_eps       = 1.0e-05
0.00.039.731 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.731 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.731 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.732 I print_info: f_logit_scale    = 0.0e+00
0.00.039.732 I print_info: n_ff             = 8192
0.00.039.732 I print_info: n_expert         = 0
0.00.039.735 I print_info: n_expert_used    = 0
0.00.039.735 I print_info: causal attn      = 1
0.00.039.735 I print_info: pooling type     = 0
0.00.039.735 I print_info: rope type        = 2
0.00.039.735 I print_info: rope scaling     = linear
0.00.039.736 I print_info: freq_base_train  = 10000.0
0.00.039.736 I print_info: freq_scale_train = 1
0.00.039.736 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.736 I print_info: rope_finetuned   = unknown
0.00.039.736 I print_info: ssm_d_conv       = 0
0.00.039.737 I print_info: ssm_d_inner      = 0
0.00.039.737 I print_info: ssm_d_state      = 0
0.00.039.737 I print_info: ssm_dt_rank      = 0
0.00.039.737 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.737 I print_info: model type       = 1.4B
0.00.039.739 I print_info: model params     = 1.41 B
0.00.039.739 I print_info: general.name     = 1.4B
0.00.039.739 I print_info: vocab type       = BPE
0.00.039.739 I print_info: n_vocab          = 50304
0.00.039.740 I print_info: n_merges         = 50009
0.00.039.740 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.741 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.741 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.741 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.742 I print_info: LF token         = 187 'Ċ'
0.00.039.742 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.742 I print_info: max token length = 1024
0.00.039.742 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.608.239 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.258 I load_tensors: offloading output layer to GPU
0.00.608.259 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.300 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.608.301 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.609.853 I llama_init_from_model: n_seq_max     = 1
0.00.609.858 I llama_init_from_model: n_ctx         = 128
0.00.609.858 I llama_init_from_model: n_ctx_per_seq = 128
0.00.609.859 I llama_init_from_model: n_batch       = 128
0.00.609.859 I llama_init_from_model: n_ubatch      = 128
0.00.609.859 I llama_init_from_model: flash_attn    = 0
0.00.609.862 I llama_init_from_model: freq_base     = 10000.0
0.00.609.862 I llama_init_from_model: freq_scale    = 1
0.00.609.863 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.609.866 I ggml_metal_init: allocating
0.00.609.946 I ggml_metal_init: found device: Apple M4
0.00.609.961 I ggml_metal_init: picking default device: Apple M4
0.00.611.539 I ggml_metal_init: using embedded metal library
0.00.618.437 I ggml_metal_init: GPU name:   Apple M4
0.00.618.447 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.618.447 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.618.448 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.618.449 I ggml_metal_init: simdgroup reduction   = true
0.00.618.449 I ggml_metal_init: simdgroup matrix mul. = true
0.00.618.449 I ggml_metal_init: has residency sets    = true
0.00.618.450 I ggml_metal_init: has bfloat            = true
0.00.618.450 I ggml_metal_init: use bfloat            = true
0.00.618.451 I ggml_metal_init: hasUnifiedMemory      = true
0.00.618.455 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.636.391 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.639.903 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.639.909 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.639.950 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.643.172 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.643.173 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.643.174 I llama_init_from_model: graph nodes  = 967
0.00.643.174 I llama_init_from_model: graph splits = 2
0.00.643.177 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.643.177 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.674.916 I 
0.00.674.982 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.674.988 I perplexity: tokenizing the input ..
0.00.682.156 I perplexity: tokenization took 7.164 ms
0.00.682.172 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.814.426 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.815.761 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.815.783 I llama_perf_context_print:        load time =     665.37 ms
0.00.815.784 I llama_perf_context_print: prompt eval time =     131.51 ms /   128 tokens (    1.03 ms per token,   973.31 tokens per second)
0.00.815.785 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.815.785 I llama_perf_context_print:       total time =     140.87 ms /   129 tokens
0.00.816.209 I ggml_metal_free: deallocating

real	0m0.830s
user	0m0.081s
sys	0m0.141s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.430 I build: 4851 (68d0027f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.026.050 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.747 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.755 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.758 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.759 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.760 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.760 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.761 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.763 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.764 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.764 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.765 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.765 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.766 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.767 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.773 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.774 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.775 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.488 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.434 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.176 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.178 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.179 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.179 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.180 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.181 I llama_model_loader: - type  f32:  194 tensors
0.00.057.181 I llama_model_loader: - type  f16:   98 tensors
0.00.057.182 I print_info: file format = GGUF V3 (latest)
0.00.057.183 I print_info: file type   = all F32 (guessed)
0.00.057.185 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.398 I load: special tokens cache size = 25
0.00.077.814 I load: token to piece cache size = 0.2984 MB
0.00.077.828 I print_info: arch             = gptneox
0.00.077.829 I print_info: vocab_only       = 0
0.00.077.830 I print_info: n_ctx_train      = 2048
0.00.077.830 I print_info: n_embd           = 2048
0.00.077.830 I print_info: n_layer          = 24
0.00.077.834 I print_info: n_head           = 16
0.00.077.834 I print_info: n_head_kv        = 16
0.00.077.835 I print_info: n_rot            = 32
0.00.077.835 I print_info: n_swa            = 0
0.00.077.835 I print_info: n_embd_head_k    = 128
0.00.077.835 I print_info: n_embd_head_v    = 128
0.00.077.836 I print_info: n_gqa            = 1
0.00.077.837 I print_info: n_embd_k_gqa     = 2048
0.00.077.837 I print_info: n_embd_v_gqa     = 2048
0.00.077.838 I print_info: f_norm_eps       = 1.0e-05
0.00.077.838 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.839 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.839 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.839 I print_info: f_logit_scale    = 0.0e+00
0.00.077.840 I print_info: n_ff             = 8192
0.00.077.840 I print_info: n_expert         = 0
0.00.077.840 I print_info: n_expert_used    = 0
0.00.077.840 I print_info: causal attn      = 1
0.00.077.840 I print_info: pooling type     = 0
0.00.077.840 I print_info: rope type        = 2
0.00.077.841 I print_info: rope scaling     = linear
0.00.077.841 I print_info: freq_base_train  = 10000.0
0.00.077.841 I print_info: freq_scale_train = 1
0.00.077.841 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.842 I print_info: rope_finetuned   = unknown
0.00.077.842 I print_info: ssm_d_conv       = 0
0.00.077.842 I print_info: ssm_d_inner      = 0
0.00.077.842 I print_info: ssm_d_state      = 0
0.00.077.842 I print_info: ssm_dt_rank      = 0
0.00.077.842 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.842 I print_info: model type       = 1.4B
0.00.077.843 I print_info: model params     = 1.41 B
0.00.077.843 I print_info: general.name     = 1.4B
0.00.077.844 I print_info: vocab type       = BPE
0.00.077.844 I print_info: n_vocab          = 50304
0.00.077.844 I print_info: n_merges         = 50009
0.00.077.844 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.844 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.844 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.845 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.846 I print_info: LF token         = 187 'Ċ'
0.00.077.846 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.847 I print_info: max token length = 1024
0.00.077.847 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.354.371 I load_tensors: offloading 24 repeating layers to GPU
0.01.354.376 I load_tensors: offloading output layer to GPU
0.01.354.377 I load_tensors: offloaded 25/25 layers to GPU
0.01.354.404 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.354.406 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.355.358 I llama_init_from_model: n_seq_max     = 1
0.01.355.359 I llama_init_from_model: n_ctx         = 128
0.01.355.359 I llama_init_from_model: n_ctx_per_seq = 128
0.01.355.359 I llama_init_from_model: n_batch       = 128
0.01.355.360 I llama_init_from_model: n_ubatch      = 128
0.01.355.360 I llama_init_from_model: flash_attn    = 0
0.01.355.361 I llama_init_from_model: freq_base     = 10000.0
0.01.355.361 I llama_init_from_model: freq_scale    = 1
0.01.355.361 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.355.362 I ggml_metal_init: allocating
0.01.355.427 I ggml_metal_init: found device: Apple M4
0.01.355.434 I ggml_metal_init: picking default device: Apple M4
0.01.356.410 I ggml_metal_init: using embedded metal library
0.01.360.514 I ggml_metal_init: GPU name:   Apple M4
0.01.360.516 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.360.516 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.360.517 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.360.517 I ggml_metal_init: simdgroup reduction   = true
0.01.360.518 I ggml_metal_init: simdgroup matrix mul. = true
0.01.360.518 I ggml_metal_init: has residency sets    = true
0.01.360.518 I ggml_metal_init: has bfloat            = true
0.01.360.518 I ggml_metal_init: use bfloat            = true
0.01.360.519 I ggml_metal_init: hasUnifiedMemory      = true
0.01.360.520 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.372.138 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.373.870 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.373.872 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.373.886 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.375.510 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.375.511 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.375.512 I llama_init_from_model: graph nodes  = 967
0.01.375.512 I llama_init_from_model: graph splits = 2
0.01.375.513 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.375.513 I 
0.01.375.550 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.375.551 I compute_imatrix: tokenizing the input ..
0.01.379.626 I compute_imatrix: tokenization took 4.074 ms
0.01.379.628 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.642.684 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.646.032 I llama_perf_context_print:        load time =    1616.63 ms
0.01.646.034 I llama_perf_context_print: prompt eval time =     261.81 ms /   128 tokens (    2.05 ms per token,   488.91 tokens per second)
0.01.646.034 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.646.035 I llama_perf_context_print:       total time =    1619.97 ms /   129 tokens
0.01.646.608 I ggml_metal_free: deallocating

real	0m1.856s
user	0m0.127s
sys	0m0.267s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4851 (68d0027f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131604710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131604db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x131605360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131605910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131605ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131606470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131606a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131606fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x131607580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x131607a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x131607f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131608480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131608fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x131609750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x131609f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13160a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13160ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13160b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13160bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13160c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13160cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13160d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13160d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13160e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13160e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13160ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13160f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13160f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13160fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1316101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131610690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x131610b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131610df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131611290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131611730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131611bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131612070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131612510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1316129b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x131612e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1316132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131613790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131613c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1316140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131614570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131614a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131614eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131615660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x131615b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131615fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131616440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1316168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131616d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131617220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1316176c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131617b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131618000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1316184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1316189f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131618e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131619150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1316195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131619a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131619f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13161a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13161a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13161ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13161b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13161b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13161baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13161bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13161c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13161c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13161ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13161d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13161d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13161de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13161e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13161e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13161ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13161f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13161f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13161fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x131620340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x131620890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x131620de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x131621330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x131621880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x131621dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131622320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x131622870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x131622dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131623310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x131623860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131623db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131624300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x131624850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131615170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131624cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x131625470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1316259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131625f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131626460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1316269b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x131626f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131627450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1316279a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x131627ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131628440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131628990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x131628ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x131629430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x131629980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131629e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13162a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13162a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13162ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13162b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121604080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121604580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1216049f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121604e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1216052d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121605e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121606140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121606640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121606b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121607040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121607540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121607a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121607f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121608440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121608940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121608e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121609340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121609840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121609d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12160a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12160a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12160ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12160b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12160b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12160bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12160c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12160c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12160ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12160cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12160d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12160d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12160de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12160e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12160e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12160ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12160f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12160f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12160fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121610140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121610640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121610b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121611040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121611540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121611a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121611f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121612440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121612940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121612e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121613340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121613840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121613d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121614240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121614740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121614c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121615140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121615640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121615b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121616040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121616540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121616a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121616f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121617440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121617940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121617e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121618340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121618840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121618d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121619240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121619740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121619c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12161a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12161a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12161ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12161b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12161b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12161ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12161bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12161c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12161cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12161d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12161d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12161db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12161e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12161e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12161eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12161ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12161f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12161f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12161ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121620470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121620910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121620db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121621600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1216218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121621e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121622420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1216229d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121622f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121623530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121623ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121624090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121624640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121624bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1216251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121625750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121625d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1216262b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121626860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121626e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1216273c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121627970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121627f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1216284d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121628a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121629030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1216295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121629b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12162a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12162a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12162aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12162b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12162b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12162bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12162c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12162c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12162cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12162d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12162da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12162dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12162e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12162eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12162f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12162f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12162fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1216301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1216307a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121630d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121631300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1216318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121631e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121632410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1216329c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121632f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121633520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121633ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121634080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121634630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121634be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121635190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121635740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121635c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121636140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121636640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121636b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121637040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121637540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121637a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121637f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121638440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121638940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121638e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121639340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121639840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121639d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12163a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12163a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12163ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12163b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12163b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12163bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12163c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12163c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12163ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12163cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12163d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12163de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12163e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12163ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12163f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12163f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12163fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1216402a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121640740 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.743.604 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.743.608 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14160c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14160c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14160c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14160cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14160d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14160d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14160db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14160df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14160e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14160e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14160ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14160f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14160fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141610670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141610e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1416115a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141611cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1416123e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141612b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1416132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1416139f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141614110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141614830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141614f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141615670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141615b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141615fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141616450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1416168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141616d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141617230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1416176d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141617990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141617e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1416182d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141618770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141618c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1416190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141619550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1416199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141619e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14161a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14161a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14161ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14161b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14161b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14161ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14161bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14161c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14161c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14161ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14161d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14161d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14161dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14161df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14161e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14161e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14161eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14161ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14161f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14161f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14161fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14161ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141620440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1416208b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141620d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141621190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141621600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141621a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141621ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141622350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1416227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141622c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1416230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141623510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141623980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141623df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141624260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1416246d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141624b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141624fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141625420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141625890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141625d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141626170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1416265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141626a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141626ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141627330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1416277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141627c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141628080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1416284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x141628960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141628dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141629240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1416296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141629b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141629f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14162a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14162a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14162ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14162b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14162b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14162ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14162bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14162c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14162c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14162cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14162d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14162d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14162d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14162ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14162e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14162e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14162eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14162ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14162f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14162f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14162fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141630130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1416305a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141630a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141630e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1416312f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141631760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141631bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141632040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1416324b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141632920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141632d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141633200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141633670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141633ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141633f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1416343c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141634830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141634ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141635110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141635580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1416359f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141635e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1416362d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141636740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141636bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141637020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141637490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141637900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141637d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1416381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141638650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141638ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141638f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1416393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141639810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141639c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14163a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14163a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14163a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14163ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14163b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14163b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14163bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14163c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14163c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14163c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14163d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14163d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14163d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14163dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14163e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14163e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14163e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14163edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14163f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14163f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14163fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14163ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1416403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141640860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141640cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141641140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1416415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141641a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141641e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141642300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141642770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141642be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141643050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1416434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141643930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141643da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141644210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141644680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141644af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141644f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1416453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141645840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141645cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141646120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141646590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x141646a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1416471d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141647490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141647a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141647f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141648620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141648ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141648f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141649400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141649c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141649f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14164a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14164aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14164b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14164b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14164bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14164c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14164c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14164cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14164d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14164d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14164dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14164e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14164e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14164eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14164f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14164fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14164ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141650570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141650b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1416510d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141651680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141651c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1416521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141652790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141652d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1416532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1416538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141653e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141654400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1416549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141654f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141655510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141655ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141656070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141656620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141656bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141657180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141657730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141657ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141658290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x141658840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141658df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1416593a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141659950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141659f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14165a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14165aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14165b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14165b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14165bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14165c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14165c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14165cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14165d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14165d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14165dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14165e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14165e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14165ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14165f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14165f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14165fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141660090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141660590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141660a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141660f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141661490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141661990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141661e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141662390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x141662890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x141662d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x141663290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x141663790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x141663c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x141664190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x141664690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x141664b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x141665090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x141665590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141665a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1416664a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141666bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1416672e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141667a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141667cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141668450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1416688f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141668d90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12162a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1216270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121624900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121633d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1216315c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12162f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12162d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121625460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121622c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121627c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121628d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12161c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12162e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12162af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121632c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121626b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12162ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1216298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12162bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121627680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12161c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1216237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121622130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12161bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1216348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121629e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12163d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121632120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1216281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12162a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12162e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121625fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12162c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1216304b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121624eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121633230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121630a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12162c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121635450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121623da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121634ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12161ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121623240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1216337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12162d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12162f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1216326d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121631010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1216292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12161e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121640a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121640cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121640f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121641240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121641500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1216417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121641a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121641d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121642290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121642550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121642810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121642ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121642d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121643050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121643310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1216435d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121643890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121643b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121643e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1216440d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121644390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121644650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121644910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121644bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121644e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121645150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121645410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1216456d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121645990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121645c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121645f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1216461d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121646490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121646750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121646a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121646cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121646f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121647250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121647510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1216477d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121647a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121647d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121648010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1216482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121648590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121648850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121648b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121648dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121649090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121649350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121649610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1216498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121649b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121649e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12164a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12164a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12164a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12164a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12164ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12164aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12164b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12164b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12164bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12164c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12164c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12164c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12164ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12164ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12164d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12164d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12164dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12164e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12164e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12164e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12164eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12164f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12164f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12164faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12164ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1216503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121650840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121650cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121651120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121651590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121651a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121651e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1216522e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121652750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121652bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121653030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1216534a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121653910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121653d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1216541f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121654660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121654ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121654f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1216553b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121655820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121655c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121656100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121656570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1216569e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121656e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1216572c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121657730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121657ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121658010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121658480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1216588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121658d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1216591d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121659640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121659ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121659f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12165a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12165a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12165ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12165b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12165b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12165b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12165be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12165c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12165c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12165cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12165cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12165d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12165d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12165dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12165e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12165e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12165ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12165ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12165f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12165f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12165fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1216600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121660530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1216609a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121660e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121661280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1216616f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121661b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121661fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121662440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1216628b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121662d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121663190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121663710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121663b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121663ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121664460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121664c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121664f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1216654b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1216659b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121666090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121666530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1216669d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121666e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1216676c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121667980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121667f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1216684e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121668a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121669040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1216695f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121669ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12166a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12166a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12166acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12166b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12166b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12166bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12166c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12166c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12166ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12166d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12166da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12166dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12166e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12166eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12166f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12166f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12166fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121670200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1216707b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121670d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121671310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1216718c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121671e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121672420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1216729d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121672f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121673530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121673ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121674090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121674640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121674bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1216751a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121675750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121675d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1216762b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121676860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121676e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1216773c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121677970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121677f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1216784d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121678a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121679030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1216795e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121679b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12167a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12167a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12167aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12167b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12167b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12167bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12167c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12167c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12167cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12167d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12167d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12167db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12167e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12167e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12167ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12167ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12167f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12167f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12167fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x121680300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x121680800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x121680d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x121681200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x121681700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x121681c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x121682100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x121682600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x121682b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x121683000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121683500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121683f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121684630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121684d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121685470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121685730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121685ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121686360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121686800 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.809s
user	0m0.284s
sys	0m0.318s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4851 (68d0027f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131f0bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131f0c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x131f0c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131f0cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131f0d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131f0d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131f0de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131f0e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x131f0e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x131f0eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x131f0f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131f0f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131f10400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x131f10bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x131f113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x131f11ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x131f12200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x131f12920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x131f13040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x131f13810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x131f13f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x131f14650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131f14d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x131f15610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x131f15d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x131f161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x131f16670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131f16d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131f171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131f17650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131f17af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x131f17f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131f18250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131f186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131f18b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131f19030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131f194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131f19970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131f19e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x131f1a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131f1a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131f1abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131f1b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131f1b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131f1b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131f1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131f1c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131f1cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x131f1cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131f1d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131f1d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131f1dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131f1e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131f1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x131f1eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131f1efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131f1f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131f1f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x131f1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131f202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131f205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x131f20a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131f20ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131f21390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x131f21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x131f21cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x131f22170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x131f22610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x131f22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x131f22f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x131f233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x131f23890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x131f23d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x131f24280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x131f247d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x131f24d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x131f25270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x131f257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x131f25d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x131f26260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x131f267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x131f26d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x131f27250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x131f277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x131f27cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x131f28240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x131f28790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x131f28ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x131f29230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131f29780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x131f29cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x131f2a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131f2a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x131f2acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131f2b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131f2b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x131f2bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131f1c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131f2c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x131f2c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x131f2ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131f2d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131f2d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x131f2de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x131f2e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131f2e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x131f2ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x131f2f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131f2f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131f2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x131f30340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x131f30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x131f30de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131f31280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131f31720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131f31bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131f32060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131f32500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131f329a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131f32e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131f332e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x131f33780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131f33c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131f340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x131f34560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131f34a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131f34ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x131f35340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131f357e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131f35c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x131f36120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131f365c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131f36a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131f36f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131f373a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131f37840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131f37ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131f38180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131f38620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x131f38ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131f38f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x131f39400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x131f398a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x131f39d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131f3a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131f3a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x131f3ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131f3afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x131f3b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x131f3b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x131f3bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131f3c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x131f3c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131f3cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131f3d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131f3d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x131f3d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x131f3de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x131f3e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x131f3e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x131f3ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131f3f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131f3f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131f3f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131f3fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131f40300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x131f407a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x131f40c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x131f410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x131f41580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x131f41a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131f41ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x131f42360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x131f42800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x131f42ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x131f43140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x131f435e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131f43a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131f43f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x131f443c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131f44860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131f44d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131f451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131f45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131f45ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131f45f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131f46420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131f468c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131f46d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131f47200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131f476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131f47b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131f47fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131f48530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131f48a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x131f48fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131f49520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131f499c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131f49e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131f4a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131f4a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x131f4ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x131f4b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131f4b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131f4bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x131f4bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131f4c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x131f4c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131f4cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x131f4d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x131f4da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x131f4df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x131f4e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x131f4e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x131f4ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x131f4f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x131f4f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131f4fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131f50440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x131f509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131f50fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x131f51550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x131f51b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x131f520b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x131f52660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x131f52c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x131f531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131f53770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131f53d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131f542d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x131f54880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131f54e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131f553e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131f55990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131f55f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x131f564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x131f56aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x131f57050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x131f57600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x131f57bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x131f58160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x131f58710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x131f58cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x131f59270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x131f59820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x131f59dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x131f5a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x131f5a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x131f5aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131f5b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131f5ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x131f5bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131f5c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131f5cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x131f5d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x131f5d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131f5dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x131f5e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x131f5e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x131f5ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x131f5f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131f5f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x131f5fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x131f60430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x131f609e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x131f60f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x131f61540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x131f61af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x131f61ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x131f624f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x131f629f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x131f62ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131f633f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x131f638f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131f63df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x131f642f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x131f647f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131f64cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x131f651f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131f656f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131f65bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x131f660f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x131f665f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x131f66af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x131f66ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x131f674f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x131f679f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x131f67ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x131f683f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x131f688f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x131f68df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x131f692f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x131f697f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x131f6a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x131f6a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131f6b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x131f6b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x131f6ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x131f6c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x131f6c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x131f6caf0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.761 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.764 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133009040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1330094b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x133009920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133009d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13300a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13300a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13300aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13300af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13300b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13300b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13300bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13300c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13300ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13300d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13300de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13300e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13300ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13300f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13300faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x133010220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133010940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x133011060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133011780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x133011ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1330125c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x133012a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x133012f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1330133a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x133013840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x133013ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133014180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133014620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1330148e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133014d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133015220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1330156c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x133015b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133016000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1330164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133016940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x133016de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133017280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133017720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133017bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133018060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x133018500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1330189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133018e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1330192e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133019780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133019c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13301a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13301a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13301aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13301aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13301b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13301b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13301baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13301bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13301c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13301c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13301cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13301cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13301d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13301d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13301dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13301e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13301e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13301e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13301ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13301f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13301f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13301fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13301fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x133020460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1330208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x133020d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1330211b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x133021620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x133021a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x133021f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x133022370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1330227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x133022c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1330230c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x133023530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1330239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x133023e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x133024280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1330246f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133024b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133024fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133025440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1330258b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133025d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x133026190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133026600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133026a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x133026ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133027350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1330277c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x133027c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1330280a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133028510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x133028980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x133028df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133029260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1330296d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x133029b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x133029fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13302a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13302a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13302ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13302b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13302b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13302ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13302bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13302c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13302c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13302cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13302d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13302d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13302d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13302ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13302e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13302e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13302eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13302ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13302f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13302f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13302fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x133030150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1330305c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x133030a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x133030ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x133031310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x133031780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x133031bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x133032060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1330324d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x133032940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x133032db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x133033220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x133033690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x133033b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133033f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1330343e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133034850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133034cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133035130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1330355a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133035a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133035e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1330362f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133036760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133036bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133037040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1330374b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133037920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133037d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133038200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x133038670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133038ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133038f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1330393c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133039830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133039fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13303a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13303a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13303ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13303afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13303b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13303b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13303bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13303c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13303c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13303ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13303ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13303d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13303d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13303dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13303e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13303e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13303e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13303ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13303f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13303f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13303fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13303ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x133040410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x133040880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133040cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133041160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1330415d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x133041a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x133041eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x133042320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134c04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134c044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134c04ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134c04fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134c054d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134c059e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134c05ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134c064a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134c069a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134c06eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134c075a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134c07860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134c07d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134c08810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134c08ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134c09090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134c09650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134c09c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134c0a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134c0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134c0ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134c0b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134c0b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134c0be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134c0c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134c0ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134c0cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134c0d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134c0db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134c0e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134c0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134c0ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134c0f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134c0f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134c0fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134c10390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134c10950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134c10f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134c114d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134c11a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134c12050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134c12610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134c12bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134c13190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134c13750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134c13d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134c142d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134c14890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134c14e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134c15410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134c159d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134c15f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134c16550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134c16b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134c170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134c17690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134c17c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134c18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134c187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134c18d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134c19350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134c19910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134c19ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134c1a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134c1aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134c1b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134c1b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134c1bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134c1c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134c1c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134c1ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134c1d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134c1d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134c1dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134c1e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134c1e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134c1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134c1efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134c1f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134c1f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134c1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134c203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134c208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134c20dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134c212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x134c217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x134c21cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x134c221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x134c226d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x134c22bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x134c230d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x134c235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x134c23ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x134c23fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x134c244d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134c249d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134c253e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134c25b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134c26220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134c26940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134c26c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134c27390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134c27650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134c27b60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131f578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131f567b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x131f53480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131f50cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131f60140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131f5d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131f5b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131f59530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x131f51810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x131f4f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x131f53fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131f550f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131f5a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x131f57310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x131f5f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x131f51dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x131f52ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x131f5a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x131f5c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x131f54b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x131f55c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x131f5b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131f57e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x131f58420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x131f52920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x131f53a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x131f606f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131f5df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131f4fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131f58f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131f4ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x131f50700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131f60ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131f56200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131f69ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131f5e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131f54590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131f56d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131f5abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x131f52370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131f5c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131f51260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131f5f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131f5ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131f589d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131f61800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131f50150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131f61250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x131f4f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131f5fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131f59ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x131f5bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131f5ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131f5d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x131f556a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x131f0b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x131f2c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x131f6bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x131f6cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x131f6d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x131f6d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x131f6d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x131f6d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x131f6db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x131f6de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x131f6e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x131f6e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x131f6e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x131f6e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x131f6ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x131f6eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x131f6f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x131f6f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x131f6f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x131f6f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x131f6fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x131f6ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x131f701f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x131f704b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x131f70770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x131f70a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x131f70cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x131f70fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x131f71270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x131f71530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x131f717f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x131f71ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x131f71d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x131f72030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131f722f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x131f725b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x131f72870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131f72b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x131f72df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131f730b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131f73370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x131f73630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131f738f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131f73bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x131f73e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x131f74130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131f743f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131f746b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x131f74970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x131f74c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131f74ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x131f751b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x131f75470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131f75730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131f759f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x131f75cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x131f75f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x131f76230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131f764f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131f767b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131f76a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131f76d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131f76ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131f772b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x131f77570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131f77830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x131f77af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131f77db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131f78070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x131f78330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131f785f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131f788b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x131f78b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131f78e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131f790f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x131f793b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x131f79670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131f79930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131f79bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131f79eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131f7a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131f7a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131f7a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131f7a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x131f7ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131f7af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x131f7b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x131f7b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x131f7b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131f7ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131f7bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x131f7bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131f7c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x131f7c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x131f7c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x131f7cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131f7cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x131f7d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131f7d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131f7d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x131f7d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x131f7db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x131f7ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x131f7e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x131f7e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x131f7e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x131f7e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x131f7ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x131f7ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x131f7f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x131f7f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x131f7f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x131f7f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x131f7fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x131f7fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x131f801b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x131f80470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x131f80730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x131f809f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x131f80cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x131f80f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x131f81230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131f814f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131f817b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x131f81a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131f81d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131f81ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131f822b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131f82570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131f82830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131f82af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131f82db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131f83070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131f83330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131f835f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131f838b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131f83b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131f83e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131f840f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131f843b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x131f84670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131f84930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131f84bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131f84eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131f85170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131f85430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x131f856f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x131f859b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131f85c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131f85f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x131f861f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131f864b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x131f86770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x131f86a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x131f86cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x131f87360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x131f87620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x131f878e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x131f87ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x131f87e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x131f88120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x131f883e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x131f886a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x131f88960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x131f88c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x131f88ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x131f891a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x131f89460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x131f89720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x131f89c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x131f8a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x131f8a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131f8ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131f8b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x131f8b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x131f8bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131f8c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x131f8c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131f8cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131f8d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x131f8d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x131f8dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x131f8e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x131f8e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x131f8ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x131f8f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x131f8f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x131f8fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x131f90160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x131f906b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x131f90c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x131f91150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x131f916a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x131f91bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131f92140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131f92690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x131f92be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131f93130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131f93680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x131f93bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x131f94120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x131f94670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x131f94bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x131f95110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x131f95660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x131f95bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x131f96100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x131f96650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x131f96ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x131f970f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x131f97640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x131f97b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x131f97e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x131f98110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x131f98610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x131f98b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x131f99010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x131f99510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x131f99a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x131f99f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x131f9a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x131f9a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131f9ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x131f9b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131f9b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131f9bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x131f9c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x131f9c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x131f9cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x131f9d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x131f9d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x131f9db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x131f9e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x131f9e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x131f9ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x131f9ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x131f9f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x131f9f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x131fa0320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x131fa0a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131fa1160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x131fa1880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x131fa1b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x131fa22d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x131fa2770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x131fa2c10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.951s
user	0m0.230s
sys	0m0.187s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
