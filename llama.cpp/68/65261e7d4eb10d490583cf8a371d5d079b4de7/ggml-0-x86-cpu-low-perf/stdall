+ gg_run_ctest_debug
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ tee /home/ggml/results/llama.cpp/68/65261e7d4eb10d490583cf8a371d5d079b4de7/ggml-0-x86-cpu-low-perf/ctest_debug.log
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ gg_check_build_requirements
+ command -v cmake
+ command -v make
+ command -v ctest
+ tee -a /home/ggml/results/llama.cpp/68/65261e7d4eb10d490583cf8a371d5d079b4de7/ggml-0-x86-cpu-low-perf/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Found OpenMP_C: -fopenmp (found version "4.5") 
-- Found OpenMP_CXX: -fopenmp (found version "4.5") 
-- Found OpenMP: TRUE (found version "4.5")  
-- OpenMP found
-- Using llamafile
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (0.6s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m0.685s
user	0m0.514s
sys	0m0.174s
+ tee -a /home/ggml/results/llama.cpp/68/65261e7d4eb10d490583cf8a371d5d079b4de7/ggml-0-x86-cpu-low-perf/ctest_debug-make.log
+ make -j
[  0%] Generating build details from Git
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-alloc.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-quants.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml.dir/llamafile/sgemm.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-backend.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml.dir/ggml-aarch64.c.o
[  6%] Built target sha256
[  6%] Built target sha1
[  6%] Built target xxhash
[  6%] Linking CXX shared library libggml.so
[  7%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  7%] Built target build_info
[  7%] Built target ggml
[  7%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[  8%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[  8%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[  9%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 10%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 10%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 10%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 11%] Linking CXX executable ../../bin/llama-gguf
[ 12%] Linking CXX executable ../../bin/llama-gguf-hash
[ 13%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 13%] Built target llama-gguf
[ 13%] Built target llama-gguf-hash
/home/ggml/work/llama.cpp/src/llama-vocab.cpp: In function ‘std::string format(const char*, ...)’:
/home/ggml/work/llama.cpp/src/llama-vocab.cpp:52:5: error: ‘va_start’ was not declared in this scope
   52 |     va_start(ap, fmt);
      |     ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-vocab.cpp:53:5: error: ‘va_copy’ was not declared in this scope
   53 |     va_copy(ap2, ap);
      |     ^~~~~~~
In file included from /home/ggml/work/llama.cpp/src/../include/llama.h:4,
                 from /home/ggml/work/llama.cpp/src/llama-impl.h:4,
                 from /home/ggml/work/llama.cpp/src/llama-vocab.h:3,
                 from /home/ggml/work/llama.cpp/src/llama-vocab.cpp:1:
/home/ggml/work/llama.cpp/src/llama-vocab.cpp:55:37: error: ‘INT_MAX’ was not declared in this scope
   55 |     GGML_ASSERT(size >= 0 && size < INT_MAX); // NOLINT
      |                                     ^~~~~~~
/home/ggml/work/llama.cpp/ggml/src/../include/ggml.h:259:15: note: in definition of macro ‘GGML_ASSERT’
  259 |         if (!(x)) { \
      |               ^
/home/ggml/work/llama.cpp/src/llama-vocab.cpp:10:1: note: ‘INT_MAX’ is defined in header ‘<climits>’; did you forget to ‘#include <climits>’?
    9 | #include <sstream>
  +++ |+#include <climits>
   10 | 
/home/ggml/work/llama.cpp/src/llama-vocab.cpp:59:5: error: ‘va_end’ was not declared in this scope
   59 |     va_end(ap2);
      |     ^~~~~~
/home/ggml/work/llama.cpp/src/llama-vocab.cpp: In member function ‘void llm_tokenizer_ugm::tokenize(const string&, std::vector<int>&)’:
/home/ggml/work/llama.cpp/src/llama-vocab.cpp:928:14: error: ‘reverse’ is not a member of ‘std’
  928 |         std::reverse(output.begin(), output.end());
      |              ^~~~~~~
/home/ggml/work/llama.cpp/src/llama-vocab.cpp: In member function ‘llm_tokenizer_ugm::normalization_result llm_tokenizer_ugm::normalize_prefix(const string&, size_t)’:
/home/ggml/work/llama.cpp/src/llama-vocab.cpp:1082:42: error: ‘strlen’ was not declared in this scope
 1082 |             return { prefix_replacement, strlen(prefix_replacement), longest_prefix_length };
      |                                          ^~~~~~
/home/ggml/work/llama.cpp/src/llama-vocab.cpp:10:1: note: ‘strlen’ is defined in header ‘<cstring>’; did you forget to ‘#include <cstring>’?
    9 | #include <sstream>
  +++ |+#include <cstring>
   10 | 
/home/ggml/work/llama.cpp/src/llama-vocab.cpp:1082:92: error: could not convert ‘{prefix_replacement, <expression error>, longest_prefix_length}’ from ‘<brace-enclosed initializer list>’ to ‘llm_tokenizer_ugm::normalization_result’
 1082 |             return { prefix_replacement, strlen(prefix_replacement), longest_prefix_length };
      |                                                                                            ^
      |                                                                                            |
      |                                                                                            <brace-enclosed initializer list>
/home/ggml/work/llama.cpp/src/llama-vocab.cpp: In lambda function:
/home/ggml/work/llama.cpp/src/llama-vocab.cpp:1586:9: error: ‘memcpy’ was not declared in this scope
 1586 |         memcpy(buf, token, size);
      |         ^~~~~~
/home/ggml/work/llama.cpp/src/llama-vocab.cpp:1586:9: note: ‘memcpy’ is defined in header ‘<cstring>’; did you forget to ‘#include <cstring>’?
/home/ggml/work/llama.cpp/src/llama-vocab.cpp: In function ‘int32_t llama_chat_apply_template(const llama_model*, const char*, const llama_chat_message*, size_t, bool, char*, int32_t)’:
/home/ggml/work/llama.cpp/src/llama-vocab.cpp:2038:9: error: ‘strncpy’ was not declared in this scope
 2038 |         strncpy(buf, formatted_chat.c_str(), length);
      |         ^~~~~~~
/home/ggml/work/llama.cpp/src/llama-vocab.cpp:2038:9: note: ‘strncpy’ is defined in header ‘<cstring>’; did you forget to ‘#include <cstring>’?
make[2]: *** [src/CMakeFiles/llama.dir/build.make:90: src/CMakeFiles/llama.dir/llama-vocab.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
make[1]: *** [CMakeFiles/Makefile2:1663: src/CMakeFiles/llama.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	0m8.243s
user	0m15.680s
sys	0m1.206s
+ cur=2
+ echo 2
+ set +x
cat: /home/ggml/results/llama.cpp/68/65261e7d4eb10d490583cf8a371d5d079b4de7/ggml-0-x86-cpu-low-perf/ctest_debug-ctest.log: No such file or directory
