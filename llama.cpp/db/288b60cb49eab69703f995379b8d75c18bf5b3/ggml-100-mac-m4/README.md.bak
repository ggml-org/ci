### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.26 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.61 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.16 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.30 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.28 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.09 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.88 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    1.12 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  191.85 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.91 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.07 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.34 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 252.89 sec*proc (29 tests)

Total Test time (real) = 252.90 sec

real	4m12.931s
user	8m33.153s
sys	0m7.163s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.15 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.22 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.05 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.92 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.21 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.79 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.18 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.22 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.46 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.39 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.58 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.40 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.10 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.25 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.58 sec*proc (29 tests)

Total Test time (real) =  54.59 sec

real	0m54.604s
user	1m16.781s
sys	0m6.146s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.164 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.028 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.568 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.023.576 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.580 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.023.581 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.582 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.023.583 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.023.583 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.023.585 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.023.586 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.023.587 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.023.588 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.023.588 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.023.592 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.023.593 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.023.594 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.023.594 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.023.595 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.023.600 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.023.601 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.028.522 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.029.760 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.762 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.029.762 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.029.763 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.029.763 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.029.764 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.029.764 I llama_model_loader: - type  f32:  124 tensors
0.00.029.765 I llama_model_loader: - type  f16:   73 tensors
0.00.029.766 I print_info: file format = GGUF V3 (latest)
0.00.029.766 I print_info: file type   = F16
0.00.029.768 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.034.458 I load: special tokens cache size = 5
0.00.036.933 I load: token to piece cache size = 0.2032 MB
0.00.036.938 I print_info: arch             = bert
0.00.036.938 I print_info: vocab_only       = 0
0.00.036.939 I print_info: n_ctx_train      = 512
0.00.036.939 I print_info: n_embd           = 384
0.00.036.939 I print_info: n_layer          = 12
0.00.036.943 I print_info: n_head           = 12
0.00.036.944 I print_info: n_head_kv        = 12
0.00.036.947 I print_info: n_rot            = 32
0.00.036.947 I print_info: n_swa            = 0
0.00.036.947 I print_info: n_embd_head_k    = 32
0.00.036.948 I print_info: n_embd_head_v    = 32
0.00.036.948 I print_info: n_gqa            = 1
0.00.036.949 I print_info: n_embd_k_gqa     = 384
0.00.036.950 I print_info: n_embd_v_gqa     = 384
0.00.036.958 I print_info: f_norm_eps       = 1.0e-12
0.00.036.959 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.036.960 I print_info: f_clamp_kqv      = 0.0e+00
0.00.036.960 I print_info: f_max_alibi_bias = 0.0e+00
0.00.036.960 I print_info: f_logit_scale    = 0.0e+00
0.00.036.961 I print_info: n_ff             = 1536
0.00.036.961 I print_info: n_expert         = 0
0.00.036.962 I print_info: n_expert_used    = 0
0.00.036.962 I print_info: causal attn      = 0
0.00.036.962 I print_info: pooling type     = 2
0.00.036.962 I print_info: rope type        = 2
0.00.036.963 I print_info: rope scaling     = linear
0.00.036.965 I print_info: freq_base_train  = 10000.0
0.00.036.965 I print_info: freq_scale_train = 1
0.00.036.965 I print_info: n_ctx_orig_yarn  = 512
0.00.036.966 I print_info: rope_finetuned   = unknown
0.00.036.966 I print_info: ssm_d_conv       = 0
0.00.036.966 I print_info: ssm_d_inner      = 0
0.00.036.967 I print_info: ssm_d_state      = 0
0.00.036.969 I print_info: ssm_dt_rank      = 0
0.00.036.969 I print_info: ssm_dt_b_c_rms   = 0
0.00.036.969 I print_info: model type       = 33M
0.00.036.970 I print_info: model params     = 33.21 M
0.00.036.970 I print_info: general.name     = Bge Small
0.00.036.971 I print_info: vocab type       = WPM
0.00.036.973 I print_info: n_vocab          = 30522
0.00.036.973 I print_info: n_merges         = 0
0.00.036.974 I print_info: BOS token        = 101 '[CLS]'
0.00.036.974 I print_info: UNK token        = 100 '[UNK]'
0.00.036.974 I print_info: SEP token        = 102 '[SEP]'
0.00.036.975 I print_info: PAD token        = 0 '[PAD]'
0.00.036.975 I print_info: MASK token       = 103 '[MASK]'
0.00.036.975 I print_info: LF token         = 0 '[PAD]'
0.00.036.976 I print_info: max token length = 21
0.00.040.288 I load_tensors: offloading 12 repeating layers to GPU
0.00.040.289 I load_tensors: offloading output layer to GPU
0.00.040.290 I load_tensors: offloaded 13/13 layers to GPU
0.00.040.316 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.040.317 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.040.557 I llama_init_from_model: n_seq_max     = 1
0.00.040.558 I llama_init_from_model: n_ctx         = 512
0.00.040.558 I llama_init_from_model: n_ctx_per_seq = 512
0.00.040.559 I llama_init_from_model: n_batch       = 2048
0.00.040.559 I llama_init_from_model: n_ubatch      = 2048
0.00.040.559 I llama_init_from_model: flash_attn    = 0
0.00.040.560 I llama_init_from_model: freq_base     = 10000.0
0.00.040.560 I llama_init_from_model: freq_scale    = 1
0.00.040.561 I ggml_metal_init: allocating
0.00.040.566 I ggml_metal_init: found device: Apple M4
0.00.040.571 I ggml_metal_init: picking default device: Apple M4
0.00.041.333 I ggml_metal_init: using embedded metal library
0.00.045.466 I ggml_metal_init: GPU name:   Apple M4
0.00.045.469 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.045.469 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.045.470 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.045.470 I ggml_metal_init: simdgroup reduction   = true
0.00.045.470 I ggml_metal_init: simdgroup matrix mul. = true
0.00.045.471 I ggml_metal_init: has residency sets    = true
0.00.045.471 I ggml_metal_init: has bfloat            = true
0.00.045.471 I ggml_metal_init: use bfloat            = true
0.00.045.472 I ggml_metal_init: hasUnifiedMemory      = true
0.00.045.472 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.058.301 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.059.015 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.059.018 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.059.039 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.060.224 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.060.225 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.060.226 I llama_init_from_model: graph nodes  = 429
0.00.060.226 I llama_init_from_model: graph splits = 2
0.00.060.228 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.060.228 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.065.955 I 
0.00.065.982 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.066.640 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.071.743 I llama_perf_context_print:        load time =      48.92 ms
0.00.071.744 I llama_perf_context_print: prompt eval time =       4.95 ms /     9 tokens (    0.55 ms per token,  1817.45 tokens per second)
0.00.071.747 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.071.747 I llama_perf_context_print:       total time =       5.79 ms /    10 tokens
0.00.071.894 I ggml_metal_free: deallocating

real	0m0.260s
user	0m0.052s
sys	0m0.030s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.045 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.697 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.444 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.448 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.449 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.450 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.452 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.452 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.452 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.453 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.454 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.454 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.455 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.455 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.457 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.457 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.458 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.458 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.458 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.459 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.971 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.646 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.647 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.648 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.648 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.649 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.649 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.649 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.650 I llama_model_loader: - type  f32:  124 tensors
0.00.015.650 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.651 I print_info: file format = GGUF V3 (latest)
0.00.015.651 I print_info: file type   = Q8_0
0.00.015.652 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.018.219 I load: special tokens cache size = 5
0.00.019.525 I load: token to piece cache size = 0.2032 MB
0.00.019.528 I print_info: arch             = bert
0.00.019.529 I print_info: vocab_only       = 0
0.00.019.529 I print_info: n_ctx_train      = 512
0.00.019.529 I print_info: n_embd           = 384
0.00.019.529 I print_info: n_layer          = 12
0.00.019.532 I print_info: n_head           = 12
0.00.019.533 I print_info: n_head_kv        = 12
0.00.019.533 I print_info: n_rot            = 32
0.00.019.533 I print_info: n_swa            = 0
0.00.019.533 I print_info: n_embd_head_k    = 32
0.00.019.533 I print_info: n_embd_head_v    = 32
0.00.019.533 I print_info: n_gqa            = 1
0.00.019.534 I print_info: n_embd_k_gqa     = 384
0.00.019.535 I print_info: n_embd_v_gqa     = 384
0.00.019.535 I print_info: f_norm_eps       = 1.0e-12
0.00.019.536 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.536 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.536 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.536 I print_info: f_logit_scale    = 0.0e+00
0.00.019.537 I print_info: n_ff             = 1536
0.00.019.537 I print_info: n_expert         = 0
0.00.019.537 I print_info: n_expert_used    = 0
0.00.019.539 I print_info: causal attn      = 0
0.00.019.540 I print_info: pooling type     = 2
0.00.019.540 I print_info: rope type        = 2
0.00.019.540 I print_info: rope scaling     = linear
0.00.019.540 I print_info: freq_base_train  = 10000.0
0.00.019.541 I print_info: freq_scale_train = 1
0.00.019.541 I print_info: n_ctx_orig_yarn  = 512
0.00.019.541 I print_info: rope_finetuned   = unknown
0.00.019.541 I print_info: ssm_d_conv       = 0
0.00.019.541 I print_info: ssm_d_inner      = 0
0.00.019.542 I print_info: ssm_d_state      = 0
0.00.019.543 I print_info: ssm_dt_rank      = 0
0.00.019.543 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.543 I print_info: model type       = 33M
0.00.019.543 I print_info: model params     = 33.21 M
0.00.019.543 I print_info: general.name     = Bge Small
0.00.019.544 I print_info: vocab type       = WPM
0.00.019.544 I print_info: n_vocab          = 30522
0.00.019.544 I print_info: n_merges         = 0
0.00.019.544 I print_info: BOS token        = 101 '[CLS]'
0.00.019.545 I print_info: UNK token        = 100 '[UNK]'
0.00.019.545 I print_info: SEP token        = 102 '[SEP]'
0.00.019.545 I print_info: PAD token        = 0 '[PAD]'
0.00.019.545 I print_info: MASK token       = 103 '[MASK]'
0.00.019.545 I print_info: LF token         = 0 '[PAD]'
0.00.019.545 I print_info: max token length = 21
0.00.021.296 I load_tensors: offloading 12 repeating layers to GPU
0.00.021.297 I load_tensors: offloading output layer to GPU
0.00.021.297 I load_tensors: offloaded 13/13 layers to GPU
0.00.021.303 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.021.303 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.021.449 I llama_init_from_model: n_seq_max     = 1
0.00.021.450 I llama_init_from_model: n_ctx         = 512
0.00.021.450 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.450 I llama_init_from_model: n_batch       = 2048
0.00.021.450 I llama_init_from_model: n_ubatch      = 2048
0.00.021.450 I llama_init_from_model: flash_attn    = 0
0.00.021.451 I llama_init_from_model: freq_base     = 10000.0
0.00.021.451 I llama_init_from_model: freq_scale    = 1
0.00.021.451 I ggml_metal_init: allocating
0.00.021.455 I ggml_metal_init: found device: Apple M4
0.00.021.458 I ggml_metal_init: picking default device: Apple M4
0.00.021.981 I ggml_metal_init: using embedded metal library
0.00.024.607 I ggml_metal_init: GPU name:   Apple M4
0.00.024.609 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.609 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.610 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.610 I ggml_metal_init: simdgroup reduction   = true
0.00.024.610 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.610 I ggml_metal_init: has residency sets    = true
0.00.024.610 I ggml_metal_init: has bfloat            = true
0.00.024.611 I ggml_metal_init: use bfloat            = true
0.00.024.611 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.612 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.035.154 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.757 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.759 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.774 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.036.796 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.036.798 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.036.798 I llama_init_from_model: graph nodes  = 429
0.00.036.798 I llama_init_from_model: graph splits = 2
0.00.036.799 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.800 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.041.010 I 
0.00.041.037 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.548 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.046.027 I llama_perf_context_print:        load time =      31.31 ms
0.00.046.030 I llama_perf_context_print: prompt eval time =       4.36 ms /     9 tokens (    0.48 ms per token,  2066.59 tokens per second)
0.00.046.031 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.046.031 I llama_perf_context_print:       total time =       5.02 ms /    10 tokens
0.00.046.205 I ggml_metal_free: deallocating

real	0m0.059s
user	0m0.032s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.275 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.632 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.739 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.745 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.747 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.748 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.752 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.753 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.754 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.755 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.755 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.756 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.757 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.757 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.760 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.762 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.763 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.763 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.764 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.876 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.739 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.034 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.035 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.036 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.036 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.037 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.037 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.037 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.038 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.038 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.038 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.039 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.048.039 I llama_model_loader: - type  f32:   40 tensors
0.00.048.040 I llama_model_loader: - type  f16:   30 tensors
0.00.048.040 I print_info: file format = GGUF V3 (latest)
0.00.048.041 I print_info: file type   = F16
0.00.048.042 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.052.258 W load: empty token at index 5
0.00.057.150 W load: model vocab missing newline token, using special_pad_id instead
0.00.058.564 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.058.600 I load: special tokens cache size = 5
0.00.323.045 I load: token to piece cache size = 1.5060 MB
0.00.323.050 I print_info: arch             = jina-bert-v2
0.00.323.050 I print_info: vocab_only       = 0
0.00.323.051 I print_info: n_ctx_train      = 8192
0.00.323.051 I print_info: n_embd           = 384
0.00.323.051 I print_info: n_layer          = 4
0.00.323.054 I print_info: n_head           = 12
0.00.323.055 I print_info: n_head_kv        = 12
0.00.323.055 I print_info: n_rot            = 32
0.00.323.056 I print_info: n_swa            = 0
0.00.323.057 I print_info: n_embd_head_k    = 32
0.00.323.057 I print_info: n_embd_head_v    = 32
0.00.323.059 I print_info: n_gqa            = 1
0.00.323.060 I print_info: n_embd_k_gqa     = 384
0.00.323.060 I print_info: n_embd_v_gqa     = 384
0.00.323.061 I print_info: f_norm_eps       = 1.0e-12
0.00.323.061 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.323.061 I print_info: f_clamp_kqv      = 0.0e+00
0.00.323.062 I print_info: f_max_alibi_bias = 8.0e+00
0.00.323.062 I print_info: f_logit_scale    = 0.0e+00
0.00.323.064 I print_info: n_ff             = 1536
0.00.323.064 I print_info: n_expert         = 0
0.00.323.064 I print_info: n_expert_used    = 0
0.00.323.065 I print_info: causal attn      = 0
0.00.323.065 I print_info: pooling type     = -1
0.00.323.065 I print_info: rope type        = -1
0.00.323.065 I print_info: rope scaling     = linear
0.00.323.065 I print_info: freq_base_train  = 10000.0
0.00.323.066 I print_info: freq_scale_train = 1
0.00.323.067 I print_info: n_ctx_orig_yarn  = 8192
0.00.323.067 I print_info: rope_finetuned   = unknown
0.00.323.068 I print_info: ssm_d_conv       = 0
0.00.323.068 I print_info: ssm_d_inner      = 0
0.00.323.068 I print_info: ssm_d_state      = 0
0.00.323.068 I print_info: ssm_dt_rank      = 0
0.00.323.068 I print_info: ssm_dt_b_c_rms   = 0
0.00.323.068 I print_info: model type       = 33M
0.00.323.069 I print_info: model params     = 32.90 M
0.00.323.069 I print_info: general.name     = Jina Bert Implementation
0.00.323.071 I print_info: vocab type       = BPE
0.00.323.071 I print_info: n_vocab          = 61056
0.00.323.071 I print_info: n_merges         = 39382
0.00.323.071 I print_info: BOS token        = 0 '<s>'
0.00.323.071 I print_info: EOS token        = 2 '</s>'
0.00.323.075 I print_info: UNK token        = 3 '<unk>'
0.00.323.075 I print_info: SEP token        = 2 '</s>'
0.00.323.075 I print_info: PAD token        = 1 '<pad>'
0.00.323.076 I print_info: MASK token       = 4 '<mask>'
0.00.323.076 I print_info: EOG token        = 2 '</s>'
0.00.323.076 I print_info: max token length = 45
0.00.324.675 I load_tensors: offloading 4 repeating layers to GPU
0.00.324.676 I load_tensors: offloading output layer to GPU
0.00.324.677 I load_tensors: offloaded 5/5 layers to GPU
0.00.324.699 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.324.700 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.324.895 I llama_init_from_model: n_seq_max     = 1
0.00.324.896 I llama_init_from_model: n_ctx         = 8192
0.00.324.896 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.324.896 I llama_init_from_model: n_batch       = 2048
0.00.324.896 I llama_init_from_model: n_ubatch      = 2048
0.00.324.897 I llama_init_from_model: flash_attn    = 0
0.00.324.897 I llama_init_from_model: freq_base     = 10000.0
0.00.324.897 I llama_init_from_model: freq_scale    = 1
0.00.324.897 I ggml_metal_init: allocating
0.00.324.900 I ggml_metal_init: found device: Apple M4
0.00.324.904 I ggml_metal_init: picking default device: Apple M4
0.00.325.567 I ggml_metal_init: using embedded metal library
0.00.328.148 I ggml_metal_init: GPU name:   Apple M4
0.00.328.150 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.328.150 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.328.150 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.328.151 I ggml_metal_init: simdgroup reduction   = true
0.00.328.151 I ggml_metal_init: simdgroup matrix mul. = true
0.00.328.151 I ggml_metal_init: has residency sets    = true
0.00.328.151 I ggml_metal_init: has bfloat            = true
0.00.328.151 I ggml_metal_init: use bfloat            = true
0.00.328.152 I ggml_metal_init: hasUnifiedMemory      = true
0.00.328.152 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.337.528 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.340.510 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.340.511 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.340.532 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.346.340 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.346.341 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.346.341 I llama_init_from_model: graph nodes  = 154
0.00.346.341 I llama_init_from_model: graph splits = 2
0.00.346.343 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.346.343 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.357.562 I 
0.00.357.597 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.357.691 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.357.692 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.357.695 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.357.695 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.357.698 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.357.698 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.358.193 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.361.904 I llama_perf_context_print:        load time =     334.92 ms
0.00.361.906 I llama_perf_context_print: prompt eval time =       3.70 ms /    62 tokens (    0.06 ms per token, 16738.66 tokens per second)
0.00.361.907 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.361.907 I llama_perf_context_print:       total time =       4.34 ms /    63 tokens
0.00.362.169 I ggml_metal_free: deallocating

real	0m1.080s
user	0m0.341s
sys	0m0.046s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.187 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.384 I main: llama backend init
0.00.000.392 I main: load the model and apply lora adapter, if any
0.00.082.619 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.095.566 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.095.595 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.095.613 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.095.614 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.095.615 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.095.616 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.095.616 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.095.620 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.095.621 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.095.621 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.095.622 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.095.623 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.095.624 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.095.625 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.095.631 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.095.632 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.095.633 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.102.520 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.104.686 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.111.553 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.111.569 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.111.570 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.111.570 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.111.571 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.111.572 I llama_model_loader: - type  f32:  194 tensors
0.00.111.573 I llama_model_loader: - type  f16:   98 tensors
0.00.111.576 I print_info: file format = GGUF V3 (latest)
0.00.111.577 I print_info: file type   = all F32 (guessed)
0.00.111.581 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.130.011 I load: special tokens cache size = 25
0.00.140.296 I load: token to piece cache size = 0.2984 MB
0.00.140.301 I print_info: arch             = gptneox
0.00.140.301 I print_info: vocab_only       = 0
0.00.140.302 I print_info: n_ctx_train      = 2048
0.00.140.302 I print_info: n_embd           = 2048
0.00.140.302 I print_info: n_layer          = 24
0.00.140.307 I print_info: n_head           = 16
0.00.140.308 I print_info: n_head_kv        = 16
0.00.140.309 I print_info: n_rot            = 32
0.00.140.310 I print_info: n_swa            = 0
0.00.140.310 I print_info: n_embd_head_k    = 128
0.00.140.310 I print_info: n_embd_head_v    = 128
0.00.140.311 I print_info: n_gqa            = 1
0.00.140.312 I print_info: n_embd_k_gqa     = 2048
0.00.140.313 I print_info: n_embd_v_gqa     = 2048
0.00.140.314 I print_info: f_norm_eps       = 1.0e-05
0.00.140.314 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.140.314 I print_info: f_clamp_kqv      = 0.0e+00
0.00.140.314 I print_info: f_max_alibi_bias = 0.0e+00
0.00.140.315 I print_info: f_logit_scale    = 0.0e+00
0.00.140.316 I print_info: n_ff             = 8192
0.00.140.316 I print_info: n_expert         = 0
0.00.140.316 I print_info: n_expert_used    = 0
0.00.140.316 I print_info: causal attn      = 1
0.00.140.316 I print_info: pooling type     = 0
0.00.140.316 I print_info: rope type        = 2
0.00.140.317 I print_info: rope scaling     = linear
0.00.140.317 I print_info: freq_base_train  = 10000.0
0.00.140.318 I print_info: freq_scale_train = 1
0.00.140.318 I print_info: n_ctx_orig_yarn  = 2048
0.00.140.318 I print_info: rope_finetuned   = unknown
0.00.140.318 I print_info: ssm_d_conv       = 0
0.00.140.318 I print_info: ssm_d_inner      = 0
0.00.140.319 I print_info: ssm_d_state      = 0
0.00.140.319 I print_info: ssm_dt_rank      = 0
0.00.140.319 I print_info: ssm_dt_b_c_rms   = 0
0.00.140.319 I print_info: model type       = 1.4B
0.00.140.320 I print_info: model params     = 1.41 B
0.00.140.320 I print_info: general.name     = 1.4B
0.00.140.320 I print_info: vocab type       = BPE
0.00.140.321 I print_info: n_vocab          = 50304
0.00.140.321 I print_info: n_merges         = 50009
0.00.140.321 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.140.321 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.140.322 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.140.322 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.140.322 I print_info: LF token         = 187 'Ċ'
0.00.140.322 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.140.323 I print_info: max token length = 1024
0.00.201.893 I load_tensors: offloading 24 repeating layers to GPU
0.00.201.899 I load_tensors: offloading output layer to GPU
0.00.201.899 I load_tensors: offloaded 25/25 layers to GPU
0.00.201.926 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.201.927 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.202.459 I llama_init_from_model: n_seq_max     = 1
0.00.202.460 I llama_init_from_model: n_ctx         = 2048
0.00.202.460 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.202.461 I llama_init_from_model: n_batch       = 2048
0.00.202.461 I llama_init_from_model: n_ubatch      = 512
0.00.202.461 I llama_init_from_model: flash_attn    = 0
0.00.202.462 I llama_init_from_model: freq_base     = 10000.0
0.00.202.462 I llama_init_from_model: freq_scale    = 1
0.00.202.464 I ggml_metal_init: allocating
0.00.202.500 I ggml_metal_init: found device: Apple M4
0.00.202.505 I ggml_metal_init: picking default device: Apple M4
0.00.203.235 I ggml_metal_init: using embedded metal library
0.00.213.674 I ggml_metal_init: GPU name:   Apple M4
0.00.213.676 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.213.676 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.213.677 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.213.677 I ggml_metal_init: simdgroup reduction   = true
0.00.213.677 I ggml_metal_init: simdgroup matrix mul. = true
0.00.213.677 I ggml_metal_init: has residency sets    = true
0.00.213.678 I ggml_metal_init: has bfloat            = true
0.00.213.678 I ggml_metal_init: use bfloat            = true
0.00.213.678 I ggml_metal_init: hasUnifiedMemory      = true
0.00.213.679 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.237.556 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.271.829 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.271.836 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.271.882 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.275.453 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.275.455 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.275.455 I llama_init_from_model: graph nodes  = 967
0.00.275.455 I llama_init_from_model: graph splits = 2
0.00.275.461 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.275.577 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.275.577 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.336.019 I main: llama threadpool init, n_threads = 4
0.00.336.064 I 
0.00.336.095 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.336.095 I 
0.00.336.240 I sampler seed: 1234
0.00.336.244 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.336.268 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.336.270 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.336.270 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.286.229 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 57028.11 tokens per second)
0.02.286.230 I llama_perf_context_print:        load time =     252.63 ms
0.02.286.230 I llama_perf_context_print: prompt eval time =      43.94 ms /     7 tokens (    6.28 ms per token,   159.32 tokens per second)
0.02.286.231 I llama_perf_context_print:        eval time =    1903.14 ms /    63 runs   (   30.21 ms per token,    33.10 tokens per second)
0.02.286.233 I llama_perf_context_print:       total time =    1950.97 ms /    70 tokens
0.02.286.554 I ggml_metal_free: deallocating

real	0m2.625s
user	0m0.137s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.835 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.518 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.038 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.043 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.045 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.050 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.051 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.051 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.052 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.054 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.055 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.055 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.056 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.056 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.056 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.057 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.059 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.059 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.059 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.410 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.557 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.454 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.456 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.457 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.457 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.457 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.458 I llama_model_loader: - type  f32:  194 tensors
0.00.054.458 I llama_model_loader: - type  f16:   98 tensors
0.00.054.459 I print_info: file format = GGUF V3 (latest)
0.00.054.460 I print_info: file type   = all F32 (guessed)
0.00.054.462 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.510 I load: special tokens cache size = 25
0.00.075.308 I load: token to piece cache size = 0.2984 MB
0.00.075.311 I print_info: arch             = gptneox
0.00.075.311 I print_info: vocab_only       = 0
0.00.075.312 I print_info: n_ctx_train      = 2048
0.00.075.312 I print_info: n_embd           = 2048
0.00.075.312 I print_info: n_layer          = 24
0.00.075.315 I print_info: n_head           = 16
0.00.075.316 I print_info: n_head_kv        = 16
0.00.075.316 I print_info: n_rot            = 32
0.00.075.316 I print_info: n_swa            = 0
0.00.075.317 I print_info: n_embd_head_k    = 128
0.00.075.317 I print_info: n_embd_head_v    = 128
0.00.075.319 I print_info: n_gqa            = 1
0.00.075.319 I print_info: n_embd_k_gqa     = 2048
0.00.075.320 I print_info: n_embd_v_gqa     = 2048
0.00.075.322 I print_info: f_norm_eps       = 1.0e-05
0.00.075.322 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.322 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.323 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.323 I print_info: f_logit_scale    = 0.0e+00
0.00.075.323 I print_info: n_ff             = 8192
0.00.075.324 I print_info: n_expert         = 0
0.00.075.324 I print_info: n_expert_used    = 0
0.00.075.324 I print_info: causal attn      = 1
0.00.075.324 I print_info: pooling type     = 0
0.00.075.324 I print_info: rope type        = 2
0.00.075.325 I print_info: rope scaling     = linear
0.00.075.325 I print_info: freq_base_train  = 10000.0
0.00.075.325 I print_info: freq_scale_train = 1
0.00.075.326 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.326 I print_info: rope_finetuned   = unknown
0.00.075.327 I print_info: ssm_d_conv       = 0
0.00.075.327 I print_info: ssm_d_inner      = 0
0.00.075.327 I print_info: ssm_d_state      = 0
0.00.075.328 I print_info: ssm_dt_rank      = 0
0.00.075.328 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.328 I print_info: model type       = 1.4B
0.00.075.328 I print_info: model params     = 1.41 B
0.00.075.328 I print_info: general.name     = 1.4B
0.00.075.329 I print_info: vocab type       = BPE
0.00.075.329 I print_info: n_vocab          = 50304
0.00.075.329 I print_info: n_merges         = 50009
0.00.075.330 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.330 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.330 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.330 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.331 I print_info: LF token         = 187 'Ċ'
0.00.075.332 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.332 I print_info: max token length = 1024
0.01.660.135 I load_tensors: offloading 24 repeating layers to GPU
0.01.660.139 I load_tensors: offloading output layer to GPU
0.01.660.139 I load_tensors: offloaded 25/25 layers to GPU
0.01.660.166 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.660.168 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.660.893 I llama_init_from_model: n_seq_max     = 1
0.01.660.894 I llama_init_from_model: n_ctx         = 128
0.01.660.894 I llama_init_from_model: n_ctx_per_seq = 128
0.01.660.895 I llama_init_from_model: n_batch       = 128
0.01.660.895 I llama_init_from_model: n_ubatch      = 128
0.01.660.895 I llama_init_from_model: flash_attn    = 0
0.01.660.898 I llama_init_from_model: freq_base     = 10000.0
0.01.660.902 I llama_init_from_model: freq_scale    = 1
0.01.660.902 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.660.906 I ggml_metal_init: allocating
0.01.660.974 I ggml_metal_init: found device: Apple M4
0.01.660.981 I ggml_metal_init: picking default device: Apple M4
0.01.662.092 I ggml_metal_init: using embedded metal library
0.01.665.844 I ggml_metal_init: GPU name:   Apple M4
0.01.665.846 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.665.847 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.665.848 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.665.848 I ggml_metal_init: simdgroup reduction   = true
0.01.665.848 I ggml_metal_init: simdgroup matrix mul. = true
0.01.665.848 I ggml_metal_init: has residency sets    = true
0.01.665.848 I ggml_metal_init: has bfloat            = true
0.01.665.848 I ggml_metal_init: use bfloat            = true
0.01.665.849 I ggml_metal_init: hasUnifiedMemory      = true
0.01.665.850 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.676.606 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.678.294 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.678.297 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.678.321 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.680.009 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.680.011 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.680.011 I llama_init_from_model: graph nodes  = 967
0.01.680.011 I llama_init_from_model: graph splits = 2
0.01.680.013 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.680.013 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.714.874 I 
0.01.714.911 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.714.916 I perplexity: tokenizing the input ..
0.01.719.767 I perplexity: tokenization took 4.849 ms
0.01.719.770 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.838.295 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.839.632 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.839.667 I llama_perf_context_print:        load time =    1691.35 ms
0.01.839.668 I llama_perf_context_print: prompt eval time =     118.22 ms /   128 tokens (    0.92 ms per token,  1082.73 tokens per second)
0.01.839.668 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.839.669 I llama_perf_context_print:       total time =     124.79 ms /   129 tokens
0.01.840.041 I ggml_metal_free: deallocating

real	0m2.029s
user	0m0.096s
sys	0m0.284s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.010.112 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.759 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.030.766 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.774 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.774 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.775 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.775 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.775 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.776 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.776 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.777 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.777 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.777 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.777 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.778 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.781 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.782 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.782 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.699 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.728 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.724 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.726 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.727 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.727 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.728 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.728 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.728 I llama_model_loader: - type  f32:  194 tensors
0.00.039.729 I llama_model_loader: - type q8_0:   98 tensors
0.00.039.730 I print_info: file format = GGUF V3 (latest)
0.00.039.735 I print_info: file type   = Q8_0
0.00.039.736 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.049.090 I load: special tokens cache size = 25
0.00.055.563 I load: token to piece cache size = 0.2984 MB
0.00.055.567 I print_info: arch             = gptneox
0.00.055.567 I print_info: vocab_only       = 0
0.00.055.568 I print_info: n_ctx_train      = 2048
0.00.055.570 I print_info: n_embd           = 2048
0.00.055.570 I print_info: n_layer          = 24
0.00.055.576 I print_info: n_head           = 16
0.00.055.578 I print_info: n_head_kv        = 16
0.00.055.578 I print_info: n_rot            = 32
0.00.055.578 I print_info: n_swa            = 0
0.00.055.579 I print_info: n_embd_head_k    = 128
0.00.055.579 I print_info: n_embd_head_v    = 128
0.00.055.580 I print_info: n_gqa            = 1
0.00.055.584 I print_info: n_embd_k_gqa     = 2048
0.00.055.585 I print_info: n_embd_v_gqa     = 2048
0.00.055.585 I print_info: f_norm_eps       = 1.0e-05
0.00.055.586 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.586 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.586 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.587 I print_info: f_logit_scale    = 0.0e+00
0.00.055.588 I print_info: n_ff             = 8192
0.00.055.588 I print_info: n_expert         = 0
0.00.055.589 I print_info: n_expert_used    = 0
0.00.055.589 I print_info: causal attn      = 1
0.00.055.589 I print_info: pooling type     = 0
0.00.055.589 I print_info: rope type        = 2
0.00.055.589 I print_info: rope scaling     = linear
0.00.055.590 I print_info: freq_base_train  = 10000.0
0.00.055.590 I print_info: freq_scale_train = 1
0.00.055.590 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.591 I print_info: rope_finetuned   = unknown
0.00.055.591 I print_info: ssm_d_conv       = 0
0.00.055.593 I print_info: ssm_d_inner      = 0
0.00.055.593 I print_info: ssm_d_state      = 0
0.00.055.593 I print_info: ssm_dt_rank      = 0
0.00.055.593 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.594 I print_info: model type       = 1.4B
0.00.055.594 I print_info: model params     = 1.41 B
0.00.055.594 I print_info: general.name     = 1.4B
0.00.055.596 I print_info: vocab type       = BPE
0.00.055.596 I print_info: n_vocab          = 50304
0.00.055.596 I print_info: n_merges         = 50009
0.00.055.596 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.597 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.597 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.597 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.597 I print_info: LF token         = 187 'Ċ'
0.00.055.598 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.598 I print_info: max token length = 1024
0.01.212.389 I load_tensors: offloading 24 repeating layers to GPU
0.01.212.394 I load_tensors: offloading output layer to GPU
0.01.212.395 I load_tensors: offloaded 25/25 layers to GPU
0.01.212.417 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.212.417 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.213.484 I llama_init_from_model: n_seq_max     = 1
0.01.213.486 I llama_init_from_model: n_ctx         = 2048
0.01.213.486 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.213.486 I llama_init_from_model: n_batch       = 2048
0.01.213.487 I llama_init_from_model: n_ubatch      = 512
0.01.213.487 I llama_init_from_model: flash_attn    = 0
0.01.213.488 I llama_init_from_model: freq_base     = 10000.0
0.01.213.488 I llama_init_from_model: freq_scale    = 1
0.01.213.489 I ggml_metal_init: allocating
0.01.213.499 I ggml_metal_init: found device: Apple M4
0.01.213.506 I ggml_metal_init: picking default device: Apple M4
0.01.214.807 I ggml_metal_init: using embedded metal library
0.01.220.204 I ggml_metal_init: GPU name:   Apple M4
0.01.220.207 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.220.208 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.220.209 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.220.209 I ggml_metal_init: simdgroup reduction   = true
0.01.220.210 I ggml_metal_init: simdgroup matrix mul. = true
0.01.220.210 I ggml_metal_init: has residency sets    = true
0.01.220.210 I ggml_metal_init: has bfloat            = true
0.01.220.210 I ggml_metal_init: use bfloat            = true
0.01.220.211 I ggml_metal_init: hasUnifiedMemory      = true
0.01.220.212 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.235.753 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.285.465 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.285.473 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.285.508 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.291.048 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.291.050 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.291.051 I llama_init_from_model: graph nodes  = 967
0.01.291.051 I llama_init_from_model: graph splits = 2
0.01.291.061 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.291.190 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.291.191 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.339.016 I main: llama threadpool init, n_threads = 4
0.01.339.058 I 
0.01.339.083 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.339.083 I 
0.01.339.217 I sampler seed: 1234
0.01.339.221 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.339.232 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.339.232 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.339.232 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.447.351 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48200.95 tokens per second)
0.02.447.352 I llama_perf_context_print:        load time =    1328.27 ms
0.02.447.353 I llama_perf_context_print: prompt eval time =      49.28 ms /     7 tokens (    7.04 ms per token,   142.05 tokens per second)
0.02.447.353 I llama_perf_context_print:        eval time =    1056.29 ms /    63 runs   (   16.77 ms per token,    59.64 tokens per second)
0.02.447.353 I llama_perf_context_print:       total time =    1108.96 ms /    70 tokens
0.02.447.697 I ggml_metal_free: deallocating

real	0m2.466s
user	0m0.109s
sys	0m0.292s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.268 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.130 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.270 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.277 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.279 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.280 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.280 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.280 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.281 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.282 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.284 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.284 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.285 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.285 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.285 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.286 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.288 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.288 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.289 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.102 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.099 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.874 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.875 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.876 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.876 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.876 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.877 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.877 I llama_model_loader: - type  f32:  194 tensors
0.00.025.878 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.879 I print_info: file format = GGUF V3 (latest)
0.00.025.879 I print_info: file type   = Q8_0
0.00.025.880 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.961 I load: special tokens cache size = 25
0.00.040.026 I load: token to piece cache size = 0.2984 MB
0.00.040.031 I print_info: arch             = gptneox
0.00.040.031 I print_info: vocab_only       = 0
0.00.040.031 I print_info: n_ctx_train      = 2048
0.00.040.031 I print_info: n_embd           = 2048
0.00.040.031 I print_info: n_layer          = 24
0.00.040.036 I print_info: n_head           = 16
0.00.040.037 I print_info: n_head_kv        = 16
0.00.040.040 I print_info: n_rot            = 32
0.00.040.040 I print_info: n_swa            = 0
0.00.040.040 I print_info: n_embd_head_k    = 128
0.00.040.040 I print_info: n_embd_head_v    = 128
0.00.040.041 I print_info: n_gqa            = 1
0.00.040.042 I print_info: n_embd_k_gqa     = 2048
0.00.040.042 I print_info: n_embd_v_gqa     = 2048
0.00.040.043 I print_info: f_norm_eps       = 1.0e-05
0.00.040.046 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.047 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.047 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.047 I print_info: f_logit_scale    = 0.0e+00
0.00.040.048 I print_info: n_ff             = 8192
0.00.040.049 I print_info: n_expert         = 0
0.00.040.050 I print_info: n_expert_used    = 0
0.00.040.050 I print_info: causal attn      = 1
0.00.040.050 I print_info: pooling type     = 0
0.00.040.051 I print_info: rope type        = 2
0.00.040.051 I print_info: rope scaling     = linear
0.00.040.051 I print_info: freq_base_train  = 10000.0
0.00.040.051 I print_info: freq_scale_train = 1
0.00.040.052 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.052 I print_info: rope_finetuned   = unknown
0.00.040.054 I print_info: ssm_d_conv       = 0
0.00.040.054 I print_info: ssm_d_inner      = 0
0.00.040.054 I print_info: ssm_d_state      = 0
0.00.040.054 I print_info: ssm_dt_rank      = 0
0.00.040.054 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.055 I print_info: model type       = 1.4B
0.00.040.055 I print_info: model params     = 1.41 B
0.00.040.055 I print_info: general.name     = 1.4B
0.00.040.058 I print_info: vocab type       = BPE
0.00.040.058 I print_info: n_vocab          = 50304
0.00.040.058 I print_info: n_merges         = 50009
0.00.040.058 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.058 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.059 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.059 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.059 I print_info: LF token         = 187 'Ċ'
0.00.040.059 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.059 I print_info: max token length = 1024
0.00.760.953 I load_tensors: offloading 24 repeating layers to GPU
0.00.760.959 I load_tensors: offloading output layer to GPU
0.00.760.960 I load_tensors: offloaded 25/25 layers to GPU
0.00.760.985 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.760.988 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.762.218 I llama_init_from_model: n_seq_max     = 1
0.00.762.220 I llama_init_from_model: n_ctx         = 128
0.00.762.220 I llama_init_from_model: n_ctx_per_seq = 128
0.00.762.220 I llama_init_from_model: n_batch       = 128
0.00.762.223 I llama_init_from_model: n_ubatch      = 128
0.00.762.223 I llama_init_from_model: flash_attn    = 0
0.00.762.224 I llama_init_from_model: freq_base     = 10000.0
0.00.762.224 I llama_init_from_model: freq_scale    = 1
0.00.762.225 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.762.228 I ggml_metal_init: allocating
0.00.762.284 I ggml_metal_init: found device: Apple M4
0.00.762.294 I ggml_metal_init: picking default device: Apple M4
0.00.763.486 I ggml_metal_init: using embedded metal library
0.00.768.721 I ggml_metal_init: GPU name:   Apple M4
0.00.768.724 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.768.725 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.768.726 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.768.726 I ggml_metal_init: simdgroup reduction   = true
0.00.768.726 I ggml_metal_init: simdgroup matrix mul. = true
0.00.768.726 I ggml_metal_init: has residency sets    = true
0.00.768.726 I ggml_metal_init: has bfloat            = true
0.00.768.727 I ggml_metal_init: use bfloat            = true
0.00.768.727 I ggml_metal_init: hasUnifiedMemory      = true
0.00.768.738 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.783.523 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.786.058 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.786.061 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.786.095 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.788.519 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.788.521 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.788.521 I llama_init_from_model: graph nodes  = 967
0.00.788.522 I llama_init_from_model: graph splits = 2
0.00.788.523 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.788.523 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.813.896 I 
0.00.813.937 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.813.942 I perplexity: tokenizing the input ..
0.00.819.716 I perplexity: tokenization took 5.772 ms
0.00.819.720 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.957.052 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.958.386 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.958.408 I llama_perf_context_print:        load time =     803.76 ms
0.00.958.409 I llama_perf_context_print: prompt eval time =     137.10 ms /   128 tokens (    1.07 ms per token,   933.62 tokens per second)
0.00.958.410 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.958.411 I llama_perf_context_print:       total time =     144.51 ms /   129 tokens
0.00.958.785 I ggml_metal_free: deallocating

real	0m0.975s
user	0m0.074s
sys	0m0.168s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.015.987 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.380 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.037.385 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.387 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.387 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.388 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.388 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.388 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.391 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.391 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.391 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.392 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.392 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.395 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.395 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.397 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.397 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.398 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.382 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.490 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.454 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.046.455 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.455 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.456 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.456 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.456 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.046.457 I llama_model_loader: - type  f32:  194 tensors
0.00.046.457 I llama_model_loader: - type q4_0:   97 tensors
0.00.046.457 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.458 I print_info: file format = GGUF V3 (latest)
0.00.046.458 I print_info: file type   = Q4_0
0.00.046.464 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.054.537 I load: special tokens cache size = 25
0.00.060.706 I load: token to piece cache size = 0.2984 MB
0.00.060.710 I print_info: arch             = gptneox
0.00.060.711 I print_info: vocab_only       = 0
0.00.060.711 I print_info: n_ctx_train      = 2048
0.00.060.711 I print_info: n_embd           = 2048
0.00.060.711 I print_info: n_layer          = 24
0.00.060.716 I print_info: n_head           = 16
0.00.060.716 I print_info: n_head_kv        = 16
0.00.060.717 I print_info: n_rot            = 32
0.00.060.717 I print_info: n_swa            = 0
0.00.060.717 I print_info: n_embd_head_k    = 128
0.00.060.717 I print_info: n_embd_head_v    = 128
0.00.060.718 I print_info: n_gqa            = 1
0.00.060.719 I print_info: n_embd_k_gqa     = 2048
0.00.060.719 I print_info: n_embd_v_gqa     = 2048
0.00.060.720 I print_info: f_norm_eps       = 1.0e-05
0.00.060.720 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.722 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.722 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.722 I print_info: f_logit_scale    = 0.0e+00
0.00.060.723 I print_info: n_ff             = 8192
0.00.060.723 I print_info: n_expert         = 0
0.00.060.723 I print_info: n_expert_used    = 0
0.00.060.723 I print_info: causal attn      = 1
0.00.060.723 I print_info: pooling type     = 0
0.00.060.723 I print_info: rope type        = 2
0.00.060.724 I print_info: rope scaling     = linear
0.00.060.724 I print_info: freq_base_train  = 10000.0
0.00.060.724 I print_info: freq_scale_train = 1
0.00.060.724 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.725 I print_info: rope_finetuned   = unknown
0.00.060.725 I print_info: ssm_d_conv       = 0
0.00.060.725 I print_info: ssm_d_inner      = 0
0.00.060.725 I print_info: ssm_d_state      = 0
0.00.060.725 I print_info: ssm_dt_rank      = 0
0.00.060.725 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.726 I print_info: model type       = 1.4B
0.00.060.726 I print_info: model params     = 1.41 B
0.00.060.726 I print_info: general.name     = 1.4B
0.00.060.727 I print_info: vocab type       = BPE
0.00.060.727 I print_info: n_vocab          = 50304
0.00.060.727 I print_info: n_merges         = 50009
0.00.060.727 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.727 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.727 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.729 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.729 I print_info: LF token         = 187 'Ċ'
0.00.060.730 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.730 I print_info: max token length = 1024
0.00.673.033 I load_tensors: offloading 24 repeating layers to GPU
0.00.673.045 I load_tensors: offloading output layer to GPU
0.00.673.046 I load_tensors: offloaded 25/25 layers to GPU
0.00.673.074 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.673.075 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.674.286 I llama_init_from_model: n_seq_max     = 1
0.00.674.297 I llama_init_from_model: n_ctx         = 2048
0.00.674.298 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.674.298 I llama_init_from_model: n_batch       = 2048
0.00.674.299 I llama_init_from_model: n_ubatch      = 512
0.00.674.299 I llama_init_from_model: flash_attn    = 0
0.00.674.301 I llama_init_from_model: freq_base     = 10000.0
0.00.674.302 I llama_init_from_model: freq_scale    = 1
0.00.674.304 I ggml_metal_init: allocating
0.00.674.364 I ggml_metal_init: found device: Apple M4
0.00.674.379 I ggml_metal_init: picking default device: Apple M4
0.00.676.155 I ggml_metal_init: using embedded metal library
0.00.681.718 I ggml_metal_init: GPU name:   Apple M4
0.00.681.730 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.681.731 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.681.732 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.681.733 I ggml_metal_init: simdgroup reduction   = true
0.00.681.733 I ggml_metal_init: simdgroup matrix mul. = true
0.00.681.733 I ggml_metal_init: has residency sets    = true
0.00.681.733 I ggml_metal_init: has bfloat            = true
0.00.681.734 I ggml_metal_init: use bfloat            = true
0.00.681.736 I ggml_metal_init: hasUnifiedMemory      = true
0.00.681.740 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.702.320 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.750.354 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.750.361 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.750.407 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.755.350 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.755.353 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.755.353 I llama_init_from_model: graph nodes  = 967
0.00.755.353 I llama_init_from_model: graph splits = 2
0.00.755.359 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.755.488 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.755.489 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.814.515 I main: llama threadpool init, n_threads = 4
0.00.814.568 I 
0.00.814.592 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.814.594 I 
0.00.814.777 I sampler seed: 1234
0.00.814.782 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.814.824 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.814.828 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.814.828 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.498.342 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52475.98 tokens per second)
0.01.498.342 I llama_perf_context_print:        load time =     797.89 ms
0.01.498.346 I llama_perf_context_print: prompt eval time =      49.53 ms /     7 tokens (    7.08 ms per token,   141.32 tokens per second)
0.01.498.348 I llama_perf_context_print:        eval time =     631.13 ms /    63 runs   (   10.02 ms per token,    99.82 tokens per second)
0.01.498.349 I llama_perf_context_print:       total time =     684.47 ms /    70 tokens
0.01.498.576 I ggml_metal_free: deallocating

real	0m1.524s
user	0m0.113s
sys	0m0.186s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.242 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.068 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.073 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.080 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.081 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.081 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.081 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.082 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.083 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.083 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.083 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.084 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.084 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.084 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.085 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.087 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.087 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.088 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.943 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.955 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.826 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.828 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.828 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.828 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.829 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.829 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.830 I llama_model_loader: - type  f32:  194 tensors
0.00.024.830 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.830 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.831 I print_info: file format = GGUF V3 (latest)
0.00.024.831 I print_info: file type   = Q4_0
0.00.024.833 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.143 I load: special tokens cache size = 25
0.00.039.004 I load: token to piece cache size = 0.2984 MB
0.00.039.009 I print_info: arch             = gptneox
0.00.039.010 I print_info: vocab_only       = 0
0.00.039.010 I print_info: n_ctx_train      = 2048
0.00.039.010 I print_info: n_embd           = 2048
0.00.039.010 I print_info: n_layer          = 24
0.00.039.014 I print_info: n_head           = 16
0.00.039.015 I print_info: n_head_kv        = 16
0.00.039.015 I print_info: n_rot            = 32
0.00.039.015 I print_info: n_swa            = 0
0.00.039.015 I print_info: n_embd_head_k    = 128
0.00.039.018 I print_info: n_embd_head_v    = 128
0.00.039.019 I print_info: n_gqa            = 1
0.00.039.019 I print_info: n_embd_k_gqa     = 2048
0.00.039.020 I print_info: n_embd_v_gqa     = 2048
0.00.039.020 I print_info: f_norm_eps       = 1.0e-05
0.00.039.024 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.024 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.026 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.026 I print_info: f_logit_scale    = 0.0e+00
0.00.039.027 I print_info: n_ff             = 8192
0.00.039.027 I print_info: n_expert         = 0
0.00.039.027 I print_info: n_expert_used    = 0
0.00.039.027 I print_info: causal attn      = 1
0.00.039.028 I print_info: pooling type     = 0
0.00.039.028 I print_info: rope type        = 2
0.00.039.028 I print_info: rope scaling     = linear
0.00.039.028 I print_info: freq_base_train  = 10000.0
0.00.039.029 I print_info: freq_scale_train = 1
0.00.039.029 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.030 I print_info: rope_finetuned   = unknown
0.00.039.030 I print_info: ssm_d_conv       = 0
0.00.039.030 I print_info: ssm_d_inner      = 0
0.00.039.030 I print_info: ssm_d_state      = 0
0.00.039.031 I print_info: ssm_dt_rank      = 0
0.00.039.031 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.031 I print_info: model type       = 1.4B
0.00.039.031 I print_info: model params     = 1.41 B
0.00.039.032 I print_info: general.name     = 1.4B
0.00.039.032 I print_info: vocab type       = BPE
0.00.039.032 I print_info: n_vocab          = 50304
0.00.039.032 I print_info: n_merges         = 50009
0.00.039.032 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.033 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.033 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.033 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.033 I print_info: LF token         = 187 'Ċ'
0.00.039.033 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.033 I print_info: max token length = 1024
0.00.569.645 I load_tensors: offloading 24 repeating layers to GPU
0.00.569.659 I load_tensors: offloading output layer to GPU
0.00.569.660 I load_tensors: offloaded 25/25 layers to GPU
0.00.569.699 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.569.700 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.571.332 I llama_init_from_model: n_seq_max     = 1
0.00.571.338 I llama_init_from_model: n_ctx         = 128
0.00.571.338 I llama_init_from_model: n_ctx_per_seq = 128
0.00.571.344 I llama_init_from_model: n_batch       = 128
0.00.571.345 I llama_init_from_model: n_ubatch      = 128
0.00.571.345 I llama_init_from_model: flash_attn    = 0
0.00.571.347 I llama_init_from_model: freq_base     = 10000.0
0.00.571.348 I llama_init_from_model: freq_scale    = 1
0.00.571.349 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.571.354 I ggml_metal_init: allocating
0.00.571.484 I ggml_metal_init: found device: Apple M4
0.00.571.499 I ggml_metal_init: picking default device: Apple M4
0.00.573.471 I ggml_metal_init: using embedded metal library
0.00.580.026 I ggml_metal_init: GPU name:   Apple M4
0.00.580.033 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.580.034 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.580.035 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.580.036 I ggml_metal_init: simdgroup reduction   = true
0.00.580.036 I ggml_metal_init: simdgroup matrix mul. = true
0.00.580.036 I ggml_metal_init: has residency sets    = true
0.00.580.037 I ggml_metal_init: has bfloat            = true
0.00.580.037 I ggml_metal_init: use bfloat            = true
0.00.580.038 I ggml_metal_init: hasUnifiedMemory      = true
0.00.580.042 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.597.972 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.601.555 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.601.561 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.601.609 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.604.883 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.604.885 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.604.885 I llama_init_from_model: graph nodes  = 967
0.00.604.885 I llama_init_from_model: graph splits = 2
0.00.604.889 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.604.889 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.628.473 I 
0.00.628.547 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.628.554 I perplexity: tokenizing the input ..
0.00.636.060 I perplexity: tokenization took 7.503 ms
0.00.636.066 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.759.931 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.761.267 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.761.291 I llama_perf_context_print:        load time =     619.22 ms
0.00.761.292 I llama_perf_context_print: prompt eval time =     123.00 ms /   128 tokens (    0.96 ms per token,  1040.66 tokens per second)
0.00.761.292 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.761.293 I llama_perf_context_print:       total time =     132.82 ms /   129 tokens
0.00.761.712 I ggml_metal_free: deallocating

real	0m0.776s
user	0m0.080s
sys	0m0.115s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.013.896 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.547 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.030.551 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.553 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.553 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.553 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.555 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.555 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.556 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.556 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.556 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.557 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.557 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.558 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.558 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.560 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.561 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.561 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.509 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.582 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.488 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.489 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.489 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.490 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.490 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.490 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.039.491 I llama_model_loader: - type  f32:  194 tensors
0.00.039.491 I llama_model_loader: - type q4_1:   97 tensors
0.00.039.491 I llama_model_loader: - type q6_K:    1 tensors
0.00.039.492 I print_info: file format = GGUF V3 (latest)
0.00.039.492 I print_info: file type   = Q4_1
0.00.039.493 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.048.672 I load: special tokens cache size = 25
0.00.055.802 I load: token to piece cache size = 0.2984 MB
0.00.055.805 I print_info: arch             = gptneox
0.00.055.805 I print_info: vocab_only       = 0
0.00.055.805 I print_info: n_ctx_train      = 2048
0.00.055.806 I print_info: n_embd           = 2048
0.00.055.806 I print_info: n_layer          = 24
0.00.055.809 I print_info: n_head           = 16
0.00.055.810 I print_info: n_head_kv        = 16
0.00.055.810 I print_info: n_rot            = 32
0.00.055.810 I print_info: n_swa            = 0
0.00.055.812 I print_info: n_embd_head_k    = 128
0.00.055.812 I print_info: n_embd_head_v    = 128
0.00.055.813 I print_info: n_gqa            = 1
0.00.055.814 I print_info: n_embd_k_gqa     = 2048
0.00.055.815 I print_info: n_embd_v_gqa     = 2048
0.00.055.815 I print_info: f_norm_eps       = 1.0e-05
0.00.055.816 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.816 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.816 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.816 I print_info: f_logit_scale    = 0.0e+00
0.00.055.817 I print_info: n_ff             = 8192
0.00.055.817 I print_info: n_expert         = 0
0.00.055.817 I print_info: n_expert_used    = 0
0.00.055.817 I print_info: causal attn      = 1
0.00.055.817 I print_info: pooling type     = 0
0.00.055.819 I print_info: rope type        = 2
0.00.055.820 I print_info: rope scaling     = linear
0.00.055.820 I print_info: freq_base_train  = 10000.0
0.00.055.821 I print_info: freq_scale_train = 1
0.00.055.821 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.821 I print_info: rope_finetuned   = unknown
0.00.055.821 I print_info: ssm_d_conv       = 0
0.00.055.821 I print_info: ssm_d_inner      = 0
0.00.055.821 I print_info: ssm_d_state      = 0
0.00.055.822 I print_info: ssm_dt_rank      = 0
0.00.055.822 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.822 I print_info: model type       = 1.4B
0.00.055.822 I print_info: model params     = 1.41 B
0.00.055.822 I print_info: general.name     = 1.4B
0.00.055.827 I print_info: vocab type       = BPE
0.00.055.827 I print_info: n_vocab          = 50304
0.00.055.828 I print_info: n_merges         = 50009
0.00.055.828 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.828 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.828 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.828 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.829 I print_info: LF token         = 187 'Ċ'
0.00.055.829 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.829 I print_info: max token length = 1024
0.00.714.382 I load_tensors: offloading 24 repeating layers to GPU
0.00.714.398 I load_tensors: offloading output layer to GPU
0.00.714.399 I load_tensors: offloaded 25/25 layers to GPU
0.00.714.431 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.714.432 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.715.875 I llama_init_from_model: n_seq_max     = 1
0.00.715.885 I llama_init_from_model: n_ctx         = 2048
0.00.715.886 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.715.886 I llama_init_from_model: n_batch       = 2048
0.00.715.886 I llama_init_from_model: n_ubatch      = 512
0.00.715.887 I llama_init_from_model: flash_attn    = 0
0.00.715.889 I llama_init_from_model: freq_base     = 10000.0
0.00.715.890 I llama_init_from_model: freq_scale    = 1
0.00.715.897 I ggml_metal_init: allocating
0.00.715.936 I ggml_metal_init: found device: Apple M4
0.00.715.946 I ggml_metal_init: picking default device: Apple M4
0.00.717.788 I ggml_metal_init: using embedded metal library
0.00.724.519 I ggml_metal_init: GPU name:   Apple M4
0.00.724.524 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.724.524 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.724.525 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.724.526 I ggml_metal_init: simdgroup reduction   = true
0.00.724.526 I ggml_metal_init: simdgroup matrix mul. = true
0.00.724.527 I ggml_metal_init: has residency sets    = true
0.00.724.527 I ggml_metal_init: has bfloat            = true
0.00.724.527 I ggml_metal_init: use bfloat            = true
0.00.724.528 I ggml_metal_init: hasUnifiedMemory      = true
0.00.724.529 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.742.688 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.788.579 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.788.587 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.788.625 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.793.142 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.793.144 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.793.144 I llama_init_from_model: graph nodes  = 967
0.00.793.145 I llama_init_from_model: graph splits = 2
0.00.793.150 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.793.283 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.793.284 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.851.813 I main: llama threadpool init, n_threads = 4
0.00.851.853 I 
0.00.851.875 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.851.875 I 
0.00.852.043 I sampler seed: 1234
0.00.852.046 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.852.057 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.852.058 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.852.058 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.582.633 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56259.90 tokens per second)
0.01.582.634 I llama_perf_context_print:        load time =     837.28 ms
0.01.582.635 I llama_perf_context_print: prompt eval time =      48.82 ms /     7 tokens (    6.97 ms per token,   143.38 tokens per second)
0.01.582.635 I llama_perf_context_print:        eval time =     679.01 ms /    63 runs   (   10.78 ms per token,    92.78 tokens per second)
0.01.582.636 I llama_perf_context_print:       total time =     731.46 ms /    70 tokens
0.01.582.942 I ggml_metal_free: deallocating

real	0m1.598s
user	0m0.113s
sys	0m0.193s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.931 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.419 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.425 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.431 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.432 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.432 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.433 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.433 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.434 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.435 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.435 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.436 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.436 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.437 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.438 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.440 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.440 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.440 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.333 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.393 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.259 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.261 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.261 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.261 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.262 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.262 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.262 I llama_model_loader: - type  f32:  194 tensors
0.00.026.263 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.263 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.264 I print_info: file format = GGUF V3 (latest)
0.00.026.264 I print_info: file type   = Q4_1
0.00.026.266 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.850 I load: special tokens cache size = 25
0.00.040.938 I load: token to piece cache size = 0.2984 MB
0.00.040.943 I print_info: arch             = gptneox
0.00.040.943 I print_info: vocab_only       = 0
0.00.040.943 I print_info: n_ctx_train      = 2048
0.00.040.943 I print_info: n_embd           = 2048
0.00.040.943 I print_info: n_layer          = 24
0.00.040.947 I print_info: n_head           = 16
0.00.040.948 I print_info: n_head_kv        = 16
0.00.040.948 I print_info: n_rot            = 32
0.00.040.948 I print_info: n_swa            = 0
0.00.040.948 I print_info: n_embd_head_k    = 128
0.00.040.948 I print_info: n_embd_head_v    = 128
0.00.040.949 I print_info: n_gqa            = 1
0.00.040.950 I print_info: n_embd_k_gqa     = 2048
0.00.040.953 I print_info: n_embd_v_gqa     = 2048
0.00.040.953 I print_info: f_norm_eps       = 1.0e-05
0.00.040.954 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.954 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.954 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.954 I print_info: f_logit_scale    = 0.0e+00
0.00.040.955 I print_info: n_ff             = 8192
0.00.040.955 I print_info: n_expert         = 0
0.00.040.955 I print_info: n_expert_used    = 0
0.00.040.955 I print_info: causal attn      = 1
0.00.040.955 I print_info: pooling type     = 0
0.00.040.955 I print_info: rope type        = 2
0.00.040.956 I print_info: rope scaling     = linear
0.00.040.956 I print_info: freq_base_train  = 10000.0
0.00.040.956 I print_info: freq_scale_train = 1
0.00.040.957 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.957 I print_info: rope_finetuned   = unknown
0.00.040.958 I print_info: ssm_d_conv       = 0
0.00.040.958 I print_info: ssm_d_inner      = 0
0.00.040.958 I print_info: ssm_d_state      = 0
0.00.040.958 I print_info: ssm_dt_rank      = 0
0.00.040.958 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.958 I print_info: model type       = 1.4B
0.00.040.960 I print_info: model params     = 1.41 B
0.00.040.960 I print_info: general.name     = 1.4B
0.00.040.960 I print_info: vocab type       = BPE
0.00.040.961 I print_info: n_vocab          = 50304
0.00.040.961 I print_info: n_merges         = 50009
0.00.040.961 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.961 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.961 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.961 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.962 I print_info: LF token         = 187 'Ċ'
0.00.040.962 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.962 I print_info: max token length = 1024
0.00.677.772 I load_tensors: offloading 24 repeating layers to GPU
0.00.677.788 I load_tensors: offloading output layer to GPU
0.00.677.788 I load_tensors: offloaded 25/25 layers to GPU
0.00.677.823 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.677.824 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.679.367 I llama_init_from_model: n_seq_max     = 1
0.00.679.374 I llama_init_from_model: n_ctx         = 128
0.00.679.374 I llama_init_from_model: n_ctx_per_seq = 128
0.00.679.375 I llama_init_from_model: n_batch       = 128
0.00.679.375 I llama_init_from_model: n_ubatch      = 128
0.00.679.376 I llama_init_from_model: flash_attn    = 0
0.00.679.378 I llama_init_from_model: freq_base     = 10000.0
0.00.679.378 I llama_init_from_model: freq_scale    = 1
0.00.679.379 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.679.382 I ggml_metal_init: allocating
0.00.679.446 I ggml_metal_init: found device: Apple M4
0.00.679.467 I ggml_metal_init: picking default device: Apple M4
0.00.681.379 I ggml_metal_init: using embedded metal library
0.00.688.534 I ggml_metal_init: GPU name:   Apple M4
0.00.688.543 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.688.544 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.688.545 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.688.545 I ggml_metal_init: simdgroup reduction   = true
0.00.688.546 I ggml_metal_init: simdgroup matrix mul. = true
0.00.688.546 I ggml_metal_init: has residency sets    = true
0.00.688.546 I ggml_metal_init: has bfloat            = true
0.00.688.546 I ggml_metal_init: use bfloat            = true
0.00.688.549 I ggml_metal_init: hasUnifiedMemory      = true
0.00.688.553 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.706.369 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.709.946 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.709.950 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.710.106 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.713.292 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.713.294 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.713.294 I llama_init_from_model: graph nodes  = 967
0.00.713.295 I llama_init_from_model: graph splits = 2
0.00.713.298 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.713.298 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.755 I 
0.00.741.832 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.840 I perplexity: tokenizing the input ..
0.00.749.139 I perplexity: tokenization took 7.295 ms
0.00.749.146 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.886.365 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.887.732 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.887.754 I llama_perf_context_print:        load time =     731.81 ms
0.00.887.755 I llama_perf_context_print: prompt eval time =     136.34 ms /   128 tokens (    1.07 ms per token,   938.82 tokens per second)
0.00.887.755 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.887.756 I llama_perf_context_print:       total time =     146.00 ms /   129 tokens
0.00.888.096 I ggml_metal_free: deallocating

real	0m0.904s
user	0m0.080s
sys	0m0.128s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.873 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.246 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.251 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.252 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.253 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.254 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.255 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.255 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.256 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.256 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.258 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.258 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.259 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.259 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.260 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.261 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.262 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.262 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.052 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.044 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.820 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.821 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.822 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.822 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.822 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.822 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.823 I llama_model_loader: - type  f32:  194 tensors
0.00.024.823 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.823 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.824 I print_info: file format = GGUF V3 (latest)
0.00.024.824 I print_info: file type   = Q5_0
0.00.024.825 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.893 I load: special tokens cache size = 25
0.00.039.009 I load: token to piece cache size = 0.2984 MB
0.00.039.011 I print_info: arch             = gptneox
0.00.039.012 I print_info: vocab_only       = 0
0.00.039.012 I print_info: n_ctx_train      = 2048
0.00.039.012 I print_info: n_embd           = 2048
0.00.039.012 I print_info: n_layer          = 24
0.00.039.015 I print_info: n_head           = 16
0.00.039.015 I print_info: n_head_kv        = 16
0.00.039.015 I print_info: n_rot            = 32
0.00.039.016 I print_info: n_swa            = 0
0.00.039.016 I print_info: n_embd_head_k    = 128
0.00.039.016 I print_info: n_embd_head_v    = 128
0.00.039.017 I print_info: n_gqa            = 1
0.00.039.018 I print_info: n_embd_k_gqa     = 2048
0.00.039.018 I print_info: n_embd_v_gqa     = 2048
0.00.039.019 I print_info: f_norm_eps       = 1.0e-05
0.00.039.019 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.019 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.020 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.020 I print_info: f_logit_scale    = 0.0e+00
0.00.039.020 I print_info: n_ff             = 8192
0.00.039.022 I print_info: n_expert         = 0
0.00.039.022 I print_info: n_expert_used    = 0
0.00.039.023 I print_info: causal attn      = 1
0.00.039.024 I print_info: pooling type     = 0
0.00.039.025 I print_info: rope type        = 2
0.00.039.027 I print_info: rope scaling     = linear
0.00.039.027 I print_info: freq_base_train  = 10000.0
0.00.039.028 I print_info: freq_scale_train = 1
0.00.039.028 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.028 I print_info: rope_finetuned   = unknown
0.00.039.028 I print_info: ssm_d_conv       = 0
0.00.039.028 I print_info: ssm_d_inner      = 0
0.00.039.029 I print_info: ssm_d_state      = 0
0.00.039.029 I print_info: ssm_dt_rank      = 0
0.00.039.029 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.030 I print_info: model type       = 1.4B
0.00.039.030 I print_info: model params     = 1.41 B
0.00.039.030 I print_info: general.name     = 1.4B
0.00.039.031 I print_info: vocab type       = BPE
0.00.039.031 I print_info: n_vocab          = 50304
0.00.039.032 I print_info: n_merges         = 50009
0.00.039.033 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.033 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.033 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.033 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.033 I print_info: LF token         = 187 'Ċ'
0.00.039.033 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.034 I print_info: max token length = 1024
0.00.681.733 I load_tensors: offloading 24 repeating layers to GPU
0.00.681.749 I load_tensors: offloading output layer to GPU
0.00.681.749 I load_tensors: offloaded 25/25 layers to GPU
0.00.681.782 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.681.783 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.683.174 I llama_init_from_model: n_seq_max     = 1
0.00.683.179 I llama_init_from_model: n_ctx         = 2048
0.00.683.179 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.683.180 I llama_init_from_model: n_batch       = 2048
0.00.683.181 I llama_init_from_model: n_ubatch      = 512
0.00.683.181 I llama_init_from_model: flash_attn    = 0
0.00.683.183 I llama_init_from_model: freq_base     = 10000.0
0.00.683.184 I llama_init_from_model: freq_scale    = 1
0.00.683.191 I ggml_metal_init: allocating
0.00.683.264 I ggml_metal_init: found device: Apple M4
0.00.683.277 I ggml_metal_init: picking default device: Apple M4
0.00.685.194 I ggml_metal_init: using embedded metal library
0.00.691.742 I ggml_metal_init: GPU name:   Apple M4
0.00.691.745 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.691.746 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.691.747 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.691.748 I ggml_metal_init: simdgroup reduction   = true
0.00.691.748 I ggml_metal_init: simdgroup matrix mul. = true
0.00.691.748 I ggml_metal_init: has residency sets    = true
0.00.691.748 I ggml_metal_init: has bfloat            = true
0.00.691.749 I ggml_metal_init: use bfloat            = true
0.00.691.749 I ggml_metal_init: hasUnifiedMemory      = true
0.00.691.751 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.709.285 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.760.107 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.760.114 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.760.149 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.764.759 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.764.761 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.764.762 I llama_init_from_model: graph nodes  = 967
0.00.764.762 I llama_init_from_model: graph splits = 2
0.00.764.773 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.764.892 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.764.893 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.820.692 I main: llama threadpool init, n_threads = 4
0.00.820.726 I 
0.00.820.747 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.820.748 I 
0.00.820.863 I sampler seed: 1234
0.00.820.867 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.820.884 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.820.885 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.820.885 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.620.500 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53064.28 tokens per second)
0.01.620.501 I llama_perf_context_print:        load time =     811.15 ms
0.01.620.501 I llama_perf_context_print: prompt eval time =      52.93 ms /     7 tokens (    7.56 ms per token,   132.24 tokens per second)
0.01.620.502 I llama_perf_context_print:        eval time =     743.78 ms /    63 runs   (   11.81 ms per token,    84.70 tokens per second)
0.01.620.503 I llama_perf_context_print:       total time =     800.48 ms /    70 tokens
0.01.620.776 I ggml_metal_free: deallocating

real	0m1.638s
user	0m0.108s
sys	0m0.195s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.465 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.722 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.034.729 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.731 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.731 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.732 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.732 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.732 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.733 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.733 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.734 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.734 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.736 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.738 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.738 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.740 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.740 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.741 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.527 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.039.588 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.446 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.448 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.448 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.449 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.453 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.454 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.043.454 I llama_model_loader: - type  f32:  194 tensors
0.00.043.454 I llama_model_loader: - type q5_0:   97 tensors
0.00.043.455 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.455 I print_info: file format = GGUF V3 (latest)
0.00.043.456 I print_info: file type   = Q5_0
0.00.043.457 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.051.534 I load: special tokens cache size = 25
0.00.058.232 I load: token to piece cache size = 0.2984 MB
0.00.058.236 I print_info: arch             = gptneox
0.00.058.237 I print_info: vocab_only       = 0
0.00.058.237 I print_info: n_ctx_train      = 2048
0.00.058.237 I print_info: n_embd           = 2048
0.00.058.237 I print_info: n_layer          = 24
0.00.058.243 I print_info: n_head           = 16
0.00.058.243 I print_info: n_head_kv        = 16
0.00.058.244 I print_info: n_rot            = 32
0.00.058.244 I print_info: n_swa            = 0
0.00.058.244 I print_info: n_embd_head_k    = 128
0.00.058.244 I print_info: n_embd_head_v    = 128
0.00.058.245 I print_info: n_gqa            = 1
0.00.058.246 I print_info: n_embd_k_gqa     = 2048
0.00.058.247 I print_info: n_embd_v_gqa     = 2048
0.00.058.247 I print_info: f_norm_eps       = 1.0e-05
0.00.058.248 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.250 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.250 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.250 I print_info: f_logit_scale    = 0.0e+00
0.00.058.251 I print_info: n_ff             = 8192
0.00.058.252 I print_info: n_expert         = 0
0.00.058.253 I print_info: n_expert_used    = 0
0.00.058.253 I print_info: causal attn      = 1
0.00.058.253 I print_info: pooling type     = 0
0.00.058.254 I print_info: rope type        = 2
0.00.058.254 I print_info: rope scaling     = linear
0.00.058.254 I print_info: freq_base_train  = 10000.0
0.00.058.255 I print_info: freq_scale_train = 1
0.00.058.255 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.255 I print_info: rope_finetuned   = unknown
0.00.058.255 I print_info: ssm_d_conv       = 0
0.00.058.255 I print_info: ssm_d_inner      = 0
0.00.058.255 I print_info: ssm_d_state      = 0
0.00.058.256 I print_info: ssm_dt_rank      = 0
0.00.058.256 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.257 I print_info: model type       = 1.4B
0.00.058.257 I print_info: model params     = 1.41 B
0.00.058.257 I print_info: general.name     = 1.4B
0.00.058.258 I print_info: vocab type       = BPE
0.00.058.258 I print_info: n_vocab          = 50304
0.00.058.258 I print_info: n_merges         = 50009
0.00.058.259 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.259 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.259 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.259 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.260 I print_info: LF token         = 187 'Ċ'
0.00.058.260 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.260 I print_info: max token length = 1024
0.00.708.946 I load_tensors: offloading 24 repeating layers to GPU
0.00.708.963 I load_tensors: offloading output layer to GPU
0.00.708.963 I load_tensors: offloaded 25/25 layers to GPU
0.00.708.999 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.709.000 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.710.185 I llama_init_from_model: n_seq_max     = 1
0.00.710.191 I llama_init_from_model: n_ctx         = 128
0.00.710.191 I llama_init_from_model: n_ctx_per_seq = 128
0.00.710.192 I llama_init_from_model: n_batch       = 128
0.00.710.192 I llama_init_from_model: n_ubatch      = 128
0.00.710.193 I llama_init_from_model: flash_attn    = 0
0.00.710.195 I llama_init_from_model: freq_base     = 10000.0
0.00.710.195 I llama_init_from_model: freq_scale    = 1
0.00.710.196 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.710.198 I ggml_metal_init: allocating
0.00.710.291 I ggml_metal_init: found device: Apple M4
0.00.710.305 I ggml_metal_init: picking default device: Apple M4
0.00.712.172 I ggml_metal_init: using embedded metal library
0.00.718.803 I ggml_metal_init: GPU name:   Apple M4
0.00.718.808 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.718.808 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.718.809 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.718.810 I ggml_metal_init: simdgroup reduction   = true
0.00.718.810 I ggml_metal_init: simdgroup matrix mul. = true
0.00.718.810 I ggml_metal_init: has residency sets    = true
0.00.718.811 I ggml_metal_init: has bfloat            = true
0.00.718.811 I ggml_metal_init: use bfloat            = true
0.00.718.812 I ggml_metal_init: hasUnifiedMemory      = true
0.00.718.813 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.736.965 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.740.537 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.740.541 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.740.584 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.743.761 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.743.764 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.743.764 I llama_init_from_model: graph nodes  = 967
0.00.743.764 I llama_init_from_model: graph splits = 2
0.00.743.768 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.743.768 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.776.726 I 
0.00.776.813 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.776.821 I perplexity: tokenizing the input ..
0.00.784.130 I perplexity: tokenization took 7.307 ms
0.00.784.138 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.931.525 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.932.868 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.932.890 I llama_perf_context_print:        load time =     768.25 ms
0.00.932.891 I llama_perf_context_print: prompt eval time =     147.00 ms /   128 tokens (    1.15 ms per token,   870.76 tokens per second)
0.00.932.891 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.932.892 I llama_perf_context_print:       total time =     156.17 ms /   129 tokens
0.00.933.269 I ggml_metal_free: deallocating

real	0m0.947s
user	0m0.080s
sys	0m0.131s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.009.947 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.788 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.793 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.794 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.795 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.795 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.796 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.797 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.799 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.800 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.800 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.801 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.801 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.801 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.802 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.804 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.804 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.805 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.664 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.718 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.518 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.519 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.519 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.520 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.520 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.520 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.521 I llama_model_loader: - type  f32:  194 tensors
0.00.026.521 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.521 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.522 I print_info: file format = GGUF V3 (latest)
0.00.026.522 I print_info: file type   = Q5_1
0.00.026.523 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.680 I load: special tokens cache size = 25
0.00.040.423 I load: token to piece cache size = 0.2984 MB
0.00.040.426 I print_info: arch             = gptneox
0.00.040.426 I print_info: vocab_only       = 0
0.00.040.426 I print_info: n_ctx_train      = 2048
0.00.040.426 I print_info: n_embd           = 2048
0.00.040.426 I print_info: n_layer          = 24
0.00.040.429 I print_info: n_head           = 16
0.00.040.430 I print_info: n_head_kv        = 16
0.00.040.430 I print_info: n_rot            = 32
0.00.040.430 I print_info: n_swa            = 0
0.00.040.431 I print_info: n_embd_head_k    = 128
0.00.040.432 I print_info: n_embd_head_v    = 128
0.00.040.433 I print_info: n_gqa            = 1
0.00.040.434 I print_info: n_embd_k_gqa     = 2048
0.00.040.435 I print_info: n_embd_v_gqa     = 2048
0.00.040.435 I print_info: f_norm_eps       = 1.0e-05
0.00.040.436 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.436 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.436 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.436 I print_info: f_logit_scale    = 0.0e+00
0.00.040.437 I print_info: n_ff             = 8192
0.00.040.437 I print_info: n_expert         = 0
0.00.040.437 I print_info: n_expert_used    = 0
0.00.040.437 I print_info: causal attn      = 1
0.00.040.438 I print_info: pooling type     = 0
0.00.040.439 I print_info: rope type        = 2
0.00.040.441 I print_info: rope scaling     = linear
0.00.040.441 I print_info: freq_base_train  = 10000.0
0.00.040.441 I print_info: freq_scale_train = 1
0.00.040.442 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.442 I print_info: rope_finetuned   = unknown
0.00.040.442 I print_info: ssm_d_conv       = 0
0.00.040.442 I print_info: ssm_d_inner      = 0
0.00.040.442 I print_info: ssm_d_state      = 0
0.00.040.442 I print_info: ssm_dt_rank      = 0
0.00.040.446 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.447 I print_info: model type       = 1.4B
0.00.040.447 I print_info: model params     = 1.41 B
0.00.040.448 I print_info: general.name     = 1.4B
0.00.040.448 I print_info: vocab type       = BPE
0.00.040.448 I print_info: n_vocab          = 50304
0.00.040.448 I print_info: n_merges         = 50009
0.00.040.449 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.449 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.449 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.449 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.449 I print_info: LF token         = 187 'Ċ'
0.00.040.450 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.450 I print_info: max token length = 1024
0.00.614.165 I load_tensors: offloading 24 repeating layers to GPU
0.00.614.175 I load_tensors: offloading output layer to GPU
0.00.614.175 I load_tensors: offloaded 25/25 layers to GPU
0.00.614.210 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.614.211 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.615.742 I llama_init_from_model: n_seq_max     = 1
0.00.615.747 I llama_init_from_model: n_ctx         = 2048
0.00.615.747 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.615.748 I llama_init_from_model: n_batch       = 2048
0.00.615.748 I llama_init_from_model: n_ubatch      = 512
0.00.615.748 I llama_init_from_model: flash_attn    = 0
0.00.615.751 I llama_init_from_model: freq_base     = 10000.0
0.00.615.751 I llama_init_from_model: freq_scale    = 1
0.00.615.763 I ggml_metal_init: allocating
0.00.615.837 I ggml_metal_init: found device: Apple M4
0.00.615.850 I ggml_metal_init: picking default device: Apple M4
0.00.617.581 I ggml_metal_init: using embedded metal library
0.00.624.133 I ggml_metal_init: GPU name:   Apple M4
0.00.624.137 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.624.138 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.624.139 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.624.140 I ggml_metal_init: simdgroup reduction   = true
0.00.624.140 I ggml_metal_init: simdgroup matrix mul. = true
0.00.624.140 I ggml_metal_init: has residency sets    = true
0.00.624.140 I ggml_metal_init: has bfloat            = true
0.00.624.141 I ggml_metal_init: use bfloat            = true
0.00.624.141 I ggml_metal_init: hasUnifiedMemory      = true
0.00.624.143 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.640.968 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.691.539 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.691.546 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.691.586 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.696.845 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.696.848 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.696.848 I llama_init_from_model: graph nodes  = 967
0.00.696.849 I llama_init_from_model: graph splits = 2
0.00.696.855 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.696.979 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.696.980 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.197 I main: llama threadpool init, n_threads = 4
0.00.753.243 I 
0.00.753.267 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.270 I 
0.00.753.436 I sampler seed: 1234
0.00.753.441 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.753.451 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.753.454 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.753.454 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.592.349 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50786.84 tokens per second)
0.01.592.349 I llama_perf_context_print:        load time =     742.60 ms
0.01.592.350 I llama_perf_context_print: prompt eval time =      42.00 ms /     7 tokens (    6.00 ms per token,   166.66 tokens per second)
0.01.592.351 I llama_perf_context_print:        eval time =     793.83 ms /    63 runs   (   12.60 ms per token,    79.36 tokens per second)
0.01.592.352 I llama_perf_context_print:       total time =     839.80 ms /    70 tokens
0.01.592.627 I ggml_metal_free: deallocating

real	0m1.610s
user	0m0.108s
sys	0m0.211s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.899 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.059 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.024.064 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.066 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.066 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.067 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.067 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.068 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.069 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.069 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.069 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.070 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.070 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.070 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.071 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.073 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.073 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.073 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.807 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.876 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.671 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.672 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.672 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.673 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.673 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.673 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.032.674 I llama_model_loader: - type  f32:  194 tensors
0.00.032.674 I llama_model_loader: - type q5_1:   97 tensors
0.00.032.674 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.675 I print_info: file format = GGUF V3 (latest)
0.00.032.675 I print_info: file type   = Q5_1
0.00.032.676 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.040.893 I load: special tokens cache size = 25
0.00.046.965 I load: token to piece cache size = 0.2984 MB
0.00.046.967 I print_info: arch             = gptneox
0.00.046.968 I print_info: vocab_only       = 0
0.00.046.968 I print_info: n_ctx_train      = 2048
0.00.046.968 I print_info: n_embd           = 2048
0.00.046.968 I print_info: n_layer          = 24
0.00.046.971 I print_info: n_head           = 16
0.00.046.971 I print_info: n_head_kv        = 16
0.00.046.972 I print_info: n_rot            = 32
0.00.046.972 I print_info: n_swa            = 0
0.00.046.974 I print_info: n_embd_head_k    = 128
0.00.046.974 I print_info: n_embd_head_v    = 128
0.00.046.975 I print_info: n_gqa            = 1
0.00.046.976 I print_info: n_embd_k_gqa     = 2048
0.00.046.976 I print_info: n_embd_v_gqa     = 2048
0.00.046.977 I print_info: f_norm_eps       = 1.0e-05
0.00.046.978 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.046.979 I print_info: f_clamp_kqv      = 0.0e+00
0.00.046.979 I print_info: f_max_alibi_bias = 0.0e+00
0.00.046.980 I print_info: f_logit_scale    = 0.0e+00
0.00.046.980 I print_info: n_ff             = 8192
0.00.046.981 I print_info: n_expert         = 0
0.00.046.981 I print_info: n_expert_used    = 0
0.00.046.982 I print_info: causal attn      = 1
0.00.046.982 I print_info: pooling type     = 0
0.00.046.982 I print_info: rope type        = 2
0.00.046.983 I print_info: rope scaling     = linear
0.00.046.983 I print_info: freq_base_train  = 10000.0
0.00.046.984 I print_info: freq_scale_train = 1
0.00.046.985 I print_info: n_ctx_orig_yarn  = 2048
0.00.046.985 I print_info: rope_finetuned   = unknown
0.00.046.985 I print_info: ssm_d_conv       = 0
0.00.046.985 I print_info: ssm_d_inner      = 0
0.00.046.985 I print_info: ssm_d_state      = 0
0.00.046.985 I print_info: ssm_dt_rank      = 0
0.00.046.985 I print_info: ssm_dt_b_c_rms   = 0
0.00.046.986 I print_info: model type       = 1.4B
0.00.046.986 I print_info: model params     = 1.41 B
0.00.046.986 I print_info: general.name     = 1.4B
0.00.046.987 I print_info: vocab type       = BPE
0.00.046.987 I print_info: n_vocab          = 50304
0.00.046.987 I print_info: n_merges         = 50009
0.00.046.987 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.046.987 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.046.988 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.046.988 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.046.988 I print_info: LF token         = 187 'Ċ'
0.00.046.988 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.046.989 I print_info: max token length = 1024
0.00.704.692 I load_tensors: offloading 24 repeating layers to GPU
0.00.704.709 I load_tensors: offloading output layer to GPU
0.00.704.710 I load_tensors: offloaded 25/25 layers to GPU
0.00.704.744 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.704.746 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.706.234 I llama_init_from_model: n_seq_max     = 1
0.00.706.238 I llama_init_from_model: n_ctx         = 128
0.00.706.238 I llama_init_from_model: n_ctx_per_seq = 128
0.00.706.239 I llama_init_from_model: n_batch       = 128
0.00.706.239 I llama_init_from_model: n_ubatch      = 128
0.00.706.240 I llama_init_from_model: flash_attn    = 0
0.00.706.241 I llama_init_from_model: freq_base     = 10000.0
0.00.706.241 I llama_init_from_model: freq_scale    = 1
0.00.706.242 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.706.243 I ggml_metal_init: allocating
0.00.706.258 I ggml_metal_init: found device: Apple M4
0.00.706.268 I ggml_metal_init: picking default device: Apple M4
0.00.707.670 I ggml_metal_init: using embedded metal library
0.00.713.996 I ggml_metal_init: GPU name:   Apple M4
0.00.714.000 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.714.000 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.714.001 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.714.002 I ggml_metal_init: simdgroup reduction   = true
0.00.714.002 I ggml_metal_init: simdgroup matrix mul. = true
0.00.714.002 I ggml_metal_init: has residency sets    = true
0.00.714.002 I ggml_metal_init: has bfloat            = true
0.00.714.003 I ggml_metal_init: use bfloat            = true
0.00.714.003 I ggml_metal_init: hasUnifiedMemory      = true
0.00.714.014 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.730.576 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.734.119 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.734.123 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.734.161 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.737.538 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.737.540 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.737.540 I llama_init_from_model: graph nodes  = 967
0.00.737.540 I llama_init_from_model: graph splits = 2
0.00.737.544 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.737.544 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.764.965 I 
0.00.765.051 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.765.058 I perplexity: tokenizing the input ..
0.00.772.625 I perplexity: tokenization took 7.562 ms
0.00.772.632 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.908.715 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.910.134 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.910.158 I llama_perf_context_print:        load time =     756.06 ms
0.00.910.159 I llama_perf_context_print: prompt eval time =     135.10 ms /   128 tokens (    1.06 ms per token,   947.42 tokens per second)
0.00.910.160 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.910.160 I llama_perf_context_print:       total time =     145.20 ms /   129 tokens
0.00.910.518 I ggml_metal_free: deallocating

real	0m0.928s
user	0m0.078s
sys	0m0.144s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.860 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.362 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.366 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.368 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.368 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.369 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.369 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.369 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.370 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.371 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.371 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.371 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.372 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.372 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.372 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.375 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.375 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.375 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.202 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.189 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.970 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.971 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.971 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.972 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.972 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.972 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.973 I llama_model_loader: - type  f32:  194 tensors
0.00.023.973 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.973 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.974 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.974 I print_info: file format = GGUF V3 (latest)
0.00.023.975 I print_info: file type   = Q2_K - Medium
0.00.023.975 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.031.843 I load: special tokens cache size = 25
0.00.037.909 I load: token to piece cache size = 0.2984 MB
0.00.037.912 I print_info: arch             = gptneox
0.00.037.913 I print_info: vocab_only       = 0
0.00.037.913 I print_info: n_ctx_train      = 2048
0.00.037.913 I print_info: n_embd           = 2048
0.00.037.913 I print_info: n_layer          = 24
0.00.037.916 I print_info: n_head           = 16
0.00.037.917 I print_info: n_head_kv        = 16
0.00.037.917 I print_info: n_rot            = 32
0.00.037.918 I print_info: n_swa            = 0
0.00.037.918 I print_info: n_embd_head_k    = 128
0.00.037.920 I print_info: n_embd_head_v    = 128
0.00.037.921 I print_info: n_gqa            = 1
0.00.037.922 I print_info: n_embd_k_gqa     = 2048
0.00.037.923 I print_info: n_embd_v_gqa     = 2048
0.00.037.924 I print_info: f_norm_eps       = 1.0e-05
0.00.037.924 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.925 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.926 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.926 I print_info: f_logit_scale    = 0.0e+00
0.00.037.927 I print_info: n_ff             = 8192
0.00.037.927 I print_info: n_expert         = 0
0.00.037.927 I print_info: n_expert_used    = 0
0.00.037.927 I print_info: causal attn      = 1
0.00.037.928 I print_info: pooling type     = 0
0.00.037.928 I print_info: rope type        = 2
0.00.037.928 I print_info: rope scaling     = linear
0.00.037.928 I print_info: freq_base_train  = 10000.0
0.00.037.929 I print_info: freq_scale_train = 1
0.00.037.929 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.929 I print_info: rope_finetuned   = unknown
0.00.037.929 I print_info: ssm_d_conv       = 0
0.00.037.929 I print_info: ssm_d_inner      = 0
0.00.037.929 I print_info: ssm_d_state      = 0
0.00.037.930 I print_info: ssm_dt_rank      = 0
0.00.037.930 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.930 I print_info: model type       = 1.4B
0.00.037.930 I print_info: model params     = 1.41 B
0.00.037.934 I print_info: general.name     = 1.4B
0.00.037.935 I print_info: vocab type       = BPE
0.00.037.935 I print_info: n_vocab          = 50304
0.00.037.935 I print_info: n_merges         = 50009
0.00.037.935 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.936 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.936 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.936 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.936 I print_info: LF token         = 187 'Ċ'
0.00.037.936 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.936 I print_info: max token length = 1024
0.00.344.506 I load_tensors: offloading 24 repeating layers to GPU
0.00.344.521 I load_tensors: offloading output layer to GPU
0.00.344.522 I load_tensors: offloaded 25/25 layers to GPU
0.00.344.555 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.344.556 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.345.959 I llama_init_from_model: n_seq_max     = 1
0.00.345.969 I llama_init_from_model: n_ctx         = 2048
0.00.345.969 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.345.970 I llama_init_from_model: n_batch       = 2048
0.00.345.970 I llama_init_from_model: n_ubatch      = 512
0.00.345.971 I llama_init_from_model: flash_attn    = 0
0.00.345.972 I llama_init_from_model: freq_base     = 10000.0
0.00.345.978 I llama_init_from_model: freq_scale    = 1
0.00.345.980 I ggml_metal_init: allocating
0.00.346.087 I ggml_metal_init: found device: Apple M4
0.00.346.101 I ggml_metal_init: picking default device: Apple M4
0.00.347.961 I ggml_metal_init: using embedded metal library
0.00.353.438 I ggml_metal_init: GPU name:   Apple M4
0.00.353.447 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.353.448 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.353.448 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.353.449 I ggml_metal_init: simdgroup reduction   = true
0.00.353.449 I ggml_metal_init: simdgroup matrix mul. = true
0.00.353.449 I ggml_metal_init: has residency sets    = true
0.00.353.449 I ggml_metal_init: has bfloat            = true
0.00.353.450 I ggml_metal_init: use bfloat            = true
0.00.353.452 I ggml_metal_init: hasUnifiedMemory      = true
0.00.353.456 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.375.037 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.429.718 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.429.728 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.429.771 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.434.241 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.434.243 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.434.244 I llama_init_from_model: graph nodes  = 967
0.00.434.244 I llama_init_from_model: graph splits = 2
0.00.434.249 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.434.377 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.434.378 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.491.777 I main: llama threadpool init, n_threads = 4
0.00.491.829 I 
0.00.491.857 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.491.858 I 
0.00.492.029 I sampler seed: 1234
0.00.492.033 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.492.045 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.492.045 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.492.045 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.166.838 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56037.88 tokens per second)
0.01.166.839 I llama_perf_context_print:        load time =     482.24 ms
0.01.166.840 I llama_perf_context_print: prompt eval time =      35.44 ms /     7 tokens (    5.06 ms per token,   197.52 tokens per second)
0.01.166.841 I llama_perf_context_print:        eval time =     636.62 ms /    63 runs   (   10.11 ms per token,    98.96 tokens per second)
0.01.166.842 I llama_perf_context_print:       total time =     675.74 ms /    70 tokens
0.01.167.070 I ggml_metal_free: deallocating

real	0m1.186s
user	0m0.111s
sys	0m0.168s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.841 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.799 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.024.803 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.805 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.805 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.805 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.806 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.806 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.807 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.807 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.808 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.808 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.808 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.811 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.811 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.813 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.813 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.814 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.538 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.695 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.747 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.749 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.749 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.749 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.750 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.750 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.033.750 I llama_model_loader: - type  f32:  194 tensors
0.00.033.751 I llama_model_loader: - type q2_K:   49 tensors
0.00.033.751 I llama_model_loader: - type q3_K:   48 tensors
0.00.033.751 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.752 I print_info: file format = GGUF V3 (latest)
0.00.033.752 I print_info: file type   = Q2_K - Medium
0.00.033.757 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.042.499 I load: special tokens cache size = 25
0.00.049.204 I load: token to piece cache size = 0.2984 MB
0.00.049.207 I print_info: arch             = gptneox
0.00.049.207 I print_info: vocab_only       = 0
0.00.049.207 I print_info: n_ctx_train      = 2048
0.00.049.207 I print_info: n_embd           = 2048
0.00.049.207 I print_info: n_layer          = 24
0.00.049.210 I print_info: n_head           = 16
0.00.049.210 I print_info: n_head_kv        = 16
0.00.049.211 I print_info: n_rot            = 32
0.00.049.211 I print_info: n_swa            = 0
0.00.049.211 I print_info: n_embd_head_k    = 128
0.00.049.211 I print_info: n_embd_head_v    = 128
0.00.049.212 I print_info: n_gqa            = 1
0.00.049.213 I print_info: n_embd_k_gqa     = 2048
0.00.049.213 I print_info: n_embd_v_gqa     = 2048
0.00.049.214 I print_info: f_norm_eps       = 1.0e-05
0.00.049.214 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.214 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.215 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.215 I print_info: f_logit_scale    = 0.0e+00
0.00.049.215 I print_info: n_ff             = 8192
0.00.049.216 I print_info: n_expert         = 0
0.00.049.216 I print_info: n_expert_used    = 0
0.00.049.216 I print_info: causal attn      = 1
0.00.049.216 I print_info: pooling type     = 0
0.00.049.216 I print_info: rope type        = 2
0.00.049.217 I print_info: rope scaling     = linear
0.00.049.217 I print_info: freq_base_train  = 10000.0
0.00.049.217 I print_info: freq_scale_train = 1
0.00.049.217 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.218 I print_info: rope_finetuned   = unknown
0.00.049.220 I print_info: ssm_d_conv       = 0
0.00.049.220 I print_info: ssm_d_inner      = 0
0.00.049.220 I print_info: ssm_d_state      = 0
0.00.049.220 I print_info: ssm_dt_rank      = 0
0.00.049.220 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.220 I print_info: model type       = 1.4B
0.00.049.221 I print_info: model params     = 1.41 B
0.00.049.221 I print_info: general.name     = 1.4B
0.00.049.221 I print_info: vocab type       = BPE
0.00.049.222 I print_info: n_vocab          = 50304
0.00.049.222 I print_info: n_merges         = 50009
0.00.049.222 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.222 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.222 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.222 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.223 I print_info: LF token         = 187 'Ċ'
0.00.049.226 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.226 I print_info: max token length = 1024
0.00.435.579 I load_tensors: offloading 24 repeating layers to GPU
0.00.435.587 I load_tensors: offloading output layer to GPU
0.00.435.588 I load_tensors: offloaded 25/25 layers to GPU
0.00.435.618 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.435.620 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.437.063 I llama_init_from_model: n_seq_max     = 1
0.00.437.070 I llama_init_from_model: n_ctx         = 128
0.00.437.071 I llama_init_from_model: n_ctx_per_seq = 128
0.00.437.071 I llama_init_from_model: n_batch       = 128
0.00.437.072 I llama_init_from_model: n_ubatch      = 128
0.00.437.072 I llama_init_from_model: flash_attn    = 0
0.00.437.074 I llama_init_from_model: freq_base     = 10000.0
0.00.437.075 I llama_init_from_model: freq_scale    = 1
0.00.437.075 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.437.078 I ggml_metal_init: allocating
0.00.437.136 I ggml_metal_init: found device: Apple M4
0.00.437.147 I ggml_metal_init: picking default device: Apple M4
0.00.439.293 I ggml_metal_init: using embedded metal library
0.00.445.334 I ggml_metal_init: GPU name:   Apple M4
0.00.445.342 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.445.343 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.445.344 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.445.345 I ggml_metal_init: simdgroup reduction   = true
0.00.445.345 I ggml_metal_init: simdgroup matrix mul. = true
0.00.445.346 I ggml_metal_init: has residency sets    = true
0.00.445.346 I ggml_metal_init: has bfloat            = true
0.00.445.346 I ggml_metal_init: use bfloat            = true
0.00.445.349 I ggml_metal_init: hasUnifiedMemory      = true
0.00.445.354 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.467.186 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.470.732 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.470.739 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.470.799 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.474.082 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.474.084 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.474.085 I llama_init_from_model: graph nodes  = 967
0.00.474.086 I llama_init_from_model: graph splits = 2
0.00.474.089 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.474.089 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.509.101 I 
0.00.509.181 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.509.189 I perplexity: tokenizing the input ..
0.00.516.173 I perplexity: tokenization took 6.981 ms
0.00.516.181 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.663.230 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.664.573 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.664.602 I llama_perf_context_print:        load time =     494.25 ms
0.00.664.603 I llama_perf_context_print: prompt eval time =     146.07 ms /   128 tokens (    1.14 ms per token,   876.27 tokens per second)
0.00.664.604 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.664.604 I llama_perf_context_print:       total time =     155.51 ms /   129 tokens
0.00.664.999 I ggml_metal_free: deallocating

real	0m0.685s
user	0m0.084s
sys	0m0.116s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.878 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.367 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.372 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.373 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.374 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.374 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.375 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.375 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.376 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.376 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.377 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.377 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.377 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.378 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.379 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.384 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.384 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.384 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.299 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.301 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.169 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.171 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.171 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.171 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.172 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.172 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.173 I llama_model_loader: - type  f32:  194 tensors
0.00.025.173 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.173 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.173 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.174 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.174 I print_info: file format = GGUF V3 (latest)
0.00.025.175 I print_info: file type   = Q3_K - Medium
0.00.025.176 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.305 I load: special tokens cache size = 25
0.00.039.301 I load: token to piece cache size = 0.2984 MB
0.00.039.304 I print_info: arch             = gptneox
0.00.039.304 I print_info: vocab_only       = 0
0.00.039.305 I print_info: n_ctx_train      = 2048
0.00.039.305 I print_info: n_embd           = 2048
0.00.039.305 I print_info: n_layer          = 24
0.00.039.307 I print_info: n_head           = 16
0.00.039.308 I print_info: n_head_kv        = 16
0.00.039.308 I print_info: n_rot            = 32
0.00.039.308 I print_info: n_swa            = 0
0.00.039.309 I print_info: n_embd_head_k    = 128
0.00.039.309 I print_info: n_embd_head_v    = 128
0.00.039.311 I print_info: n_gqa            = 1
0.00.039.311 I print_info: n_embd_k_gqa     = 2048
0.00.039.314 I print_info: n_embd_v_gqa     = 2048
0.00.039.314 I print_info: f_norm_eps       = 1.0e-05
0.00.039.315 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.315 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.316 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.316 I print_info: f_logit_scale    = 0.0e+00
0.00.039.317 I print_info: n_ff             = 8192
0.00.039.317 I print_info: n_expert         = 0
0.00.039.317 I print_info: n_expert_used    = 0
0.00.039.319 I print_info: causal attn      = 1
0.00.039.320 I print_info: pooling type     = 0
0.00.039.320 I print_info: rope type        = 2
0.00.039.320 I print_info: rope scaling     = linear
0.00.039.321 I print_info: freq_base_train  = 10000.0
0.00.039.321 I print_info: freq_scale_train = 1
0.00.039.321 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.321 I print_info: rope_finetuned   = unknown
0.00.039.322 I print_info: ssm_d_conv       = 0
0.00.039.322 I print_info: ssm_d_inner      = 0
0.00.039.322 I print_info: ssm_d_state      = 0
0.00.039.322 I print_info: ssm_dt_rank      = 0
0.00.039.322 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.322 I print_info: model type       = 1.4B
0.00.039.327 I print_info: model params     = 1.41 B
0.00.039.329 I print_info: general.name     = 1.4B
0.00.039.331 I print_info: vocab type       = BPE
0.00.039.331 I print_info: n_vocab          = 50304
0.00.039.331 I print_info: n_merges         = 50009
0.00.039.331 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.331 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.331 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.332 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.332 I print_info: LF token         = 187 'Ċ'
0.00.039.332 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.332 I print_info: max token length = 1024
0.00.439.083 I load_tensors: offloading 24 repeating layers to GPU
0.00.439.098 I load_tensors: offloading output layer to GPU
0.00.439.099 I load_tensors: offloaded 25/25 layers to GPU
0.00.439.130 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.439.132 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.440.675 I llama_init_from_model: n_seq_max     = 1
0.00.440.680 I llama_init_from_model: n_ctx         = 2048
0.00.440.680 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.440.681 I llama_init_from_model: n_batch       = 2048
0.00.440.681 I llama_init_from_model: n_ubatch      = 512
0.00.440.681 I llama_init_from_model: flash_attn    = 0
0.00.440.689 I llama_init_from_model: freq_base     = 10000.0
0.00.440.693 I llama_init_from_model: freq_scale    = 1
0.00.440.695 I ggml_metal_init: allocating
0.00.440.768 I ggml_metal_init: found device: Apple M4
0.00.440.781 I ggml_metal_init: picking default device: Apple M4
0.00.442.719 I ggml_metal_init: using embedded metal library
0.00.448.245 I ggml_metal_init: GPU name:   Apple M4
0.00.448.249 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.448.250 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.448.251 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.448.252 I ggml_metal_init: simdgroup reduction   = true
0.00.448.253 I ggml_metal_init: simdgroup matrix mul. = true
0.00.448.253 I ggml_metal_init: has residency sets    = true
0.00.448.253 I ggml_metal_init: has bfloat            = true
0.00.448.253 I ggml_metal_init: use bfloat            = true
0.00.448.255 I ggml_metal_init: hasUnifiedMemory      = true
0.00.448.256 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.468.284 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.524.262 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.524.270 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.524.304 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.530.292 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.530.294 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.530.295 I llama_init_from_model: graph nodes  = 967
0.00.530.295 I llama_init_from_model: graph splits = 2
0.00.530.303 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.530.426 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.530.427 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.586.551 I main: llama threadpool init, n_threads = 4
0.00.586.597 I 
0.00.586.622 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.586.622 I 
0.00.586.791 I sampler seed: 1234
0.00.586.796 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.586.831 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.586.835 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.586.835 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.336.793 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52129.22 tokens per second)
0.01.336.793 I llama_perf_context_print:        load time =     577.03 ms
0.01.336.794 I llama_perf_context_print: prompt eval time =      50.11 ms /     7 tokens (    7.16 ms per token,   139.68 tokens per second)
0.01.336.796 I llama_perf_context_print:        eval time =     696.88 ms /    63 runs   (   11.06 ms per token,    90.40 tokens per second)
0.01.336.796 I llama_perf_context_print:       total time =     750.88 ms /    70 tokens
0.01.336.990 I ggml_metal_free: deallocating

real	0m1.354s
user	0m0.110s
sys	0m0.183s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.097 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.121 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.024.127 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.128 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.130 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.130 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.130 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.131 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.132 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.132 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.132 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.132 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.133 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.133 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.133 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.135 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.135 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.136 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.789 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.774 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.443 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.444 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.444 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.445 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.445 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.445 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.032.446 I llama_model_loader: - type  f32:  194 tensors
0.00.032.446 I llama_model_loader: - type q3_K:   25 tensors
0.00.032.446 I llama_model_loader: - type q4_K:   71 tensors
0.00.032.447 I llama_model_loader: - type q5_K:    1 tensors
0.00.032.447 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.447 I print_info: file format = GGUF V3 (latest)
0.00.032.449 I print_info: file type   = Q3_K - Medium
0.00.032.450 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.041.257 I load: special tokens cache size = 25
0.00.047.851 I load: token to piece cache size = 0.2984 MB
0.00.047.854 I print_info: arch             = gptneox
0.00.047.854 I print_info: vocab_only       = 0
0.00.047.854 I print_info: n_ctx_train      = 2048
0.00.047.854 I print_info: n_embd           = 2048
0.00.047.854 I print_info: n_layer          = 24
0.00.047.857 I print_info: n_head           = 16
0.00.047.858 I print_info: n_head_kv        = 16
0.00.047.858 I print_info: n_rot            = 32
0.00.047.859 I print_info: n_swa            = 0
0.00.047.859 I print_info: n_embd_head_k    = 128
0.00.047.860 I print_info: n_embd_head_v    = 128
0.00.047.860 I print_info: n_gqa            = 1
0.00.047.861 I print_info: n_embd_k_gqa     = 2048
0.00.047.862 I print_info: n_embd_v_gqa     = 2048
0.00.047.862 I print_info: f_norm_eps       = 1.0e-05
0.00.047.863 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.863 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.863 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.865 I print_info: f_logit_scale    = 0.0e+00
0.00.047.865 I print_info: n_ff             = 8192
0.00.047.865 I print_info: n_expert         = 0
0.00.047.866 I print_info: n_expert_used    = 0
0.00.047.866 I print_info: causal attn      = 1
0.00.047.866 I print_info: pooling type     = 0
0.00.047.866 I print_info: rope type        = 2
0.00.047.866 I print_info: rope scaling     = linear
0.00.047.867 I print_info: freq_base_train  = 10000.0
0.00.047.867 I print_info: freq_scale_train = 1
0.00.047.867 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.867 I print_info: rope_finetuned   = unknown
0.00.047.869 I print_info: ssm_d_conv       = 0
0.00.047.869 I print_info: ssm_d_inner      = 0
0.00.047.869 I print_info: ssm_d_state      = 0
0.00.047.869 I print_info: ssm_dt_rank      = 0
0.00.047.869 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.870 I print_info: model type       = 1.4B
0.00.047.870 I print_info: model params     = 1.41 B
0.00.047.870 I print_info: general.name     = 1.4B
0.00.047.871 I print_info: vocab type       = BPE
0.00.047.871 I print_info: n_vocab          = 50304
0.00.047.871 I print_info: n_merges         = 50009
0.00.047.871 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.872 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.872 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.872 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.876 I print_info: LF token         = 187 'Ċ'
0.00.047.877 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.877 I print_info: max token length = 1024
0.00.527.629 I load_tensors: offloading 24 repeating layers to GPU
0.00.527.637 I load_tensors: offloading output layer to GPU
0.00.527.637 I load_tensors: offloaded 25/25 layers to GPU
0.00.527.670 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.527.671 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.529.166 I llama_init_from_model: n_seq_max     = 1
0.00.529.170 I llama_init_from_model: n_ctx         = 128
0.00.529.170 I llama_init_from_model: n_ctx_per_seq = 128
0.00.529.171 I llama_init_from_model: n_batch       = 128
0.00.529.171 I llama_init_from_model: n_ubatch      = 128
0.00.529.172 I llama_init_from_model: flash_attn    = 0
0.00.529.191 I llama_init_from_model: freq_base     = 10000.0
0.00.529.192 I llama_init_from_model: freq_scale    = 1
0.00.529.193 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.529.195 I ggml_metal_init: allocating
0.00.529.275 I ggml_metal_init: found device: Apple M4
0.00.529.325 I ggml_metal_init: picking default device: Apple M4
0.00.531.037 I ggml_metal_init: using embedded metal library
0.00.536.925 I ggml_metal_init: GPU name:   Apple M4
0.00.536.930 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.536.931 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.536.932 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.536.933 I ggml_metal_init: simdgroup reduction   = true
0.00.536.933 I ggml_metal_init: simdgroup matrix mul. = true
0.00.536.934 I ggml_metal_init: has residency sets    = true
0.00.536.934 I ggml_metal_init: has bfloat            = true
0.00.536.934 I ggml_metal_init: use bfloat            = true
0.00.536.935 I ggml_metal_init: hasUnifiedMemory      = true
0.00.536.940 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.556.012 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.559.502 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.559.508 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.559.577 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.562.737 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.562.738 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.562.739 I llama_init_from_model: graph nodes  = 967
0.00.562.740 I llama_init_from_model: graph splits = 2
0.00.562.743 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.562.743 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.593.880 I 
0.00.593.988 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.594.010 I perplexity: tokenizing the input ..
0.00.600.862 I perplexity: tokenization took 6.849 ms
0.00.600.868 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.747.389 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.748.733 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.748.754 I llama_perf_context_print:        load time =     584.77 ms
0.00.748.759 I llama_perf_context_print: prompt eval time =     145.56 ms /   128 tokens (    1.14 ms per token,   879.37 tokens per second)
0.00.748.760 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.748.760 I llama_perf_context_print:       total time =     154.88 ms /   129 tokens
0.00.749.144 I ggml_metal_free: deallocating

real	0m0.763s
user	0m0.080s
sys	0m0.112s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.011.736 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.283 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.019.288 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.293 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.294 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.294 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.294 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.295 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.296 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.296 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.296 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.297 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.297 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.298 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.298 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.299 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.300 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.300 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.116 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.103 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.835 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.837 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.837 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.837 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.837 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.838 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.838 I llama_model_loader: - type  f32:  194 tensors
0.00.027.839 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.839 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.839 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.840 I print_info: file format = GGUF V3 (latest)
0.00.027.840 I print_info: file type   = Q4_K - Medium
0.00.027.841 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.035.735 I load: special tokens cache size = 25
0.00.041.492 I load: token to piece cache size = 0.2984 MB
0.00.041.495 I print_info: arch             = gptneox
0.00.041.495 I print_info: vocab_only       = 0
0.00.041.496 I print_info: n_ctx_train      = 2048
0.00.041.496 I print_info: n_embd           = 2048
0.00.041.496 I print_info: n_layer          = 24
0.00.041.498 I print_info: n_head           = 16
0.00.041.499 I print_info: n_head_kv        = 16
0.00.041.499 I print_info: n_rot            = 32
0.00.041.499 I print_info: n_swa            = 0
0.00.041.500 I print_info: n_embd_head_k    = 128
0.00.041.500 I print_info: n_embd_head_v    = 128
0.00.041.500 I print_info: n_gqa            = 1
0.00.041.501 I print_info: n_embd_k_gqa     = 2048
0.00.041.502 I print_info: n_embd_v_gqa     = 2048
0.00.041.503 I print_info: f_norm_eps       = 1.0e-05
0.00.041.503 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.503 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.503 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.506 I print_info: f_logit_scale    = 0.0e+00
0.00.041.506 I print_info: n_ff             = 8192
0.00.041.507 I print_info: n_expert         = 0
0.00.041.507 I print_info: n_expert_used    = 0
0.00.041.507 I print_info: causal attn      = 1
0.00.041.508 I print_info: pooling type     = 0
0.00.041.509 I print_info: rope type        = 2
0.00.041.509 I print_info: rope scaling     = linear
0.00.041.509 I print_info: freq_base_train  = 10000.0
0.00.041.509 I print_info: freq_scale_train = 1
0.00.041.511 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.511 I print_info: rope_finetuned   = unknown
0.00.041.511 I print_info: ssm_d_conv       = 0
0.00.041.511 I print_info: ssm_d_inner      = 0
0.00.041.511 I print_info: ssm_d_state      = 0
0.00.041.512 I print_info: ssm_dt_rank      = 0
0.00.041.513 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.513 I print_info: model type       = 1.4B
0.00.041.513 I print_info: model params     = 1.41 B
0.00.041.513 I print_info: general.name     = 1.4B
0.00.041.515 I print_info: vocab type       = BPE
0.00.041.516 I print_info: n_vocab          = 50304
0.00.041.516 I print_info: n_merges         = 50009
0.00.041.516 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.516 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.516 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.517 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.517 I print_info: LF token         = 187 'Ċ'
0.00.041.517 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.517 I print_info: max token length = 1024
0.00.517.404 I load_tensors: offloading 24 repeating layers to GPU
0.00.517.419 I load_tensors: offloading output layer to GPU
0.00.517.420 I load_tensors: offloaded 25/25 layers to GPU
0.00.517.458 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.517.459 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.518.778 I llama_init_from_model: n_seq_max     = 1
0.00.518.782 I llama_init_from_model: n_ctx         = 2048
0.00.518.783 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.518.783 I llama_init_from_model: n_batch       = 2048
0.00.518.783 I llama_init_from_model: n_ubatch      = 512
0.00.518.784 I llama_init_from_model: flash_attn    = 0
0.00.518.786 I llama_init_from_model: freq_base     = 10000.0
0.00.518.787 I llama_init_from_model: freq_scale    = 1
0.00.518.789 I ggml_metal_init: allocating
0.00.518.867 I ggml_metal_init: found device: Apple M4
0.00.518.881 I ggml_metal_init: picking default device: Apple M4
0.00.520.753 I ggml_metal_init: using embedded metal library
0.00.527.429 I ggml_metal_init: GPU name:   Apple M4
0.00.527.433 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.527.434 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.527.435 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.527.436 I ggml_metal_init: simdgroup reduction   = true
0.00.527.436 I ggml_metal_init: simdgroup matrix mul. = true
0.00.527.436 I ggml_metal_init: has residency sets    = true
0.00.527.436 I ggml_metal_init: has bfloat            = true
0.00.527.437 I ggml_metal_init: use bfloat            = true
0.00.527.437 I ggml_metal_init: hasUnifiedMemory      = true
0.00.527.439 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.545.713 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.594.970 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.594.976 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.595.012 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.599.682 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.599.684 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.599.684 I llama_init_from_model: graph nodes  = 967
0.00.599.685 I llama_init_from_model: graph splits = 2
0.00.599.690 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.599.824 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.599.825 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.659.839 I main: llama threadpool init, n_threads = 4
0.00.659.879 I 
0.00.659.903 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.659.903 I 
0.00.660.054 I sampler seed: 1234
0.00.660.059 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.660.069 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.660.070 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.660.070 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.426.410 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52592.59 tokens per second)
0.01.426.410 I llama_perf_context_print:        load time =     647.43 ms
0.01.426.411 I llama_perf_context_print: prompt eval time =      57.53 ms /     7 tokens (    8.22 ms per token,   121.67 tokens per second)
0.01.426.412 I llama_perf_context_print:        eval time =     705.91 ms /    63 runs   (   11.20 ms per token,    89.25 tokens per second)
0.01.426.412 I llama_perf_context_print:       total time =     767.24 ms /    70 tokens
0.01.426.649 I ggml_metal_free: deallocating

real	0m1.443s
user	0m0.109s
sys	0m0.183s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.847 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.122 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.026.126 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.130 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.131 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.133 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.133 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.133 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.134 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.135 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.135 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.136 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.139 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.140 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.140 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.142 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.142 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.143 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.786 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.808 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.621 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.622 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.622 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.622 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.623 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.623 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.034.624 I llama_model_loader: - type  f32:  194 tensors
0.00.034.624 I llama_model_loader: - type q4_K:   61 tensors
0.00.034.624 I llama_model_loader: - type q5_K:   24 tensors
0.00.034.624 I llama_model_loader: - type q6_K:   13 tensors
0.00.034.625 I print_info: file format = GGUF V3 (latest)
0.00.034.626 I print_info: file type   = Q4_K - Medium
0.00.034.627 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.042.599 I load: special tokens cache size = 25
0.00.048.914 I load: token to piece cache size = 0.2984 MB
0.00.048.919 I print_info: arch             = gptneox
0.00.048.919 I print_info: vocab_only       = 0
0.00.048.919 I print_info: n_ctx_train      = 2048
0.00.048.919 I print_info: n_embd           = 2048
0.00.048.919 I print_info: n_layer          = 24
0.00.048.922 I print_info: n_head           = 16
0.00.048.923 I print_info: n_head_kv        = 16
0.00.048.923 I print_info: n_rot            = 32
0.00.048.923 I print_info: n_swa            = 0
0.00.048.923 I print_info: n_embd_head_k    = 128
0.00.048.924 I print_info: n_embd_head_v    = 128
0.00.048.924 I print_info: n_gqa            = 1
0.00.048.925 I print_info: n_embd_k_gqa     = 2048
0.00.048.925 I print_info: n_embd_v_gqa     = 2048
0.00.048.926 I print_info: f_norm_eps       = 1.0e-05
0.00.048.926 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.926 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.927 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.927 I print_info: f_logit_scale    = 0.0e+00
0.00.048.927 I print_info: n_ff             = 8192
0.00.048.927 I print_info: n_expert         = 0
0.00.048.928 I print_info: n_expert_used    = 0
0.00.048.928 I print_info: causal attn      = 1
0.00.048.928 I print_info: pooling type     = 0
0.00.048.928 I print_info: rope type        = 2
0.00.048.928 I print_info: rope scaling     = linear
0.00.048.929 I print_info: freq_base_train  = 10000.0
0.00.048.929 I print_info: freq_scale_train = 1
0.00.048.929 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.929 I print_info: rope_finetuned   = unknown
0.00.048.929 I print_info: ssm_d_conv       = 0
0.00.048.929 I print_info: ssm_d_inner      = 0
0.00.048.931 I print_info: ssm_d_state      = 0
0.00.048.931 I print_info: ssm_dt_rank      = 0
0.00.048.931 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.932 I print_info: model type       = 1.4B
0.00.048.932 I print_info: model params     = 1.41 B
0.00.048.932 I print_info: general.name     = 1.4B
0.00.048.933 I print_info: vocab type       = BPE
0.00.048.933 I print_info: n_vocab          = 50304
0.00.048.933 I print_info: n_merges         = 50009
0.00.048.933 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.933 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.933 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.934 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.937 I print_info: LF token         = 187 'Ċ'
0.00.048.938 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.938 I print_info: max token length = 1024
0.00.572.386 I load_tensors: offloading 24 repeating layers to GPU
0.00.572.401 I load_tensors: offloading output layer to GPU
0.00.572.402 I load_tensors: offloaded 25/25 layers to GPU
0.00.572.436 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.572.438 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.574.019 I llama_init_from_model: n_seq_max     = 1
0.00.574.025 I llama_init_from_model: n_ctx         = 128
0.00.574.025 I llama_init_from_model: n_ctx_per_seq = 128
0.00.574.026 I llama_init_from_model: n_batch       = 128
0.00.574.026 I llama_init_from_model: n_ubatch      = 128
0.00.574.026 I llama_init_from_model: flash_attn    = 0
0.00.574.029 I llama_init_from_model: freq_base     = 10000.0
0.00.574.029 I llama_init_from_model: freq_scale    = 1
0.00.574.030 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.574.032 I ggml_metal_init: allocating
0.00.574.106 I ggml_metal_init: found device: Apple M4
0.00.574.120 I ggml_metal_init: picking default device: Apple M4
0.00.575.938 I ggml_metal_init: using embedded metal library
0.00.582.420 I ggml_metal_init: GPU name:   Apple M4
0.00.582.425 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.582.426 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.582.427 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.582.428 I ggml_metal_init: simdgroup reduction   = true
0.00.582.428 I ggml_metal_init: simdgroup matrix mul. = true
0.00.582.429 I ggml_metal_init: has residency sets    = true
0.00.582.429 I ggml_metal_init: has bfloat            = true
0.00.582.429 I ggml_metal_init: use bfloat            = true
0.00.582.430 I ggml_metal_init: hasUnifiedMemory      = true
0.00.582.442 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.601.272 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.604.883 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.604.889 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.604.956 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.608.190 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.608.192 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.608.192 I llama_init_from_model: graph nodes  = 967
0.00.608.193 I llama_init_from_model: graph splits = 2
0.00.608.195 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.608.196 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.636.238 I 
0.00.636.315 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.636.322 I perplexity: tokenizing the input ..
0.00.643.823 I perplexity: tokenization took 7.498 ms
0.00.643.829 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.792.538 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.793.945 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.793.964 I llama_perf_context_print:        load time =     627.38 ms
0.00.793.965 I llama_perf_context_print: prompt eval time =     147.80 ms /   128 tokens (    1.15 ms per token,   866.02 tokens per second)
0.00.793.966 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.793.966 I llama_perf_context_print:       total time =     157.73 ms /   129 tokens
0.00.794.418 I ggml_metal_free: deallocating

real	0m0.808s
user	0m0.080s
sys	0m0.122s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.787 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.135 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.140 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.146 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.146 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.147 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.147 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.147 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.148 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.149 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.149 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.151 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.152 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.152 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.152 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.154 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.155 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.155 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.050 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.023 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.826 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.827 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.828 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.828 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.828 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.829 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.829 I llama_model_loader: - type  f32:  194 tensors
0.00.025.830 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.830 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.830 I print_info: file format = GGUF V3 (latest)
0.00.025.831 I print_info: file type   = Q5_K - Medium
0.00.025.835 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.094 I load: special tokens cache size = 25
0.00.039.902 I load: token to piece cache size = 0.2984 MB
0.00.039.905 I print_info: arch             = gptneox
0.00.039.905 I print_info: vocab_only       = 0
0.00.039.905 I print_info: n_ctx_train      = 2048
0.00.039.905 I print_info: n_embd           = 2048
0.00.039.906 I print_info: n_layer          = 24
0.00.039.908 I print_info: n_head           = 16
0.00.039.909 I print_info: n_head_kv        = 16
0.00.039.911 I print_info: n_rot            = 32
0.00.039.911 I print_info: n_swa            = 0
0.00.039.911 I print_info: n_embd_head_k    = 128
0.00.039.912 I print_info: n_embd_head_v    = 128
0.00.039.912 I print_info: n_gqa            = 1
0.00.039.913 I print_info: n_embd_k_gqa     = 2048
0.00.039.914 I print_info: n_embd_v_gqa     = 2048
0.00.039.914 I print_info: f_norm_eps       = 1.0e-05
0.00.039.915 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.915 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.915 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.915 I print_info: f_logit_scale    = 0.0e+00
0.00.039.916 I print_info: n_ff             = 8192
0.00.039.916 I print_info: n_expert         = 0
0.00.039.916 I print_info: n_expert_used    = 0
0.00.039.916 I print_info: causal attn      = 1
0.00.039.917 I print_info: pooling type     = 0
0.00.039.917 I print_info: rope type        = 2
0.00.039.917 I print_info: rope scaling     = linear
0.00.039.918 I print_info: freq_base_train  = 10000.0
0.00.039.918 I print_info: freq_scale_train = 1
0.00.039.918 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.919 I print_info: rope_finetuned   = unknown
0.00.039.919 I print_info: ssm_d_conv       = 0
0.00.039.920 I print_info: ssm_d_inner      = 0
0.00.039.920 I print_info: ssm_d_state      = 0
0.00.039.920 I print_info: ssm_dt_rank      = 0
0.00.039.921 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.921 I print_info: model type       = 1.4B
0.00.039.921 I print_info: model params     = 1.41 B
0.00.039.921 I print_info: general.name     = 1.4B
0.00.039.922 I print_info: vocab type       = BPE
0.00.039.922 I print_info: n_vocab          = 50304
0.00.039.922 I print_info: n_merges         = 50009
0.00.039.923 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.923 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.923 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.923 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.924 I print_info: LF token         = 187 'Ċ'
0.00.039.926 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.926 I print_info: max token length = 1024
0.00.625.495 I load_tensors: offloading 24 repeating layers to GPU
0.00.625.501 I load_tensors: offloading output layer to GPU
0.00.625.502 I load_tensors: offloaded 25/25 layers to GPU
0.00.625.519 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.625.520 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.626.346 I llama_init_from_model: n_seq_max     = 1
0.00.626.351 I llama_init_from_model: n_ctx         = 2048
0.00.626.351 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.626.352 I llama_init_from_model: n_batch       = 2048
0.00.626.352 I llama_init_from_model: n_ubatch      = 512
0.00.626.352 I llama_init_from_model: flash_attn    = 0
0.00.626.353 I llama_init_from_model: freq_base     = 10000.0
0.00.626.354 I llama_init_from_model: freq_scale    = 1
0.00.626.355 I ggml_metal_init: allocating
0.00.626.393 I ggml_metal_init: found device: Apple M4
0.00.626.404 I ggml_metal_init: picking default device: Apple M4
0.00.627.574 I ggml_metal_init: using embedded metal library
0.00.632.268 I ggml_metal_init: GPU name:   Apple M4
0.00.632.273 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.632.274 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.632.275 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.632.275 I ggml_metal_init: simdgroup reduction   = true
0.00.632.275 I ggml_metal_init: simdgroup matrix mul. = true
0.00.632.276 I ggml_metal_init: has residency sets    = true
0.00.632.276 I ggml_metal_init: has bfloat            = true
0.00.632.276 I ggml_metal_init: use bfloat            = true
0.00.632.278 I ggml_metal_init: hasUnifiedMemory      = true
0.00.632.282 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.646.420 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.674.968 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.674.974 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.675.009 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.679.696 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.679.698 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.679.699 I llama_init_from_model: graph nodes  = 967
0.00.679.699 I llama_init_from_model: graph splits = 2
0.00.679.704 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.679.835 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.679.836 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.509 I main: llama threadpool init, n_threads = 4
0.00.741.548 I 
0.00.741.566 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.566 I 
0.00.741.713 I sampler seed: 1234
0.00.741.717 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.741.735 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.741.735 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.741.735 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.586.439 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50354.61 tokens per second)
0.01.586.442 I llama_perf_context_print:        load time =     732.10 ms
0.01.586.443 I llama_perf_context_print: prompt eval time =      51.23 ms /     7 tokens (    7.32 ms per token,   136.63 tokens per second)
0.01.586.443 I llama_perf_context_print:        eval time =     790.66 ms /    63 runs   (   12.55 ms per token,    79.68 tokens per second)
0.01.586.444 I llama_perf_context_print:       total time =     845.56 ms /    70 tokens
0.01.586.665 I ggml_metal_free: deallocating

real	0m1.604s
user	0m0.103s
sys	0m0.187s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.434 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.286 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.028.290 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.292 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.292 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.292 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.293 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.293 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.294 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.294 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.295 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.295 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.296 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.296 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.296 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.297 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.298 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.298 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.417 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.664 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.869 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.870 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.870 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.871 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.871 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.871 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.037.872 I llama_model_loader: - type  f32:  194 tensors
0.00.037.872 I llama_model_loader: - type q5_K:   61 tensors
0.00.037.872 I llama_model_loader: - type q6_K:   37 tensors
0.00.037.873 I print_info: file format = GGUF V3 (latest)
0.00.037.873 I print_info: file type   = Q5_K - Medium
0.00.037.874 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.047.032 I load: special tokens cache size = 25
0.00.053.395 I load: token to piece cache size = 0.2984 MB
0.00.053.398 I print_info: arch             = gptneox
0.00.053.398 I print_info: vocab_only       = 0
0.00.053.398 I print_info: n_ctx_train      = 2048
0.00.053.399 I print_info: n_embd           = 2048
0.00.053.399 I print_info: n_layer          = 24
0.00.053.402 I print_info: n_head           = 16
0.00.053.404 I print_info: n_head_kv        = 16
0.00.053.404 I print_info: n_rot            = 32
0.00.053.404 I print_info: n_swa            = 0
0.00.053.404 I print_info: n_embd_head_k    = 128
0.00.053.404 I print_info: n_embd_head_v    = 128
0.00.053.405 I print_info: n_gqa            = 1
0.00.053.405 I print_info: n_embd_k_gqa     = 2048
0.00.053.409 I print_info: n_embd_v_gqa     = 2048
0.00.053.410 I print_info: f_norm_eps       = 1.0e-05
0.00.053.410 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.410 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.411 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.411 I print_info: f_logit_scale    = 0.0e+00
0.00.053.412 I print_info: n_ff             = 8192
0.00.053.412 I print_info: n_expert         = 0
0.00.053.412 I print_info: n_expert_used    = 0
0.00.053.412 I print_info: causal attn      = 1
0.00.053.412 I print_info: pooling type     = 0
0.00.053.412 I print_info: rope type        = 2
0.00.053.413 I print_info: rope scaling     = linear
0.00.053.413 I print_info: freq_base_train  = 10000.0
0.00.053.413 I print_info: freq_scale_train = 1
0.00.053.413 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.413 I print_info: rope_finetuned   = unknown
0.00.053.413 I print_info: ssm_d_conv       = 0
0.00.053.414 I print_info: ssm_d_inner      = 0
0.00.053.414 I print_info: ssm_d_state      = 0
0.00.053.414 I print_info: ssm_dt_rank      = 0
0.00.053.414 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.414 I print_info: model type       = 1.4B
0.00.053.414 I print_info: model params     = 1.41 B
0.00.053.415 I print_info: general.name     = 1.4B
0.00.053.415 I print_info: vocab type       = BPE
0.00.053.415 I print_info: n_vocab          = 50304
0.00.053.415 I print_info: n_merges         = 50009
0.00.053.416 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.416 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.416 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.416 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.417 I print_info: LF token         = 187 'Ċ'
0.00.053.417 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.417 I print_info: max token length = 1024
0.00.752.952 I load_tensors: offloading 24 repeating layers to GPU
0.00.752.966 I load_tensors: offloading output layer to GPU
0.00.752.967 I load_tensors: offloaded 25/25 layers to GPU
0.00.753.001 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.753.002 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.754.476 I llama_init_from_model: n_seq_max     = 1
0.00.754.481 I llama_init_from_model: n_ctx         = 128
0.00.754.481 I llama_init_from_model: n_ctx_per_seq = 128
0.00.754.482 I llama_init_from_model: n_batch       = 128
0.00.754.482 I llama_init_from_model: n_ubatch      = 128
0.00.754.483 I llama_init_from_model: flash_attn    = 0
0.00.754.486 I llama_init_from_model: freq_base     = 10000.0
0.00.754.486 I llama_init_from_model: freq_scale    = 1
0.00.754.487 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.754.492 I ggml_metal_init: allocating
0.00.754.579 I ggml_metal_init: found device: Apple M4
0.00.754.592 I ggml_metal_init: picking default device: Apple M4
0.00.756.490 I ggml_metal_init: using embedded metal library
0.00.762.923 I ggml_metal_init: GPU name:   Apple M4
0.00.762.926 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.762.927 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.762.928 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.762.929 I ggml_metal_init: simdgroup reduction   = true
0.00.762.929 I ggml_metal_init: simdgroup matrix mul. = true
0.00.762.930 I ggml_metal_init: has residency sets    = true
0.00.762.930 I ggml_metal_init: has bfloat            = true
0.00.762.930 I ggml_metal_init: use bfloat            = true
0.00.762.931 I ggml_metal_init: hasUnifiedMemory      = true
0.00.762.932 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.780.217 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.783.675 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.783.679 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.783.721 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.786.878 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.786.880 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.786.881 I llama_init_from_model: graph nodes  = 967
0.00.786.881 I llama_init_from_model: graph splits = 2
0.00.786.885 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.786.885 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.821.240 I 
0.00.821.325 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.821.333 I perplexity: tokenizing the input ..
0.00.828.646 I perplexity: tokenization took 7.31 ms
0.00.828.661 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.970.595 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.971.938 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.971.960 I llama_perf_context_print:        load time =     805.80 ms
0.00.971.961 I llama_perf_context_print: prompt eval time =     141.04 ms /   128 tokens (    1.10 ms per token,   907.54 tokens per second)
0.00.971.961 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.971.962 I llama_perf_context_print:       total time =     150.72 ms /   129 tokens
0.00.972.323 I ggml_metal_free: deallocating

real	0m0.996s
user	0m0.083s
sys	0m0.159s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.748 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.086 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.091 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.094 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.094 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.095 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.095 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.095 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.096 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.097 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.097 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.097 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.098 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.098 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.098 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.100 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.100 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.101 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.906 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.903 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.709 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.710 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.711 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.711 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.711 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.712 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.712 I llama_model_loader: - type  f32:  194 tensors
0.00.025.713 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.713 I print_info: file format = GGUF V3 (latest)
0.00.025.714 I print_info: file type   = Q6_K
0.00.025.714 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.648 I load: special tokens cache size = 25
0.00.039.699 I load: token to piece cache size = 0.2984 MB
0.00.039.702 I print_info: arch             = gptneox
0.00.039.702 I print_info: vocab_only       = 0
0.00.039.702 I print_info: n_ctx_train      = 2048
0.00.039.702 I print_info: n_embd           = 2048
0.00.039.703 I print_info: n_layer          = 24
0.00.039.705 I print_info: n_head           = 16
0.00.039.706 I print_info: n_head_kv        = 16
0.00.039.706 I print_info: n_rot            = 32
0.00.039.708 I print_info: n_swa            = 0
0.00.039.708 I print_info: n_embd_head_k    = 128
0.00.039.708 I print_info: n_embd_head_v    = 128
0.00.039.709 I print_info: n_gqa            = 1
0.00.039.710 I print_info: n_embd_k_gqa     = 2048
0.00.039.711 I print_info: n_embd_v_gqa     = 2048
0.00.039.711 I print_info: f_norm_eps       = 1.0e-05
0.00.039.712 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.712 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.712 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.712 I print_info: f_logit_scale    = 0.0e+00
0.00.039.713 I print_info: n_ff             = 8192
0.00.039.713 I print_info: n_expert         = 0
0.00.039.713 I print_info: n_expert_used    = 0
0.00.039.713 I print_info: causal attn      = 1
0.00.039.714 I print_info: pooling type     = 0
0.00.039.714 I print_info: rope type        = 2
0.00.039.714 I print_info: rope scaling     = linear
0.00.039.714 I print_info: freq_base_train  = 10000.0
0.00.039.715 I print_info: freq_scale_train = 1
0.00.039.715 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.715 I print_info: rope_finetuned   = unknown
0.00.039.716 I print_info: ssm_d_conv       = 0
0.00.039.716 I print_info: ssm_d_inner      = 0
0.00.039.716 I print_info: ssm_d_state      = 0
0.00.039.716 I print_info: ssm_dt_rank      = 0
0.00.039.716 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.716 I print_info: model type       = 1.4B
0.00.039.717 I print_info: model params     = 1.41 B
0.00.039.717 I print_info: general.name     = 1.4B
0.00.039.717 I print_info: vocab type       = BPE
0.00.039.718 I print_info: n_vocab          = 50304
0.00.039.718 I print_info: n_merges         = 50009
0.00.039.718 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.719 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.719 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.719 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.719 I print_info: LF token         = 187 'Ċ'
0.00.039.720 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.720 I print_info: max token length = 1024
0.00.649.559 I load_tensors: offloading 24 repeating layers to GPU
0.00.649.562 I load_tensors: offloading output layer to GPU
0.00.649.562 I load_tensors: offloaded 25/25 layers to GPU
0.00.649.585 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.649.587 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.650.846 I llama_init_from_model: n_seq_max     = 1
0.00.650.848 I llama_init_from_model: n_ctx         = 2048
0.00.650.848 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.650.848 I llama_init_from_model: n_batch       = 2048
0.00.650.849 I llama_init_from_model: n_ubatch      = 512
0.00.650.849 I llama_init_from_model: flash_attn    = 0
0.00.650.850 I llama_init_from_model: freq_base     = 10000.0
0.00.650.851 I llama_init_from_model: freq_scale    = 1
0.00.650.852 I ggml_metal_init: allocating
0.00.650.894 I ggml_metal_init: found device: Apple M4
0.00.650.904 I ggml_metal_init: picking default device: Apple M4
0.00.652.402 I ggml_metal_init: using embedded metal library
0.00.658.512 I ggml_metal_init: GPU name:   Apple M4
0.00.658.515 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.658.516 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.658.517 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.658.517 I ggml_metal_init: simdgroup reduction   = true
0.00.658.517 I ggml_metal_init: simdgroup matrix mul. = true
0.00.658.517 I ggml_metal_init: has residency sets    = true
0.00.658.518 I ggml_metal_init: has bfloat            = true
0.00.658.518 I ggml_metal_init: use bfloat            = true
0.00.658.519 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.520 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.674.999 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.724.211 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.724.217 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.724.263 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.728.461 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.728.463 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.728.463 I llama_init_from_model: graph nodes  = 967
0.00.728.463 I llama_init_from_model: graph splits = 2
0.00.728.469 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.728.593 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.728.593 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.793.442 I main: llama threadpool init, n_threads = 4
0.00.793.485 I 
0.00.793.508 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.793.508 I 
0.00.793.689 I sampler seed: 1234
0.00.793.694 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.793.738 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.793.742 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.793.742 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.668.929 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52359.88 tokens per second)
0.01.668.929 I llama_perf_context_print:        load time =     783.96 ms
0.01.668.930 I llama_perf_context_print: prompt eval time =      54.41 ms /     7 tokens (    7.77 ms per token,   128.66 tokens per second)
0.01.668.931 I llama_perf_context_print:        eval time =     817.80 ms /    63 runs   (   12.98 ms per token,    77.04 tokens per second)
0.01.668.931 I llama_perf_context_print:       total time =     876.22 ms /    70 tokens
0.01.669.219 I ggml_metal_free: deallocating

real	0m1.687s
user	0m0.108s
sys	0m0.220s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4636 (db288b60) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.987 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.981 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.987 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.992 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.992 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.993 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.993 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.993 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.994 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.995 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.995 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.995 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.996 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.998 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.998 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.000 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.000 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.000 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.775 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.823 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.479 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.481 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.481 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.481 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.482 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.482 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.483 I llama_model_loader: - type  f32:  194 tensors
0.00.026.483 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.484 I print_info: file format = GGUF V3 (latest)
0.00.026.484 I print_info: file type   = Q6_K
0.00.026.489 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.263 I load: special tokens cache size = 25
0.00.040.309 I load: token to piece cache size = 0.2984 MB
0.00.040.311 I print_info: arch             = gptneox
0.00.040.311 I print_info: vocab_only       = 0
0.00.040.312 I print_info: n_ctx_train      = 2048
0.00.040.312 I print_info: n_embd           = 2048
0.00.040.312 I print_info: n_layer          = 24
0.00.040.316 I print_info: n_head           = 16
0.00.040.316 I print_info: n_head_kv        = 16
0.00.040.317 I print_info: n_rot            = 32
0.00.040.317 I print_info: n_swa            = 0
0.00.040.317 I print_info: n_embd_head_k    = 128
0.00.040.317 I print_info: n_embd_head_v    = 128
0.00.040.318 I print_info: n_gqa            = 1
0.00.040.318 I print_info: n_embd_k_gqa     = 2048
0.00.040.321 I print_info: n_embd_v_gqa     = 2048
0.00.040.321 I print_info: f_norm_eps       = 1.0e-05
0.00.040.322 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.322 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.322 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.322 I print_info: f_logit_scale    = 0.0e+00
0.00.040.323 I print_info: n_ff             = 8192
0.00.040.329 I print_info: n_expert         = 0
0.00.040.332 I print_info: n_expert_used    = 0
0.00.040.332 I print_info: causal attn      = 1
0.00.040.333 I print_info: pooling type     = 0
0.00.040.333 I print_info: rope type        = 2
0.00.040.333 I print_info: rope scaling     = linear
0.00.040.334 I print_info: freq_base_train  = 10000.0
0.00.040.334 I print_info: freq_scale_train = 1
0.00.040.334 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.335 I print_info: rope_finetuned   = unknown
0.00.040.335 I print_info: ssm_d_conv       = 0
0.00.040.336 I print_info: ssm_d_inner      = 0
0.00.040.336 I print_info: ssm_d_state      = 0
0.00.040.336 I print_info: ssm_dt_rank      = 0
0.00.040.336 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.336 I print_info: model type       = 1.4B
0.00.040.337 I print_info: model params     = 1.41 B
0.00.040.338 I print_info: general.name     = 1.4B
0.00.040.338 I print_info: vocab type       = BPE
0.00.040.338 I print_info: n_vocab          = 50304
0.00.040.338 I print_info: n_merges         = 50009
0.00.040.339 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.339 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.339 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.339 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.340 I print_info: LF token         = 187 'Ċ'
0.00.040.340 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.340 I print_info: max token length = 1024
0.00.686.650 I load_tensors: offloading 24 repeating layers to GPU
0.00.686.667 I load_tensors: offloading output layer to GPU
0.00.686.668 I load_tensors: offloaded 25/25 layers to GPU
0.00.686.700 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.686.701 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.688.203 I llama_init_from_model: n_seq_max     = 1
0.00.688.209 I llama_init_from_model: n_ctx         = 128
0.00.688.210 I llama_init_from_model: n_ctx_per_seq = 128
0.00.688.210 I llama_init_from_model: n_batch       = 128
0.00.688.214 I llama_init_from_model: n_ubatch      = 128
0.00.688.215 I llama_init_from_model: flash_attn    = 0
0.00.688.218 I llama_init_from_model: freq_base     = 10000.0
0.00.688.231 I llama_init_from_model: freq_scale    = 1
0.00.688.232 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.688.235 I ggml_metal_init: allocating
0.00.688.352 I ggml_metal_init: found device: Apple M4
0.00.688.366 I ggml_metal_init: picking default device: Apple M4
0.00.690.171 I ggml_metal_init: using embedded metal library
0.00.696.890 I ggml_metal_init: GPU name:   Apple M4
0.00.696.895 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.696.896 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.696.896 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.696.897 I ggml_metal_init: simdgroup reduction   = true
0.00.696.897 I ggml_metal_init: simdgroup matrix mul. = true
0.00.696.897 I ggml_metal_init: has residency sets    = true
0.00.696.898 I ggml_metal_init: has bfloat            = true
0.00.696.898 I ggml_metal_init: use bfloat            = true
0.00.696.899 I ggml_metal_init: hasUnifiedMemory      = true
0.00.696.907 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.714.485 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.718.030 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.718.034 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.718.078 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.721.371 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.721.372 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.721.373 I llama_init_from_model: graph nodes  = 967
0.00.721.373 I llama_init_from_model: graph splits = 2
0.00.721.377 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.721.377 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.708 I 
0.00.753.788 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.795 I perplexity: tokenizing the input ..
0.00.760.722 I perplexity: tokenization took 6.924 ms
0.00.760.730 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.900.899 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.902.201 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.902.222 I llama_perf_context_print:        load time =     744.71 ms
0.00.902.223 I llama_perf_context_print: prompt eval time =     139.36 ms /   128 tokens (    1.09 ms per token,   918.51 tokens per second)
0.00.902.224 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.902.224 I llama_perf_context_print:       total time =     148.52 ms /   129 tokens
0.00.902.548 I ggml_metal_free: deallocating

real	0m0.935s
user	0m0.078s
sys	0m0.148s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4636 (db288b60)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13f904bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13f905320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13f9058d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13f905e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13f906430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13f9069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13f906f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13f907540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13f907af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13f907ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13f9084f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13f9089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13f909510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13f909cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13f90a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13f90abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13f90b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13f90ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13f90c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13f90c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13f90d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13f90d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13f90de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13f90e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13f90ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13f90f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13f90f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13f910380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13f9108c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13f910b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1057076f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1057079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105707e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x105708290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105708700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105708e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105709300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1057097d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105709ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10570a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10570a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10570ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10570afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10570b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10570b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10570bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10570c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10570cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10570ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10570d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10570d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10570dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10570e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10570e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10570e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10570efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10570f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10570f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10570fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10570ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x105710530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x105710a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x105710f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x105711430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x105711930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x105711e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x105712330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x105712830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x105712d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x105713230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x105713730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x105713c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x105714130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x105714630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x105714be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x105715190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x105715740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x105715cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1057162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x105716850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x105716e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1057173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x105717960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x105717f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1057184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x105718a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x105719020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1057195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x105719b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10571a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10571a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10571ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10571b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10571b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10571bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10571c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10571c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10570c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10571d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10571d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10571d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10571def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10571e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10571ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10571f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10571f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10571fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x105720110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1057206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x105720c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x105721220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1057217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x105721d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x105722330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x105722830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x105722d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x105723230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x105723730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x105723c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x105724130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x105724630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x105724b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x105725030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x105725530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x105725a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x105725f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x105726430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x105726930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x105726e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x105727330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x105727830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x105727d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x105728230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x105728730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x105728c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x105729130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x105729630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x105729b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10572a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10572a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10572aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10572af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10572b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10572b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10572be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10572c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10572c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10572cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10572d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10572d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10572dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10572e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10572e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10572eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10572f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10572f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10572fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10572ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x105730430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x105730930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105730e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105731330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x105731830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x105731d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x105732230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x105732730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x105732c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x105733130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x105733630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x105733b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x105734030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x105734530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x105734a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x105734f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x105735430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x105735930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x105735e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x105736330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x105736830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x105736d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x105737230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x105737730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x105737c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x105738130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x105738630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x105738b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x105739030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x105739530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x105739a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x105739f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10573a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10573a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10573ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10573b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10573b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10573be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10573c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10573c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10573d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10573d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10573dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10573e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10573e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10573eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10573f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10573f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10573ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x105740420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1057408c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x105740d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x105741510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x105741a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x105741fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x105742500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x105742a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x105742fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1057434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x105743a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x105743f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1057444e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x105744a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x105744f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1057454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x105745a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x105745f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1057464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x105746a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x105746f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1057474b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x105747a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x105747f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1057484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1057489f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x105748f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x105749490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1057499e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x105749f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10574a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10574a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10574af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10574b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10574b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10574bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10574c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10574c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10574cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10574d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10574d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10574def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10574e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10574e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10574eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10574f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10574f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10574fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x105750420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x105750970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x105750ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x105751410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x105751960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x105751eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x105752400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x105752950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x105752ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1057533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x105753940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x105753e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x105754330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1057547d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x105754c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x105755110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1057555b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x105755a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x105755ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x105756390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x105756830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x105756cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x105757170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x105757610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x105757ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x105757f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1057583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x105758940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x105759060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x105759780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x105759ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10575a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10575a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10575b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10575b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10575b940 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.758.181 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.758.184 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x109704dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x109705240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1097056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x109705b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x109705f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x109706400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x109706870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x109706ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x109707150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1097075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x109707a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x109708120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x109708c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1097093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x109709c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10970a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10970aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10970b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10970b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10970bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10970c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10970cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10970d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10970dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10970e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10970e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10970e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10970ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10970f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10970f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10970fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10970ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x109710430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1097106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x109710b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x109710fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x109711440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1097118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x109711d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x109712190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x109712600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x109712a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x109712ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x109713350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1097137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x109713c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1097140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x109714510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x109714980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x109714df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x109715260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1097156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x109715b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x109715fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x109716420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x109716890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x109716e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x109717300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x109717770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x109717be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x109718050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1097184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x109718930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x109718da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x109719210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x109719680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x109719af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x105715450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x105714ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10571a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1057148f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10571c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x105719e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1057214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x105720f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x105720980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10571c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x105716b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10571ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10573bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10571bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x105716560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x105719890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1057181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10571e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10573b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1057203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10571b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x105715fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1057192e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x105717c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10571e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10571fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10571af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x105715a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x105718d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10571dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10571f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10571a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x105718780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10571f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1057089c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10573ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10573d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10575b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10573ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10573d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10573f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10570c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10570ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10575ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10571cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10573fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10573dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10575bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10575c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10575c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10575c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10575c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10575cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10575ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10575d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10575d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10575d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10575d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10575dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10575dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10575e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10575e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10575e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10575e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10575ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10575ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10575f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10575f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10575f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10575fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10575fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10575ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x105760260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x105760520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1057607e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x105760aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x105760d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x105761020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1057612e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1057615a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x105761860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x105761b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x105761de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1057620a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105762360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105762620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1057628e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x105762ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x105762e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x105763120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1057633e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1057636a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x105763960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105763c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105763ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1057641a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x105764460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x105764720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1057649e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x105764ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x105764f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x105765220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1057654e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1057657a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x105765a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x105765d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x105765fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1057662a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x105766560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x105766820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x105766ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x105766da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x105767060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x105767320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1057675e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1057678a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x105767b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x109719f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10971a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10971a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10971ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10971b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10971b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10971bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10971bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10971c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10971c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10971cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10971d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10971d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10971d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10971de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10971e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10971e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10971eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10971f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10971f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10971f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10971fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1097201d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x109720640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x109720ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x109720f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x109721b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x109721dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x109722080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1097224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x109722960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x109722dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x109723240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1097236b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x109723b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x109723f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x109724400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x109724870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x109724ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x109725150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1097255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x109725a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x109725ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x109726310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x109726780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x109726bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x109727060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1097274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x109727940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x109727db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x109728220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x109728690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x109728b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x109728f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1097293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x109729850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x109729cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10972a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10972a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10972aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10972ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10972b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10972b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10972bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10972c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10972c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10972c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10972cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10972d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10972d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10972dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10972df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10972e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10972e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10972eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10972f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10972f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10972f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10972fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1097302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x109730740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x109730bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x109731020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x109731490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x109731900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x109731d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1097321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x109732650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x109732ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x109732f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1097333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x109733810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x109733c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1097340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x109734560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1097349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x109734e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1097352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x109735720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x109736190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1097368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x109736fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1097376f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1097379b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x109737e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x109738420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x109738a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1096086f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x109606500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x109609130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x109609680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x109609bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10960a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10960a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10960abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10960b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10960b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10960b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10960bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10960c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10960ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10960d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10960dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10960e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10960eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10960f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10960fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x109610360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x109610a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1096111a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1096118c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x109611fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1096122a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1096128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x109612ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1096134d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x109613cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x109614160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x109614420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x109614cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1096151f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1096154b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x109615950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x109615df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x109616290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x109616730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x109616bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x109617070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x109617510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1096179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x109617e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x109618110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x109618720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x109618d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x109619340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x109619950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x109619f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10961a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10961ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10961b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10961b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10961bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10961c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10961c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10961cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10961d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10961d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10961de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10961e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10961e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10961ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10961f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10961f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10961f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10961fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x109620330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1096207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x109620c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x109621110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1096215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x109621b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x109622050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1096225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x109622af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x109623040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x109623590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x109623ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x109624030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x109624580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x109624ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x109625020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x109625570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x109625ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x109626010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x109626560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x109626ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x109627000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x109627550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x109627aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x109627ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x109628540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x109628a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x109628fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x109629530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x109629a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x109629fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10962a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10962aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10962afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10962b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10962ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10962bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10962c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10962ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10962cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10962d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10962da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10962df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10962e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10962ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10962eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10962f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10962f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10962fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x109630150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1096305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x109630a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x109630f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1096313d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x109631870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x109631d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1096321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x109632650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x109632af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x109632f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x109633430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1096338d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x109633d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x109634210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1096346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x109634b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x109634ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x109635490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x109635930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x109635dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x109636270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x109636710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x109636bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x109637050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1096374f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x109637990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x109637e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1096382d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x109638770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x109638c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1096390b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x109639550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1096399f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x109639e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10963a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10963a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10963ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10963b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10963b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10963ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10963bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10963c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10963c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10963ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10963d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10963d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10963dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10963df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10963e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10963e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10963ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10963f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10963f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10963fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10963ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x109640450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1096408f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x109640d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x109641230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1096416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x109641b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x109642010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1096424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x109642950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x109642df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x109643290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x109643730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x109643bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x109644070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x109644510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1096449b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x109644e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1096452f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x109645790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x109645c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x109646180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1096466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x109646c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x109647170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x109647430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x109647a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x109648050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x109648660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x109648e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1096492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1096495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x109649bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10964a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10964a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10964ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10964b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10964b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10964bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10964c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10964c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10964cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10964d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10964d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10964df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10964e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10964e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10964ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10964f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10964f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10964ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x109650460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1096509b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x109650f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x109651450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1096519a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x109651ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x109652440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x109652990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x109652ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x109653430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x109653980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x109653ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x109654420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x109654970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x109654ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x109655410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x109655960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x109655eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x109656400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x109656950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x109656ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1096573f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x109657940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x109657e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1096583e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x109658930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x109658e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1096593d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x109659920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x109659e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10965a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10965a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10965ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10965b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10965b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10965be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10965c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10965c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10965ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10965d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10965d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10965de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10965e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10965e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10965ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10965f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10965f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10965fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10965fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x109660490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x109660930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x109660dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x109661270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x109661710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x109661bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x109662050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1096624f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x109662990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x109662e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x109663380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x109663aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1096641c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1096648e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x109665000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1096652c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x109665ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x109665d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x109666380 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.795s
user	0m0.258s
sys	0m0.312s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4636 (db288b60)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15500a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15500aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15500b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15500b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15500bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15500c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15500c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15500cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15500d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15500d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15500dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15500e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15500ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15500f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15500fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1550103c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x155010ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x155011200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x155011920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1550120f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x155012810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x155012f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x155013650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x155013ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x155014610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1550148d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x155014ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x155015b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x155016090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x155016350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1550167f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x155016ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155017340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155017880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155017b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x155017fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155018480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155018920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155018dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155019260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x155019700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x155019ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15501a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15501a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15501a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15501adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15501b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15501bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15501c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15501c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15501cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15501d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15501db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15501e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15501e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15501edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15501f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15501f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15501fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155020330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1550205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155020a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x155020f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1550213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155021870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x155021d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1550221b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x155022650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x155022af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x155022f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x155023430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1550238d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x155023d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1550242c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x155024810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x155024d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1550252b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x155025800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x155025d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1550262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1550267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x155026d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x155027290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1550277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x155027d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x155028280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1550287d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x155028d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x155029270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1550297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x155029d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15502a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15502a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15502ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15502b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15502b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15502bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15501b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15502c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15502c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15502ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15502d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15502d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15502de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15502e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15502e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15502ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15502f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15502f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15502fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x155030380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1550308d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x155030e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1550312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x155031760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x155031c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1550320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x155032540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1550329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x155032e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x155033320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1550337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x155033c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x155034100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1550345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x155034a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x155034ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x155035380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x155035820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x155035cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x155036160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x155036600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x155036aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x155036f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1550373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x155037880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x155037d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1550381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x155038660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155038b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x155038fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x155039440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1550398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x155039d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15503a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15503a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15503ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15503b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15503b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15503b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15503bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15503c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15503c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15503cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15503d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15503d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15503d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15503de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15503e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15503e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15503ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15503f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15503f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15503fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15503fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x155040340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1550407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x155040c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x155041120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1550415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x155041a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x155041f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1550423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x155042840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x155042ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x155043180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x155043620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x155043ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155043f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x155044400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1550448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x155044d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1550451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x155045680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x155045b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x155045fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x155046460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x155046900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x155046da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x155047240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1550476e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x155047b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x155048020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x155048570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155048ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x155049010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x155049560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x155049820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x155049e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15504a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15504aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15504b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15504b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15504b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15504bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15504c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15504cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15504d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15504d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15504db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15504e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15504e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15504ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15504f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15504f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15504fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x155050320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155050870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x155050dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x155051310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x155051860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x155051db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x155052300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x155052850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x155052da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1550532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x155053840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x155053d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1550542e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x155054830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x155054d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1550552d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x155055820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x155055d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1550562c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155056810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x155056d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1550572b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x155057800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x155057d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1550582a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1550587f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x155058d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x155059290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1550597e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x155059d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15505a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15505a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15505ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15505b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15505b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15505bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15505c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15505c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15505cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15505d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15505d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15505dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15505e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15505e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15505ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15505f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15505f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15505fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x155060220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x155060770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x155060cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x155061160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x155061600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x155061aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x155061f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1550623e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x155062880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x155062d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1550631c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x155063660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x155063b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x155063fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x155064440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1550648e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x155064d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x155065220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x155065770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x155065e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1550665b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x155066cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1550673f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1550676b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x155067ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x155068160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x155068770 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.098.948 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.952 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x155068420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15504a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x155049ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15504a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15501d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15501d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15501f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15504c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x155014b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15501b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15501bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15501c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15501aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15501cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x155013b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15501fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15502c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x155067970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x155016d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x155017030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15504c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15504ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1550151a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x155015460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x155015720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x155068bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x155068e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x155069150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x155069410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1550696d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x155069990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x155069c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155069f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15506a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15506a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15506a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15506aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15506acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15506af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15506b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15506b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15506b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15506ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15506bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15506c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15506c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15506c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15506c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15506cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15506cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15506d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15506d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15506d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15506d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15506db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15506de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15506e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15506e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15506e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15506e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15506ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15506eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15506f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15506f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15506f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15506f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15506fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15506ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x155070210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1550704d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146b04430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146b048a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146b04d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x146b05180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146b055f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146b05a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x146b05ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146b06340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146b067b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146b06c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146b07090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146b07500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146b07970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146b07de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146b08250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146b086c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146b08b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146b08fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146b09410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146b09880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146b09cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146b0a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146b0a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146b0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146b0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146b0b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146b0b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146b0bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146b0c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146b0c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146b0c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146b0cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146b0d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146b0d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146b0db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146b0df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146b0e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146b0e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146b0ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x146b0f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146b0f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146b0fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146b0fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146b10300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146b10770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146b10be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146b11050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146b114c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146b11930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146b11da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146b12210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146b12680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146b12af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146b12f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146b133d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146b13840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146b13cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146b14120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146b14590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146b14a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146b14e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146b152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146b15750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146b15bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146b16030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146b164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146b16910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146b16d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146b171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146b17660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146b17ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146b17f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146b183b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146b18820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146b18c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146b19100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146b19570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146b199e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146b19e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146b1a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146b1a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146b1aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146b1b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146b1b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146b1b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146b1bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146b1c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146b1c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146b1cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146b1cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146b1d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146b1d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146b1dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146b1e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146b1e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146b1e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146b1f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146b1f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146b1fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146b20000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146b20470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146b208e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146b20d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146b211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146b21630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146b21aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146b21f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146b22380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146b227f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146b22c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146b230d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146b23540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146b239b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146b23e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146b24290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146b24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146b24b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146b24fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146b25450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146b258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146b25d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146b261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146b26610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146b26a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146b26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146b27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146b277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146b27c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146b280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146b28520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146b28990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146b28e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146b29360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146b29870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146b29ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146b2a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146b2a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146b2aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146b2af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146b2b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146b2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146b2c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146b2c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146b2ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146b2d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146b2d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146b2df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146b2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146b2ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146b2f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146b2f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146b2fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146b301d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146b30790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146b30d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146b31310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146b318d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146b31e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146b32450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146b32a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146b32fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146b33590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146b33b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146b34110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146b346d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146b34c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146b35250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146b35810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146b35dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146b36390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146b36950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146b36f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146b374d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146b37a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146b38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146b38610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146b38bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146b39190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146b39750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146b39d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146b3a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146b3a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146b3ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146b3b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146b3b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146b3bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146b3c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146b3cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146b3d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146b3d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146b3dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146b3e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146b3e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146b3ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146b3f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146b3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146b3fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146b40490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146b40990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146b40e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146b41390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146b41890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146b41d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146b42290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146b42790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146b42c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146b43190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146b43690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146b43b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146b44090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146b44590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146b44a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146b44f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146b459a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146b460c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146b467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146b46f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146b471c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146b479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146b47c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146b48280 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15360af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15360b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15360b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15360bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15360c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15360c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15360ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15360ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15360d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15360d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15360dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15360e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15360ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15360f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15360fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x153610520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x153610c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x153611360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x153611a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x153612250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x153612970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x153613090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1536137b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x153613ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1536145f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1536148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x153614b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x153614fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x153615450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1536158c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x153615dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1536162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x153616740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x153616a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x153616e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1536172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x153617840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x153617d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x153618240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x153618740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x153618c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x153619140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x153619640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x153619b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15361a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15361a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15361a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15361ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15361b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15361b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15361bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15361bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15361c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15361c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15361cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15361d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15361d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15361dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15361e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15361e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15361ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15361f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15361f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15361fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1536200f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x153620590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x153620a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x153620ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x153621370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x153621810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x153621cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x153622150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1536225f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x153622b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x153623090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1536235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x153623b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x153624080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1536245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x153624b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x153625070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1536255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x153625b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x153626060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1536265b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x153626b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x153627050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1536275a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x153627af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x153628040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x153628590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x153628ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x153629030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x153629580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x153629ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15362a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15362a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15362aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15362b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15362b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15362bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15362c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15362c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15362caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15362cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15362d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15362da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15362dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15362e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15362ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15362efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15362f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15362fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15362ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1536303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x153630850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x153630cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x153631190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x153631630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x153631ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x153631f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x153632410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1536328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x153632d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1536331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x153633690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x153633b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x153633fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x153634470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x153634910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x153634db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x153635250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1536356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x153635b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x153636030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1536364d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x153636970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x153636e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1536372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x153637750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x153637bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x153638090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x153638530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1536389d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x153638e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x153639310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1536397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x153639c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15363a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15363a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15363aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15363aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15363b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15363b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15363bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15363c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15363c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15363ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15363cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15363d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15363d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15363dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15363e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15363e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15363eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15363ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15363f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15363f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15363fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x153640210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1536406b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x153640b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x153640ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x153641490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x153641930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x153641dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x153642270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x153642710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x153642bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x153643050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1536434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x153643990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x153643e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1536442d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x153644770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x153644c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1536450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x153645550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1536459f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x153645e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x153646330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1536467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x153646c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1536471c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x153647710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x153647c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1536481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x153648470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x153648a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x153649090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1536496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x153649e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15364a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15364a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15364ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15364b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15364ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15364bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15364c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15364c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15364cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15364d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15364da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15364df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15364e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15364ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15364ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15364f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15364fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15364ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1536504b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x153650a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x153650f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1536514a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1536519f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x153651f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x153652490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1536529e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x153652f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x153653480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1536539d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x153653f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x153654470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1536549c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x153654f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x153655460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1536559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x153655f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x153656450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1536569a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x153656ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x153657440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x153657990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x153657ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x153658430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x153658980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x153658ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x153659420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x153659970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x153659ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15365a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15365a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15365aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15365b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15365b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15365bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15365c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15365c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15365ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15365d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15365d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15365de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15365e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15365e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15365ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15365f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15365f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15365fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x153660250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1536606f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x153660b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x153661030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1536614d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x153661970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x153661e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1536622b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x153662750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x153662bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x153663090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x153663530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1536639d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x153663e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1536643c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x153664ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x153665200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x153665920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x153666040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x153666300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x153666af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x153666db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1536673c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.958s
user	0m0.236s
sys	0m0.190s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.47 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.32 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.79 sec*proc (2 tests)

Total Test time (real) =   1.80 sec
        1.83 real         0.53 user         0.21 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.24 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.31 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.55 sec*proc (2 tests)

Total Test time (real) =   0.56 sec
        0.56 real         0.13 user         0.08 sys
```
