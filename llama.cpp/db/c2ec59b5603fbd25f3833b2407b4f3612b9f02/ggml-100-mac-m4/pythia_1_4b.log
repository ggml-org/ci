Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.560s
user	0m0.865s
sys	0m1.254s
++ nproc
+ make -j10
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Built target sha1
[  4%] Built target build_info
[  4%] Built target sha256
[  4%] Built target xxhash
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Built target llama-gguf-hash
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Linking CXX executable ../../bin/llama-quantize-stats
[ 29%] Linking CXX executable ../../bin/llama-simple
[ 30%] Linking CXX executable ../../bin/llama-simple-chat
[ 30%] Built target llava
[ 31%] Linking C executable ../bin/test-c
[ 32%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX static library libllava_static.a
[ 35%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 35%] Built target llama-simple-chat
[ 35%] Built target llama-quantize-stats
[ 35%] Built target test-c
[ 36%] Built target llama-simple
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 36%] Built target common
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-log
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-sampling
[ 48%] Built target test-grammar-integration
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Built target test-chat
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-barrier
[ 59%] Linking CXX executable ../bin/test-gguf
[ 60%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 62%] Built target test-arg-parser
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-barrier
[ 62%] Built target test-gguf
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-backend-ops
[ 63%] Built target test-autorelease
[ 63%] Built target test-quantize-fns
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Built target test-quantize-perf
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 67%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Built target test-rope
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-batched-bench
[ 72%] Built target llama-batched
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-embedding
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-eval-callback
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-infill
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Built target llama-gritlm
[ 73%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Built target llama-imatrix
[ 77%] Linking CXX executable ../../bin/llama-lookahead
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Built target llama-bench
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-lookup
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-cli
[ 81%] Built target llama-lookup-merge
[ 81%] Generating loading.html.hpp
[ 82%] Generating index.html.gz.hpp
[ 82%] Built target llama-lookup-stats
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Built target llama-parallel
[ 82%] Built target llama-lookup-create
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Built target llama-passkey
[ 83%] Built target llama-perplexity
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Built target llama-quantize
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 84%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 85%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-retrieval
[ 91%] Linking CXX executable ../../bin/llama-run
[ 91%] Built target llama-save-load-state
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-speculative-simple
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-speculative
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-tts
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-run
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.149s
user	0m6.635s
sys	0m10.194s

main: quantize time =  2362.30 ms
main:    total time =  2362.30 ms

main: quantize time =  2105.46 ms
main:    total time =  2105.46 ms

main: quantize time =  3013.63 ms
main:    total time =  3013.63 ms

main: quantize time =  1888.14 ms
main:    total time =  1888.14 ms

main: quantize time =  2600.38 ms
main:    total time =  2600.38 ms

main: quantize time =  5681.06 ms
main:    total time =  5681.06 ms

main: quantize time =  6183.05 ms
main:    total time =  6183.05 ms

main: quantize time =  7150.21 ms
main:    total time =  7150.21 ms

main: quantize time =  6549.55 ms
main:    total time =  6549.55 ms

main: quantize time =  4363.39 ms
main:    total time =  4363.39 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.150 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.350 I main: llama backend init
0.00.000.357 I main: load the model and apply lora adapter, if any
0.00.092.206 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.104.554 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.104.567 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.104.571 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.104.578 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.104.578 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.104.579 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.104.580 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.104.582 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.104.583 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.104.584 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.104.584 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.104.585 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.104.586 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.104.587 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.104.593 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.104.593 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.104.594 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.111.554 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.113.705 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.120.440 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.120.447 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.120.447 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.120.448 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.120.449 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.120.451 I llama_model_loader: - type  f32:  194 tensors
0.00.120.451 I llama_model_loader: - type  f16:   98 tensors
0.00.120.453 I print_info: file format = GGUF V3 (latest)
0.00.120.455 I print_info: file type   = all F32 (guessed)
0.00.120.458 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.138.786 I load: special tokens cache size = 25
0.00.148.777 I load: token to piece cache size = 0.2984 MB
0.00.148.783 I print_info: arch             = gptneox
0.00.148.783 I print_info: vocab_only       = 0
0.00.148.784 I print_info: n_ctx_train      = 2048
0.00.148.786 I print_info: n_embd           = 2048
0.00.148.786 I print_info: n_layer          = 24
0.00.148.790 I print_info: n_head           = 16
0.00.148.791 I print_info: n_head_kv        = 16
0.00.148.792 I print_info: n_rot            = 32
0.00.148.792 I print_info: n_swa            = 0
0.00.148.792 I print_info: n_embd_head_k    = 128
0.00.148.792 I print_info: n_embd_head_v    = 128
0.00.148.793 I print_info: n_gqa            = 1
0.00.148.794 I print_info: n_embd_k_gqa     = 2048
0.00.148.795 I print_info: n_embd_v_gqa     = 2048
0.00.148.796 I print_info: f_norm_eps       = 1.0e-05
0.00.148.796 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.148.797 I print_info: f_clamp_kqv      = 0.0e+00
0.00.148.797 I print_info: f_max_alibi_bias = 0.0e+00
0.00.148.797 I print_info: f_logit_scale    = 0.0e+00
0.00.148.798 I print_info: n_ff             = 8192
0.00.148.799 I print_info: n_expert         = 0
0.00.148.799 I print_info: n_expert_used    = 0
0.00.148.802 I print_info: causal attn      = 1
0.00.148.802 I print_info: pooling type     = 0
0.00.148.802 I print_info: rope type        = 2
0.00.148.802 I print_info: rope scaling     = linear
0.00.148.803 I print_info: freq_base_train  = 10000.0
0.00.148.803 I print_info: freq_scale_train = 1
0.00.148.803 I print_info: n_ctx_orig_yarn  = 2048
0.00.148.804 I print_info: rope_finetuned   = unknown
0.00.148.804 I print_info: ssm_d_conv       = 0
0.00.148.804 I print_info: ssm_d_inner      = 0
0.00.148.804 I print_info: ssm_d_state      = 0
0.00.148.805 I print_info: ssm_dt_rank      = 0
0.00.148.805 I print_info: ssm_dt_b_c_rms   = 0
0.00.148.805 I print_info: model type       = 1.4B
0.00.148.805 I print_info: model params     = 1.41 B
0.00.148.806 I print_info: general.name     = 1.4B
0.00.148.806 I print_info: vocab type       = BPE
0.00.148.807 I print_info: n_vocab          = 50304
0.00.148.808 I print_info: n_merges         = 50009
0.00.148.808 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.148.810 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.148.810 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.148.810 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.148.811 I print_info: LF token         = 187 'Ċ'
0.00.148.811 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.148.811 I print_info: max token length = 1024
0.00.148.813 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.196.382 I load_tensors: offloading 24 repeating layers to GPU
0.00.196.385 I load_tensors: offloading output layer to GPU
0.00.196.385 I load_tensors: offloaded 25/25 layers to GPU
0.00.196.408 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.196.410 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.197.039 I llama_init_from_model: n_seq_max     = 1
0.00.197.040 I llama_init_from_model: n_ctx         = 2048
0.00.197.040 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.197.040 I llama_init_from_model: n_batch       = 2048
0.00.197.040 I llama_init_from_model: n_ubatch      = 512
0.00.197.040 I llama_init_from_model: flash_attn    = 0
0.00.197.041 I llama_init_from_model: freq_base     = 10000.0
0.00.197.041 I llama_init_from_model: freq_scale    = 1
0.00.197.041 I ggml_metal_init: allocating
0.00.197.064 I ggml_metal_init: found device: Apple M4
0.00.197.073 I ggml_metal_init: picking default device: Apple M4
0.00.197.693 I ggml_metal_init: using embedded metal library
0.00.402.271 I ggml_metal_init: GPU name:   Apple M4
0.00.402.286 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.402.287 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.402.288 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.402.288 I ggml_metal_init: simdgroup reduction   = true
0.00.402.288 I ggml_metal_init: simdgroup matrix mul. = true
0.00.402.289 I ggml_metal_init: has residency sets    = true
0.00.402.289 I ggml_metal_init: has bfloat            = true
0.00.402.289 I ggml_metal_init: use bfloat            = true
0.00.402.291 I ggml_metal_init: hasUnifiedMemory      = true
0.00.402.297 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.458.106 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.498.968 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.498.981 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.499.028 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.502.665 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.502.668 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.502.668 I llama_init_from_model: graph nodes  = 967
0.00.502.669 I llama_init_from_model: graph splits = 2
0.00.502.675 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.502.804 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.502.805 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.571.000 I main: llama threadpool init, n_threads = 4
0.00.571.042 I 
0.00.571.056 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.571.056 I 
0.00.571.214 I sampler seed: 1234
0.00.571.219 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.571.244 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.571.246 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.571.246 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.407.805 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57396.93 tokens per second)
0.02.407.806 I llama_perf_context_print:        load time =     477.95 ms
0.02.407.806 I llama_perf_context_print: prompt eval time =      43.64 ms /     7 tokens (    6.23 ms per token,   160.40 tokens per second)
0.02.407.807 I llama_perf_context_print:        eval time =    1789.94 ms /    63 runs   (   28.41 ms per token,    35.20 tokens per second)
0.02.407.807 I llama_perf_context_print:       total time =    1837.62 ms /    70 tokens
0.02.408.023 I ggml_metal_free: deallocating

real	0m2.710s
user	0m0.145s
sys	0m0.161s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.010.340 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.365 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.373 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.379 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.380 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.380 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.380 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.381 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.382 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.382 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.382 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.383 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.383 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.383 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.383 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.386 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.386 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.386 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.431 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.511 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.476 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.478 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.478 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.478 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.479 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.479 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.480 I llama_model_loader: - type  f32:  194 tensors
0.00.038.480 I llama_model_loader: - type q8_0:   98 tensors
0.00.038.481 I print_info: file format = GGUF V3 (latest)
0.00.038.481 I print_info: file type   = Q8_0
0.00.038.483 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.048.156 I load: special tokens cache size = 25
0.00.055.253 I load: token to piece cache size = 0.2984 MB
0.00.055.257 I print_info: arch             = gptneox
0.00.055.257 I print_info: vocab_only       = 0
0.00.055.257 I print_info: n_ctx_train      = 2048
0.00.055.257 I print_info: n_embd           = 2048
0.00.055.258 I print_info: n_layer          = 24
0.00.055.263 I print_info: n_head           = 16
0.00.055.263 I print_info: n_head_kv        = 16
0.00.055.264 I print_info: n_rot            = 32
0.00.055.264 I print_info: n_swa            = 0
0.00.055.264 I print_info: n_embd_head_k    = 128
0.00.055.266 I print_info: n_embd_head_v    = 128
0.00.055.267 I print_info: n_gqa            = 1
0.00.055.268 I print_info: n_embd_k_gqa     = 2048
0.00.055.268 I print_info: n_embd_v_gqa     = 2048
0.00.055.269 I print_info: f_norm_eps       = 1.0e-05
0.00.055.269 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.270 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.270 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.270 I print_info: f_logit_scale    = 0.0e+00
0.00.055.271 I print_info: n_ff             = 8192
0.00.055.273 I print_info: n_expert         = 0
0.00.055.273 I print_info: n_expert_used    = 0
0.00.055.273 I print_info: causal attn      = 1
0.00.055.273 I print_info: pooling type     = 0
0.00.055.273 I print_info: rope type        = 2
0.00.055.274 I print_info: rope scaling     = linear
0.00.055.274 I print_info: freq_base_train  = 10000.0
0.00.055.274 I print_info: freq_scale_train = 1
0.00.055.275 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.275 I print_info: rope_finetuned   = unknown
0.00.055.275 I print_info: ssm_d_conv       = 0
0.00.055.277 I print_info: ssm_d_inner      = 0
0.00.055.277 I print_info: ssm_d_state      = 0
0.00.055.277 I print_info: ssm_dt_rank      = 0
0.00.055.277 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.278 I print_info: model type       = 1.4B
0.00.055.278 I print_info: model params     = 1.41 B
0.00.055.278 I print_info: general.name     = 1.4B
0.00.055.279 I print_info: vocab type       = BPE
0.00.055.279 I print_info: n_vocab          = 50304
0.00.055.279 I print_info: n_merges         = 50009
0.00.055.279 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.280 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.280 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.280 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.281 I print_info: LF token         = 187 'Ċ'
0.00.055.281 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.281 I print_info: max token length = 1024
0.00.055.281 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.216.408 I load_tensors: offloading 24 repeating layers to GPU
0.01.216.413 I load_tensors: offloading output layer to GPU
0.01.216.414 I load_tensors: offloaded 25/25 layers to GPU
0.01.216.437 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.216.440 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.217.526 I llama_init_from_model: n_seq_max     = 1
0.01.217.529 I llama_init_from_model: n_ctx         = 2048
0.01.217.529 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.217.529 I llama_init_from_model: n_batch       = 2048
0.01.217.530 I llama_init_from_model: n_ubatch      = 512
0.01.217.530 I llama_init_from_model: flash_attn    = 0
0.01.217.531 I llama_init_from_model: freq_base     = 10000.0
0.01.217.531 I llama_init_from_model: freq_scale    = 1
0.01.217.532 I ggml_metal_init: allocating
0.01.217.544 I ggml_metal_init: found device: Apple M4
0.01.217.552 I ggml_metal_init: picking default device: Apple M4
0.01.218.759 I ggml_metal_init: using embedded metal library
0.01.223.934 I ggml_metal_init: GPU name:   Apple M4
0.01.223.937 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.223.938 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.223.939 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.223.939 I ggml_metal_init: simdgroup reduction   = true
0.01.223.940 I ggml_metal_init: simdgroup matrix mul. = true
0.01.223.940 I ggml_metal_init: has residency sets    = true
0.01.223.940 I ggml_metal_init: has bfloat            = true
0.01.223.940 I ggml_metal_init: use bfloat            = true
0.01.223.941 I ggml_metal_init: hasUnifiedMemory      = true
0.01.223.942 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.239.652 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.292.067 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.292.074 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.292.110 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.296.221 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.296.223 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.296.223 I llama_init_from_model: graph nodes  = 967
0.01.296.224 I llama_init_from_model: graph splits = 2
0.01.296.228 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.296.353 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.296.354 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.352.254 I main: llama threadpool init, n_threads = 4
0.01.352.300 I 
0.01.352.317 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.352.317 I 
0.01.352.496 I sampler seed: 1234
0.01.352.500 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.352.539 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.352.542 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.352.542 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.458.678 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51263.54 tokens per second)
0.02.458.678 I llama_perf_context_print:        load time =    1341.22 ms
0.02.458.679 I llama_perf_context_print: prompt eval time =      48.81 ms /     7 tokens (    6.97 ms per token,   143.42 tokens per second)
0.02.458.680 I llama_perf_context_print:        eval time =    1054.53 ms /    63 runs   (   16.74 ms per token,    59.74 tokens per second)
0.02.458.681 I llama_perf_context_print:       total time =    1107.11 ms /    70 tokens
0.02.458.934 I ggml_metal_free: deallocating

real	0m2.477s
user	0m0.112s
sys	0m0.269s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.012.582 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.125 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.022.131 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.133 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.134 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.134 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.135 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.135 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.137 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.137 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.137 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.138 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.138 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.138 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.139 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.141 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.141 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.141 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.151 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.222 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.088 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.089 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.090 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.090 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.090 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.091 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.031.091 I llama_model_loader: - type  f32:  194 tensors
0.00.031.092 I llama_model_loader: - type q4_0:   97 tensors
0.00.031.092 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.093 I print_info: file format = GGUF V3 (latest)
0.00.031.093 I print_info: file type   = Q4_0
0.00.031.094 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.039.303 I load: special tokens cache size = 25
0.00.045.441 I load: token to piece cache size = 0.2984 MB
0.00.045.444 I print_info: arch             = gptneox
0.00.045.445 I print_info: vocab_only       = 0
0.00.045.445 I print_info: n_ctx_train      = 2048
0.00.045.445 I print_info: n_embd           = 2048
0.00.045.445 I print_info: n_layer          = 24
0.00.045.450 I print_info: n_head           = 16
0.00.045.451 I print_info: n_head_kv        = 16
0.00.045.451 I print_info: n_rot            = 32
0.00.045.452 I print_info: n_swa            = 0
0.00.045.452 I print_info: n_embd_head_k    = 128
0.00.045.452 I print_info: n_embd_head_v    = 128
0.00.045.453 I print_info: n_gqa            = 1
0.00.045.454 I print_info: n_embd_k_gqa     = 2048
0.00.045.454 I print_info: n_embd_v_gqa     = 2048
0.00.045.455 I print_info: f_norm_eps       = 1.0e-05
0.00.045.459 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.459 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.459 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.459 I print_info: f_logit_scale    = 0.0e+00
0.00.045.460 I print_info: n_ff             = 8192
0.00.045.460 I print_info: n_expert         = 0
0.00.045.461 I print_info: n_expert_used    = 0
0.00.045.461 I print_info: causal attn      = 1
0.00.045.461 I print_info: pooling type     = 0
0.00.045.461 I print_info: rope type        = 2
0.00.045.461 I print_info: rope scaling     = linear
0.00.045.462 I print_info: freq_base_train  = 10000.0
0.00.045.462 I print_info: freq_scale_train = 1
0.00.045.462 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.462 I print_info: rope_finetuned   = unknown
0.00.045.462 I print_info: ssm_d_conv       = 0
0.00.045.462 I print_info: ssm_d_inner      = 0
0.00.045.468 I print_info: ssm_d_state      = 0
0.00.045.470 I print_info: ssm_dt_rank      = 0
0.00.045.470 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.470 I print_info: model type       = 1.4B
0.00.045.471 I print_info: model params     = 1.41 B
0.00.045.471 I print_info: general.name     = 1.4B
0.00.045.472 I print_info: vocab type       = BPE
0.00.045.472 I print_info: n_vocab          = 50304
0.00.045.472 I print_info: n_merges         = 50009
0.00.045.472 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.472 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.473 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.473 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.473 I print_info: LF token         = 187 'Ċ'
0.00.045.476 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.476 I print_info: max token length = 1024
0.00.045.476 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.626.600 I load_tensors: offloading 24 repeating layers to GPU
0.00.626.617 I load_tensors: offloading output layer to GPU
0.00.626.618 I load_tensors: offloaded 25/25 layers to GPU
0.00.626.651 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.626.653 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.628.288 I llama_init_from_model: n_seq_max     = 1
0.00.628.291 I llama_init_from_model: n_ctx         = 2048
0.00.628.292 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.628.292 I llama_init_from_model: n_batch       = 2048
0.00.628.293 I llama_init_from_model: n_ubatch      = 512
0.00.628.293 I llama_init_from_model: flash_attn    = 0
0.00.628.296 I llama_init_from_model: freq_base     = 10000.0
0.00.628.296 I llama_init_from_model: freq_scale    = 1
0.00.628.300 I ggml_metal_init: allocating
0.00.628.379 I ggml_metal_init: found device: Apple M4
0.00.628.392 I ggml_metal_init: picking default device: Apple M4
0.00.630.261 I ggml_metal_init: using embedded metal library
0.00.636.520 I ggml_metal_init: GPU name:   Apple M4
0.00.636.525 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.636.526 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.636.527 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.636.528 I ggml_metal_init: simdgroup reduction   = true
0.00.636.528 I ggml_metal_init: simdgroup matrix mul. = true
0.00.636.528 I ggml_metal_init: has residency sets    = true
0.00.636.529 I ggml_metal_init: has bfloat            = true
0.00.636.529 I ggml_metal_init: use bfloat            = true
0.00.636.530 I ggml_metal_init: hasUnifiedMemory      = true
0.00.636.531 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.654.882 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.710.121 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.710.130 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.710.169 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.715.145 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.715.148 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.715.148 I llama_init_from_model: graph nodes  = 967
0.00.715.149 I llama_init_from_model: graph splits = 2
0.00.715.154 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.715.279 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.715.279 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.771.438 I main: llama threadpool init, n_threads = 4
0.00.771.482 I 
0.00.771.507 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.771.507 I 
0.00.771.674 I sampler seed: 1234
0.00.771.678 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.771.699 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.771.699 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.771.699 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.458.813 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52709.73 tokens per second)
0.01.458.813 I llama_perf_context_print:        load time =     758.16 ms
0.01.458.814 I llama_perf_context_print: prompt eval time =      48.96 ms /     7 tokens (    6.99 ms per token,   142.98 tokens per second)
0.01.458.815 I llama_perf_context_print:        eval time =     635.27 ms /    63 runs   (   10.08 ms per token,    99.17 tokens per second)
0.01.458.819 I llama_perf_context_print:       total time =     688.07 ms /    70 tokens
0.01.459.104 I ggml_metal_free: deallocating

real	0m1.478s
user	0m0.109s
sys	0m0.200s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.802 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.950 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.955 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.957 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.957 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.959 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.959 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.960 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.961 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.961 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.961 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.962 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.962 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.962 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.963 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.966 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.967 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.967 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.851 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.910 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.809 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.811 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.811 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.811 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.812 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.812 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.812 I llama_model_loader: - type  f32:  194 tensors
0.00.025.813 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.813 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.814 I print_info: file format = GGUF V3 (latest)
0.00.025.814 I print_info: file type   = Q4_1
0.00.025.815 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.705 I load: special tokens cache size = 25
0.00.039.812 I load: token to piece cache size = 0.2984 MB
0.00.039.815 I print_info: arch             = gptneox
0.00.039.816 I print_info: vocab_only       = 0
0.00.039.816 I print_info: n_ctx_train      = 2048
0.00.039.816 I print_info: n_embd           = 2048
0.00.039.816 I print_info: n_layer          = 24
0.00.039.819 I print_info: n_head           = 16
0.00.039.820 I print_info: n_head_kv        = 16
0.00.039.820 I print_info: n_rot            = 32
0.00.039.820 I print_info: n_swa            = 0
0.00.039.820 I print_info: n_embd_head_k    = 128
0.00.039.820 I print_info: n_embd_head_v    = 128
0.00.039.821 I print_info: n_gqa            = 1
0.00.039.822 I print_info: n_embd_k_gqa     = 2048
0.00.039.822 I print_info: n_embd_v_gqa     = 2048
0.00.039.823 I print_info: f_norm_eps       = 1.0e-05
0.00.039.825 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.826 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.826 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.826 I print_info: f_logit_scale    = 0.0e+00
0.00.039.826 I print_info: n_ff             = 8192
0.00.039.827 I print_info: n_expert         = 0
0.00.039.827 I print_info: n_expert_used    = 0
0.00.039.827 I print_info: causal attn      = 1
0.00.039.827 I print_info: pooling type     = 0
0.00.039.829 I print_info: rope type        = 2
0.00.039.830 I print_info: rope scaling     = linear
0.00.039.830 I print_info: freq_base_train  = 10000.0
0.00.039.830 I print_info: freq_scale_train = 1
0.00.039.831 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.831 I print_info: rope_finetuned   = unknown
0.00.039.831 I print_info: ssm_d_conv       = 0
0.00.039.831 I print_info: ssm_d_inner      = 0
0.00.039.831 I print_info: ssm_d_state      = 0
0.00.039.831 I print_info: ssm_dt_rank      = 0
0.00.039.831 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.832 I print_info: model type       = 1.4B
0.00.039.832 I print_info: model params     = 1.41 B
0.00.039.832 I print_info: general.name     = 1.4B
0.00.039.833 I print_info: vocab type       = BPE
0.00.039.833 I print_info: n_vocab          = 50304
0.00.039.833 I print_info: n_merges         = 50009
0.00.039.833 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.833 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.837 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.838 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.840 I print_info: LF token         = 187 'Ċ'
0.00.039.840 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.840 I print_info: max token length = 1024
0.00.039.840 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.626.820 I load_tensors: offloading 24 repeating layers to GPU
0.00.626.832 I load_tensors: offloading output layer to GPU
0.00.626.833 I load_tensors: offloaded 25/25 layers to GPU
0.00.626.866 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.626.868 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.628.392 I llama_init_from_model: n_seq_max     = 1
0.00.628.395 I llama_init_from_model: n_ctx         = 2048
0.00.628.396 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.628.396 I llama_init_from_model: n_batch       = 2048
0.00.628.397 I llama_init_from_model: n_ubatch      = 512
0.00.628.397 I llama_init_from_model: flash_attn    = 0
0.00.628.399 I llama_init_from_model: freq_base     = 10000.0
0.00.628.400 I llama_init_from_model: freq_scale    = 1
0.00.628.404 I ggml_metal_init: allocating
0.00.628.476 I ggml_metal_init: found device: Apple M4
0.00.628.489 I ggml_metal_init: picking default device: Apple M4
0.00.630.406 I ggml_metal_init: using embedded metal library
0.00.637.073 I ggml_metal_init: GPU name:   Apple M4
0.00.637.077 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.637.078 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.637.079 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.637.080 I ggml_metal_init: simdgroup reduction   = true
0.00.637.080 I ggml_metal_init: simdgroup matrix mul. = true
0.00.637.080 I ggml_metal_init: has residency sets    = true
0.00.637.080 I ggml_metal_init: has bfloat            = true
0.00.637.081 I ggml_metal_init: use bfloat            = true
0.00.637.081 I ggml_metal_init: hasUnifiedMemory      = true
0.00.637.082 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.654.973 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.708.146 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.708.153 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.708.187 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.712.198 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.712.200 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.712.200 I llama_init_from_model: graph nodes  = 967
0.00.712.201 I llama_init_from_model: graph splits = 2
0.00.712.211 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.712.346 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.712.346 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.766.158 I main: llama threadpool init, n_threads = 4
0.00.766.202 I 
0.00.766.218 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.766.219 I 
0.00.766.380 I sampler seed: 1234
0.00.766.385 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.766.395 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.766.396 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.766.397 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.493.652 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57443.37 tokens per second)
0.01.493.653 I llama_perf_context_print:        load time =     756.66 ms
0.01.493.654 I llama_perf_context_print: prompt eval time =      49.13 ms /     7 tokens (    7.02 ms per token,   142.46 tokens per second)
0.01.493.655 I llama_perf_context_print:        eval time =     675.38 ms /    63 runs   (   10.72 ms per token,    93.28 tokens per second)
0.01.493.655 I llama_perf_context_print:       total time =     728.18 ms /    70 tokens
0.01.493.887 I ggml_metal_free: deallocating

real	0m1.510s
user	0m0.110s
sys	0m0.192s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.314 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.039 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.044 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.046 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.046 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.047 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.047 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.047 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.048 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.049 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.049 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.049 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.050 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.050 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.051 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.052 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.052 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.053 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.879 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.907 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.715 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.716 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.716 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.717 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.717 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.717 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.718 I llama_model_loader: - type  f32:  194 tensors
0.00.025.718 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.719 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.719 I print_info: file format = GGUF V3 (latest)
0.00.025.720 I print_info: file type   = Q5_0
0.00.025.720 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.890 I load: special tokens cache size = 25
0.00.039.877 I load: token to piece cache size = 0.2984 MB
0.00.039.879 I print_info: arch             = gptneox
0.00.039.880 I print_info: vocab_only       = 0
0.00.039.880 I print_info: n_ctx_train      = 2048
0.00.039.880 I print_info: n_embd           = 2048
0.00.039.880 I print_info: n_layer          = 24
0.00.039.883 I print_info: n_head           = 16
0.00.039.883 I print_info: n_head_kv        = 16
0.00.039.884 I print_info: n_rot            = 32
0.00.039.884 I print_info: n_swa            = 0
0.00.039.884 I print_info: n_embd_head_k    = 128
0.00.039.884 I print_info: n_embd_head_v    = 128
0.00.039.886 I print_info: n_gqa            = 1
0.00.039.887 I print_info: n_embd_k_gqa     = 2048
0.00.039.887 I print_info: n_embd_v_gqa     = 2048
0.00.039.888 I print_info: f_norm_eps       = 1.0e-05
0.00.039.888 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.888 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.889 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.889 I print_info: f_logit_scale    = 0.0e+00
0.00.039.890 I print_info: n_ff             = 8192
0.00.039.894 I print_info: n_expert         = 0
0.00.039.896 I print_info: n_expert_used    = 0
0.00.039.896 I print_info: causal attn      = 1
0.00.039.897 I print_info: pooling type     = 0
0.00.039.897 I print_info: rope type        = 2
0.00.039.897 I print_info: rope scaling     = linear
0.00.039.897 I print_info: freq_base_train  = 10000.0
0.00.039.898 I print_info: freq_scale_train = 1
0.00.039.898 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.898 I print_info: rope_finetuned   = unknown
0.00.039.898 I print_info: ssm_d_conv       = 0
0.00.039.898 I print_info: ssm_d_inner      = 0
0.00.039.899 I print_info: ssm_d_state      = 0
0.00.039.899 I print_info: ssm_dt_rank      = 0
0.00.039.899 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.899 I print_info: model type       = 1.4B
0.00.039.900 I print_info: model params     = 1.41 B
0.00.039.900 I print_info: general.name     = 1.4B
0.00.039.900 I print_info: vocab type       = BPE
0.00.039.900 I print_info: n_vocab          = 50304
0.00.039.901 I print_info: n_merges         = 50009
0.00.039.901 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.901 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.901 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.901 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.902 I print_info: LF token         = 187 'Ċ'
0.00.039.902 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.902 I print_info: max token length = 1024
0.00.039.903 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.725.956 I load_tensors: offloading 24 repeating layers to GPU
0.00.725.970 I load_tensors: offloading output layer to GPU
0.00.725.971 I load_tensors: offloaded 25/25 layers to GPU
0.00.726.007 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.726.008 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.727.626 I llama_init_from_model: n_seq_max     = 1
0.00.727.629 I llama_init_from_model: n_ctx         = 2048
0.00.727.630 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.727.631 I llama_init_from_model: n_batch       = 2048
0.00.727.631 I llama_init_from_model: n_ubatch      = 512
0.00.727.631 I llama_init_from_model: flash_attn    = 0
0.00.727.634 I llama_init_from_model: freq_base     = 10000.0
0.00.727.634 I llama_init_from_model: freq_scale    = 1
0.00.727.636 I ggml_metal_init: allocating
0.00.727.759 I ggml_metal_init: found device: Apple M4
0.00.727.772 I ggml_metal_init: picking default device: Apple M4
0.00.729.713 I ggml_metal_init: using embedded metal library
0.00.736.439 I ggml_metal_init: GPU name:   Apple M4
0.00.736.444 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.736.444 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.736.445 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.736.445 I ggml_metal_init: simdgroup reduction   = true
0.00.736.446 I ggml_metal_init: simdgroup matrix mul. = true
0.00.736.446 I ggml_metal_init: has residency sets    = true
0.00.736.446 I ggml_metal_init: has bfloat            = true
0.00.736.447 I ggml_metal_init: use bfloat            = true
0.00.736.447 I ggml_metal_init: hasUnifiedMemory      = true
0.00.736.449 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.755.421 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.810.981 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.810.989 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.811.027 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.816.815 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.816.818 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.816.818 I llama_init_from_model: graph nodes  = 967
0.00.816.818 I llama_init_from_model: graph splits = 2
0.00.816.825 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.816.949 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.816.949 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.874.500 I main: llama threadpool init, n_threads = 4
0.00.874.546 I 
0.00.874.561 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.874.561 I 
0.00.874.743 I sampler seed: 1234
0.00.874.748 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.874.774 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.874.775 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.874.775 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.653.414 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51115.91 tokens per second)
0.01.653.414 I llama_perf_context_print:        load time =     864.48 ms
0.01.653.418 I llama_perf_context_print: prompt eval time =      43.13 ms /     7 tokens (    6.16 ms per token,   162.28 tokens per second)
0.01.653.419 I llama_perf_context_print:        eval time =     732.57 ms /    63 runs   (   11.63 ms per token,    86.00 tokens per second)
0.01.653.419 I llama_perf_context_print:       total time =     779.61 ms /    70 tokens
0.01.653.640 I ggml_metal_free: deallocating

real	0m1.671s
user	0m0.110s
sys	0m0.231s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.778 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.209 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.213 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.215 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.215 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.216 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.220 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.220 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.222 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.223 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.223 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.223 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.224 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.224 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.227 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.230 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.231 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.231 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.070 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.039 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.801 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.803 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.803 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.803 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.803 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.804 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.804 I llama_model_loader: - type  f32:  194 tensors
0.00.025.804 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.804 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.805 I print_info: file format = GGUF V3 (latest)
0.00.025.805 I print_info: file type   = Q5_1
0.00.025.806 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.650 I load: special tokens cache size = 25
0.00.039.603 I load: token to piece cache size = 0.2984 MB
0.00.039.606 I print_info: arch             = gptneox
0.00.039.606 I print_info: vocab_only       = 0
0.00.039.606 I print_info: n_ctx_train      = 2048
0.00.039.607 I print_info: n_embd           = 2048
0.00.039.607 I print_info: n_layer          = 24
0.00.039.609 I print_info: n_head           = 16
0.00.039.610 I print_info: n_head_kv        = 16
0.00.039.610 I print_info: n_rot            = 32
0.00.039.611 I print_info: n_swa            = 0
0.00.039.612 I print_info: n_embd_head_k    = 128
0.00.039.612 I print_info: n_embd_head_v    = 128
0.00.039.613 I print_info: n_gqa            = 1
0.00.039.613 I print_info: n_embd_k_gqa     = 2048
0.00.039.614 I print_info: n_embd_v_gqa     = 2048
0.00.039.615 I print_info: f_norm_eps       = 1.0e-05
0.00.039.615 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.615 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.615 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.617 I print_info: f_logit_scale    = 0.0e+00
0.00.039.618 I print_info: n_ff             = 8192
0.00.039.618 I print_info: n_expert         = 0
0.00.039.618 I print_info: n_expert_used    = 0
0.00.039.618 I print_info: causal attn      = 1
0.00.039.618 I print_info: pooling type     = 0
0.00.039.620 I print_info: rope type        = 2
0.00.039.621 I print_info: rope scaling     = linear
0.00.039.621 I print_info: freq_base_train  = 10000.0
0.00.039.622 I print_info: freq_scale_train = 1
0.00.039.622 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.622 I print_info: rope_finetuned   = unknown
0.00.039.622 I print_info: ssm_d_conv       = 0
0.00.039.622 I print_info: ssm_d_inner      = 0
0.00.039.623 I print_info: ssm_d_state      = 0
0.00.039.623 I print_info: ssm_dt_rank      = 0
0.00.039.623 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.623 I print_info: model type       = 1.4B
0.00.039.623 I print_info: model params     = 1.41 B
0.00.039.623 I print_info: general.name     = 1.4B
0.00.039.624 I print_info: vocab type       = BPE
0.00.039.628 I print_info: n_vocab          = 50304
0.00.039.628 I print_info: n_merges         = 50009
0.00.039.628 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.628 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.628 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.628 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.629 I print_info: LF token         = 187 'Ċ'
0.00.039.629 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.629 I print_info: max token length = 1024
0.00.039.629 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.625.283 I load_tensors: offloading 24 repeating layers to GPU
0.00.625.298 I load_tensors: offloading output layer to GPU
0.00.625.298 I load_tensors: offloaded 25/25 layers to GPU
0.00.625.331 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.625.332 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.626.840 I llama_init_from_model: n_seq_max     = 1
0.00.626.843 I llama_init_from_model: n_ctx         = 2048
0.00.626.844 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.626.844 I llama_init_from_model: n_batch       = 2048
0.00.626.844 I llama_init_from_model: n_ubatch      = 512
0.00.626.845 I llama_init_from_model: flash_attn    = 0
0.00.626.847 I llama_init_from_model: freq_base     = 10000.0
0.00.626.848 I llama_init_from_model: freq_scale    = 1
0.00.626.858 I ggml_metal_init: allocating
0.00.626.938 I ggml_metal_init: found device: Apple M4
0.00.626.951 I ggml_metal_init: picking default device: Apple M4
0.00.628.858 I ggml_metal_init: using embedded metal library
0.00.635.459 I ggml_metal_init: GPU name:   Apple M4
0.00.635.463 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.635.464 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.635.464 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.635.465 I ggml_metal_init: simdgroup reduction   = true
0.00.635.465 I ggml_metal_init: simdgroup matrix mul. = true
0.00.635.466 I ggml_metal_init: has residency sets    = true
0.00.635.466 I ggml_metal_init: has bfloat            = true
0.00.635.466 I ggml_metal_init: use bfloat            = true
0.00.635.467 I ggml_metal_init: hasUnifiedMemory      = true
0.00.635.468 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.653.195 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.708.543 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.708.551 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.708.604 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.713.024 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.713.026 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.713.026 I llama_init_from_model: graph nodes  = 967
0.00.713.026 I llama_init_from_model: graph splits = 2
0.00.713.033 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.713.161 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.713.162 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.960 I main: llama threadpool init, n_threads = 4
0.00.769.002 I 
0.00.769.019 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.020 I 
0.00.769.171 I sampler seed: 1234
0.00.769.175 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.187 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.187 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.187 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.598.376 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54364.47 tokens per second)
0.01.598.376 I llama_perf_context_print:        load time =     758.49 ms
0.01.598.377 I llama_perf_context_print: prompt eval time =      42.19 ms /     7 tokens (    6.03 ms per token,   165.92 tokens per second)
0.01.598.378 I llama_perf_context_print:        eval time =     784.10 ms /    63 runs   (   12.45 ms per token,    80.35 tokens per second)
0.01.598.378 I llama_perf_context_print:       total time =     830.10 ms /    70 tokens
0.01.598.596 I ggml_metal_free: deallocating

real	0m1.617s
user	0m0.108s
sys	0m0.227s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.926 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.584 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.590 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.591 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.592 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.592 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.593 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.593 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.595 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.595 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.596 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.596 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.596 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.597 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.597 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.599 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.599 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.599 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.605 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.678 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.562 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.563 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.564 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.564 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.564 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.565 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.565 I llama_model_loader: - type  f32:  194 tensors
0.00.024.566 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.566 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.566 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.567 I print_info: file format = GGUF V3 (latest)
0.00.024.567 I print_info: file type   = Q2_K - Medium
0.00.024.568 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.710 I load: special tokens cache size = 25
0.00.038.813 I load: token to piece cache size = 0.2984 MB
0.00.038.816 I print_info: arch             = gptneox
0.00.038.816 I print_info: vocab_only       = 0
0.00.038.816 I print_info: n_ctx_train      = 2048
0.00.038.817 I print_info: n_embd           = 2048
0.00.038.817 I print_info: n_layer          = 24
0.00.038.819 I print_info: n_head           = 16
0.00.038.820 I print_info: n_head_kv        = 16
0.00.038.820 I print_info: n_rot            = 32
0.00.038.820 I print_info: n_swa            = 0
0.00.038.821 I print_info: n_embd_head_k    = 128
0.00.038.821 I print_info: n_embd_head_v    = 128
0.00.038.822 I print_info: n_gqa            = 1
0.00.038.822 I print_info: n_embd_k_gqa     = 2048
0.00.038.824 I print_info: n_embd_v_gqa     = 2048
0.00.038.825 I print_info: f_norm_eps       = 1.0e-05
0.00.038.825 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.825 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.827 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.828 I print_info: f_logit_scale    = 0.0e+00
0.00.038.828 I print_info: n_ff             = 8192
0.00.038.828 I print_info: n_expert         = 0
0.00.038.828 I print_info: n_expert_used    = 0
0.00.038.829 I print_info: causal attn      = 1
0.00.038.829 I print_info: pooling type     = 0
0.00.038.829 I print_info: rope type        = 2
0.00.038.829 I print_info: rope scaling     = linear
0.00.038.830 I print_info: freq_base_train  = 10000.0
0.00.038.830 I print_info: freq_scale_train = 1
0.00.038.830 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.831 I print_info: rope_finetuned   = unknown
0.00.038.831 I print_info: ssm_d_conv       = 0
0.00.038.831 I print_info: ssm_d_inner      = 0
0.00.038.831 I print_info: ssm_d_state      = 0
0.00.038.831 I print_info: ssm_dt_rank      = 0
0.00.038.831 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.831 I print_info: model type       = 1.4B
0.00.038.832 I print_info: model params     = 1.41 B
0.00.038.832 I print_info: general.name     = 1.4B
0.00.038.833 I print_info: vocab type       = BPE
0.00.038.833 I print_info: n_vocab          = 50304
0.00.038.833 I print_info: n_merges         = 50009
0.00.038.833 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.834 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.834 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.834 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.834 I print_info: LF token         = 187 'Ċ'
0.00.038.834 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.835 I print_info: max token length = 1024
0.00.038.835 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.349.471 I load_tensors: offloading 24 repeating layers to GPU
0.00.349.487 I load_tensors: offloading output layer to GPU
0.00.349.488 I load_tensors: offloaded 25/25 layers to GPU
0.00.349.536 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.349.541 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.351.250 I llama_init_from_model: n_seq_max     = 1
0.00.351.257 I llama_init_from_model: n_ctx         = 2048
0.00.351.257 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.351.258 I llama_init_from_model: n_batch       = 2048
0.00.351.258 I llama_init_from_model: n_ubatch      = 512
0.00.351.259 I llama_init_from_model: flash_attn    = 0
0.00.351.261 I llama_init_from_model: freq_base     = 10000.0
0.00.351.261 I llama_init_from_model: freq_scale    = 1
0.00.351.263 I ggml_metal_init: allocating
0.00.351.372 I ggml_metal_init: found device: Apple M4
0.00.351.387 I ggml_metal_init: picking default device: Apple M4
0.00.353.314 I ggml_metal_init: using embedded metal library
0.00.359.039 I ggml_metal_init: GPU name:   Apple M4
0.00.359.051 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.359.052 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.359.053 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.359.053 I ggml_metal_init: simdgroup reduction   = true
0.00.359.054 I ggml_metal_init: simdgroup matrix mul. = true
0.00.359.054 I ggml_metal_init: has residency sets    = true
0.00.359.054 I ggml_metal_init: has bfloat            = true
0.00.359.054 I ggml_metal_init: use bfloat            = true
0.00.359.058 I ggml_metal_init: hasUnifiedMemory      = true
0.00.359.063 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.380.249 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.436.831 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.436.839 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.436.877 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.440.925 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.440.926 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.440.927 I llama_init_from_model: graph nodes  = 967
0.00.440.927 I llama_init_from_model: graph splits = 2
0.00.440.933 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.441.043 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.441.043 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.500.789 I main: llama threadpool init, n_threads = 4
0.00.500.834 I 
0.00.500.849 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.500.849 I 
0.00.501.014 I sampler seed: 1234
0.00.501.020 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.501.064 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.501.067 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.501.067 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.183.722 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52748.89 tokens per second)
0.01.183.722 I llama_perf_context_print:        load time =     491.15 ms
0.01.183.723 I llama_perf_context_print: prompt eval time =      44.46 ms /     7 tokens (    6.35 ms per token,   157.44 tokens per second)
0.01.183.724 I llama_perf_context_print:        eval time =     635.37 ms /    63 runs   (   10.09 ms per token,    99.15 tokens per second)
0.01.183.724 I llama_perf_context_print:       total time =     683.64 ms /    70 tokens
0.01.183.951 I ggml_metal_free: deallocating

real	0m1.200s
user	0m0.112s
sys	0m0.170s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.008.706 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.125 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.130 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.131 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.132 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.133 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.133 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.134 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.135 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.135 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.135 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.136 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.136 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.136 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.137 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.139 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.140 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.140 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.950 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.004 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.815 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.816 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.816 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.816 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.817 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.817 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.818 I llama_model_loader: - type  f32:  194 tensors
0.00.024.818 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.818 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.818 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.818 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.819 I print_info: file format = GGUF V3 (latest)
0.00.024.820 I print_info: file type   = Q3_K - Medium
0.00.024.820 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.639 I load: special tokens cache size = 25
0.00.038.690 I load: token to piece cache size = 0.2984 MB
0.00.038.693 I print_info: arch             = gptneox
0.00.038.693 I print_info: vocab_only       = 0
0.00.038.693 I print_info: n_ctx_train      = 2048
0.00.038.693 I print_info: n_embd           = 2048
0.00.038.693 I print_info: n_layer          = 24
0.00.038.696 I print_info: n_head           = 16
0.00.038.697 I print_info: n_head_kv        = 16
0.00.038.697 I print_info: n_rot            = 32
0.00.038.698 I print_info: n_swa            = 0
0.00.038.698 I print_info: n_embd_head_k    = 128
0.00.038.698 I print_info: n_embd_head_v    = 128
0.00.038.699 I print_info: n_gqa            = 1
0.00.038.699 I print_info: n_embd_k_gqa     = 2048
0.00.038.700 I print_info: n_embd_v_gqa     = 2048
0.00.038.701 I print_info: f_norm_eps       = 1.0e-05
0.00.038.701 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.701 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.701 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.702 I print_info: f_logit_scale    = 0.0e+00
0.00.038.702 I print_info: n_ff             = 8192
0.00.038.702 I print_info: n_expert         = 0
0.00.038.703 I print_info: n_expert_used    = 0
0.00.038.704 I print_info: causal attn      = 1
0.00.038.706 I print_info: pooling type     = 0
0.00.038.706 I print_info: rope type        = 2
0.00.038.706 I print_info: rope scaling     = linear
0.00.038.706 I print_info: freq_base_train  = 10000.0
0.00.038.707 I print_info: freq_scale_train = 1
0.00.038.707 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.707 I print_info: rope_finetuned   = unknown
0.00.038.707 I print_info: ssm_d_conv       = 0
0.00.038.707 I print_info: ssm_d_inner      = 0
0.00.038.708 I print_info: ssm_d_state      = 0
0.00.038.708 I print_info: ssm_dt_rank      = 0
0.00.038.708 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.709 I print_info: model type       = 1.4B
0.00.038.710 I print_info: model params     = 1.41 B
0.00.038.710 I print_info: general.name     = 1.4B
0.00.038.710 I print_info: vocab type       = BPE
0.00.038.711 I print_info: n_vocab          = 50304
0.00.038.711 I print_info: n_merges         = 50009
0.00.038.711 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.711 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.711 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.712 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.712 I print_info: LF token         = 187 'Ċ'
0.00.038.712 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.712 I print_info: max token length = 1024
0.00.038.713 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.446.946 I load_tensors: offloading 24 repeating layers to GPU
0.00.446.963 I load_tensors: offloading output layer to GPU
0.00.446.964 I load_tensors: offloaded 25/25 layers to GPU
0.00.446.994 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.446.999 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.448.700 I llama_init_from_model: n_seq_max     = 1
0.00.448.707 I llama_init_from_model: n_ctx         = 2048
0.00.448.708 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.448.708 I llama_init_from_model: n_batch       = 2048
0.00.448.709 I llama_init_from_model: n_ubatch      = 512
0.00.448.709 I llama_init_from_model: flash_attn    = 0
0.00.448.711 I llama_init_from_model: freq_base     = 10000.0
0.00.448.711 I llama_init_from_model: freq_scale    = 1
0.00.448.713 I ggml_metal_init: allocating
0.00.448.786 I ggml_metal_init: found device: Apple M4
0.00.448.799 I ggml_metal_init: picking default device: Apple M4
0.00.450.652 I ggml_metal_init: using embedded metal library
0.00.456.681 I ggml_metal_init: GPU name:   Apple M4
0.00.456.687 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.456.688 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.456.688 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.456.689 I ggml_metal_init: simdgroup reduction   = true
0.00.456.690 I ggml_metal_init: simdgroup matrix mul. = true
0.00.456.690 I ggml_metal_init: has residency sets    = true
0.00.456.690 I ggml_metal_init: has bfloat            = true
0.00.456.691 I ggml_metal_init: use bfloat            = true
0.00.456.692 I ggml_metal_init: hasUnifiedMemory      = true
0.00.456.699 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.475.900 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.536.730 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.536.737 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.536.779 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.541.449 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.541.452 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.541.452 I llama_init_from_model: graph nodes  = 967
0.00.541.452 I llama_init_from_model: graph splits = 2
0.00.541.457 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.541.594 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.541.595 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.597.339 I main: llama threadpool init, n_threads = 4
0.00.597.385 I 
0.00.597.402 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.597.402 I 
0.00.597.563 I sampler seed: 1234
0.00.597.569 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.597.636 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.597.641 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.597.641 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.349.109 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50247.70 tokens per second)
0.01.349.110 I llama_perf_context_print:        load time =     587.93 ms
0.01.349.110 I llama_perf_context_print: prompt eval time =      49.81 ms /     7 tokens (    7.12 ms per token,   140.54 tokens per second)
0.01.349.111 I llama_perf_context_print:        eval time =     698.86 ms /    63 runs   (   11.09 ms per token,    90.15 tokens per second)
0.01.349.111 I llama_perf_context_print:       total time =     752.47 ms /    70 tokens
0.01.349.391 I ggml_metal_free: deallocating

real	0m1.365s
user	0m0.110s
sys	0m0.184s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.485 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.682 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.688 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.689 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.689 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.690 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.690 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.690 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.691 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.691 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.692 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.692 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.692 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.692 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.693 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.694 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.695 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.695 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.505 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.544 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.529 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.530 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.531 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.531 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.531 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.532 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.532 I llama_model_loader: - type  f32:  194 tensors
0.00.026.532 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.533 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.533 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.533 I print_info: file format = GGUF V3 (latest)
0.00.026.534 I print_info: file type   = Q4_K - Medium
0.00.026.535 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.456 I load: special tokens cache size = 25
0.00.040.766 I load: token to piece cache size = 0.2984 MB
0.00.040.769 I print_info: arch             = gptneox
0.00.040.769 I print_info: vocab_only       = 0
0.00.040.769 I print_info: n_ctx_train      = 2048
0.00.040.769 I print_info: n_embd           = 2048
0.00.040.769 I print_info: n_layer          = 24
0.00.040.772 I print_info: n_head           = 16
0.00.040.773 I print_info: n_head_kv        = 16
0.00.040.773 I print_info: n_rot            = 32
0.00.040.773 I print_info: n_swa            = 0
0.00.040.774 I print_info: n_embd_head_k    = 128
0.00.040.774 I print_info: n_embd_head_v    = 128
0.00.040.774 I print_info: n_gqa            = 1
0.00.040.775 I print_info: n_embd_k_gqa     = 2048
0.00.040.776 I print_info: n_embd_v_gqa     = 2048
0.00.040.776 I print_info: f_norm_eps       = 1.0e-05
0.00.040.777 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.777 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.777 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.777 I print_info: f_logit_scale    = 0.0e+00
0.00.040.778 I print_info: n_ff             = 8192
0.00.040.778 I print_info: n_expert         = 0
0.00.040.778 I print_info: n_expert_used    = 0
0.00.040.779 I print_info: causal attn      = 1
0.00.040.779 I print_info: pooling type     = 0
0.00.040.781 I print_info: rope type        = 2
0.00.040.781 I print_info: rope scaling     = linear
0.00.040.781 I print_info: freq_base_train  = 10000.0
0.00.040.782 I print_info: freq_scale_train = 1
0.00.040.782 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.782 I print_info: rope_finetuned   = unknown
0.00.040.782 I print_info: ssm_d_conv       = 0
0.00.040.782 I print_info: ssm_d_inner      = 0
0.00.040.783 I print_info: ssm_d_state      = 0
0.00.040.783 I print_info: ssm_dt_rank      = 0
0.00.040.783 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.783 I print_info: model type       = 1.4B
0.00.040.783 I print_info: model params     = 1.41 B
0.00.040.783 I print_info: general.name     = 1.4B
0.00.040.784 I print_info: vocab type       = BPE
0.00.040.784 I print_info: n_vocab          = 50304
0.00.040.785 I print_info: n_merges         = 50009
0.00.040.786 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.786 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.786 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.787 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.787 I print_info: LF token         = 187 'Ċ'
0.00.040.788 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.788 I print_info: max token length = 1024
0.00.040.788 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.519.335 I load_tensors: offloading 24 repeating layers to GPU
0.00.519.339 I load_tensors: offloading output layer to GPU
0.00.519.340 I load_tensors: offloaded 25/25 layers to GPU
0.00.519.357 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.519.358 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.520.193 I llama_init_from_model: n_seq_max     = 1
0.00.520.197 I llama_init_from_model: n_ctx         = 2048
0.00.520.197 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.520.197 I llama_init_from_model: n_batch       = 2048
0.00.520.198 I llama_init_from_model: n_ubatch      = 512
0.00.520.198 I llama_init_from_model: flash_attn    = 0
0.00.520.199 I llama_init_from_model: freq_base     = 10000.0
0.00.520.200 I llama_init_from_model: freq_scale    = 1
0.00.520.201 I ggml_metal_init: allocating
0.00.520.235 I ggml_metal_init: found device: Apple M4
0.00.520.246 I ggml_metal_init: picking default device: Apple M4
0.00.521.285 I ggml_metal_init: using embedded metal library
0.00.525.423 I ggml_metal_init: GPU name:   Apple M4
0.00.525.432 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.525.432 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.525.433 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.525.433 I ggml_metal_init: simdgroup reduction   = true
0.00.525.434 I ggml_metal_init: simdgroup matrix mul. = true
0.00.525.434 I ggml_metal_init: has residency sets    = true
0.00.525.434 I ggml_metal_init: has bfloat            = true
0.00.525.435 I ggml_metal_init: use bfloat            = true
0.00.525.436 I ggml_metal_init: hasUnifiedMemory      = true
0.00.525.438 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.539.822 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.570.354 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.570.361 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.570.398 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.574.473 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.574.475 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.574.475 I llama_init_from_model: graph nodes  = 967
0.00.574.475 I llama_init_from_model: graph splits = 2
0.00.574.481 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.574.603 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.574.603 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.630.093 I main: llama threadpool init, n_threads = 4
0.00.630.128 I 
0.00.630.141 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.630.141 I 
0.00.630.305 I sampler seed: 1234
0.00.630.309 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.630.328 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.630.329 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.630.329 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.398.901 I llama_perf_sampler_print:    sampling time =       1.50 ms /    71 runs   (    0.02 ms per token, 47207.45 tokens per second)
0.01.398.902 I llama_perf_context_print:        load time =     619.91 ms
0.01.398.903 I llama_perf_context_print: prompt eval time =      55.29 ms /     7 tokens (    7.90 ms per token,   126.60 tokens per second)
0.01.398.903 I llama_perf_context_print:        eval time =     710.77 ms /    63 runs   (   11.28 ms per token,    88.64 tokens per second)
0.01.398.904 I llama_perf_context_print:       total time =     769.50 ms /    70 tokens
0.01.399.144 I ggml_metal_free: deallocating

real	0m1.418s
user	0m0.104s
sys	0m0.143s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.009.198 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.940 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.946 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.948 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.948 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.949 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.949 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.949 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.950 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.951 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.951 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.951 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.952 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.952 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.953 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.955 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.955 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.955 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.959 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.195 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.317 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.319 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.319 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.320 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.320 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.320 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.321 I llama_model_loader: - type  f32:  194 tensors
0.00.025.321 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.322 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.322 I print_info: file format = GGUF V3 (latest)
0.00.025.323 I print_info: file type   = Q5_K - Medium
0.00.025.324 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.629 I load: special tokens cache size = 25
0.00.039.724 I load: token to piece cache size = 0.2984 MB
0.00.039.727 I print_info: arch             = gptneox
0.00.039.727 I print_info: vocab_only       = 0
0.00.039.727 I print_info: n_ctx_train      = 2048
0.00.039.728 I print_info: n_embd           = 2048
0.00.039.728 I print_info: n_layer          = 24
0.00.039.731 I print_info: n_head           = 16
0.00.039.732 I print_info: n_head_kv        = 16
0.00.039.732 I print_info: n_rot            = 32
0.00.039.732 I print_info: n_swa            = 0
0.00.039.733 I print_info: n_embd_head_k    = 128
0.00.039.733 I print_info: n_embd_head_v    = 128
0.00.039.734 I print_info: n_gqa            = 1
0.00.039.734 I print_info: n_embd_k_gqa     = 2048
0.00.039.738 I print_info: n_embd_v_gqa     = 2048
0.00.039.738 I print_info: f_norm_eps       = 1.0e-05
0.00.039.738 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.739 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.739 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.739 I print_info: f_logit_scale    = 0.0e+00
0.00.039.739 I print_info: n_ff             = 8192
0.00.039.740 I print_info: n_expert         = 0
0.00.039.741 I print_info: n_expert_used    = 0
0.00.039.741 I print_info: causal attn      = 1
0.00.039.741 I print_info: pooling type     = 0
0.00.039.741 I print_info: rope type        = 2
0.00.039.742 I print_info: rope scaling     = linear
0.00.039.745 I print_info: freq_base_train  = 10000.0
0.00.039.745 I print_info: freq_scale_train = 1
0.00.039.745 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.746 I print_info: rope_finetuned   = unknown
0.00.039.746 I print_info: ssm_d_conv       = 0
0.00.039.747 I print_info: ssm_d_inner      = 0
0.00.039.747 I print_info: ssm_d_state      = 0
0.00.039.747 I print_info: ssm_dt_rank      = 0
0.00.039.747 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.747 I print_info: model type       = 1.4B
0.00.039.748 I print_info: model params     = 1.41 B
0.00.039.748 I print_info: general.name     = 1.4B
0.00.039.748 I print_info: vocab type       = BPE
0.00.039.748 I print_info: n_vocab          = 50304
0.00.039.749 I print_info: n_merges         = 50009
0.00.039.749 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.749 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.749 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.750 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.750 I print_info: LF token         = 187 'Ċ'
0.00.039.750 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.751 I print_info: max token length = 1024
0.00.039.752 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.644.894 I load_tensors: offloading 24 repeating layers to GPU
0.00.644.902 I load_tensors: offloading output layer to GPU
0.00.644.903 I load_tensors: offloaded 25/25 layers to GPU
0.00.644.923 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.644.924 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.645.793 I llama_init_from_model: n_seq_max     = 1
0.00.645.797 I llama_init_from_model: n_ctx         = 2048
0.00.645.797 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.645.797 I llama_init_from_model: n_batch       = 2048
0.00.645.798 I llama_init_from_model: n_ubatch      = 512
0.00.645.798 I llama_init_from_model: flash_attn    = 0
0.00.645.800 I llama_init_from_model: freq_base     = 10000.0
0.00.645.800 I llama_init_from_model: freq_scale    = 1
0.00.645.801 I ggml_metal_init: allocating
0.00.645.854 I ggml_metal_init: found device: Apple M4
0.00.645.865 I ggml_metal_init: picking default device: Apple M4
0.00.646.967 I ggml_metal_init: using embedded metal library
0.00.651.155 I ggml_metal_init: GPU name:   Apple M4
0.00.651.160 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.651.161 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.651.161 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.651.162 I ggml_metal_init: simdgroup reduction   = true
0.00.651.162 I ggml_metal_init: simdgroup matrix mul. = true
0.00.651.163 I ggml_metal_init: has residency sets    = true
0.00.651.163 I ggml_metal_init: has bfloat            = true
0.00.651.163 I ggml_metal_init: use bfloat            = true
0.00.651.164 I ggml_metal_init: hasUnifiedMemory      = true
0.00.651.167 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.665.099 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.694.599 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.694.605 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.694.634 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.698.585 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.698.587 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.698.587 I llama_init_from_model: graph nodes  = 967
0.00.698.587 I llama_init_from_model: graph splits = 2
0.00.698.594 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.698.723 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.698.724 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.766.222 I main: llama threadpool init, n_threads = 4
0.00.766.265 I 
0.00.766.281 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.766.281 I 
0.00.766.465 I sampler seed: 1234
0.00.766.469 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.766.480 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.766.480 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.766.480 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.611.721 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55905.51 tokens per second)
0.01.611.722 I llama_perf_context_print:        load time =     756.32 ms
0.01.611.723 I llama_perf_context_print: prompt eval time =      51.58 ms /     7 tokens (    7.37 ms per token,   135.72 tokens per second)
0.01.611.723 I llama_perf_context_print:        eval time =     790.82 ms /    63 runs   (   12.55 ms per token,    79.66 tokens per second)
0.01.611.724 I llama_perf_context_print:       total time =     846.19 ms /    70 tokens
0.01.611.938 I ggml_metal_free: deallocating

real	0m1.630s
user	0m0.104s
sys	0m0.163s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.834 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.629 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.634 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.636 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.636 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.637 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.637 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.637 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.638 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.638 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.639 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.639 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.639 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.641 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.641 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.643 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.643 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.643 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.498 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.548 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.385 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.386 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.386 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.387 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.387 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.387 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.388 I llama_model_loader: - type  f32:  194 tensors
0.00.024.388 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.389 I print_info: file format = GGUF V3 (latest)
0.00.024.389 I print_info: file type   = Q6_K
0.00.024.390 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.552 I load: special tokens cache size = 25
0.00.038.405 I load: token to piece cache size = 0.2984 MB
0.00.038.408 I print_info: arch             = gptneox
0.00.038.408 I print_info: vocab_only       = 0
0.00.038.408 I print_info: n_ctx_train      = 2048
0.00.038.409 I print_info: n_embd           = 2048
0.00.038.409 I print_info: n_layer          = 24
0.00.038.412 I print_info: n_head           = 16
0.00.038.412 I print_info: n_head_kv        = 16
0.00.038.413 I print_info: n_rot            = 32
0.00.038.414 I print_info: n_swa            = 0
0.00.038.414 I print_info: n_embd_head_k    = 128
0.00.038.414 I print_info: n_embd_head_v    = 128
0.00.038.415 I print_info: n_gqa            = 1
0.00.038.416 I print_info: n_embd_k_gqa     = 2048
0.00.038.416 I print_info: n_embd_v_gqa     = 2048
0.00.038.417 I print_info: f_norm_eps       = 1.0e-05
0.00.038.417 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.418 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.418 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.418 I print_info: f_logit_scale    = 0.0e+00
0.00.038.418 I print_info: n_ff             = 8192
0.00.038.419 I print_info: n_expert         = 0
0.00.038.419 I print_info: n_expert_used    = 0
0.00.038.419 I print_info: causal attn      = 1
0.00.038.419 I print_info: pooling type     = 0
0.00.038.419 I print_info: rope type        = 2
0.00.038.419 I print_info: rope scaling     = linear
0.00.038.420 I print_info: freq_base_train  = 10000.0
0.00.038.420 I print_info: freq_scale_train = 1
0.00.038.420 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.421 I print_info: rope_finetuned   = unknown
0.00.038.423 I print_info: ssm_d_conv       = 0
0.00.038.423 I print_info: ssm_d_inner      = 0
0.00.038.423 I print_info: ssm_d_state      = 0
0.00.038.423 I print_info: ssm_dt_rank      = 0
0.00.038.423 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.423 I print_info: model type       = 1.4B
0.00.038.424 I print_info: model params     = 1.41 B
0.00.038.424 I print_info: general.name     = 1.4B
0.00.038.424 I print_info: vocab type       = BPE
0.00.038.424 I print_info: n_vocab          = 50304
0.00.038.425 I print_info: n_merges         = 50009
0.00.038.425 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.425 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.425 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.425 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.426 I print_info: LF token         = 187 'Ċ'
0.00.038.426 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.426 I print_info: max token length = 1024
0.00.038.426 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.641.822 I load_tensors: offloading 24 repeating layers to GPU
0.00.641.825 I load_tensors: offloading output layer to GPU
0.00.641.826 I load_tensors: offloaded 25/25 layers to GPU
0.00.641.849 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.641.852 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.643.316 I llama_init_from_model: n_seq_max     = 1
0.00.643.318 I llama_init_from_model: n_ctx         = 2048
0.00.643.318 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.643.319 I llama_init_from_model: n_batch       = 2048
0.00.643.319 I llama_init_from_model: n_ubatch      = 512
0.00.643.320 I llama_init_from_model: flash_attn    = 0
0.00.643.321 I llama_init_from_model: freq_base     = 10000.0
0.00.643.321 I llama_init_from_model: freq_scale    = 1
0.00.643.322 I ggml_metal_init: allocating
0.00.643.374 I ggml_metal_init: found device: Apple M4
0.00.643.384 I ggml_metal_init: picking default device: Apple M4
0.00.644.891 I ggml_metal_init: using embedded metal library
0.00.650.801 I ggml_metal_init: GPU name:   Apple M4
0.00.650.804 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.650.805 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.650.806 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.650.806 I ggml_metal_init: simdgroup reduction   = true
0.00.650.806 I ggml_metal_init: simdgroup matrix mul. = true
0.00.650.807 I ggml_metal_init: has residency sets    = true
0.00.650.807 I ggml_metal_init: has bfloat            = true
0.00.650.807 I ggml_metal_init: use bfloat            = true
0.00.650.808 I ggml_metal_init: hasUnifiedMemory      = true
0.00.650.818 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.667.881 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.716.550 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.716.555 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.716.590 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.720.801 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.720.803 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.720.803 I llama_init_from_model: graph nodes  = 967
0.00.720.804 I llama_init_from_model: graph splits = 2
0.00.720.810 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.720.941 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.720.941 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.789.042 I main: llama threadpool init, n_threads = 4
0.00.789.082 I 
0.00.789.096 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.789.097 I 
0.00.789.275 I sampler seed: 1234
0.00.789.280 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.789.291 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.789.291 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.789.291 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.665.471 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52670.62 tokens per second)
0.01.665.472 I llama_perf_context_print:        load time =     779.45 ms
0.01.665.472 I llama_perf_context_print: prompt eval time =      54.08 ms /     7 tokens (    7.73 ms per token,   129.43 tokens per second)
0.01.665.474 I llama_perf_context_print:        eval time =     819.23 ms /    63 runs   (   13.00 ms per token,    76.90 tokens per second)
0.01.665.475 I llama_perf_context_print:       total time =     877.18 ms /    70 tokens
0.01.665.727 I ggml_metal_free: deallocating

real	0m1.683s
user	0m0.108s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.738 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.099 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.042.048 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.042.054 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.056 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.057 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.059 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.059 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.059 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.060 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.061 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.061 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.061 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.062 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.062 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.063 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.065 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.066 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.066 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.835 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.858 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.485 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.058.487 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.487 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.488 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.488 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.489 I llama_model_loader: - type  f32:  194 tensors
0.00.058.489 I llama_model_loader: - type  f16:   98 tensors
0.00.058.490 I print_info: file format = GGUF V3 (latest)
0.00.058.491 I print_info: file type   = all F32 (guessed)
0.00.058.492 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.904 I load: special tokens cache size = 25
0.00.077.773 I load: token to piece cache size = 0.2984 MB
0.00.077.776 I print_info: arch             = gptneox
0.00.077.777 I print_info: vocab_only       = 0
0.00.077.777 I print_info: n_ctx_train      = 2048
0.00.077.777 I print_info: n_embd           = 2048
0.00.077.777 I print_info: n_layer          = 24
0.00.077.781 I print_info: n_head           = 16
0.00.077.781 I print_info: n_head_kv        = 16
0.00.077.782 I print_info: n_rot            = 32
0.00.077.782 I print_info: n_swa            = 0
0.00.077.782 I print_info: n_embd_head_k    = 128
0.00.077.782 I print_info: n_embd_head_v    = 128
0.00.077.783 I print_info: n_gqa            = 1
0.00.077.784 I print_info: n_embd_k_gqa     = 2048
0.00.077.784 I print_info: n_embd_v_gqa     = 2048
0.00.077.790 I print_info: f_norm_eps       = 1.0e-05
0.00.077.790 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.790 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.790 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.791 I print_info: f_logit_scale    = 0.0e+00
0.00.077.791 I print_info: n_ff             = 8192
0.00.077.791 I print_info: n_expert         = 0
0.00.077.792 I print_info: n_expert_used    = 0
0.00.077.792 I print_info: causal attn      = 1
0.00.077.792 I print_info: pooling type     = 0
0.00.077.792 I print_info: rope type        = 2
0.00.077.792 I print_info: rope scaling     = linear
0.00.077.793 I print_info: freq_base_train  = 10000.0
0.00.077.793 I print_info: freq_scale_train = 1
0.00.077.793 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.794 I print_info: rope_finetuned   = unknown
0.00.077.795 I print_info: ssm_d_conv       = 0
0.00.077.795 I print_info: ssm_d_inner      = 0
0.00.077.796 I print_info: ssm_d_state      = 0
0.00.077.796 I print_info: ssm_dt_rank      = 0
0.00.077.796 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.796 I print_info: model type       = 1.4B
0.00.077.796 I print_info: model params     = 1.41 B
0.00.077.796 I print_info: general.name     = 1.4B
0.00.077.797 I print_info: vocab type       = BPE
0.00.077.797 I print_info: n_vocab          = 50304
0.00.077.797 I print_info: n_merges         = 50009
0.00.077.798 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.798 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.798 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.798 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.799 I print_info: LF token         = 187 'Ċ'
0.00.077.799 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.799 I print_info: max token length = 1024
0.00.077.799 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.440.879 I load_tensors: offloading 24 repeating layers to GPU
0.01.440.886 I load_tensors: offloading output layer to GPU
0.01.440.887 I load_tensors: offloaded 25/25 layers to GPU
0.01.440.922 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.440.924 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.441.771 I llama_init_from_model: n_seq_max     = 1
0.01.441.772 I llama_init_from_model: n_ctx         = 128
0.01.441.773 I llama_init_from_model: n_ctx_per_seq = 128
0.01.441.773 I llama_init_from_model: n_batch       = 128
0.01.441.773 I llama_init_from_model: n_ubatch      = 128
0.01.441.774 I llama_init_from_model: flash_attn    = 0
0.01.441.775 I llama_init_from_model: freq_base     = 10000.0
0.01.441.775 I llama_init_from_model: freq_scale    = 1
0.01.441.775 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.441.777 I ggml_metal_init: allocating
0.01.441.892 I ggml_metal_init: found device: Apple M4
0.01.441.908 I ggml_metal_init: picking default device: Apple M4
0.01.443.115 I ggml_metal_init: using embedded metal library
0.01.447.085 I ggml_metal_init: GPU name:   Apple M4
0.01.447.088 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.447.089 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.447.090 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.447.090 I ggml_metal_init: simdgroup reduction   = true
0.01.447.090 I ggml_metal_init: simdgroup matrix mul. = true
0.01.447.091 I ggml_metal_init: has residency sets    = true
0.01.447.091 I ggml_metal_init: has bfloat            = true
0.01.447.091 I ggml_metal_init: use bfloat            = true
0.01.447.091 I ggml_metal_init: hasUnifiedMemory      = true
0.01.447.098 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.457.797 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.459.535 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.459.538 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.459.563 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.461.388 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.461.390 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.461.390 I llama_init_from_model: graph nodes  = 967
0.01.461.390 I llama_init_from_model: graph splits = 2
0.01.461.392 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.461.392 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.497.209 I 
0.01.497.238 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.497.245 I perplexity: tokenizing the input ..
0.01.502.697 I perplexity: tokenization took 5.451 ms
0.01.502.703 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.621.813 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.624.486 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.624.522 I llama_perf_context_print:        load time =    1473.08 ms
0.01.624.522 I llama_perf_context_print: prompt eval time =     118.79 ms /   128 tokens (    0.93 ms per token,  1077.53 tokens per second)
0.01.624.523 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.624.525 I llama_perf_context_print:       total time =     127.31 ms /   129 tokens
0.01.624.942 I ggml_metal_free: deallocating

real	0m1.811s
user	0m0.106s
sys	0m0.268s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.191 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.728 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.734 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.740 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.741 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.741 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.742 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.742 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.743 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.743 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.744 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.744 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.744 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.746 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.746 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.748 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.748 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.748 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.548 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.625 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.443 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.445 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.445 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.446 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.446 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.446 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.447 I llama_model_loader: - type  f32:  194 tensors
0.00.025.447 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.448 I print_info: file format = GGUF V3 (latest)
0.00.025.449 I print_info: file type   = Q8_0
0.00.025.450 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.619 I load: special tokens cache size = 25
0.00.039.871 I load: token to piece cache size = 0.2984 MB
0.00.039.876 I print_info: arch             = gptneox
0.00.039.876 I print_info: vocab_only       = 0
0.00.039.876 I print_info: n_ctx_train      = 2048
0.00.039.876 I print_info: n_embd           = 2048
0.00.039.876 I print_info: n_layer          = 24
0.00.039.881 I print_info: n_head           = 16
0.00.039.881 I print_info: n_head_kv        = 16
0.00.039.882 I print_info: n_rot            = 32
0.00.039.882 I print_info: n_swa            = 0
0.00.039.882 I print_info: n_embd_head_k    = 128
0.00.039.882 I print_info: n_embd_head_v    = 128
0.00.039.883 I print_info: n_gqa            = 1
0.00.039.884 I print_info: n_embd_k_gqa     = 2048
0.00.039.884 I print_info: n_embd_v_gqa     = 2048
0.00.039.885 I print_info: f_norm_eps       = 1.0e-05
0.00.039.885 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.885 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.886 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.888 I print_info: f_logit_scale    = 0.0e+00
0.00.039.888 I print_info: n_ff             = 8192
0.00.039.889 I print_info: n_expert         = 0
0.00.039.889 I print_info: n_expert_used    = 0
0.00.039.889 I print_info: causal attn      = 1
0.00.039.890 I print_info: pooling type     = 0
0.00.039.891 I print_info: rope type        = 2
0.00.039.891 I print_info: rope scaling     = linear
0.00.039.891 I print_info: freq_base_train  = 10000.0
0.00.039.891 I print_info: freq_scale_train = 1
0.00.039.892 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.895 I print_info: rope_finetuned   = unknown
0.00.039.895 I print_info: ssm_d_conv       = 0
0.00.039.896 I print_info: ssm_d_inner      = 0
0.00.039.896 I print_info: ssm_d_state      = 0
0.00.039.896 I print_info: ssm_dt_rank      = 0
0.00.039.896 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.896 I print_info: model type       = 1.4B
0.00.039.897 I print_info: model params     = 1.41 B
0.00.039.897 I print_info: general.name     = 1.4B
0.00.039.897 I print_info: vocab type       = BPE
0.00.039.897 I print_info: n_vocab          = 50304
0.00.039.898 I print_info: n_merges         = 50009
0.00.039.898 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.898 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.899 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.900 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.900 I print_info: LF token         = 187 'Ċ'
0.00.039.900 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.901 I print_info: max token length = 1024
0.00.039.901 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.881.061 I load_tensors: offloading 24 repeating layers to GPU
0.00.881.069 I load_tensors: offloading output layer to GPU
0.00.881.070 I load_tensors: offloaded 25/25 layers to GPU
0.00.881.098 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.881.100 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.882.610 I llama_init_from_model: n_seq_max     = 1
0.00.882.611 I llama_init_from_model: n_ctx         = 128
0.00.882.612 I llama_init_from_model: n_ctx_per_seq = 128
0.00.882.612 I llama_init_from_model: n_batch       = 128
0.00.882.613 I llama_init_from_model: n_ubatch      = 128
0.00.882.613 I llama_init_from_model: flash_attn    = 0
0.00.882.614 I llama_init_from_model: freq_base     = 10000.0
0.00.882.615 I llama_init_from_model: freq_scale    = 1
0.00.882.615 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.882.616 I ggml_metal_init: allocating
0.00.882.706 I ggml_metal_init: found device: Apple M4
0.00.882.718 I ggml_metal_init: picking default device: Apple M4
0.00.884.071 I ggml_metal_init: using embedded metal library
0.00.889.233 I ggml_metal_init: GPU name:   Apple M4
0.00.889.237 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.889.237 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.889.238 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.889.239 I ggml_metal_init: simdgroup reduction   = true
0.00.889.239 I ggml_metal_init: simdgroup matrix mul. = true
0.00.889.239 I ggml_metal_init: has residency sets    = true
0.00.889.239 I ggml_metal_init: has bfloat            = true
0.00.889.239 I ggml_metal_init: use bfloat            = true
0.00.889.240 I ggml_metal_init: hasUnifiedMemory      = true
0.00.889.244 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.903.984 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.906.655 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.906.658 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.906.764 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.909.266 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.909.268 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.909.269 I llama_init_from_model: graph nodes  = 967
0.00.909.269 I llama_init_from_model: graph splits = 2
0.00.909.271 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.909.271 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.935.122 I 
0.00.935.151 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.935.156 I perplexity: tokenizing the input ..
0.00.941.288 I perplexity: tokenization took 6.129 ms
0.00.941.293 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.079.247 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.080.598 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.080.624 I llama_perf_context_print:        load time =     925.92 ms
0.01.080.625 I llama_perf_context_print: prompt eval time =     137.57 ms /   128 tokens (    1.07 ms per token,   930.44 tokens per second)
0.01.080.625 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.080.626 I llama_perf_context_print:       total time =     145.50 ms /   129 tokens
0.01.080.992 I ggml_metal_free: deallocating

real	0m1.096s
user	0m0.075s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.630 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.982 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.988 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.990 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.995 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.995 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.996 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.996 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.997 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.997 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.998 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.998 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.998 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.999 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.999 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.001 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.002 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.002 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.887 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.930 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.767 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.768 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.769 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.769 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.769 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.770 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.770 I llama_model_loader: - type  f32:  194 tensors
0.00.025.771 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.771 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.772 I print_info: file format = GGUF V3 (latest)
0.00.025.776 I print_info: file type   = Q4_0
0.00.025.778 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.362 I load: special tokens cache size = 25
0.00.040.451 I load: token to piece cache size = 0.2984 MB
0.00.040.455 I print_info: arch             = gptneox
0.00.040.455 I print_info: vocab_only       = 0
0.00.040.456 I print_info: n_ctx_train      = 2048
0.00.040.456 I print_info: n_embd           = 2048
0.00.040.456 I print_info: n_layer          = 24
0.00.040.460 I print_info: n_head           = 16
0.00.040.461 I print_info: n_head_kv        = 16
0.00.040.461 I print_info: n_rot            = 32
0.00.040.462 I print_info: n_swa            = 0
0.00.040.462 I print_info: n_embd_head_k    = 128
0.00.040.462 I print_info: n_embd_head_v    = 128
0.00.040.463 I print_info: n_gqa            = 1
0.00.040.463 I print_info: n_embd_k_gqa     = 2048
0.00.040.467 I print_info: n_embd_v_gqa     = 2048
0.00.040.467 I print_info: f_norm_eps       = 1.0e-05
0.00.040.467 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.468 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.468 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.468 I print_info: f_logit_scale    = 0.0e+00
0.00.040.469 I print_info: n_ff             = 8192
0.00.040.469 I print_info: n_expert         = 0
0.00.040.469 I print_info: n_expert_used    = 0
0.00.040.469 I print_info: causal attn      = 1
0.00.040.469 I print_info: pooling type     = 0
0.00.040.469 I print_info: rope type        = 2
0.00.040.470 I print_info: rope scaling     = linear
0.00.040.470 I print_info: freq_base_train  = 10000.0
0.00.040.470 I print_info: freq_scale_train = 1
0.00.040.470 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.471 I print_info: rope_finetuned   = unknown
0.00.040.471 I print_info: ssm_d_conv       = 0
0.00.040.471 I print_info: ssm_d_inner      = 0
0.00.040.471 I print_info: ssm_d_state      = 0
0.00.040.471 I print_info: ssm_dt_rank      = 0
0.00.040.471 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.472 I print_info: model type       = 1.4B
0.00.040.473 I print_info: model params     = 1.41 B
0.00.040.473 I print_info: general.name     = 1.4B
0.00.040.473 I print_info: vocab type       = BPE
0.00.040.474 I print_info: n_vocab          = 50304
0.00.040.474 I print_info: n_merges         = 50009
0.00.040.474 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.474 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.474 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.474 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.475 I print_info: LF token         = 187 'Ċ'
0.00.040.475 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.475 I print_info: max token length = 1024
0.00.040.475 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.627.424 I load_tensors: offloading 24 repeating layers to GPU
0.00.627.438 I load_tensors: offloading output layer to GPU
0.00.627.439 I load_tensors: offloaded 25/25 layers to GPU
0.00.627.469 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.627.471 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.629.030 I llama_init_from_model: n_seq_max     = 1
0.00.629.036 I llama_init_from_model: n_ctx         = 128
0.00.629.036 I llama_init_from_model: n_ctx_per_seq = 128
0.00.629.037 I llama_init_from_model: n_batch       = 128
0.00.629.038 I llama_init_from_model: n_ubatch      = 128
0.00.629.038 I llama_init_from_model: flash_attn    = 0
0.00.629.039 I llama_init_from_model: freq_base     = 10000.0
0.00.629.040 I llama_init_from_model: freq_scale    = 1
0.00.629.041 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.629.043 I ggml_metal_init: allocating
0.00.629.114 I ggml_metal_init: found device: Apple M4
0.00.629.127 I ggml_metal_init: picking default device: Apple M4
0.00.630.874 I ggml_metal_init: using embedded metal library
0.00.636.476 I ggml_metal_init: GPU name:   Apple M4
0.00.636.489 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.636.490 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.636.491 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.636.492 I ggml_metal_init: simdgroup reduction   = true
0.00.636.492 I ggml_metal_init: simdgroup matrix mul. = true
0.00.636.492 I ggml_metal_init: has residency sets    = true
0.00.636.492 I ggml_metal_init: has bfloat            = true
0.00.636.493 I ggml_metal_init: use bfloat            = true
0.00.636.495 I ggml_metal_init: hasUnifiedMemory      = true
0.00.636.498 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.657.084 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.660.832 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.660.836 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.660.879 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.664.186 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.664.188 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.664.189 I llama_init_from_model: graph nodes  = 967
0.00.664.189 I llama_init_from_model: graph splits = 2
0.00.664.192 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.664.193 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.189 I 
0.00.691.260 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.267 I perplexity: tokenizing the input ..
0.00.698.382 I perplexity: tokenization took 7.114 ms
0.00.698.388 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.828.439 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.829.764 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.829.787 I llama_perf_context_print:        load time =     681.54 ms
0.00.829.788 I llama_perf_context_print: prompt eval time =     129.15 ms /   128 tokens (    1.01 ms per token,   991.06 tokens per second)
0.00.829.789 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.829.789 I llama_perf_context_print:       total time =     138.61 ms /   129 tokens
0.00.830.205 I ggml_metal_free: deallocating

real	0m0.845s
user	0m0.082s
sys	0m0.133s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.991 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.541 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.547 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.554 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.554 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.555 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.555 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.555 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.556 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.557 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.557 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.558 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.558 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.558 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.559 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.561 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.561 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.561 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.370 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.442 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.263 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.265 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.265 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.265 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.266 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.266 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.267 I llama_model_loader: - type  f32:  194 tensors
0.00.025.267 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.267 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.268 I print_info: file format = GGUF V3 (latest)
0.00.025.269 I print_info: file type   = Q4_1
0.00.025.270 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.804 I load: special tokens cache size = 25
0.00.039.884 I load: token to piece cache size = 0.2984 MB
0.00.039.889 I print_info: arch             = gptneox
0.00.039.889 I print_info: vocab_only       = 0
0.00.039.889 I print_info: n_ctx_train      = 2048
0.00.039.889 I print_info: n_embd           = 2048
0.00.039.890 I print_info: n_layer          = 24
0.00.039.894 I print_info: n_head           = 16
0.00.039.895 I print_info: n_head_kv        = 16
0.00.039.895 I print_info: n_rot            = 32
0.00.039.895 I print_info: n_swa            = 0
0.00.039.895 I print_info: n_embd_head_k    = 128
0.00.039.895 I print_info: n_embd_head_v    = 128
0.00.039.896 I print_info: n_gqa            = 1
0.00.039.897 I print_info: n_embd_k_gqa     = 2048
0.00.039.898 I print_info: n_embd_v_gqa     = 2048
0.00.039.898 I print_info: f_norm_eps       = 1.0e-05
0.00.039.899 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.899 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.899 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.899 I print_info: f_logit_scale    = 0.0e+00
0.00.039.900 I print_info: n_ff             = 8192
0.00.039.900 I print_info: n_expert         = 0
0.00.039.900 I print_info: n_expert_used    = 0
0.00.039.900 I print_info: causal attn      = 1
0.00.039.900 I print_info: pooling type     = 0
0.00.039.901 I print_info: rope type        = 2
0.00.039.901 I print_info: rope scaling     = linear
0.00.039.901 I print_info: freq_base_train  = 10000.0
0.00.039.901 I print_info: freq_scale_train = 1
0.00.039.902 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.902 I print_info: rope_finetuned   = unknown
0.00.039.902 I print_info: ssm_d_conv       = 0
0.00.039.902 I print_info: ssm_d_inner      = 0
0.00.039.902 I print_info: ssm_d_state      = 0
0.00.039.902 I print_info: ssm_dt_rank      = 0
0.00.039.903 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.903 I print_info: model type       = 1.4B
0.00.039.903 I print_info: model params     = 1.41 B
0.00.039.903 I print_info: general.name     = 1.4B
0.00.039.904 I print_info: vocab type       = BPE
0.00.039.904 I print_info: n_vocab          = 50304
0.00.039.904 I print_info: n_merges         = 50009
0.00.039.904 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.907 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.907 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.907 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.908 I print_info: LF token         = 187 'Ċ'
0.00.039.908 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.945 I print_info: max token length = 1024
0.00.039.947 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.622.541 I load_tensors: offloading 24 repeating layers to GPU
0.00.622.558 I load_tensors: offloading output layer to GPU
0.00.622.559 I load_tensors: offloaded 25/25 layers to GPU
0.00.622.593 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.622.595 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.624.298 I llama_init_from_model: n_seq_max     = 1
0.00.624.301 I llama_init_from_model: n_ctx         = 128
0.00.624.302 I llama_init_from_model: n_ctx_per_seq = 128
0.00.624.302 I llama_init_from_model: n_batch       = 128
0.00.624.303 I llama_init_from_model: n_ubatch      = 128
0.00.624.303 I llama_init_from_model: flash_attn    = 0
0.00.624.307 I llama_init_from_model: freq_base     = 10000.0
0.00.624.307 I llama_init_from_model: freq_scale    = 1
0.00.624.308 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.624.312 I ggml_metal_init: allocating
0.00.624.392 I ggml_metal_init: found device: Apple M4
0.00.624.407 I ggml_metal_init: picking default device: Apple M4
0.00.626.199 I ggml_metal_init: using embedded metal library
0.00.632.778 I ggml_metal_init: GPU name:   Apple M4
0.00.632.784 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.632.784 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.632.785 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.632.786 I ggml_metal_init: simdgroup reduction   = true
0.00.632.786 I ggml_metal_init: simdgroup matrix mul. = true
0.00.632.787 I ggml_metal_init: has residency sets    = true
0.00.632.787 I ggml_metal_init: has bfloat            = true
0.00.632.787 I ggml_metal_init: use bfloat            = true
0.00.632.788 I ggml_metal_init: hasUnifiedMemory      = true
0.00.632.794 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.651.365 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.654.870 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.654.880 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.654.932 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.658.146 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.658.148 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.658.149 I llama_init_from_model: graph nodes  = 967
0.00.658.149 I llama_init_from_model: graph splits = 2
0.00.658.152 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.658.153 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.687.241 I 
0.00.687.301 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.687.308 I perplexity: tokenizing the input ..
0.00.694.415 I perplexity: tokenization took 7.105 ms
0.00.694.421 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.831.720 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.833.070 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.833.093 I llama_perf_context_print:        load time =     678.23 ms
0.00.833.094 I llama_perf_context_print: prompt eval time =     136.36 ms /   128 tokens (    1.07 ms per token,   938.71 tokens per second)
0.00.833.095 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.833.095 I llama_perf_context_print:       total time =     145.86 ms /   129 tokens
0.00.833.466 I ggml_metal_free: deallocating

real	0m0.848s
user	0m0.081s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.940 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.163 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.169 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.176 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.176 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.177 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.177 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.177 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.178 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.179 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.179 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.179 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.180 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.180 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.182 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.184 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.185 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.185 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.054 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.052 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.905 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.906 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.907 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.907 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.908 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.908 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.908 I llama_model_loader: - type  f32:  194 tensors
0.00.024.909 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.909 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.910 I print_info: file format = GGUF V3 (latest)
0.00.024.911 I print_info: file type   = Q5_0
0.00.024.912 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.924 I load: special tokens cache size = 25
0.00.039.149 I load: token to piece cache size = 0.2984 MB
0.00.039.154 I print_info: arch             = gptneox
0.00.039.154 I print_info: vocab_only       = 0
0.00.039.154 I print_info: n_ctx_train      = 2048
0.00.039.154 I print_info: n_embd           = 2048
0.00.039.154 I print_info: n_layer          = 24
0.00.039.159 I print_info: n_head           = 16
0.00.039.159 I print_info: n_head_kv        = 16
0.00.039.160 I print_info: n_rot            = 32
0.00.039.160 I print_info: n_swa            = 0
0.00.039.160 I print_info: n_embd_head_k    = 128
0.00.039.160 I print_info: n_embd_head_v    = 128
0.00.039.161 I print_info: n_gqa            = 1
0.00.039.162 I print_info: n_embd_k_gqa     = 2048
0.00.039.165 I print_info: n_embd_v_gqa     = 2048
0.00.039.166 I print_info: f_norm_eps       = 1.0e-05
0.00.039.166 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.166 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.166 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.167 I print_info: f_logit_scale    = 0.0e+00
0.00.039.167 I print_info: n_ff             = 8192
0.00.039.167 I print_info: n_expert         = 0
0.00.039.168 I print_info: n_expert_used    = 0
0.00.039.168 I print_info: causal attn      = 1
0.00.039.168 I print_info: pooling type     = 0
0.00.039.168 I print_info: rope type        = 2
0.00.039.169 I print_info: rope scaling     = linear
0.00.039.170 I print_info: freq_base_train  = 10000.0
0.00.039.170 I print_info: freq_scale_train = 1
0.00.039.170 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.170 I print_info: rope_finetuned   = unknown
0.00.039.170 I print_info: ssm_d_conv       = 0
0.00.039.171 I print_info: ssm_d_inner      = 0
0.00.039.171 I print_info: ssm_d_state      = 0
0.00.039.171 I print_info: ssm_dt_rank      = 0
0.00.039.171 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.171 I print_info: model type       = 1.4B
0.00.039.172 I print_info: model params     = 1.41 B
0.00.039.172 I print_info: general.name     = 1.4B
0.00.039.172 I print_info: vocab type       = BPE
0.00.039.173 I print_info: n_vocab          = 50304
0.00.039.173 I print_info: n_merges         = 50009
0.00.039.173 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.173 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.173 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.173 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.174 I print_info: LF token         = 187 'Ċ'
0.00.039.174 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.174 I print_info: max token length = 1024
0.00.039.174 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.704.786 I load_tensors: offloading 24 repeating layers to GPU
0.00.704.802 I load_tensors: offloading output layer to GPU
0.00.704.802 I load_tensors: offloaded 25/25 layers to GPU
0.00.704.832 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.704.834 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.706.592 I llama_init_from_model: n_seq_max     = 1
0.00.706.595 I llama_init_from_model: n_ctx         = 128
0.00.706.595 I llama_init_from_model: n_ctx_per_seq = 128
0.00.706.596 I llama_init_from_model: n_batch       = 128
0.00.706.596 I llama_init_from_model: n_ubatch      = 128
0.00.706.597 I llama_init_from_model: flash_attn    = 0
0.00.706.599 I llama_init_from_model: freq_base     = 10000.0
0.00.706.600 I llama_init_from_model: freq_scale    = 1
0.00.706.600 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.706.603 I ggml_metal_init: allocating
0.00.706.712 I ggml_metal_init: found device: Apple M4
0.00.706.725 I ggml_metal_init: picking default device: Apple M4
0.00.708.659 I ggml_metal_init: using embedded metal library
0.00.715.362 I ggml_metal_init: GPU name:   Apple M4
0.00.715.367 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.715.368 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.715.369 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.715.369 I ggml_metal_init: simdgroup reduction   = true
0.00.715.370 I ggml_metal_init: simdgroup matrix mul. = true
0.00.715.370 I ggml_metal_init: has residency sets    = true
0.00.715.370 I ggml_metal_init: has bfloat            = true
0.00.715.370 I ggml_metal_init: use bfloat            = true
0.00.715.371 I ggml_metal_init: hasUnifiedMemory      = true
0.00.715.373 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.733.321 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.736.900 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.736.904 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.736.946 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.740.328 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.740.330 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.740.330 I llama_init_from_model: graph nodes  = 967
0.00.740.330 I llama_init_from_model: graph splits = 2
0.00.740.334 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.740.334 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.767.271 I 
0.00.767.320 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.767.328 I perplexity: tokenizing the input ..
0.00.774.107 I perplexity: tokenization took 6.775 ms
0.00.774.113 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.909.715 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.911.023 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.911.046 I llama_perf_context_print:        load time =     758.32 ms
0.00.911.047 I llama_perf_context_print: prompt eval time =     134.69 ms /   128 tokens (    1.05 ms per token,   950.34 tokens per second)
0.00.911.048 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.911.049 I llama_perf_context_print:       total time =     143.78 ms /   129 tokens
0.00.911.424 I ggml_metal_free: deallocating

real	0m0.925s
user	0m0.080s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.800 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.045 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.050 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.051 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.052 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.052 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.053 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.053 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.054 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.054 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.055 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.055 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.055 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.056 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.056 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.058 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.058 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.059 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.888 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.006 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.873 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.874 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.875 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.875 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.875 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.876 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.876 I llama_model_loader: - type  f32:  194 tensors
0.00.025.877 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.877 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.878 I print_info: file format = GGUF V3 (latest)
0.00.025.878 I print_info: file type   = Q5_1
0.00.025.879 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.170 I load: special tokens cache size = 25
0.00.040.281 I load: token to piece cache size = 0.2984 MB
0.00.040.286 I print_info: arch             = gptneox
0.00.040.286 I print_info: vocab_only       = 0
0.00.040.286 I print_info: n_ctx_train      = 2048
0.00.040.286 I print_info: n_embd           = 2048
0.00.040.287 I print_info: n_layer          = 24
0.00.040.290 I print_info: n_head           = 16
0.00.040.293 I print_info: n_head_kv        = 16
0.00.040.293 I print_info: n_rot            = 32
0.00.040.293 I print_info: n_swa            = 0
0.00.040.294 I print_info: n_embd_head_k    = 128
0.00.040.294 I print_info: n_embd_head_v    = 128
0.00.040.294 I print_info: n_gqa            = 1
0.00.040.299 I print_info: n_embd_k_gqa     = 2048
0.00.040.300 I print_info: n_embd_v_gqa     = 2048
0.00.040.300 I print_info: f_norm_eps       = 1.0e-05
0.00.040.301 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.301 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.301 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.301 I print_info: f_logit_scale    = 0.0e+00
0.00.040.302 I print_info: n_ff             = 8192
0.00.040.302 I print_info: n_expert         = 0
0.00.040.303 I print_info: n_expert_used    = 0
0.00.040.303 I print_info: causal attn      = 1
0.00.040.303 I print_info: pooling type     = 0
0.00.040.303 I print_info: rope type        = 2
0.00.040.303 I print_info: rope scaling     = linear
0.00.040.304 I print_info: freq_base_train  = 10000.0
0.00.040.304 I print_info: freq_scale_train = 1
0.00.040.304 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.304 I print_info: rope_finetuned   = unknown
0.00.040.305 I print_info: ssm_d_conv       = 0
0.00.040.305 I print_info: ssm_d_inner      = 0
0.00.040.305 I print_info: ssm_d_state      = 0
0.00.040.305 I print_info: ssm_dt_rank      = 0
0.00.040.305 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.306 I print_info: model type       = 1.4B
0.00.040.307 I print_info: model params     = 1.41 B
0.00.040.307 I print_info: general.name     = 1.4B
0.00.040.307 I print_info: vocab type       = BPE
0.00.040.308 I print_info: n_vocab          = 50304
0.00.040.308 I print_info: n_merges         = 50009
0.00.040.308 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.308 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.308 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.308 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.309 I print_info: LF token         = 187 'Ċ'
0.00.040.309 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.309 I print_info: max token length = 1024
0.00.040.309 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.633.308 I load_tensors: offloading 24 repeating layers to GPU
0.00.633.313 I load_tensors: offloading output layer to GPU
0.00.633.314 I load_tensors: offloaded 25/25 layers to GPU
0.00.633.339 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.633.342 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.634.844 I llama_init_from_model: n_seq_max     = 1
0.00.634.846 I llama_init_from_model: n_ctx         = 128
0.00.634.847 I llama_init_from_model: n_ctx_per_seq = 128
0.00.634.847 I llama_init_from_model: n_batch       = 128
0.00.634.848 I llama_init_from_model: n_ubatch      = 128
0.00.634.848 I llama_init_from_model: flash_attn    = 0
0.00.634.849 I llama_init_from_model: freq_base     = 10000.0
0.00.634.850 I llama_init_from_model: freq_scale    = 1
0.00.634.851 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.634.852 I ggml_metal_init: allocating
0.00.634.872 I ggml_metal_init: found device: Apple M4
0.00.634.881 I ggml_metal_init: picking default device: Apple M4
0.00.636.271 I ggml_metal_init: using embedded metal library
0.00.642.085 I ggml_metal_init: GPU name:   Apple M4
0.00.642.089 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.642.090 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.642.091 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.642.092 I ggml_metal_init: simdgroup reduction   = true
0.00.642.092 I ggml_metal_init: simdgroup matrix mul. = true
0.00.642.092 I ggml_metal_init: has residency sets    = true
0.00.642.092 I ggml_metal_init: has bfloat            = true
0.00.642.093 I ggml_metal_init: use bfloat            = true
0.00.642.093 I ggml_metal_init: hasUnifiedMemory      = true
0.00.642.101 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.658.794 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.662.166 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.662.169 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.662.217 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.665.345 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.665.347 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.665.348 I llama_init_from_model: graph nodes  = 967
0.00.665.348 I llama_init_from_model: graph splits = 2
0.00.665.351 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.665.351 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.112 I 
0.00.699.166 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.174 I perplexity: tokenizing the input ..
0.00.705.976 I perplexity: tokenization took 6.801 ms
0.00.705.984 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.848.008 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.849.351 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.849.373 I llama_perf_context_print:        load time =     689.30 ms
0.00.849.374 I llama_perf_context_print: prompt eval time =     141.74 ms /   128 tokens (    1.11 ms per token,   903.09 tokens per second)
0.00.849.375 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.849.375 I llama_perf_context_print:       total time =     150.27 ms /   129 tokens
0.00.849.720 I ggml_metal_free: deallocating

real	0m0.865s
user	0m0.078s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.940 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.227 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.233 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.235 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.235 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.236 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.236 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.236 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.237 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.238 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.238 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.238 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.239 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.239 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.242 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.244 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.244 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.245 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.199 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.253 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.161 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.163 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.163 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.163 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.164 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.164 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.165 I llama_model_loader: - type  f32:  194 tensors
0.00.025.165 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.165 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.166 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.167 I print_info: file format = GGUF V3 (latest)
0.00.025.172 I print_info: file type   = Q2_K - Medium
0.00.025.173 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.226 I load: special tokens cache size = 25
0.00.039.230 I load: token to piece cache size = 0.2984 MB
0.00.039.233 I print_info: arch             = gptneox
0.00.039.234 I print_info: vocab_only       = 0
0.00.039.234 I print_info: n_ctx_train      = 2048
0.00.039.234 I print_info: n_embd           = 2048
0.00.039.234 I print_info: n_layer          = 24
0.00.039.239 I print_info: n_head           = 16
0.00.039.240 I print_info: n_head_kv        = 16
0.00.039.240 I print_info: n_rot            = 32
0.00.039.241 I print_info: n_swa            = 0
0.00.039.241 I print_info: n_embd_head_k    = 128
0.00.039.241 I print_info: n_embd_head_v    = 128
0.00.039.243 I print_info: n_gqa            = 1
0.00.039.244 I print_info: n_embd_k_gqa     = 2048
0.00.039.245 I print_info: n_embd_v_gqa     = 2048
0.00.039.246 I print_info: f_norm_eps       = 1.0e-05
0.00.039.246 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.246 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.246 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.246 I print_info: f_logit_scale    = 0.0e+00
0.00.039.247 I print_info: n_ff             = 8192
0.00.039.247 I print_info: n_expert         = 0
0.00.039.247 I print_info: n_expert_used    = 0
0.00.039.248 I print_info: causal attn      = 1
0.00.039.248 I print_info: pooling type     = 0
0.00.039.248 I print_info: rope type        = 2
0.00.039.248 I print_info: rope scaling     = linear
0.00.039.249 I print_info: freq_base_train  = 10000.0
0.00.039.249 I print_info: freq_scale_train = 1
0.00.039.249 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.249 I print_info: rope_finetuned   = unknown
0.00.039.249 I print_info: ssm_d_conv       = 0
0.00.039.249 I print_info: ssm_d_inner      = 0
0.00.039.250 I print_info: ssm_d_state      = 0
0.00.039.250 I print_info: ssm_dt_rank      = 0
0.00.039.250 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.250 I print_info: model type       = 1.4B
0.00.039.250 I print_info: model params     = 1.41 B
0.00.039.251 I print_info: general.name     = 1.4B
0.00.039.251 I print_info: vocab type       = BPE
0.00.039.251 I print_info: n_vocab          = 50304
0.00.039.251 I print_info: n_merges         = 50009
0.00.039.252 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.252 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.252 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.252 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.253 I print_info: LF token         = 187 'Ċ'
0.00.039.253 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.253 I print_info: max token length = 1024
0.00.039.253 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.351.448 I load_tensors: offloading 24 repeating layers to GPU
0.00.351.462 I load_tensors: offloading output layer to GPU
0.00.351.463 I load_tensors: offloaded 25/25 layers to GPU
0.00.351.496 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.351.497 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.353.197 I llama_init_from_model: n_seq_max     = 1
0.00.353.203 I llama_init_from_model: n_ctx         = 128
0.00.353.204 I llama_init_from_model: n_ctx_per_seq = 128
0.00.353.204 I llama_init_from_model: n_batch       = 128
0.00.353.205 I llama_init_from_model: n_ubatch      = 128
0.00.353.205 I llama_init_from_model: flash_attn    = 0
0.00.353.207 I llama_init_from_model: freq_base     = 10000.0
0.00.353.207 I llama_init_from_model: freq_scale    = 1
0.00.353.208 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.353.218 I ggml_metal_init: allocating
0.00.353.331 I ggml_metal_init: found device: Apple M4
0.00.353.345 I ggml_metal_init: picking default device: Apple M4
0.00.355.224 I ggml_metal_init: using embedded metal library
0.00.360.659 I ggml_metal_init: GPU name:   Apple M4
0.00.360.668 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.360.668 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.360.669 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.360.670 I ggml_metal_init: simdgroup reduction   = true
0.00.360.670 I ggml_metal_init: simdgroup matrix mul. = true
0.00.360.670 I ggml_metal_init: has residency sets    = true
0.00.360.671 I ggml_metal_init: has bfloat            = true
0.00.360.671 I ggml_metal_init: use bfloat            = true
0.00.360.673 I ggml_metal_init: hasUnifiedMemory      = true
0.00.360.677 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.382.146 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.385.741 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.385.745 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.385.789 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.389.292 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.389.294 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.389.295 I llama_init_from_model: graph nodes  = 967
0.00.389.296 I llama_init_from_model: graph splits = 2
0.00.389.299 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.389.299 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.421.213 I 
0.00.421.277 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.421.285 I perplexity: tokenizing the input ..
0.00.428.468 I perplexity: tokenization took 7.179 ms
0.00.428.481 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.568.608 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.569.941 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.569.968 I llama_perf_context_print:        load time =     412.26 ms
0.00.569.968 I llama_perf_context_print: prompt eval time =     139.18 ms /   128 tokens (    1.09 ms per token,   919.69 tokens per second)
0.00.569.969 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.569.969 I llama_perf_context_print:       total time =     148.76 ms /   129 tokens
0.00.570.342 I ggml_metal_free: deallocating

real	0m0.584s
user	0m0.082s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.767 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.795 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.802 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.809 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.809 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.810 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.810 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.810 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.813 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.813 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.814 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.814 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.814 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.815 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.815 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.818 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.818 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.819 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.768 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.805 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.717 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.718 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.719 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.719 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.719 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.720 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.720 I llama_model_loader: - type  f32:  194 tensors
0.00.024.721 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.724 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.724 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.724 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.725 I print_info: file format = GGUF V3 (latest)
0.00.024.725 I print_info: file type   = Q3_K - Medium
0.00.024.726 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.493 I load: special tokens cache size = 25
0.00.039.505 I load: token to piece cache size = 0.2984 MB
0.00.039.509 I print_info: arch             = gptneox
0.00.039.509 I print_info: vocab_only       = 0
0.00.039.509 I print_info: n_ctx_train      = 2048
0.00.039.509 I print_info: n_embd           = 2048
0.00.039.510 I print_info: n_layer          = 24
0.00.039.513 I print_info: n_head           = 16
0.00.039.514 I print_info: n_head_kv        = 16
0.00.039.514 I print_info: n_rot            = 32
0.00.039.516 I print_info: n_swa            = 0
0.00.039.516 I print_info: n_embd_head_k    = 128
0.00.039.518 I print_info: n_embd_head_v    = 128
0.00.039.518 I print_info: n_gqa            = 1
0.00.039.519 I print_info: n_embd_k_gqa     = 2048
0.00.039.520 I print_info: n_embd_v_gqa     = 2048
0.00.039.521 I print_info: f_norm_eps       = 1.0e-05
0.00.039.521 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.521 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.521 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.521 I print_info: f_logit_scale    = 0.0e+00
0.00.039.522 I print_info: n_ff             = 8192
0.00.039.522 I print_info: n_expert         = 0
0.00.039.522 I print_info: n_expert_used    = 0
0.00.039.523 I print_info: causal attn      = 1
0.00.039.523 I print_info: pooling type     = 0
0.00.039.523 I print_info: rope type        = 2
0.00.039.524 I print_info: rope scaling     = linear
0.00.039.525 I print_info: freq_base_train  = 10000.0
0.00.039.525 I print_info: freq_scale_train = 1
0.00.039.525 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.526 I print_info: rope_finetuned   = unknown
0.00.039.526 I print_info: ssm_d_conv       = 0
0.00.039.526 I print_info: ssm_d_inner      = 0
0.00.039.526 I print_info: ssm_d_state      = 0
0.00.039.526 I print_info: ssm_dt_rank      = 0
0.00.039.526 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.526 I print_info: model type       = 1.4B
0.00.039.527 I print_info: model params     = 1.41 B
0.00.039.527 I print_info: general.name     = 1.4B
0.00.039.527 I print_info: vocab type       = BPE
0.00.039.528 I print_info: n_vocab          = 50304
0.00.039.528 I print_info: n_merges         = 50009
0.00.039.531 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.531 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.532 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.532 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.532 I print_info: LF token         = 187 'Ċ'
0.00.039.533 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.534 I print_info: max token length = 1024
0.00.039.534 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.451.073 I load_tensors: offloading 24 repeating layers to GPU
0.00.451.087 I load_tensors: offloading output layer to GPU
0.00.451.088 I load_tensors: offloaded 25/25 layers to GPU
0.00.451.119 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.451.124 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.452.876 I llama_init_from_model: n_seq_max     = 1
0.00.452.885 I llama_init_from_model: n_ctx         = 128
0.00.452.886 I llama_init_from_model: n_ctx_per_seq = 128
0.00.452.886 I llama_init_from_model: n_batch       = 128
0.00.452.887 I llama_init_from_model: n_ubatch      = 128
0.00.452.887 I llama_init_from_model: flash_attn    = 0
0.00.452.889 I llama_init_from_model: freq_base     = 10000.0
0.00.452.889 I llama_init_from_model: freq_scale    = 1
0.00.452.890 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.452.892 I ggml_metal_init: allocating
0.00.452.967 I ggml_metal_init: found device: Apple M4
0.00.452.980 I ggml_metal_init: picking default device: Apple M4
0.00.454.768 I ggml_metal_init: using embedded metal library
0.00.460.154 I ggml_metal_init: GPU name:   Apple M4
0.00.460.164 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.460.165 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.460.166 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.460.166 I ggml_metal_init: simdgroup reduction   = true
0.00.460.166 I ggml_metal_init: simdgroup matrix mul. = true
0.00.460.167 I ggml_metal_init: has residency sets    = true
0.00.460.167 I ggml_metal_init: has bfloat            = true
0.00.460.167 I ggml_metal_init: use bfloat            = true
0.00.460.169 I ggml_metal_init: hasUnifiedMemory      = true
0.00.460.174 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.480.137 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.483.761 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.483.768 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.483.823 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.487.288 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.487.290 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.487.291 I llama_init_from_model: graph nodes  = 967
0.00.487.291 I llama_init_from_model: graph splits = 2
0.00.487.294 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.487.295 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.517.352 I 
0.00.517.420 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.517.427 I perplexity: tokenizing the input ..
0.00.524.396 I perplexity: tokenization took 6.965 ms
0.00.524.404 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.666.274 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.667.615 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.667.641 I llama_perf_context_print:        load time =     508.57 ms
0.00.667.642 I llama_perf_context_print: prompt eval time =     140.91 ms /   128 tokens (    1.10 ms per token,   908.40 tokens per second)
0.00.667.643 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.667.643 I llama_perf_context_print:       total time =     150.30 ms /   129 tokens
0.00.668.020 I ggml_metal_free: deallocating

real	0m0.682s
user	0m0.081s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.837 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.434 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.440 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.449 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.450 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.450 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.450 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.451 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.452 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.453 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.454 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.454 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.454 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.455 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.455 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.457 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.458 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.458 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.222 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.233 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.017 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.018 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.018 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.019 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.019 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.019 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.020 I llama_model_loader: - type  f32:  194 tensors
0.00.025.020 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.021 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.021 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.021 I print_info: file format = GGUF V3 (latest)
0.00.025.022 I print_info: file type   = Q4_K - Medium
0.00.025.023 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.895 I load: special tokens cache size = 25
0.00.038.971 I load: token to piece cache size = 0.2984 MB
0.00.038.975 I print_info: arch             = gptneox
0.00.038.975 I print_info: vocab_only       = 0
0.00.038.975 I print_info: n_ctx_train      = 2048
0.00.038.975 I print_info: n_embd           = 2048
0.00.038.976 I print_info: n_layer          = 24
0.00.038.979 I print_info: n_head           = 16
0.00.038.980 I print_info: n_head_kv        = 16
0.00.038.980 I print_info: n_rot            = 32
0.00.038.981 I print_info: n_swa            = 0
0.00.038.982 I print_info: n_embd_head_k    = 128
0.00.038.984 I print_info: n_embd_head_v    = 128
0.00.038.985 I print_info: n_gqa            = 1
0.00.038.986 I print_info: n_embd_k_gqa     = 2048
0.00.038.986 I print_info: n_embd_v_gqa     = 2048
0.00.038.987 I print_info: f_norm_eps       = 1.0e-05
0.00.038.988 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.988 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.988 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.988 I print_info: f_logit_scale    = 0.0e+00
0.00.038.989 I print_info: n_ff             = 8192
0.00.038.989 I print_info: n_expert         = 0
0.00.038.989 I print_info: n_expert_used    = 0
0.00.038.989 I print_info: causal attn      = 1
0.00.038.990 I print_info: pooling type     = 0
0.00.038.990 I print_info: rope type        = 2
0.00.038.990 I print_info: rope scaling     = linear
0.00.038.990 I print_info: freq_base_train  = 10000.0
0.00.038.991 I print_info: freq_scale_train = 1
0.00.038.991 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.991 I print_info: rope_finetuned   = unknown
0.00.038.991 I print_info: ssm_d_conv       = 0
0.00.038.991 I print_info: ssm_d_inner      = 0
0.00.038.992 I print_info: ssm_d_state      = 0
0.00.038.992 I print_info: ssm_dt_rank      = 0
0.00.038.992 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.993 I print_info: model type       = 1.4B
0.00.038.993 I print_info: model params     = 1.41 B
0.00.038.993 I print_info: general.name     = 1.4B
0.00.038.998 I print_info: vocab type       = BPE
0.00.038.999 I print_info: n_vocab          = 50304
0.00.038.999 I print_info: n_merges         = 50009
0.00.038.999 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.000 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.000 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.000 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.000 I print_info: LF token         = 187 'Ċ'
0.00.039.000 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.001 I print_info: max token length = 1024
0.00.039.001 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.545.217 I load_tensors: offloading 24 repeating layers to GPU
0.00.545.231 I load_tensors: offloading output layer to GPU
0.00.545.232 I load_tensors: offloaded 25/25 layers to GPU
0.00.545.267 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.545.269 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.546.837 I llama_init_from_model: n_seq_max     = 1
0.00.546.839 I llama_init_from_model: n_ctx         = 128
0.00.546.840 I llama_init_from_model: n_ctx_per_seq = 128
0.00.546.841 I llama_init_from_model: n_batch       = 128
0.00.546.841 I llama_init_from_model: n_ubatch      = 128
0.00.546.842 I llama_init_from_model: flash_attn    = 0
0.00.546.844 I llama_init_from_model: freq_base     = 10000.0
0.00.546.845 I llama_init_from_model: freq_scale    = 1
0.00.546.845 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.546.847 I ggml_metal_init: allocating
0.00.546.921 I ggml_metal_init: found device: Apple M4
0.00.546.935 I ggml_metal_init: picking default device: Apple M4
0.00.548.748 I ggml_metal_init: using embedded metal library
0.00.555.319 I ggml_metal_init: GPU name:   Apple M4
0.00.555.325 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.555.326 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.555.327 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.555.328 I ggml_metal_init: simdgroup reduction   = true
0.00.555.328 I ggml_metal_init: simdgroup matrix mul. = true
0.00.555.328 I ggml_metal_init: has residency sets    = true
0.00.555.329 I ggml_metal_init: has bfloat            = true
0.00.555.329 I ggml_metal_init: use bfloat            = true
0.00.555.330 I ggml_metal_init: hasUnifiedMemory      = true
0.00.555.332 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.574.219 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.577.765 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.577.772 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.577.814 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.581.261 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.581.263 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.581.263 I llama_init_from_model: graph nodes  = 967
0.00.581.264 I llama_init_from_model: graph splits = 2
0.00.581.267 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.581.267 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.613.452 I 
0.00.613.506 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.613.512 I perplexity: tokenizing the input ..
0.00.618.968 I perplexity: tokenization took 5.455 ms
0.00.618.975 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.752.802 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.754.117 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.754.139 I llama_perf_context_print:        load time =     603.60 ms
0.00.754.140 I llama_perf_context_print: prompt eval time =     133.59 ms /   128 tokens (    1.04 ms per token,   958.15 tokens per second)
0.00.754.140 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.754.141 I llama_perf_context_print:       total time =     140.69 ms /   129 tokens
0.00.754.467 I ggml_metal_free: deallocating

real	0m0.770s
user	0m0.077s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.608 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.697 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.702 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.708 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.708 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.709 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.709 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.709 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.710 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.711 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.712 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.713 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.713 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.713 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.714 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.715 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.715 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.715 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.688 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.761 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.759 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.762 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.762 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.763 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.763 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.763 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.764 I llama_model_loader: - type  f32:  194 tensors
0.00.024.764 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.764 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.765 I print_info: file format = GGUF V3 (latest)
0.00.024.766 I print_info: file type   = Q5_K - Medium
0.00.024.767 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.592 I load: special tokens cache size = 25
0.00.038.631 I load: token to piece cache size = 0.2984 MB
0.00.038.635 I print_info: arch             = gptneox
0.00.038.636 I print_info: vocab_only       = 0
0.00.038.636 I print_info: n_ctx_train      = 2048
0.00.038.636 I print_info: n_embd           = 2048
0.00.038.636 I print_info: n_layer          = 24
0.00.038.641 I print_info: n_head           = 16
0.00.038.642 I print_info: n_head_kv        = 16
0.00.038.642 I print_info: n_rot            = 32
0.00.038.642 I print_info: n_swa            = 0
0.00.038.642 I print_info: n_embd_head_k    = 128
0.00.038.642 I print_info: n_embd_head_v    = 128
0.00.038.643 I print_info: n_gqa            = 1
0.00.038.644 I print_info: n_embd_k_gqa     = 2048
0.00.038.645 I print_info: n_embd_v_gqa     = 2048
0.00.038.645 I print_info: f_norm_eps       = 1.0e-05
0.00.038.646 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.647 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.647 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.647 I print_info: f_logit_scale    = 0.0e+00
0.00.038.648 I print_info: n_ff             = 8192
0.00.038.648 I print_info: n_expert         = 0
0.00.038.648 I print_info: n_expert_used    = 0
0.00.038.648 I print_info: causal attn      = 1
0.00.038.648 I print_info: pooling type     = 0
0.00.038.649 I print_info: rope type        = 2
0.00.038.649 I print_info: rope scaling     = linear
0.00.038.649 I print_info: freq_base_train  = 10000.0
0.00.038.649 I print_info: freq_scale_train = 1
0.00.038.650 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.650 I print_info: rope_finetuned   = unknown
0.00.038.650 I print_info: ssm_d_conv       = 0
0.00.038.650 I print_info: ssm_d_inner      = 0
0.00.038.650 I print_info: ssm_d_state      = 0
0.00.038.650 I print_info: ssm_dt_rank      = 0
0.00.038.650 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.651 I print_info: model type       = 1.4B
0.00.038.651 I print_info: model params     = 1.41 B
0.00.038.651 I print_info: general.name     = 1.4B
0.00.038.652 I print_info: vocab type       = BPE
0.00.038.652 I print_info: n_vocab          = 50304
0.00.038.652 I print_info: n_merges         = 50009
0.00.038.652 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.652 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.653 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.653 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.653 I print_info: LF token         = 187 'Ċ'
0.00.038.653 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.653 I print_info: max token length = 1024
0.00.038.654 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.586.418 I load_tensors: offloading 24 repeating layers to GPU
0.00.586.426 I load_tensors: offloading output layer to GPU
0.00.586.426 I load_tensors: offloaded 25/25 layers to GPU
0.00.586.447 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.586.448 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.587.370 I llama_init_from_model: n_seq_max     = 1
0.00.587.373 I llama_init_from_model: n_ctx         = 128
0.00.587.374 I llama_init_from_model: n_ctx_per_seq = 128
0.00.587.374 I llama_init_from_model: n_batch       = 128
0.00.587.375 I llama_init_from_model: n_ubatch      = 128
0.00.587.375 I llama_init_from_model: flash_attn    = 0
0.00.587.376 I llama_init_from_model: freq_base     = 10000.0
0.00.587.377 I llama_init_from_model: freq_scale    = 1
0.00.587.377 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.587.381 I ggml_metal_init: allocating
0.00.587.448 I ggml_metal_init: found device: Apple M4
0.00.587.460 I ggml_metal_init: picking default device: Apple M4
0.00.588.546 I ggml_metal_init: using embedded metal library
0.00.592.608 I ggml_metal_init: GPU name:   Apple M4
0.00.592.616 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.592.617 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.592.618 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.592.618 I ggml_metal_init: simdgroup reduction   = true
0.00.592.618 I ggml_metal_init: simdgroup matrix mul. = true
0.00.592.619 I ggml_metal_init: has residency sets    = true
0.00.592.619 I ggml_metal_init: has bfloat            = true
0.00.592.619 I ggml_metal_init: use bfloat            = true
0.00.592.621 I ggml_metal_init: hasUnifiedMemory      = true
0.00.592.623 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.605.907 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.607.470 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.607.472 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.607.498 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.609.088 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.609.089 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.609.089 I llama_init_from_model: graph nodes  = 967
0.00.609.090 I llama_init_from_model: graph splits = 2
0.00.609.091 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.609.091 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.637.060 I 
0.00.637.093 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.637.099 I perplexity: tokenizing the input ..
0.00.641.088 I perplexity: tokenization took 3.987 ms
0.00.641.093 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.780.435 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.781.578 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.781.597 I llama_perf_context_print:        load time =     628.44 ms
0.00.781.598 I llama_perf_context_print: prompt eval time =     139.11 ms /   128 tokens (    1.09 ms per token,   920.14 tokens per second)
0.00.781.599 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.781.601 I llama_perf_context_print:       total time =     144.54 ms /   129 tokens
0.00.781.947 I ggml_metal_free: deallocating

real	0m0.795s
user	0m0.068s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.718 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.141 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.146 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.151 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.152 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.152 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.153 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.153 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.154 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.154 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.155 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.155 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.156 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.156 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.156 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.158 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.158 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.159 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.971 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.941 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.776 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.777 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.777 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.778 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.778 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.778 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.778 I llama_model_loader: - type  f32:  194 tensors
0.00.023.779 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.779 I print_info: file format = GGUF V3 (latest)
0.00.023.780 I print_info: file type   = Q6_K
0.00.023.780 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.580 I load: special tokens cache size = 25
0.00.037.394 I load: token to piece cache size = 0.2984 MB
0.00.037.396 I print_info: arch             = gptneox
0.00.037.397 I print_info: vocab_only       = 0
0.00.037.397 I print_info: n_ctx_train      = 2048
0.00.037.397 I print_info: n_embd           = 2048
0.00.037.397 I print_info: n_layer          = 24
0.00.037.400 I print_info: n_head           = 16
0.00.037.400 I print_info: n_head_kv        = 16
0.00.037.401 I print_info: n_rot            = 32
0.00.037.401 I print_info: n_swa            = 0
0.00.037.401 I print_info: n_embd_head_k    = 128
0.00.037.401 I print_info: n_embd_head_v    = 128
0.00.037.403 I print_info: n_gqa            = 1
0.00.037.404 I print_info: n_embd_k_gqa     = 2048
0.00.037.405 I print_info: n_embd_v_gqa     = 2048
0.00.037.406 I print_info: f_norm_eps       = 1.0e-05
0.00.037.406 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.407 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.407 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.407 I print_info: f_logit_scale    = 0.0e+00
0.00.037.409 I print_info: n_ff             = 8192
0.00.037.409 I print_info: n_expert         = 0
0.00.037.409 I print_info: n_expert_used    = 0
0.00.037.410 I print_info: causal attn      = 1
0.00.037.410 I print_info: pooling type     = 0
0.00.037.410 I print_info: rope type        = 2
0.00.037.410 I print_info: rope scaling     = linear
0.00.037.410 I print_info: freq_base_train  = 10000.0
0.00.037.412 I print_info: freq_scale_train = 1
0.00.037.412 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.412 I print_info: rope_finetuned   = unknown
0.00.037.412 I print_info: ssm_d_conv       = 0
0.00.037.412 I print_info: ssm_d_inner      = 0
0.00.037.413 I print_info: ssm_d_state      = 0
0.00.037.414 I print_info: ssm_dt_rank      = 0
0.00.037.414 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.414 I print_info: model type       = 1.4B
0.00.037.414 I print_info: model params     = 1.41 B
0.00.037.415 I print_info: general.name     = 1.4B
0.00.037.415 I print_info: vocab type       = BPE
0.00.037.415 I print_info: n_vocab          = 50304
0.00.037.415 I print_info: n_merges         = 50009
0.00.037.416 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.416 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.416 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.416 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.417 I print_info: LF token         = 187 'Ċ'
0.00.037.417 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.417 I print_info: max token length = 1024
0.00.037.417 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.620.584 I load_tensors: offloading 24 repeating layers to GPU
0.00.620.590 I load_tensors: offloading output layer to GPU
0.00.620.591 I load_tensors: offloaded 25/25 layers to GPU
0.00.620.616 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.620.619 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.622.123 I llama_init_from_model: n_seq_max     = 1
0.00.622.125 I llama_init_from_model: n_ctx         = 128
0.00.622.125 I llama_init_from_model: n_ctx_per_seq = 128
0.00.622.126 I llama_init_from_model: n_batch       = 128
0.00.622.126 I llama_init_from_model: n_ubatch      = 128
0.00.622.126 I llama_init_from_model: flash_attn    = 0
0.00.622.127 I llama_init_from_model: freq_base     = 10000.0
0.00.622.128 I llama_init_from_model: freq_scale    = 1
0.00.622.128 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.622.130 I ggml_metal_init: allocating
0.00.622.147 I ggml_metal_init: found device: Apple M4
0.00.622.155 I ggml_metal_init: picking default device: Apple M4
0.00.623.449 I ggml_metal_init: using embedded metal library
0.00.629.118 I ggml_metal_init: GPU name:   Apple M4
0.00.629.122 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.629.122 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.629.123 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.629.124 I ggml_metal_init: simdgroup reduction   = true
0.00.629.124 I ggml_metal_init: simdgroup matrix mul. = true
0.00.629.124 I ggml_metal_init: has residency sets    = true
0.00.629.124 I ggml_metal_init: has bfloat            = true
0.00.629.125 I ggml_metal_init: use bfloat            = true
0.00.629.126 I ggml_metal_init: hasUnifiedMemory      = true
0.00.629.127 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.644.987 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.648.453 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.648.457 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.648.498 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.651.658 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.651.659 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.651.660 I llama_init_from_model: graph nodes  = 967
0.00.651.660 I llama_init_from_model: graph splits = 2
0.00.651.663 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.651.663 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.703 I 
0.00.689.769 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.777 I perplexity: tokenizing the input ..
0.00.697.179 I perplexity: tokenization took 7.399 ms
0.00.697.187 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.838.800 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.840.127 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.840.150 I llama_perf_context_print:        load time =     680.97 ms
0.00.840.151 I llama_perf_context_print: prompt eval time =     140.64 ms /   128 tokens (    1.10 ms per token,   910.14 tokens per second)
0.00.840.152 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.840.152 I llama_perf_context_print:       total time =     150.45 ms /   129 tokens
0.00.840.526 I ggml_metal_free: deallocating

real	0m0.854s
user	0m0.077s
sys	0m0.155s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.272 I build: 4716 (dbc2ec59) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.439 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.431 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.437 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.440 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.441 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.441 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.442 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.442 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.444 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.444 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.445 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.445 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.446 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.447 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.447 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.450 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.450 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.451 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.653 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.476 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.644 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.646 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.646 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.647 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.647 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.648 I llama_model_loader: - type  f32:  194 tensors
0.00.051.648 I llama_model_loader: - type  f16:   98 tensors
0.00.051.649 I print_info: file format = GGUF V3 (latest)
0.00.051.649 I print_info: file type   = all F32 (guessed)
0.00.051.650 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.063.515 I load: special tokens cache size = 25
0.00.071.199 I load: token to piece cache size = 0.2984 MB
0.00.071.202 I print_info: arch             = gptneox
0.00.071.202 I print_info: vocab_only       = 0
0.00.071.203 I print_info: n_ctx_train      = 2048
0.00.071.203 I print_info: n_embd           = 2048
0.00.071.203 I print_info: n_layer          = 24
0.00.071.207 I print_info: n_head           = 16
0.00.071.207 I print_info: n_head_kv        = 16
0.00.071.208 I print_info: n_rot            = 32
0.00.071.210 I print_info: n_swa            = 0
0.00.071.211 I print_info: n_embd_head_k    = 128
0.00.071.211 I print_info: n_embd_head_v    = 128
0.00.071.211 I print_info: n_gqa            = 1
0.00.071.212 I print_info: n_embd_k_gqa     = 2048
0.00.071.213 I print_info: n_embd_v_gqa     = 2048
0.00.071.215 I print_info: f_norm_eps       = 1.0e-05
0.00.071.215 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.071.215 I print_info: f_clamp_kqv      = 0.0e+00
0.00.071.216 I print_info: f_max_alibi_bias = 0.0e+00
0.00.071.216 I print_info: f_logit_scale    = 0.0e+00
0.00.071.216 I print_info: n_ff             = 8192
0.00.071.217 I print_info: n_expert         = 0
0.00.071.217 I print_info: n_expert_used    = 0
0.00.071.217 I print_info: causal attn      = 1
0.00.071.217 I print_info: pooling type     = 0
0.00.071.217 I print_info: rope type        = 2
0.00.071.217 I print_info: rope scaling     = linear
0.00.071.218 I print_info: freq_base_train  = 10000.0
0.00.071.218 I print_info: freq_scale_train = 1
0.00.071.218 I print_info: n_ctx_orig_yarn  = 2048
0.00.071.220 I print_info: rope_finetuned   = unknown
0.00.071.220 I print_info: ssm_d_conv       = 0
0.00.071.220 I print_info: ssm_d_inner      = 0
0.00.071.220 I print_info: ssm_d_state      = 0
0.00.071.220 I print_info: ssm_dt_rank      = 0
0.00.071.220 I print_info: ssm_dt_b_c_rms   = 0
0.00.071.220 I print_info: model type       = 1.4B
0.00.071.221 I print_info: model params     = 1.41 B
0.00.071.221 I print_info: general.name     = 1.4B
0.00.071.222 I print_info: vocab type       = BPE
0.00.071.222 I print_info: n_vocab          = 50304
0.00.071.222 I print_info: n_merges         = 50009
0.00.071.222 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.071.222 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.071.223 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.071.223 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.071.223 I print_info: LF token         = 187 'Ċ'
0.00.071.223 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.071.224 I print_info: max token length = 1024
0.00.071.224 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.314.535 I load_tensors: offloading 24 repeating layers to GPU
0.01.314.539 I load_tensors: offloading output layer to GPU
0.01.314.539 I load_tensors: offloaded 25/25 layers to GPU
0.01.314.563 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.314.564 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.315.583 I llama_init_from_model: n_seq_max     = 1
0.01.315.585 I llama_init_from_model: n_ctx         = 128
0.01.315.585 I llama_init_from_model: n_ctx_per_seq = 128
0.01.315.585 I llama_init_from_model: n_batch       = 128
0.01.315.585 I llama_init_from_model: n_ubatch      = 128
0.01.315.586 I llama_init_from_model: flash_attn    = 0
0.01.315.586 I llama_init_from_model: freq_base     = 10000.0
0.01.315.587 I llama_init_from_model: freq_scale    = 1
0.01.315.587 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.315.588 I ggml_metal_init: allocating
0.01.315.621 I ggml_metal_init: found device: Apple M4
0.01.315.628 I ggml_metal_init: picking default device: Apple M4
0.01.316.677 I ggml_metal_init: using embedded metal library
0.01.320.447 I ggml_metal_init: GPU name:   Apple M4
0.01.320.450 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.320.450 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.320.451 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.320.452 I ggml_metal_init: simdgroup reduction   = true
0.01.320.452 I ggml_metal_init: simdgroup matrix mul. = true
0.01.320.452 I ggml_metal_init: has residency sets    = true
0.01.320.452 I ggml_metal_init: has bfloat            = true
0.01.320.452 I ggml_metal_init: use bfloat            = true
0.01.320.453 I ggml_metal_init: hasUnifiedMemory      = true
0.01.320.454 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.331.040 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.332.705 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.332.708 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.332.746 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.334.441 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.334.442 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.334.442 I llama_init_from_model: graph nodes  = 967
0.01.334.443 I llama_init_from_model: graph splits = 2
0.01.334.444 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.334.444 I 
0.01.334.472 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.334.473 I compute_imatrix: tokenizing the input ..
0.01.338.501 I compute_imatrix: tokenization took 4.027 ms
0.01.338.503 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.606.301 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.609.001 I llama_perf_context_print:        load time =    1585.85 ms
0.01.609.002 I llama_perf_context_print: prompt eval time =     266.02 ms /   128 tokens (    2.08 ms per token,   481.16 tokens per second)
0.01.609.003 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.609.003 I llama_perf_context_print:       total time =    1588.54 ms /   129 tokens
0.01.609.579 I ggml_metal_free: deallocating

real	0m1.794s
user	0m0.123s
sys	0m0.248s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4716 (dbc2ec59)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125e04e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125e085e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125e08a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125e08ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125e09330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125e097a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125e09c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125e0a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125e0a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125e0a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125e0add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125e0b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125e0bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x125e0c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125e0cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x125e0d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x125e0dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x125e0e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x125e0ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125e0f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x125e0fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x125e101e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125e10900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x125e111a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x125e118c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125e11b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125e11e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125e122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125e129d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125e12e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125e13400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x125e13910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125e13d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125e14040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125e144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125e14920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125e14d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125e15200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125e15670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125e15ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125e15f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125e163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125e16830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125e16ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125e17110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125e17580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125e179f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125e17e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125e185f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125e18a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x125e18ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125e19340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x125e197b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x125e19c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125e1a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125e1a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125e1ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125e1af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125e1b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125e1ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125e1be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125e1c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125e1c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125e1cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125e1d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125e1d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125e1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125e1df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125e1e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125e1e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x125e1ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125e1f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125e1f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x125e1fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x125e202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x125e20870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x125e20e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x125e213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x125e21980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x125e21f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x125e224e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x125e22a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x125e23040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x125e235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x125e23ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x125e24150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x125e24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x125e24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x125e25260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x125e25810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x125e25dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x125e26370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125e26920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125e26ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125e27480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x125e27a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125e27fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x125e18120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x125e28740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x125e28bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x125e29020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x125e295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x125e29b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x125e2a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x125e2a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x125e2ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x125e2b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x125e2b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x125e2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x125e2c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x125e2c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x125e2ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x125e2d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125e2da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x125e2df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x125e2e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125e2e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x125e2ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125e2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125e2f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x125e2fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125e30210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125e30710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125e30c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125e31110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125e31610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125e31b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125e32010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x125e32510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125e32a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125e32f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x125e33410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125e33910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125e33e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125e34310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x125e34810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125e34d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125e35210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x125e35710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125e35c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125e36110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x125e36610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125e36b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125e37010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x125e37510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125e37a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125e37f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125e38410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125e38910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125e38e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125e39310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125e39810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125e39d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125e3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125e3a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125e3ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x125e3b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125e3b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125e3bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125e3c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125e3c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125e3ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125e3cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x125e3d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125e3d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125e3de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125e3e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125e3e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125e3ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125e3f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x125e3f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125e3fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125e40110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125e40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125e40b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125e41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125e41510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125e41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125e41f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125e42410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125e42910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x125e42e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125e43310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125e43810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x125e43d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125e44210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125e44710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125e44c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125e45110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125e45610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125e45b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125e46010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125e46510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125e46a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125e46fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125e47570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125e47b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125e480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125e486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x125e48cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125e49300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x125e49af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x125e49f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x125e4a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125e4a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x125e4ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125e4b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125e4bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125e4bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125e4c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125e4cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125e4d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125e4d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125e4dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125e4e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125e4e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125e4ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125e4f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125e4f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125e4fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125e50110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125e50660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125e50bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125e51100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125e51650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125e51ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125e520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125e52640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125e52b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125e530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125e53630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125e53b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125e540d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125e54620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125e54b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125e550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125e55610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125e55b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125e560b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125e56600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125e56b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x125e570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125e575f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x125e57b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125e58090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125e585e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x125e58b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125e59080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125e595d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125e59b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125e5a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125e5a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125e5ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125e5b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125e5b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125e5bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125e5c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x125e5c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x125e5caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125e5d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125e5d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125e5dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125e5e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125e5e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125e5ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125e5f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125e5f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x125e5fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x125e5feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125e60350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125e607f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125e60c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125e61130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125e615d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125e61a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125e61f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125e623b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125e62850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125e62cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125e63190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125e63630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125e63ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125e64020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125e64740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125e64e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125e65580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125e65ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125e65f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125e66750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125e66a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125e67020 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.747.180 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.747.184 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125e20b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125e20580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125e25ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125e1ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125e27cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125e25520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125e2cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125e2c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125e2c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125e27740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125e221f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125e2a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125e47280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x125e27190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125e21c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x125e24f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x125e238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x125e29e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x125e46cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x125e2bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x125e26be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x125e21690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125e249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x125e23300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x125e29890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125e2b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125e26630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125e210e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125e24410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125e292e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x125e2af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x125e26080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125e23e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125e2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125e66cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125e48390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125e48fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125e4ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125e10e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125e0b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125e047e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125e1b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125e66220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125e1a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125e282a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125e4b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125e495c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125e12570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x125e67480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125e67740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x125e67a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125e67cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x125e67f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x125e68240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125e68500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125e687c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125e68a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125e68d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125e69000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125e692c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125e69580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125e69840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125e69b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125e69dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125e6a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125e6a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125e6a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125e6a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125e6ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125e6ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x125e6b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125e6b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125e6b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x125e6b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x125e6bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x125e6bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x125e6c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x125e6c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x125e6c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x125e6c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x125e6cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x125e6cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x125e6d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x125e6d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x125e6d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x125e6da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x125e6dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x125e6dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x125e6e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x125e6e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x125e6e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x125e6eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125e6ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125e6f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125e6f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x125e6f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125e6f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x125e6fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x125e6fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x125e700c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x125e70380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x125e70640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x125e70900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x125e70bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x125e70e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x125e71140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x125e71400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x125e716c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x125e71980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x125e71c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x125e71f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x125e721c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x125e72480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125e72740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x125e72a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x125e72cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125e72f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x125e73240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125e73500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125e737c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x125e73a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125e73d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125e74000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125e742c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125e74580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125e74840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125e74b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125e74dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x125e75080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125e75340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125e75600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x125e758c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125e75b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x125e75e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125e76100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x125e763c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125e76680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125e76940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x125e76c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125e76ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125e77180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x125e77440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125e77700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125e779c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x125e77c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125e77f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125e78200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x125e784c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125e78780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125e78a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125e78d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125e78fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125e79280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125e79540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125e79800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125e79ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x125e79d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125e7a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125e7a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125e7a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125e7a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125e7ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125e7ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x125e7b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125e7b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125e7b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125e7b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125e7bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125e7be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125e7c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x125e7c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125e7c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125e7c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125e7cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125e7cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125e7d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125e7d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125e7d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125e7da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125e7dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125e7df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x125e7e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125e7e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125e7e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x125e7ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125e7ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125e7f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125e7f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125e7f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x125e7f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125e7fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125e7fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x125e80080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125e80340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125e80600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x125e808c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125e80b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125e80e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125e81100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x125e813c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125e81680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x125e81940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x125e81c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x125e81ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125e82180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x125e82440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125e82700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125e829c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125e82c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125e82f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125e83510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125e837d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125e83a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125e83d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125e84010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125e842d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125e84820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125e84d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125e852c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125e85810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125e85d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125e862b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125e86800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125e86d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125e872a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125e877f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125e87d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125e88290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125e887e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125e88d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125e89280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125e897d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125e89d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125e8a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125e8a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125e8ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125e8b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125e8b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125e8bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125e8c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125e8c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x125e8ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125e8d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x125e8d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125e8dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125e8e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x125e8e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125e8ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125e8f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125e8f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125e8fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125e90210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125e90760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125e90cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125e91200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x125e91750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x125e91ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x125e921f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x125e92740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x125e92c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x125e931e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125e93730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x125e93c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125e941d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125e94720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125e94c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125e951c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x125e95660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x125e95b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125e95fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125e96440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125e968e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125e96d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125e97220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125e976c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125e97b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125e98000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125e984a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125e98940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125e98de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125e99280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125e99720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125e99c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125e9a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125e9aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125e9b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125e9b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125e9bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125e9c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125e9c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125e9cc70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x116f04640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x116f04ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x116f04f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x116f05390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x116f05800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x116f05c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x116f060e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x116f06550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x116f069c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x116f06e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x116f072a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x116f07920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x116f08440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x116f08bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x116f09400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x116f09b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x116f0a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x116f0a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x116f0b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x116f0b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x116f0bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x116f0c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x116f0cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x116f0d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x116f0dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x116f0deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x116f0e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x116f0e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x116f0ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x116f0eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x116f0f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x116f0f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x116f0fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x116f0ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x116f10400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x116f10870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x116f10ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x116f11150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x116f115c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x116f11a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x116f11ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x116f12310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x116f12780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x116f12bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x116f13060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x116f134d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x116f13940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x116f13db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x116f14220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x116f14690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x116f14b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x116f14f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x116f153e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x116f15850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x116f15cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x116f16130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x116f166a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x116f16ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x116f17010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x116f17480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x116f178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x116f17d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x116f181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x116f18640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x116f18ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x116f18f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x116f19390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x116f19800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x116f19c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x116f1a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x116f1a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x116f1a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x116f1ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x116f1b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x116f1b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x116f1bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x116f1bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x116f1c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x116f1c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x116f1cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x116f1d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x116f1d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x116f1da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x116f1df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x116f1e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x116f1e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x116f1ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x116f1f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x116f1f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x116f1f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x116f1fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x116f20280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x116f206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x116f20b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x116f20fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x116f21440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x116f218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x116f21d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x116f22190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x116f22600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x116f22a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x116f22ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x116f23350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x116f23be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x116f23ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x116f24310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x116f24780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x116f24bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x116f25060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x116f254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x116f25940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x116f25db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x116f26220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x116f26690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x116f26b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x116f26f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x116f273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x116f27850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x116f27cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x116f28130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x116f285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x116f28a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x116f28e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x116f292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x116f29760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x116f29bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x116f2a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x116f2a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x116f2a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x116f2ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x116f2b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x116f2b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x116f2bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x116f2bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x116f2c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x116f2c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x116f2cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x116f2d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x116f2d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x116f2d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x116f2de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x116f2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x116f2e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x116f2ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x116f2f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x116f2f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x116f2f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x116f2fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x116f301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x116f30650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x116f30ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x116f30f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x116f313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x116f31810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x116f31c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x116f320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x116f32560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x116f329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x116f32e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x116f332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x116f33720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x116f33b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x116f34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x116f34470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x116f348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x116f34d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x116f351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x116f35630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x116f35aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x116f35f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x116f36380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x116f367f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x116f36c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x116f370d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x116f37540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x116f379b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x116f37e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x116f38290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x116f38700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x116f38b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x116f38fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x116f39450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x116f398c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x116f39d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x116f3a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x116f3a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x116f3aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x116f3aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x116f3b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x116f3b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x116f3bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x116f3c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x116f3c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x116f3c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x116f3ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x116f3d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x116f3d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x116f3db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x116f3dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x116f3e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x116f3e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x116f3ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x116f3f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x116f3f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x116f3fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x116f3fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x116f40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x116f407b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x116f40c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x116f41090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x116f41c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x116f41ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x116f42190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x116f42600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x116f42a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x116f42ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x116f43350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x116f437c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x116f43c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x116f440a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x116f44510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x116f44980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x116f44df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x116f45260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x116f456d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x116f45b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x116f45fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x116f46420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x116f46890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x116f46d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x116f47170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x116f475e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x116f47a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x116f47ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x116f48330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x116f487a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x116f48c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x116f49080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x116f494f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x116f49960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x116f49dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x116f4a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x116f4a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x116f4ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x116f4af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x116f4b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x116f4b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x116f4bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x116f4c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x116f4c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x116f4ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x116f4cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x116f4d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x116f4d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x116f4dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x116f4e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x116f4e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x116f4e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x116f4edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x116f4f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x116f4f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x116f4fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x116f4ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x116f503e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x116f50850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x116f50cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x116f51130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x116f515a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x116f51a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x116f51e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x116f522f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x116f52760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x116f52bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x116f53040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x116f534b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x116f53920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x116f53d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x116f54200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x116f54670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x116f54ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x116f54f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x116f553c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x116f55830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x116f562a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x116f569c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x116f570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x116f57800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x116f57ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x116f57f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x116f58530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x116f58b40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.792s
user	0m0.279s
sys	0m0.318s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4716 (dbc2ec59)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12b70d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12b70dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12b70e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12b70e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12b70ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12b70f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12b70f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12b70fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12b7103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12b7108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12b710db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12b7112b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12b711dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12b712580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12b712d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12b7134b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12b713bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12b7142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12b714a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12b7151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12b715900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12b716020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12b716740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12b716fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12b717700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12b7179c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12b717fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12b718c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12b719180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12b719440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12b7198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12b719ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12b71a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12b71a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12b71ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12b71b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12b71b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12b71ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12b71beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12b71c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12b71c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12b71cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12b71d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12b71d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12b71d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12b71dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12b71e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12b71edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12b71f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12b71f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12b720000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12b720610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12b720c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12b721230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12b721a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12b721ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12b722360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12b722620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12b722c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12b723420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12b7236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12b723b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12b724020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12b7244c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12b724960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12b724e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12b7252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12b725740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12b725be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12b726080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12b726520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12b7269c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12b726e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12b7273b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12b727900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12b727e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12b7283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12b7288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12b728e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12b729390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12b7298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12b729e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12b72a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12b72a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12b72ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12b72b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12b72b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12b72be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12b72c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12b72c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12b72ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12b72d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12b72d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12b72ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12b72e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12b72e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12b72ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12b71eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12b72f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12b72fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12b72ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12b7304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12b7309f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12b730f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12b731490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12b7319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12b731f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12b732480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12b7329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12b732f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12b733470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12b7339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12b733f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12b7343b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12b734850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12b734cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12b735190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12b735630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12b735ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12b735f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12b736410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12b7368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12b736d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12b7371f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12b737690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12b737b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12b737fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12b738470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12b738910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12b738db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12b739250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12b7396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12b739b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12b73a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12b73a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12b73a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12b73ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12b73b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12b73b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12b73bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12b73c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12b73c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12b73c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12b73ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12b73d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12b73d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12b73dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12b73e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12b73e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12b73ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12b73eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12b73f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12b73f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12b73fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12b740150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12b7405f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12b740a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12b740f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12b7413d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12b741870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12b741d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12b7421b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12b742650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12b742af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12b742f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12b743430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12b7438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12b743d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12b744210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12b7446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12b744b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12b744ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12b745490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12b745930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12b745dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12b746270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12b746710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12b746bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12b747050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12b7474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12b747990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12b747e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12b7482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12b748770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12b748c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12b7490b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12b749550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12b7499f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12b749e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12b74a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12b74a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12b74ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12b74b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12b74b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12b74bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12b74c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12b74c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12b74c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12b74cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12b74d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12b74db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12b74e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12b74e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12b74ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12b74f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12b74f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12b74fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12b750340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12b7507e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12b750c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12b751430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12b751980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12b751ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12b752420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12b752970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12b752ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12b753410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12b753960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12b753eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12b754400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12b754950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12b754ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12b7553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12b755940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12b755e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12b7563e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12b756930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12b756e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12b7573d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12b757920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12b757e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12b7583c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12b758910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12b758e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12b7593b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12b759900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12b759e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12b75a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12b75a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12b75ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12b75b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12b75b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12b75be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12b75c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12b75c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12b75ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12b75d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12b75d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12b75de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12b75e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12b75e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12b75ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12b75f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12b75f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12b75fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12b760340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12b760890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12b760de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12b761330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12b761880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12b761dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12b762320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12b762870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12b762dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12b763310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12b763860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12b763db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12b764250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12b7646f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12b764b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12b765030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12b7654d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12b765970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12b765e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12b7662b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12b766750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12b766bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12b767090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12b767530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12b7679d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12b767e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12b768310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12b768860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12b768f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12b7696a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12b769dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12b76a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12b76a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12b76af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12b76b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12b76b860 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.099.918 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.922 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12b76b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12b74d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12b74cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12b74d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12b7208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12b7202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12b7228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12b74f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12b717c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12b71e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12b71f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12b71f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12b71db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12b71fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12b716c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12b722ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12b72f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12b76aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12b719e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12b71a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12b74f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12b74de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12b718290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12b718550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12b718810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12b76bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12b76bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12b76c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12b76c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12b76c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12b76ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12b76cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12b76d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12b76d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12b76d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12b76d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12b76db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12b76ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12b76e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12b76e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12b76e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12b76e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12b76eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12b76ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12b76f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12b76f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12b76f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12b76f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12b76fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12b76fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12b770180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12b770440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12b770700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12b7709c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12b770c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12b770f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12b771200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12b7714c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12b771780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12b771a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12b771d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12b771fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12b772280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12b772540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12b772800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12b772ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12b772d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12b773040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12b773300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12b7735c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12b773880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12b773b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12b773e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12b7740c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12b774380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12b774640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12b774900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12b774bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12b774e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12b775140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12b775400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12b7756c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12b775980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12b775c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12b775f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12b7761c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12b776480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12b776740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12b776a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12b776cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12b776f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12b777240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12b777500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12b7777c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12b777a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12b777d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12b778000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12b7782c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12b778580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12b778840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12b778b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12b778dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12b779080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12b779340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12b779600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12b7798c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12b779b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12b779e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12b77a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12b77a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12b77a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12b77a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12b77ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12b77aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12b77b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12b77b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12b77b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12b77b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12b77bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12b77bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12b77c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12b77c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12b77c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12b77ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12b77cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12b77cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12b77d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12b77d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12b77d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12b77dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12b77dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12b77e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12b77e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12b77e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12b77e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12b77eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12b77ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12b77f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12b77f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12b77f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12b77f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12b77fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12b77fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12b780140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12b780400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12b7806c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12b780980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12b780c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12b780f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12b7811c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12b781480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12b781740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12b781a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12b781cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12b781f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12b782240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12b782500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12b7827c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12b782a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12b782d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12b783000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12b7832c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12b783580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12b783840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12b783b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12b783dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12b784080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12b784340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12b784600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12b7848c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12b784b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12b784e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12b785100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12b7853c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12b785680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12b785940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12b785c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12b785ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12b786180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12b786440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12b786700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12b7869c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12b786c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12b786f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12b787200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12b7874c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12b787780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12b787a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12b787d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12b787fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12b788280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12b788540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12b788800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12b788ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12b788d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12b789040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12b789300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12b7895c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12b789880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12b789b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12b789e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12b78a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12b78a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12b78a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12b78a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12b78ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12b78b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12b78b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12b78ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12b78bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12b78c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12b78c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12b78cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12b78d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12b78d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12b78d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12b78ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12b78e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12b78e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12b78eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12b78ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12b78f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12b78f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12b78fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12b790150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12b7905c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12b790a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12b790ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12b791310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12b791780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12b791bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12b792060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12b7924d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12b792940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12b792db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12b793220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12b793690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12b793b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12b793f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12b7943e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12b794850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12b794cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12b795130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12b7955a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12b795a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12b795e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12b7962f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12b796760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12b796bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12b797040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12b7974b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12b797920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12b797d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12b798200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12b798670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12b798ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12b798f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12b7993c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12b799830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12b799ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12b79a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12b79a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12b79a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12b79ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12b79b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12b79b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12b79bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12b79c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12b79c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12b79c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12b79cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12b79d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12b79d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12b79dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12b79df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12b79e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12b79e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12b79ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12b79f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12b79f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12b79f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12b79fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12b7a02b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12b7a0d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12b7a1440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12b7a1b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12b7a2280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12b7a2540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12b7a2d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12b7a2ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12b7a3600 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12d0044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12d004950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12d004dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12d005230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12d0056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12d005b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12d005f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12d0063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12d006860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12d006cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12d007140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12d007820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12d008340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12d008af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12d009300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12d009a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12d00a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12d00a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12d00af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12d00b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12d00be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12d00c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12d00ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12d00d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12d00daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12d00ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12d00e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12d00e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12d00e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12d00edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12d00f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12d00f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12d00fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12d00fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12d010300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12d010770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12d010be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12d011050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12d0114c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12d011930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12d011da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12d012210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12d012680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12d012af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12d012f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12d0133d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12d013840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12d013cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12d014120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12d014590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12d014a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12d014e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12d0152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12d015750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12d015bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12d016030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12d0165a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12d016aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12d016f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12d017380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12d0177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12d017c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12d0180d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12d018540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12d0189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12d018e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12d019290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12d019700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12d019b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12d019fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12d01a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12d01a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12d01ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12d01b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12d01b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12d01ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12d01bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12d01c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12d01c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12d01cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12d01d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12d01d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12d01d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12d01de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12d01e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12d01e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12d01eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12d01efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12d01f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12d01f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12d01fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12d020180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12d0205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12d020a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12d020ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12d021340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12d0217b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12d021c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12d022090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12d022500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12d022970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12d022de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12d023250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12d023ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12d023da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12d024210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12d024680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12d024af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12d024f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12d0253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12d025840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12d025cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12d026120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12d026590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12d026a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12d026e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12d0272e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12d027750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12d027bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12d028030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12d0284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12d028910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12d028d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12d0291f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12d029660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12d029ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12d029f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12d02a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12d02a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12d02ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12d02b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12d02b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12d02b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12d02be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12d02c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12d02c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12d02cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12d02d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12d02d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12d02d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12d02dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12d02e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12d02e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12d02eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12d02ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12d02f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12d02f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12d02fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12d0300e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12d030550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12d0309c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12d030e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12d0312a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12d031710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12d031b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12d031ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12d032460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12d0328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12d032d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12d0331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12d033620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12d033a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12d033f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12d034370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12d0347e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12d034c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12d0350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12d035530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12d0359a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12d035e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12d036280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12d0366f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12d036b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12d036fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12d037440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12d0378b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12d037d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12d038190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12d038600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12d038a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12d038ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12d039350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12d0397c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12d039c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12d03a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12d03a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12d03a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12d03adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12d03b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12d03b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12d03bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12d03bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12d03c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12d03c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12d03cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12d03d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12d03d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12d03da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12d03dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12d03e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12d03e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12d03ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12d03f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12d03f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12d03f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12d03fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12d040240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12d0406b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12d040b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12d040f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12d041b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12d041dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12d042090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12d042500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12d042970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12d042de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12d043250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12d0436c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12d043b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12d043fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12d044410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12d044880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12d044cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12d045160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12d0455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12d045a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12d045eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12d046320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12d046790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12d046c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12d047070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12d0474e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12d047950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12d047dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12d048230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12d0486a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12d048b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12d048f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12d0493f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12d049860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12d049cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12d04a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12d04a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12d04aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12d04ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12d04b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12d04b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12d04bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12d04c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12d04c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12d04c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12d04cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12d04d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12d04d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12d04daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12d04df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12d04e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12d04e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12d04ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12d04f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12d04f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12d04fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12d04fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12d0502e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12d050750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12d050bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12d051030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12d0514a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12d051910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12d051d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12d0521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12d052660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12d052ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12d052f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12d0533b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12d053820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12d053c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12d054100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12d054570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12d0549e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12d054e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12d0552c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12d055730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12d0561a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12d0568c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12d056fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12d057700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12d0579c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12d057e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12d058430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12d058a40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.956s
user	0m0.230s
sys	0m0.190s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
