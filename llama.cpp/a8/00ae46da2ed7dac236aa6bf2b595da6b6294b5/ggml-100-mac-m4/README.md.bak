### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.22 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.08 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.21 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.21 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.25 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.57 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.25 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.04 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.21 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.29 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.90 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.91 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  103.76 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.84 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.77 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.23 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 164.33 sec*proc (29 tests)

Total Test time (real) = 164.34 sec

real	2m44.475s
user	4m36.737s
sys	0m5.805s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.13 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.21 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.13 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.88 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.20 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.75 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.17 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.35 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.22 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.44 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.47 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.27 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.07 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.22 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.23 sec*proc (29 tests)

Total Test time (real) =  48.24 sec

real	0m48.254s
user	0m54.740s
sys	0m5.149s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.118 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.161 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.341 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.023.350 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.353 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.023.354 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.355 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.023.356 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.023.356 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.023.358 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.023.359 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.023.360 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.023.360 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.023.361 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.023.365 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.023.365 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.023.366 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.023.367 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.023.368 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.023.369 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.023.369 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.028.711 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.030.079 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.082 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.030.082 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.030.083 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.030.084 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.030.084 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.030.085 I llama_model_loader: - type  f32:  124 tensors
0.00.030.085 I llama_model_loader: - type  f16:   73 tensors
0.00.030.086 I print_info: file format = GGUF V3 (latest)
0.00.030.087 I print_info: file type   = F16
0.00.030.088 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.033.039 I load: special tokens cache size = 5
0.00.034.349 I load: token to piece cache size = 0.2032 MB
0.00.034.352 I print_info: arch             = bert
0.00.034.353 I print_info: vocab_only       = 0
0.00.034.353 I print_info: n_ctx_train      = 512
0.00.034.353 I print_info: n_embd           = 384
0.00.034.353 I print_info: n_layer          = 12
0.00.034.355 I print_info: n_head           = 12
0.00.034.356 I print_info: n_head_kv        = 12
0.00.034.356 I print_info: n_rot            = 32
0.00.034.356 I print_info: n_swa            = 0
0.00.034.356 I print_info: n_embd_head_k    = 32
0.00.034.356 I print_info: n_embd_head_v    = 32
0.00.034.357 I print_info: n_gqa            = 1
0.00.034.357 I print_info: n_embd_k_gqa     = 384
0.00.034.360 I print_info: n_embd_v_gqa     = 384
0.00.034.361 I print_info: f_norm_eps       = 1.0e-12
0.00.034.361 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.034.362 I print_info: f_clamp_kqv      = 0.0e+00
0.00.034.362 I print_info: f_max_alibi_bias = 0.0e+00
0.00.034.363 I print_info: f_logit_scale    = 0.0e+00
0.00.034.363 I print_info: n_ff             = 1536
0.00.034.363 I print_info: n_expert         = 0
0.00.034.363 I print_info: n_expert_used    = 0
0.00.034.364 I print_info: causal attn      = 0
0.00.034.364 I print_info: pooling type     = 2
0.00.034.364 I print_info: rope type        = 2
0.00.034.364 I print_info: rope scaling     = linear
0.00.034.364 I print_info: freq_base_train  = 10000.0
0.00.034.365 I print_info: freq_scale_train = 1
0.00.034.365 I print_info: n_ctx_orig_yarn  = 512
0.00.034.365 I print_info: rope_finetuned   = unknown
0.00.034.365 I print_info: ssm_d_conv       = 0
0.00.034.367 I print_info: ssm_d_inner      = 0
0.00.034.367 I print_info: ssm_d_state      = 0
0.00.034.367 I print_info: ssm_dt_rank      = 0
0.00.034.367 I print_info: ssm_dt_b_c_rms   = 0
0.00.034.367 I print_info: model type       = 33M
0.00.034.367 I print_info: model params     = 33.21 M
0.00.034.368 I print_info: general.name     = Bge Small
0.00.034.368 I print_info: vocab type       = WPM
0.00.034.368 I print_info: n_vocab          = 30522
0.00.034.368 I print_info: n_merges         = 0
0.00.034.369 I print_info: BOS token        = 101 '[CLS]'
0.00.034.369 I print_info: UNK token        = 100 '[UNK]'
0.00.034.369 I print_info: SEP token        = 102 '[SEP]'
0.00.034.369 I print_info: PAD token        = 0 '[PAD]'
0.00.034.372 I print_info: MASK token       = 103 '[MASK]'
0.00.034.372 I print_info: LF token         = 0 '[PAD]'
0.00.034.373 I print_info: max token length = 21
0.00.034.373 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.036.708 I load_tensors: offloading 12 repeating layers to GPU
0.00.036.710 I load_tensors: offloading output layer to GPU
0.00.036.710 I load_tensors: offloaded 13/13 layers to GPU
0.00.036.735 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.036.737 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.037.071 I llama_init_from_model: n_seq_max     = 1
0.00.037.072 I llama_init_from_model: n_ctx         = 512
0.00.037.072 I llama_init_from_model: n_ctx_per_seq = 512
0.00.037.073 I llama_init_from_model: n_batch       = 2048
0.00.037.073 I llama_init_from_model: n_ubatch      = 2048
0.00.037.073 I llama_init_from_model: flash_attn    = 0
0.00.037.074 I llama_init_from_model: freq_base     = 10000.0
0.00.037.075 I llama_init_from_model: freq_scale    = 1
0.00.037.075 I ggml_metal_init: allocating
0.00.037.086 I ggml_metal_init: found device: Apple M4
0.00.037.093 I ggml_metal_init: picking default device: Apple M4
0.00.037.888 I ggml_metal_init: using embedded metal library
0.00.041.334 I ggml_metal_init: GPU name:   Apple M4
0.00.041.336 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.041.336 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.041.337 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.041.337 I ggml_metal_init: simdgroup reduction   = true
0.00.041.337 I ggml_metal_init: simdgroup matrix mul. = true
0.00.041.337 I ggml_metal_init: has residency sets    = true
0.00.041.337 I ggml_metal_init: has bfloat            = true
0.00.041.338 I ggml_metal_init: use bfloat            = true
0.00.041.338 I ggml_metal_init: hasUnifiedMemory      = true
0.00.041.339 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.051.547 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.052.159 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.052.162 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.052.183 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.053.242 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.053.243 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.053.244 I llama_init_from_model: graph nodes  = 429
0.00.053.244 I llama_init_from_model: graph splits = 2
0.00.053.245 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.053.245 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.057.729 I 
0.00.057.763 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.058.305 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.062.682 I llama_perf_context_print:        load time =      38.56 ms
0.00.062.683 I llama_perf_context_print: prompt eval time =       4.25 ms /     9 tokens (    0.47 ms per token,  2118.15 tokens per second)
0.00.062.684 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.062.684 I llama_perf_context_print:       total time =       4.95 ms /    10 tokens
0.00.062.896 I ggml_metal_free: deallocating

real	0m0.235s
user	0m0.043s
sys	0m0.027s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.045 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.870 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.282 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.285 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.287 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.288 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.288 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.288 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.289 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.290 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.290 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.290 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.291 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.291 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.293 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.294 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.294 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.295 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.295 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.295 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.391 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.013.995 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.013.996 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.013.996 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.013.997 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.013.997 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.013.998 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.013.998 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.013.998 I llama_model_loader: - type  f32:  124 tensors
0.00.013.999 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.000 I print_info: file format = GGUF V3 (latest)
0.00.014.000 I print_info: file type   = Q8_0
0.00.014.001 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.307 I load: special tokens cache size = 5
0.00.017.388 I load: token to piece cache size = 0.2032 MB
0.00.017.391 I print_info: arch             = bert
0.00.017.391 I print_info: vocab_only       = 0
0.00.017.391 I print_info: n_ctx_train      = 512
0.00.017.391 I print_info: n_embd           = 384
0.00.017.392 I print_info: n_layer          = 12
0.00.017.394 I print_info: n_head           = 12
0.00.017.395 I print_info: n_head_kv        = 12
0.00.017.395 I print_info: n_rot            = 32
0.00.017.395 I print_info: n_swa            = 0
0.00.017.395 I print_info: n_embd_head_k    = 32
0.00.017.395 I print_info: n_embd_head_v    = 32
0.00.017.396 I print_info: n_gqa            = 1
0.00.017.397 I print_info: n_embd_k_gqa     = 384
0.00.017.397 I print_info: n_embd_v_gqa     = 384
0.00.017.398 I print_info: f_norm_eps       = 1.0e-12
0.00.017.398 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.017.399 I print_info: f_clamp_kqv      = 0.0e+00
0.00.017.399 I print_info: f_max_alibi_bias = 0.0e+00
0.00.017.399 I print_info: f_logit_scale    = 0.0e+00
0.00.017.400 I print_info: n_ff             = 1536
0.00.017.400 I print_info: n_expert         = 0
0.00.017.400 I print_info: n_expert_used    = 0
0.00.017.401 I print_info: causal attn      = 0
0.00.017.401 I print_info: pooling type     = 2
0.00.017.403 I print_info: rope type        = 2
0.00.017.403 I print_info: rope scaling     = linear
0.00.017.404 I print_info: freq_base_train  = 10000.0
0.00.017.404 I print_info: freq_scale_train = 1
0.00.017.404 I print_info: n_ctx_orig_yarn  = 512
0.00.017.404 I print_info: rope_finetuned   = unknown
0.00.017.404 I print_info: ssm_d_conv       = 0
0.00.017.404 I print_info: ssm_d_inner      = 0
0.00.017.405 I print_info: ssm_d_state      = 0
0.00.017.405 I print_info: ssm_dt_rank      = 0
0.00.017.405 I print_info: ssm_dt_b_c_rms   = 0
0.00.017.405 I print_info: model type       = 33M
0.00.017.405 I print_info: model params     = 33.21 M
0.00.017.407 I print_info: general.name     = Bge Small
0.00.017.407 I print_info: vocab type       = WPM
0.00.017.407 I print_info: n_vocab          = 30522
0.00.017.407 I print_info: n_merges         = 0
0.00.017.407 I print_info: BOS token        = 101 '[CLS]'
0.00.017.408 I print_info: UNK token        = 100 '[UNK]'
0.00.017.408 I print_info: SEP token        = 102 '[SEP]'
0.00.017.408 I print_info: PAD token        = 0 '[PAD]'
0.00.017.408 I print_info: MASK token       = 103 '[MASK]'
0.00.017.408 I print_info: LF token         = 0 '[PAD]'
0.00.017.408 I print_info: max token length = 21
0.00.017.409 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.019.044 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.045 I load_tensors: offloading output layer to GPU
0.00.019.045 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.052 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.052 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.234 I llama_init_from_model: n_seq_max     = 1
0.00.019.235 I llama_init_from_model: n_ctx         = 512
0.00.019.236 I llama_init_from_model: n_ctx_per_seq = 512
0.00.019.236 I llama_init_from_model: n_batch       = 2048
0.00.019.236 I llama_init_from_model: n_ubatch      = 2048
0.00.019.236 I llama_init_from_model: flash_attn    = 0
0.00.019.236 I llama_init_from_model: freq_base     = 10000.0
0.00.019.237 I llama_init_from_model: freq_scale    = 1
0.00.019.237 I ggml_metal_init: allocating
0.00.019.245 I ggml_metal_init: found device: Apple M4
0.00.019.249 I ggml_metal_init: picking default device: Apple M4
0.00.019.777 I ggml_metal_init: using embedded metal library
0.00.022.197 I ggml_metal_init: GPU name:   Apple M4
0.00.022.199 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.199 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.200 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.200 I ggml_metal_init: simdgroup reduction   = true
0.00.022.200 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.200 I ggml_metal_init: has residency sets    = true
0.00.022.200 I ggml_metal_init: has bfloat            = true
0.00.022.200 I ggml_metal_init: use bfloat            = true
0.00.022.201 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.202 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.004 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.616 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.618 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.633 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.034.640 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.034.641 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.034.641 I llama_init_from_model: graph nodes  = 429
0.00.034.641 I llama_init_from_model: graph splits = 2
0.00.034.643 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.643 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.617 I 
0.00.038.641 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.157 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.519 I llama_perf_context_print:        load time =      29.74 ms
0.00.042.520 I llama_perf_context_print: prompt eval time =       3.23 ms /     9 tokens (    0.36 ms per token,  2788.10 tokens per second)
0.00.042.521 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.521 I llama_perf_context_print:       total time =       3.90 ms /    10 tokens
0.00.042.693 I ggml_metal_free: deallocating

real	0m0.053s
user	0m0.029s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.234 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.132 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.262 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.267 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.270 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.038.270 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.271 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.038.272 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.038.272 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.038.279 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.038.280 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.038.280 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.038.281 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.038.282 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.038.285 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.038.285 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.038.286 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.038.287 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.287 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.045.570 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.047.841 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.520 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.052.522 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.522 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.052.523 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.052.523 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.052.524 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.052.524 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.052.524 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.052.525 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.052.525 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.052.525 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.052.526 I llama_model_loader: - type  f32:   40 tensors
0.00.052.526 I llama_model_loader: - type  f16:   30 tensors
0.00.052.527 I print_info: file format = GGUF V3 (latest)
0.00.052.527 I print_info: file type   = F16
0.00.052.529 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.056.868 W load: empty token at index 5
0.00.062.008 W load: model vocab missing newline token, using special_pad_id instead
0.00.063.421 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.063.456 I load: special tokens cache size = 5
0.00.327.577 I load: token to piece cache size = 1.5060 MB
0.00.327.589 I print_info: arch             = jina-bert-v2
0.00.327.589 I print_info: vocab_only       = 0
0.00.327.590 I print_info: n_ctx_train      = 8192
0.00.327.590 I print_info: n_embd           = 384
0.00.327.590 I print_info: n_layer          = 4
0.00.327.597 I print_info: n_head           = 12
0.00.327.598 I print_info: n_head_kv        = 12
0.00.327.598 I print_info: n_rot            = 32
0.00.327.598 I print_info: n_swa            = 0
0.00.327.599 I print_info: n_embd_head_k    = 32
0.00.327.599 I print_info: n_embd_head_v    = 32
0.00.327.605 I print_info: n_gqa            = 1
0.00.327.605 I print_info: n_embd_k_gqa     = 384
0.00.327.606 I print_info: n_embd_v_gqa     = 384
0.00.327.606 I print_info: f_norm_eps       = 1.0e-12
0.00.327.607 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.327.607 I print_info: f_clamp_kqv      = 0.0e+00
0.00.327.607 I print_info: f_max_alibi_bias = 8.0e+00
0.00.327.607 I print_info: f_logit_scale    = 0.0e+00
0.00.327.608 I print_info: n_ff             = 1536
0.00.327.608 I print_info: n_expert         = 0
0.00.327.608 I print_info: n_expert_used    = 0
0.00.327.608 I print_info: causal attn      = 0
0.00.327.609 I print_info: pooling type     = -1
0.00.327.609 I print_info: rope type        = -1
0.00.327.609 I print_info: rope scaling     = linear
0.00.327.609 I print_info: freq_base_train  = 10000.0
0.00.327.615 I print_info: freq_scale_train = 1
0.00.327.615 I print_info: n_ctx_orig_yarn  = 8192
0.00.327.616 I print_info: rope_finetuned   = unknown
0.00.327.616 I print_info: ssm_d_conv       = 0
0.00.327.616 I print_info: ssm_d_inner      = 0
0.00.327.616 I print_info: ssm_d_state      = 0
0.00.327.616 I print_info: ssm_dt_rank      = 0
0.00.327.616 I print_info: ssm_dt_b_c_rms   = 0
0.00.327.617 I print_info: model type       = 33M
0.00.327.617 I print_info: model params     = 32.90 M
0.00.327.618 I print_info: general.name     = Jina Bert Implementation
0.00.327.618 I print_info: vocab type       = BPE
0.00.327.619 I print_info: n_vocab          = 61056
0.00.327.619 I print_info: n_merges         = 39382
0.00.327.622 I print_info: BOS token        = 0 '<s>'
0.00.327.622 I print_info: EOS token        = 2 '</s>'
0.00.327.622 I print_info: UNK token        = 3 '<unk>'
0.00.327.622 I print_info: SEP token        = 2 '</s>'
0.00.327.623 I print_info: PAD token        = 1 '<pad>'
0.00.327.623 I print_info: MASK token       = 4 '<mask>'
0.00.327.623 I print_info: EOG token        = 2 '</s>'
0.00.327.623 I print_info: max token length = 45
0.00.327.624 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.329.682 I load_tensors: offloading 4 repeating layers to GPU
0.00.329.683 I load_tensors: offloading output layer to GPU
0.00.329.683 I load_tensors: offloaded 5/5 layers to GPU
0.00.329.708 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.329.709 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.330.020 I llama_init_from_model: n_seq_max     = 1
0.00.330.021 I llama_init_from_model: n_ctx         = 8192
0.00.330.021 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.330.021 I llama_init_from_model: n_batch       = 2048
0.00.330.021 I llama_init_from_model: n_ubatch      = 2048
0.00.330.021 I llama_init_from_model: flash_attn    = 0
0.00.330.022 I llama_init_from_model: freq_base     = 10000.0
0.00.330.022 I llama_init_from_model: freq_scale    = 1
0.00.330.023 I ggml_metal_init: allocating
0.00.330.026 I ggml_metal_init: found device: Apple M4
0.00.330.029 I ggml_metal_init: picking default device: Apple M4
0.00.330.888 I ggml_metal_init: using embedded metal library
0.00.333.730 I ggml_metal_init: GPU name:   Apple M4
0.00.333.732 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.333.732 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.333.733 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.333.733 I ggml_metal_init: simdgroup reduction   = true
0.00.333.733 I ggml_metal_init: simdgroup matrix mul. = true
0.00.333.733 I ggml_metal_init: has residency sets    = true
0.00.333.733 I ggml_metal_init: has bfloat            = true
0.00.333.733 I ggml_metal_init: use bfloat            = true
0.00.333.734 I ggml_metal_init: hasUnifiedMemory      = true
0.00.333.734 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.343.363 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.346.428 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.346.429 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.346.450 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.353.516 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.353.517 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.353.517 I llama_init_from_model: graph nodes  = 154
0.00.353.518 I llama_init_from_model: graph splits = 2
0.00.353.519 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.353.519 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.364.978 I 
0.00.365.018 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.365.124 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.365.125 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.365.128 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.365.129 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.365.132 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.365.132 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.365.672 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.369.145 I llama_perf_context_print:        load time =     340.84 ms
0.00.369.146 I llama_perf_context_print: prompt eval time =       3.46 ms /    62 tokens (    0.06 ms per token, 17893.22 tokens per second)
0.00.369.147 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.369.148 I llama_perf_context_print:       total time =       4.17 ms /    63 tokens
0.00.369.398 I ggml_metal_free: deallocating

real	0m1.082s
user	0m0.346s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.234 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.468 I main: llama backend init
0.00.000.483 I main: load the model and apply lora adapter, if any
0.00.055.454 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.070.807 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.070.821 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.070.825 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.070.837 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.070.838 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.070.839 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.070.839 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.070.842 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.070.843 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.070.844 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.070.844 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.070.849 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.070.850 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.070.851 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.070.854 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.070.855 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.070.855 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.079.122 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.081.437 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.088.464 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.088.466 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.088.467 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.088.467 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.088.467 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.088.468 I llama_model_loader: - type  f32:  194 tensors
0.00.088.469 I llama_model_loader: - type  f16:   98 tensors
0.00.088.470 I print_info: file format = GGUF V3 (latest)
0.00.088.471 I print_info: file type   = all F32 (guessed)
0.00.088.472 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.101.203 I load: special tokens cache size = 25
0.00.109.291 I load: token to piece cache size = 0.2984 MB
0.00.109.294 I print_info: arch             = gptneox
0.00.109.294 I print_info: vocab_only       = 0
0.00.109.295 I print_info: n_ctx_train      = 2048
0.00.109.295 I print_info: n_embd           = 2048
0.00.109.295 I print_info: n_layer          = 24
0.00.109.298 I print_info: n_head           = 16
0.00.109.299 I print_info: n_head_kv        = 16
0.00.109.300 I print_info: n_rot            = 32
0.00.109.300 I print_info: n_swa            = 0
0.00.109.300 I print_info: n_embd_head_k    = 128
0.00.109.300 I print_info: n_embd_head_v    = 128
0.00.109.301 I print_info: n_gqa            = 1
0.00.109.302 I print_info: n_embd_k_gqa     = 2048
0.00.109.302 I print_info: n_embd_v_gqa     = 2048
0.00.109.303 I print_info: f_norm_eps       = 1.0e-05
0.00.109.303 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.109.303 I print_info: f_clamp_kqv      = 0.0e+00
0.00.109.303 I print_info: f_max_alibi_bias = 0.0e+00
0.00.109.304 I print_info: f_logit_scale    = 0.0e+00
0.00.109.304 I print_info: n_ff             = 8192
0.00.109.304 I print_info: n_expert         = 0
0.00.109.304 I print_info: n_expert_used    = 0
0.00.109.305 I print_info: causal attn      = 1
0.00.109.305 I print_info: pooling type     = 0
0.00.109.305 I print_info: rope type        = 2
0.00.109.305 I print_info: rope scaling     = linear
0.00.109.305 I print_info: freq_base_train  = 10000.0
0.00.109.306 I print_info: freq_scale_train = 1
0.00.109.310 I print_info: n_ctx_orig_yarn  = 2048
0.00.109.310 I print_info: rope_finetuned   = unknown
0.00.109.310 I print_info: ssm_d_conv       = 0
0.00.109.310 I print_info: ssm_d_inner      = 0
0.00.109.311 I print_info: ssm_d_state      = 0
0.00.109.311 I print_info: ssm_dt_rank      = 0
0.00.109.311 I print_info: ssm_dt_b_c_rms   = 0
0.00.109.311 I print_info: model type       = 1.4B
0.00.109.311 I print_info: model params     = 1.41 B
0.00.109.312 I print_info: general.name     = 1.4B
0.00.109.312 I print_info: vocab type       = BPE
0.00.109.312 I print_info: n_vocab          = 50304
0.00.109.314 I print_info: n_merges         = 50009
0.00.109.314 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.109.314 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.109.315 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.109.315 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.109.315 I print_info: LF token         = 187 ''
0.00.109.315 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.109.317 I print_info: max token length = 1024
0.00.109.317 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.153.924 I load_tensors: offloading 24 repeating layers to GPU
0.00.153.928 I load_tensors: offloading output layer to GPU
0.00.153.929 I load_tensors: offloaded 25/25 layers to GPU
0.00.153.951 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.153.952 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.154.437 I llama_init_from_model: n_seq_max     = 1
0.00.154.438 I llama_init_from_model: n_ctx         = 2048
0.00.154.438 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.154.438 I llama_init_from_model: n_batch       = 2048
0.00.154.438 I llama_init_from_model: n_ubatch      = 512
0.00.154.438 I llama_init_from_model: flash_attn    = 0
0.00.154.439 I llama_init_from_model: freq_base     = 10000.0
0.00.154.439 I llama_init_from_model: freq_scale    = 1
0.00.154.440 I ggml_metal_init: allocating
0.00.154.457 I ggml_metal_init: found device: Apple M4
0.00.154.462 I ggml_metal_init: picking default device: Apple M4
0.00.155.079 I ggml_metal_init: using embedded metal library
0.00.353.599 I ggml_metal_init: GPU name:   Apple M4
0.00.353.608 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.353.609 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.353.609 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.353.610 I ggml_metal_init: simdgroup reduction   = true
0.00.353.610 I ggml_metal_init: simdgroup matrix mul. = true
0.00.353.610 I ggml_metal_init: has residency sets    = true
0.00.353.611 I ggml_metal_init: has bfloat            = true
0.00.353.611 I ggml_metal_init: use bfloat            = true
0.00.353.612 I ggml_metal_init: hasUnifiedMemory      = true
0.00.353.615 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.390.433 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.428.332 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.428.342 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.428.389 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.432.738 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.432.742 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.432.742 I llama_init_from_model: graph nodes  = 967
0.00.432.743 I llama_init_from_model: graph splits = 2
0.00.432.748 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.432.876 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.432.877 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.499.223 I main: llama threadpool init, n_threads = 4
0.00.499.267 I 
0.00.499.301 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.499.302 I 
0.00.499.480 I sampler seed: 1234
0.00.499.485 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.499.509 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.499.511 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.499.511 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.329.362 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60016.91 tokens per second)
0.02.329.363 I llama_perf_context_print:        load time =     442.84 ms
0.02.329.364 I llama_perf_context_print: prompt eval time =      43.81 ms /     7 tokens (    6.26 ms per token,   159.80 tokens per second)
0.02.329.365 I llama_perf_context_print:        eval time =    1783.13 ms /    63 runs   (   28.30 ms per token,    35.33 tokens per second)
0.02.329.365 I llama_perf_context_print:       total time =    1831.06 ms /    70 tokens
0.02.329.554 I ggml_metal_free: deallocating

real	0m2.698s
user	0m0.142s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.594 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.443 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.350 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.356 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.357 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.358 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.358 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.361 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.361 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.362 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.363 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.363 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.363 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.364 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.364 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.365 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.367 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.367 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.369 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.618 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.725 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.748 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.750 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.751 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.751 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.752 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.752 I llama_model_loader: - type  f32:  194 tensors
0.00.055.753 I llama_model_loader: - type  f16:   98 tensors
0.00.055.754 I print_info: file format = GGUF V3 (latest)
0.00.055.755 I print_info: file type   = all F32 (guessed)
0.00.055.756 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.094 I load: special tokens cache size = 25
0.00.076.311 I load: token to piece cache size = 0.2984 MB
0.00.076.314 I print_info: arch             = gptneox
0.00.076.314 I print_info: vocab_only       = 0
0.00.076.314 I print_info: n_ctx_train      = 2048
0.00.076.314 I print_info: n_embd           = 2048
0.00.076.315 I print_info: n_layer          = 24
0.00.076.318 I print_info: n_head           = 16
0.00.076.319 I print_info: n_head_kv        = 16
0.00.076.319 I print_info: n_rot            = 32
0.00.076.319 I print_info: n_swa            = 0
0.00.076.319 I print_info: n_embd_head_k    = 128
0.00.076.319 I print_info: n_embd_head_v    = 128
0.00.076.320 I print_info: n_gqa            = 1
0.00.076.321 I print_info: n_embd_k_gqa     = 2048
0.00.076.325 I print_info: n_embd_v_gqa     = 2048
0.00.076.325 I print_info: f_norm_eps       = 1.0e-05
0.00.076.326 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.326 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.326 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.326 I print_info: f_logit_scale    = 0.0e+00
0.00.076.327 I print_info: n_ff             = 8192
0.00.076.327 I print_info: n_expert         = 0
0.00.076.327 I print_info: n_expert_used    = 0
0.00.076.328 I print_info: causal attn      = 1
0.00.076.328 I print_info: pooling type     = 0
0.00.076.339 I print_info: rope type        = 2
0.00.076.341 I print_info: rope scaling     = linear
0.00.076.342 I print_info: freq_base_train  = 10000.0
0.00.076.342 I print_info: freq_scale_train = 1
0.00.076.342 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.343 I print_info: rope_finetuned   = unknown
0.00.076.343 I print_info: ssm_d_conv       = 0
0.00.076.343 I print_info: ssm_d_inner      = 0
0.00.076.343 I print_info: ssm_d_state      = 0
0.00.076.343 I print_info: ssm_dt_rank      = 0
0.00.076.347 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.347 I print_info: model type       = 1.4B
0.00.076.347 I print_info: model params     = 1.41 B
0.00.076.349 I print_info: general.name     = 1.4B
0.00.076.350 I print_info: vocab type       = BPE
0.00.076.350 I print_info: n_vocab          = 50304
0.00.076.350 I print_info: n_merges         = 50009
0.00.076.351 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.351 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.351 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.351 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.351 I print_info: LF token         = 187 ''
0.00.076.356 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.356 I print_info: max token length = 1024
0.00.076.356 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.439.275 I load_tensors: offloading 24 repeating layers to GPU
0.01.439.279 I load_tensors: offloading output layer to GPU
0.01.439.279 I load_tensors: offloaded 25/25 layers to GPU
0.01.439.305 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.439.307 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.440.489 I llama_init_from_model: n_seq_max     = 1
0.01.440.490 I llama_init_from_model: n_ctx         = 128
0.01.440.490 I llama_init_from_model: n_ctx_per_seq = 128
0.01.440.491 I llama_init_from_model: n_batch       = 128
0.01.440.491 I llama_init_from_model: n_ubatch      = 128
0.01.440.491 I llama_init_from_model: flash_attn    = 0
0.01.440.492 I llama_init_from_model: freq_base     = 10000.0
0.01.440.492 I llama_init_from_model: freq_scale    = 1
0.01.440.492 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.440.493 I ggml_metal_init: allocating
0.01.440.559 I ggml_metal_init: found device: Apple M4
0.01.440.566 I ggml_metal_init: picking default device: Apple M4
0.01.441.700 I ggml_metal_init: using embedded metal library
0.01.445.556 I ggml_metal_init: GPU name:   Apple M4
0.01.445.558 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.445.558 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.445.559 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.445.559 I ggml_metal_init: simdgroup reduction   = true
0.01.445.559 I ggml_metal_init: simdgroup matrix mul. = true
0.01.445.559 I ggml_metal_init: has residency sets    = true
0.01.445.559 I ggml_metal_init: has bfloat            = true
0.01.445.560 I ggml_metal_init: use bfloat            = true
0.01.445.560 I ggml_metal_init: hasUnifiedMemory      = true
0.01.445.561 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.457.817 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.459.587 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.459.592 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.459.626 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.461.338 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.461.340 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.461.340 I llama_init_from_model: graph nodes  = 967
0.01.461.340 I llama_init_from_model: graph splits = 2
0.01.461.341 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.461.341 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.495.931 I 
0.01.495.968 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.495.972 I perplexity: tokenizing the input ..
0.01.501.322 I perplexity: tokenization took 5.348 ms
0.01.501.326 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.620.662 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.623.019 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.623.050 I llama_perf_context_print:        load time =    1471.48 ms
0.01.623.051 I llama_perf_context_print: prompt eval time =     119.07 ms /   128 tokens (    0.93 ms per token,  1074.98 tokens per second)
0.01.623.052 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.623.052 I llama_perf_context_print:       total time =     127.12 ms /   129 tokens
0.01.623.420 I ggml_metal_free: deallocating

real	0m1.821s
user	0m0.103s
sys	0m0.266s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.099 I main: load the model and apply lora adapter, if any
0.00.010.037 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.897 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.903 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.906 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.907 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.907 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.907 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.908 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.909 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.909 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.910 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.910 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.910 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.911 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.913 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.915 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.915 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.915 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.739 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.919 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.748 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.750 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.750 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.751 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.751 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.751 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.752 I llama_model_loader: - type  f32:  194 tensors
0.00.034.752 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.753 I print_info: file format = GGUF V3 (latest)
0.00.034.753 I print_info: file type   = Q8_0
0.00.034.754 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.043.632 I load: special tokens cache size = 25
0.00.049.706 I load: token to piece cache size = 0.2984 MB
0.00.049.711 I print_info: arch             = gptneox
0.00.049.711 I print_info: vocab_only       = 0
0.00.049.712 I print_info: n_ctx_train      = 2048
0.00.049.714 I print_info: n_embd           = 2048
0.00.049.714 I print_info: n_layer          = 24
0.00.049.720 I print_info: n_head           = 16
0.00.049.721 I print_info: n_head_kv        = 16
0.00.049.724 I print_info: n_rot            = 32
0.00.049.724 I print_info: n_swa            = 0
0.00.049.724 I print_info: n_embd_head_k    = 128
0.00.049.724 I print_info: n_embd_head_v    = 128
0.00.049.725 I print_info: n_gqa            = 1
0.00.049.726 I print_info: n_embd_k_gqa     = 2048
0.00.049.726 I print_info: n_embd_v_gqa     = 2048
0.00.049.727 I print_info: f_norm_eps       = 1.0e-05
0.00.049.728 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.728 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.728 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.728 I print_info: f_logit_scale    = 0.0e+00
0.00.049.729 I print_info: n_ff             = 8192
0.00.049.729 I print_info: n_expert         = 0
0.00.049.729 I print_info: n_expert_used    = 0
0.00.049.730 I print_info: causal attn      = 1
0.00.049.730 I print_info: pooling type     = 0
0.00.049.730 I print_info: rope type        = 2
0.00.049.732 I print_info: rope scaling     = linear
0.00.049.732 I print_info: freq_base_train  = 10000.0
0.00.049.732 I print_info: freq_scale_train = 1
0.00.049.732 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.732 I print_info: rope_finetuned   = unknown
0.00.049.733 I print_info: ssm_d_conv       = 0
0.00.049.733 I print_info: ssm_d_inner      = 0
0.00.049.733 I print_info: ssm_d_state      = 0
0.00.049.733 I print_info: ssm_dt_rank      = 0
0.00.049.733 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.733 I print_info: model type       = 1.4B
0.00.049.734 I print_info: model params     = 1.41 B
0.00.049.734 I print_info: general.name     = 1.4B
0.00.049.735 I print_info: vocab type       = BPE
0.00.049.735 I print_info: n_vocab          = 50304
0.00.049.735 I print_info: n_merges         = 50009
0.00.049.735 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.735 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.735 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.736 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.736 I print_info: LF token         = 187 ''
0.00.049.736 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.736 I print_info: max token length = 1024
0.00.049.737 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.317.675 I load_tensors: offloading 24 repeating layers to GPU
0.01.317.681 I load_tensors: offloading output layer to GPU
0.01.317.683 I load_tensors: offloaded 25/25 layers to GPU
0.01.317.705 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.317.708 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.318.516 I llama_init_from_model: n_seq_max     = 1
0.01.318.518 I llama_init_from_model: n_ctx         = 2048
0.01.318.518 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.318.519 I llama_init_from_model: n_batch       = 2048
0.01.318.519 I llama_init_from_model: n_ubatch      = 512
0.01.318.520 I llama_init_from_model: flash_attn    = 0
0.01.318.521 I llama_init_from_model: freq_base     = 10000.0
0.01.318.521 I llama_init_from_model: freq_scale    = 1
0.01.318.522 I ggml_metal_init: allocating
0.01.318.535 I ggml_metal_init: found device: Apple M4
0.01.318.544 I ggml_metal_init: picking default device: Apple M4
0.01.319.885 I ggml_metal_init: using embedded metal library
0.01.325.257 I ggml_metal_init: GPU name:   Apple M4
0.01.325.261 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.325.262 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.325.262 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.325.263 I ggml_metal_init: simdgroup reduction   = true
0.01.325.263 I ggml_metal_init: simdgroup matrix mul. = true
0.01.325.263 I ggml_metal_init: has residency sets    = true
0.01.325.263 I ggml_metal_init: has bfloat            = true
0.01.325.263 I ggml_metal_init: use bfloat            = true
0.01.325.264 I ggml_metal_init: hasUnifiedMemory      = true
0.01.325.265 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.342.132 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.396.298 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.396.307 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.396.344 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.400.570 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.400.572 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.400.572 I llama_init_from_model: graph nodes  = 967
0.01.400.572 I llama_init_from_model: graph splits = 2
0.01.400.578 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.400.701 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.400.702 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.453.772 I main: llama threadpool init, n_threads = 4
0.01.453.820 I 
0.01.453.843 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.453.844 I 
0.01.453.987 I sampler seed: 1234
0.01.453.992 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.454.003 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.454.004 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.454.004 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.546.541 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55081.46 tokens per second)
0.02.546.542 I llama_perf_context_print:        load time =    1443.02 ms
0.02.546.543 I llama_perf_context_print: prompt eval time =      48.81 ms /     7 tokens (    6.97 ms per token,   143.42 tokens per second)
0.02.546.543 I llama_perf_context_print:        eval time =    1040.88 ms /    63 runs   (   16.52 ms per token,    60.53 tokens per second)
0.02.546.544 I llama_perf_context_print:       total time =    1093.48 ms /    70 tokens
0.02.546.806 I ggml_metal_free: deallocating

real	0m2.567s
user	0m0.108s
sys	0m0.265s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.283 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.199 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.152 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.158 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.160 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.161 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.161 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.161 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.167 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.167 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.168 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.168 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.169 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.169 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.169 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.170 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.173 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.173 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.173 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.926 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.121 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.837 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.838 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.839 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.839 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.839 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.840 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.840 I llama_model_loader: - type  f32:  194 tensors
0.00.025.841 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.841 I print_info: file format = GGUF V3 (latest)
0.00.025.842 I print_info: file type   = Q8_0
0.00.025.843 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.878 I load: special tokens cache size = 25
0.00.039.964 I load: token to piece cache size = 0.2984 MB
0.00.039.969 I print_info: arch             = gptneox
0.00.039.969 I print_info: vocab_only       = 0
0.00.039.969 I print_info: n_ctx_train      = 2048
0.00.039.970 I print_info: n_embd           = 2048
0.00.039.970 I print_info: n_layer          = 24
0.00.039.974 I print_info: n_head           = 16
0.00.039.975 I print_info: n_head_kv        = 16
0.00.039.975 I print_info: n_rot            = 32
0.00.039.975 I print_info: n_swa            = 0
0.00.039.975 I print_info: n_embd_head_k    = 128
0.00.039.975 I print_info: n_embd_head_v    = 128
0.00.039.976 I print_info: n_gqa            = 1
0.00.039.977 I print_info: n_embd_k_gqa     = 2048
0.00.039.980 I print_info: n_embd_v_gqa     = 2048
0.00.039.981 I print_info: f_norm_eps       = 1.0e-05
0.00.039.981 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.981 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.981 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.981 I print_info: f_logit_scale    = 0.0e+00
0.00.039.982 I print_info: n_ff             = 8192
0.00.039.982 I print_info: n_expert         = 0
0.00.039.982 I print_info: n_expert_used    = 0
0.00.039.982 I print_info: causal attn      = 1
0.00.039.983 I print_info: pooling type     = 0
0.00.039.983 I print_info: rope type        = 2
0.00.039.983 I print_info: rope scaling     = linear
0.00.039.983 I print_info: freq_base_train  = 10000.0
0.00.039.984 I print_info: freq_scale_train = 1
0.00.039.986 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.987 I print_info: rope_finetuned   = unknown
0.00.039.987 I print_info: ssm_d_conv       = 0
0.00.039.987 I print_info: ssm_d_inner      = 0
0.00.039.987 I print_info: ssm_d_state      = 0
0.00.039.987 I print_info: ssm_dt_rank      = 0
0.00.039.987 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.988 I print_info: model type       = 1.4B
0.00.039.988 I print_info: model params     = 1.41 B
0.00.039.988 I print_info: general.name     = 1.4B
0.00.039.988 I print_info: vocab type       = BPE
0.00.039.989 I print_info: n_vocab          = 50304
0.00.039.989 I print_info: n_merges         = 50009
0.00.039.989 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.989 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.989 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.989 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.990 I print_info: LF token         = 187 ''
0.00.039.990 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.990 I print_info: max token length = 1024
0.00.039.990 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.908.960 I load_tensors: offloading 24 repeating layers to GPU
0.00.908.966 I load_tensors: offloading output layer to GPU
0.00.908.967 I load_tensors: offloaded 25/25 layers to GPU
0.00.908.996 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.908.999 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.910.429 I llama_init_from_model: n_seq_max     = 1
0.00.910.431 I llama_init_from_model: n_ctx         = 128
0.00.910.431 I llama_init_from_model: n_ctx_per_seq = 128
0.00.910.432 I llama_init_from_model: n_batch       = 128
0.00.910.432 I llama_init_from_model: n_ubatch      = 128
0.00.910.432 I llama_init_from_model: flash_attn    = 0
0.00.910.433 I llama_init_from_model: freq_base     = 10000.0
0.00.910.434 I llama_init_from_model: freq_scale    = 1
0.00.910.435 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.910.436 I ggml_metal_init: allocating
0.00.910.502 I ggml_metal_init: found device: Apple M4
0.00.910.512 I ggml_metal_init: picking default device: Apple M4
0.00.911.883 I ggml_metal_init: using embedded metal library
0.00.917.505 I ggml_metal_init: GPU name:   Apple M4
0.00.917.509 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.917.509 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.917.510 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.917.511 I ggml_metal_init: simdgroup reduction   = true
0.00.917.511 I ggml_metal_init: simdgroup matrix mul. = true
0.00.917.511 I ggml_metal_init: has residency sets    = true
0.00.917.511 I ggml_metal_init: has bfloat            = true
0.00.917.512 I ggml_metal_init: use bfloat            = true
0.00.917.513 I ggml_metal_init: hasUnifiedMemory      = true
0.00.917.515 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.933.483 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.936.200 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.936.204 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.936.243 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.938.535 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.938.537 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.938.537 I llama_init_from_model: graph nodes  = 967
0.00.938.538 I llama_init_from_model: graph splits = 2
0.00.938.539 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.938.539 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.963.773 I 
0.00.963.836 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.963.848 I perplexity: tokenizing the input ..
0.00.970.671 I perplexity: tokenization took 6.821 ms
0.00.970.677 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.108.388 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.109.740 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.109.764 I llama_perf_context_print:        load time =     953.57 ms
0.01.109.765 I llama_perf_context_print: prompt eval time =     137.16 ms /   128 tokens (    1.07 ms per token,   933.20 tokens per second)
0.01.109.766 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.109.766 I llama_perf_context_print:       total time =     145.99 ms /   129 tokens
0.01.110.160 I ggml_metal_free: deallocating

real	0m1.131s
user	0m0.079s
sys	0m0.174s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.099 I main: llama backend init
0.00.000.101 I main: load the model and apply lora adapter, if any
0.00.010.880 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.026 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.032 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.034 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.034 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.035 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.035 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.035 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.038 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.039 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.039 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.039 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.040 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.040 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.040 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.043 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.043 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.043 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.806 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.011 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.810 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.811 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.811 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.811 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.812 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.812 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.813 I llama_model_loader: - type  f32:  194 tensors
0.00.027.813 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.814 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.814 I print_info: file format = GGUF V3 (latest)
0.00.027.815 I print_info: file type   = Q4_0
0.00.027.816 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.295 I load: special tokens cache size = 25
0.00.042.294 I load: token to piece cache size = 0.2984 MB
0.00.042.297 I print_info: arch             = gptneox
0.00.042.297 I print_info: vocab_only       = 0
0.00.042.297 I print_info: n_ctx_train      = 2048
0.00.042.297 I print_info: n_embd           = 2048
0.00.042.298 I print_info: n_layer          = 24
0.00.042.302 I print_info: n_head           = 16
0.00.042.303 I print_info: n_head_kv        = 16
0.00.042.303 I print_info: n_rot            = 32
0.00.042.303 I print_info: n_swa            = 0
0.00.042.304 I print_info: n_embd_head_k    = 128
0.00.042.304 I print_info: n_embd_head_v    = 128
0.00.042.305 I print_info: n_gqa            = 1
0.00.042.305 I print_info: n_embd_k_gqa     = 2048
0.00.042.306 I print_info: n_embd_v_gqa     = 2048
0.00.042.306 I print_info: f_norm_eps       = 1.0e-05
0.00.042.307 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.307 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.307 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.307 I print_info: f_logit_scale    = 0.0e+00
0.00.042.308 I print_info: n_ff             = 8192
0.00.042.308 I print_info: n_expert         = 0
0.00.042.308 I print_info: n_expert_used    = 0
0.00.042.309 I print_info: causal attn      = 1
0.00.042.309 I print_info: pooling type     = 0
0.00.042.309 I print_info: rope type        = 2
0.00.042.312 I print_info: rope scaling     = linear
0.00.042.313 I print_info: freq_base_train  = 10000.0
0.00.042.313 I print_info: freq_scale_train = 1
0.00.042.313 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.313 I print_info: rope_finetuned   = unknown
0.00.042.314 I print_info: ssm_d_conv       = 0
0.00.042.314 I print_info: ssm_d_inner      = 0
0.00.042.314 I print_info: ssm_d_state      = 0
0.00.042.314 I print_info: ssm_dt_rank      = 0
0.00.042.314 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.314 I print_info: model type       = 1.4B
0.00.042.315 I print_info: model params     = 1.41 B
0.00.042.315 I print_info: general.name     = 1.4B
0.00.042.316 I print_info: vocab type       = BPE
0.00.042.320 I print_info: n_vocab          = 50304
0.00.042.320 I print_info: n_merges         = 50009
0.00.042.320 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.321 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.321 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.321 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.321 I print_info: LF token         = 187 ''
0.00.042.322 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.322 I print_info: max token length = 1024
0.00.042.322 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.593.079 I load_tensors: offloading 24 repeating layers to GPU
0.00.593.092 I load_tensors: offloading output layer to GPU
0.00.593.093 I load_tensors: offloaded 25/25 layers to GPU
0.00.593.127 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.593.128 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.594.803 I llama_init_from_model: n_seq_max     = 1
0.00.594.806 I llama_init_from_model: n_ctx         = 2048
0.00.594.807 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.594.808 I llama_init_from_model: n_batch       = 2048
0.00.594.808 I llama_init_from_model: n_ubatch      = 512
0.00.594.808 I llama_init_from_model: flash_attn    = 0
0.00.594.811 I llama_init_from_model: freq_base     = 10000.0
0.00.594.811 I llama_init_from_model: freq_scale    = 1
0.00.594.813 I ggml_metal_init: allocating
0.00.594.888 I ggml_metal_init: found device: Apple M4
0.00.594.901 I ggml_metal_init: picking default device: Apple M4
0.00.596.748 I ggml_metal_init: using embedded metal library
0.00.602.715 I ggml_metal_init: GPU name:   Apple M4
0.00.602.720 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.602.721 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.602.722 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.602.723 I ggml_metal_init: simdgroup reduction   = true
0.00.602.723 I ggml_metal_init: simdgroup matrix mul. = true
0.00.602.724 I ggml_metal_init: has residency sets    = true
0.00.602.724 I ggml_metal_init: has bfloat            = true
0.00.602.724 I ggml_metal_init: use bfloat            = true
0.00.602.725 I ggml_metal_init: hasUnifiedMemory      = true
0.00.602.727 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.622.496 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.680.832 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.680.837 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.680.876 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.684.822 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.684.824 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.684.825 I llama_init_from_model: graph nodes  = 967
0.00.684.825 I llama_init_from_model: graph splits = 2
0.00.684.837 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.684.969 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.684.970 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.566 I main: llama threadpool init, n_threads = 4
0.00.742.613 I 
0.00.742.635 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.742.635 I 
0.00.742.808 I sampler seed: 1234
0.00.742.813 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.742.824 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.742.825 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.742.825 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.432.279 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49511.85 tokens per second)
0.01.432.279 I llama_perf_context_print:        load time =     730.97 ms
0.01.432.281 I llama_perf_context_print: prompt eval time =      48.84 ms /     7 tokens (    6.98 ms per token,   143.31 tokens per second)
0.01.432.282 I llama_perf_context_print:        eval time =     637.69 ms /    63 runs   (   10.12 ms per token,    98.79 tokens per second)
0.01.432.282 I llama_perf_context_print:       total time =     690.42 ms /    70 tokens
0.01.432.547 I ggml_metal_free: deallocating

real	0m1.452s
user	0m0.111s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.278 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.091 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.322 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.328 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.334 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.335 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.335 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.336 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.336 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.337 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.337 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.338 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.339 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.339 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.340 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.340 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.342 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.342 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.342 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.074 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.250 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.046 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.047 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.047 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.048 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.048 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.048 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.049 I llama_model_loader: - type  f32:  194 tensors
0.00.026.049 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.050 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.050 I print_info: file format = GGUF V3 (latest)
0.00.026.051 I print_info: file type   = Q4_0
0.00.026.052 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.103 I load: special tokens cache size = 25
0.00.040.155 I load: token to piece cache size = 0.2984 MB
0.00.040.159 I print_info: arch             = gptneox
0.00.040.160 I print_info: vocab_only       = 0
0.00.040.160 I print_info: n_ctx_train      = 2048
0.00.040.160 I print_info: n_embd           = 2048
0.00.040.160 I print_info: n_layer          = 24
0.00.040.164 I print_info: n_head           = 16
0.00.040.165 I print_info: n_head_kv        = 16
0.00.040.165 I print_info: n_rot            = 32
0.00.040.166 I print_info: n_swa            = 0
0.00.040.166 I print_info: n_embd_head_k    = 128
0.00.040.166 I print_info: n_embd_head_v    = 128
0.00.040.169 I print_info: n_gqa            = 1
0.00.040.170 I print_info: n_embd_k_gqa     = 2048
0.00.040.171 I print_info: n_embd_v_gqa     = 2048
0.00.040.171 I print_info: f_norm_eps       = 1.0e-05
0.00.040.172 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.172 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.172 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.172 I print_info: f_logit_scale    = 0.0e+00
0.00.040.173 I print_info: n_ff             = 8192
0.00.040.173 I print_info: n_expert         = 0
0.00.040.173 I print_info: n_expert_used    = 0
0.00.040.173 I print_info: causal attn      = 1
0.00.040.173 I print_info: pooling type     = 0
0.00.040.173 I print_info: rope type        = 2
0.00.040.174 I print_info: rope scaling     = linear
0.00.040.174 I print_info: freq_base_train  = 10000.0
0.00.040.174 I print_info: freq_scale_train = 1
0.00.040.174 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.175 I print_info: rope_finetuned   = unknown
0.00.040.175 I print_info: ssm_d_conv       = 0
0.00.040.175 I print_info: ssm_d_inner      = 0
0.00.040.175 I print_info: ssm_d_state      = 0
0.00.040.175 I print_info: ssm_dt_rank      = 0
0.00.040.175 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.176 I print_info: model type       = 1.4B
0.00.040.176 I print_info: model params     = 1.41 B
0.00.040.176 I print_info: general.name     = 1.4B
0.00.040.177 I print_info: vocab type       = BPE
0.00.040.177 I print_info: n_vocab          = 50304
0.00.040.178 I print_info: n_merges         = 50009
0.00.040.179 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.179 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.179 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.179 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.179 I print_info: LF token         = 187 ''
0.00.040.180 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.180 I print_info: max token length = 1024
0.00.040.180 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.583.565 I load_tensors: offloading 24 repeating layers to GPU
0.00.583.582 I load_tensors: offloading output layer to GPU
0.00.583.582 I load_tensors: offloaded 25/25 layers to GPU
0.00.583.615 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.583.616 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.585.134 I llama_init_from_model: n_seq_max     = 1
0.00.585.137 I llama_init_from_model: n_ctx         = 128
0.00.585.138 I llama_init_from_model: n_ctx_per_seq = 128
0.00.585.138 I llama_init_from_model: n_batch       = 128
0.00.585.138 I llama_init_from_model: n_ubatch      = 128
0.00.585.139 I llama_init_from_model: flash_attn    = 0
0.00.585.140 I llama_init_from_model: freq_base     = 10000.0
0.00.585.141 I llama_init_from_model: freq_scale    = 1
0.00.585.142 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.585.146 I ggml_metal_init: allocating
0.00.585.247 I ggml_metal_init: found device: Apple M4
0.00.585.263 I ggml_metal_init: picking default device: Apple M4
0.00.587.101 I ggml_metal_init: using embedded metal library
0.00.593.153 I ggml_metal_init: GPU name:   Apple M4
0.00.593.161 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.593.162 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.593.163 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.593.164 I ggml_metal_init: simdgroup reduction   = true
0.00.593.164 I ggml_metal_init: simdgroup matrix mul. = true
0.00.593.164 I ggml_metal_init: has residency sets    = true
0.00.593.165 I ggml_metal_init: has bfloat            = true
0.00.593.165 I ggml_metal_init: use bfloat            = true
0.00.593.166 I ggml_metal_init: hasUnifiedMemory      = true
0.00.593.169 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.612.983 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.616.529 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.616.543 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.616.602 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.619.735 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.619.737 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.619.738 I llama_init_from_model: graph nodes  = 967
0.00.619.738 I llama_init_from_model: graph splits = 2
0.00.619.740 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.619.741 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.647.778 I 
0.00.647.852 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.647.859 I perplexity: tokenizing the input ..
0.00.654.750 I perplexity: tokenization took 6.888 ms
0.00.654.756 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.788.220 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.789.563 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.789.582 I llama_perf_context_print:        load time =     637.68 ms
0.00.789.583 I llama_perf_context_print: prompt eval time =     132.50 ms /   128 tokens (    1.04 ms per token,   966.01 tokens per second)
0.00.789.583 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.789.585 I llama_perf_context_print:       total time =     141.81 ms /   129 tokens
0.00.789.962 I ggml_metal_free: deallocating

real	0m0.805s
user	0m0.080s
sys	0m0.117s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.886 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.014 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.019 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.021 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.021 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.023 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.024 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.024 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.025 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.025 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.026 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.026 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.026 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.028 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.029 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.030 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.030 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.031 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.764 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.820 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.529 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.531 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.531 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.531 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.531 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.532 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.532 I llama_model_loader: - type  f32:  194 tensors
0.00.025.533 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.533 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.533 I print_info: file format = GGUF V3 (latest)
0.00.025.534 I print_info: file type   = Q4_1
0.00.025.535 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.344 I load: special tokens cache size = 25
0.00.039.249 I load: token to piece cache size = 0.2984 MB
0.00.039.252 I print_info: arch             = gptneox
0.00.039.252 I print_info: vocab_only       = 0
0.00.039.252 I print_info: n_ctx_train      = 2048
0.00.039.252 I print_info: n_embd           = 2048
0.00.039.253 I print_info: n_layer          = 24
0.00.039.255 I print_info: n_head           = 16
0.00.039.256 I print_info: n_head_kv        = 16
0.00.039.256 I print_info: n_rot            = 32
0.00.039.256 I print_info: n_swa            = 0
0.00.039.257 I print_info: n_embd_head_k    = 128
0.00.039.257 I print_info: n_embd_head_v    = 128
0.00.039.258 I print_info: n_gqa            = 1
0.00.039.258 I print_info: n_embd_k_gqa     = 2048
0.00.039.259 I print_info: n_embd_v_gqa     = 2048
0.00.039.264 I print_info: f_norm_eps       = 1.0e-05
0.00.039.264 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.264 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.265 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.265 I print_info: f_logit_scale    = 0.0e+00
0.00.039.266 I print_info: n_ff             = 8192
0.00.039.266 I print_info: n_expert         = 0
0.00.039.266 I print_info: n_expert_used    = 0
0.00.039.266 I print_info: causal attn      = 1
0.00.039.266 I print_info: pooling type     = 0
0.00.039.268 I print_info: rope type        = 2
0.00.039.269 I print_info: rope scaling     = linear
0.00.039.269 I print_info: freq_base_train  = 10000.0
0.00.039.270 I print_info: freq_scale_train = 1
0.00.039.270 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.270 I print_info: rope_finetuned   = unknown
0.00.039.270 I print_info: ssm_d_conv       = 0
0.00.039.270 I print_info: ssm_d_inner      = 0
0.00.039.270 I print_info: ssm_d_state      = 0
0.00.039.270 I print_info: ssm_dt_rank      = 0
0.00.039.270 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.271 I print_info: model type       = 1.4B
0.00.039.271 I print_info: model params     = 1.41 B
0.00.039.271 I print_info: general.name     = 1.4B
0.00.039.275 I print_info: vocab type       = BPE
0.00.039.275 I print_info: n_vocab          = 50304
0.00.039.275 I print_info: n_merges         = 50009
0.00.039.275 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.275 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.276 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.276 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.276 I print_info: LF token         = 187 ''
0.00.039.276 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.277 I print_info: max token length = 1024
0.00.039.277 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.633.676 I load_tensors: offloading 24 repeating layers to GPU
0.00.633.691 I load_tensors: offloading output layer to GPU
0.00.633.692 I load_tensors: offloaded 25/25 layers to GPU
0.00.633.726 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.633.727 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.635.301 I llama_init_from_model: n_seq_max     = 1
0.00.635.304 I llama_init_from_model: n_ctx         = 2048
0.00.635.304 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.635.306 I llama_init_from_model: n_batch       = 2048
0.00.635.306 I llama_init_from_model: n_ubatch      = 512
0.00.635.306 I llama_init_from_model: flash_attn    = 0
0.00.635.309 I llama_init_from_model: freq_base     = 10000.0
0.00.635.309 I llama_init_from_model: freq_scale    = 1
0.00.635.311 I ggml_metal_init: allocating
0.00.635.390 I ggml_metal_init: found device: Apple M4
0.00.635.405 I ggml_metal_init: picking default device: Apple M4
0.00.637.306 I ggml_metal_init: using embedded metal library
0.00.643.944 I ggml_metal_init: GPU name:   Apple M4
0.00.643.950 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.643.951 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.643.952 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.643.953 I ggml_metal_init: simdgroup reduction   = true
0.00.643.953 I ggml_metal_init: simdgroup matrix mul. = true
0.00.643.953 I ggml_metal_init: has residency sets    = true
0.00.643.954 I ggml_metal_init: has bfloat            = true
0.00.643.954 I ggml_metal_init: use bfloat            = true
0.00.643.955 I ggml_metal_init: hasUnifiedMemory      = true
0.00.643.957 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.663.057 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.719.649 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.719.656 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.719.692 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.724.127 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.724.129 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.724.130 I llama_init_from_model: graph nodes  = 967
0.00.724.130 I llama_init_from_model: graph splits = 2
0.00.724.141 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.724.269 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.724.270 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.780.419 I main: llama threadpool init, n_threads = 4
0.00.780.463 I 
0.00.780.487 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.780.489 I 
0.00.780.632 I sampler seed: 1234
0.00.780.637 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.780.647 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.780.648 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.780.648 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.510.311 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57304.28 tokens per second)
0.01.510.311 I llama_perf_context_print:        load time =     770.78 ms
0.01.510.312 I llama_perf_context_print: prompt eval time =      47.45 ms /     7 tokens (    6.78 ms per token,   147.52 tokens per second)
0.01.510.313 I llama_perf_context_print:        eval time =     679.54 ms /    63 runs   (   10.79 ms per token,    92.71 tokens per second)
0.01.510.313 I llama_perf_context_print:       total time =     730.65 ms /    70 tokens
0.01.510.542 I ggml_metal_free: deallocating

real	0m1.527s
user	0m0.109s
sys	0m0.199s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.929 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.421 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.427 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.433 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.434 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.434 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.434 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.434 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.436 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.436 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.438 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.438 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.438 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.439 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.439 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.441 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.441 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.442 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.201 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.312 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.030 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.032 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.032 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.033 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.033 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.033 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.034 I llama_model_loader: - type  f32:  194 tensors
0.00.025.034 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.035 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.035 I print_info: file format = GGUF V3 (latest)
0.00.025.036 I print_info: file type   = Q4_1
0.00.025.037 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.081 I load: special tokens cache size = 25
0.00.039.084 I load: token to piece cache size = 0.2984 MB
0.00.039.088 I print_info: arch             = gptneox
0.00.039.088 I print_info: vocab_only       = 0
0.00.039.088 I print_info: n_ctx_train      = 2048
0.00.039.089 I print_info: n_embd           = 2048
0.00.039.089 I print_info: n_layer          = 24
0.00.039.093 I print_info: n_head           = 16
0.00.039.094 I print_info: n_head_kv        = 16
0.00.039.094 I print_info: n_rot            = 32
0.00.039.094 I print_info: n_swa            = 0
0.00.039.094 I print_info: n_embd_head_k    = 128
0.00.039.094 I print_info: n_embd_head_v    = 128
0.00.039.095 I print_info: n_gqa            = 1
0.00.039.096 I print_info: n_embd_k_gqa     = 2048
0.00.039.097 I print_info: n_embd_v_gqa     = 2048
0.00.039.097 I print_info: f_norm_eps       = 1.0e-05
0.00.039.097 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.098 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.098 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.098 I print_info: f_logit_scale    = 0.0e+00
0.00.039.098 I print_info: n_ff             = 8192
0.00.039.099 I print_info: n_expert         = 0
0.00.039.101 I print_info: n_expert_used    = 0
0.00.039.101 I print_info: causal attn      = 1
0.00.039.102 I print_info: pooling type     = 0
0.00.039.102 I print_info: rope type        = 2
0.00.039.102 I print_info: rope scaling     = linear
0.00.039.102 I print_info: freq_base_train  = 10000.0
0.00.039.103 I print_info: freq_scale_train = 1
0.00.039.103 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.103 I print_info: rope_finetuned   = unknown
0.00.039.103 I print_info: ssm_d_conv       = 0
0.00.039.103 I print_info: ssm_d_inner      = 0
0.00.039.104 I print_info: ssm_d_state      = 0
0.00.039.104 I print_info: ssm_dt_rank      = 0
0.00.039.104 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.105 I print_info: model type       = 1.4B
0.00.039.105 I print_info: model params     = 1.41 B
0.00.039.105 I print_info: general.name     = 1.4B
0.00.039.106 I print_info: vocab type       = BPE
0.00.039.106 I print_info: n_vocab          = 50304
0.00.039.106 I print_info: n_merges         = 50009
0.00.039.106 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.106 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.108 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.108 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.108 I print_info: LF token         = 187 ''
0.00.039.108 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.108 I print_info: max token length = 1024
0.00.039.109 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.625.672 I load_tensors: offloading 24 repeating layers to GPU
0.00.625.685 I load_tensors: offloading output layer to GPU
0.00.625.686 I load_tensors: offloaded 25/25 layers to GPU
0.00.625.723 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.625.724 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.627.426 I llama_init_from_model: n_seq_max     = 1
0.00.627.429 I llama_init_from_model: n_ctx         = 128
0.00.627.430 I llama_init_from_model: n_ctx_per_seq = 128
0.00.627.430 I llama_init_from_model: n_batch       = 128
0.00.627.430 I llama_init_from_model: n_ubatch      = 128
0.00.627.431 I llama_init_from_model: flash_attn    = 0
0.00.627.433 I llama_init_from_model: freq_base     = 10000.0
0.00.627.433 I llama_init_from_model: freq_scale    = 1
0.00.627.434 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.627.437 I ggml_metal_init: allocating
0.00.627.519 I ggml_metal_init: found device: Apple M4
0.00.627.532 I ggml_metal_init: picking default device: Apple M4
0.00.629.408 I ggml_metal_init: using embedded metal library
0.00.636.015 I ggml_metal_init: GPU name:   Apple M4
0.00.636.023 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.636.024 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.636.025 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.636.025 I ggml_metal_init: simdgroup reduction   = true
0.00.636.025 I ggml_metal_init: simdgroup matrix mul. = true
0.00.636.026 I ggml_metal_init: has residency sets    = true
0.00.636.026 I ggml_metal_init: has bfloat            = true
0.00.636.026 I ggml_metal_init: use bfloat            = true
0.00.636.027 I ggml_metal_init: hasUnifiedMemory      = true
0.00.636.032 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.655.364 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.659.015 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.659.019 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.659.062 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.662.206 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.662.207 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.662.208 I llama_init_from_model: graph nodes  = 967
0.00.662.208 I llama_init_from_model: graph splits = 2
0.00.662.211 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.662.212 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.309 I 
0.00.691.396 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.402 I perplexity: tokenizing the input ..
0.00.698.463 I perplexity: tokenization took 7.058 ms
0.00.698.470 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.834.817 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.836.110 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.836.132 I llama_perf_context_print:        load time =     682.37 ms
0.00.836.133 I llama_perf_context_print: prompt eval time =     135.42 ms /   128 tokens (    1.06 ms per token,   945.19 tokens per second)
0.00.836.134 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.836.134 I llama_perf_context_print:       total time =     144.83 ms /   129 tokens
0.00.836.517 I ggml_metal_free: deallocating

real	0m0.850s
user	0m0.080s
sys	0m0.121s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.012.603 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.394 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.020.399 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.401 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.401 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.401 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.404 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.404 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.408 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.409 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.409 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.410 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.410 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.410 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.411 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.414 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.415 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.415 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.100 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.191 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.864 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.866 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.866 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.866 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.867 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.867 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.867 I llama_model_loader: - type  f32:  194 tensors
0.00.028.868 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.868 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.869 I print_info: file format = GGUF V3 (latest)
0.00.028.869 I print_info: file type   = Q5_0
0.00.028.870 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.036.690 I load: special tokens cache size = 25
0.00.042.659 I load: token to piece cache size = 0.2984 MB
0.00.042.662 I print_info: arch             = gptneox
0.00.042.662 I print_info: vocab_only       = 0
0.00.042.662 I print_info: n_ctx_train      = 2048
0.00.042.662 I print_info: n_embd           = 2048
0.00.042.663 I print_info: n_layer          = 24
0.00.042.665 I print_info: n_head           = 16
0.00.042.666 I print_info: n_head_kv        = 16
0.00.042.666 I print_info: n_rot            = 32
0.00.042.667 I print_info: n_swa            = 0
0.00.042.667 I print_info: n_embd_head_k    = 128
0.00.042.667 I print_info: n_embd_head_v    = 128
0.00.042.668 I print_info: n_gqa            = 1
0.00.042.669 I print_info: n_embd_k_gqa     = 2048
0.00.042.670 I print_info: n_embd_v_gqa     = 2048
0.00.042.671 I print_info: f_norm_eps       = 1.0e-05
0.00.042.671 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.671 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.671 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.671 I print_info: f_logit_scale    = 0.0e+00
0.00.042.672 I print_info: n_ff             = 8192
0.00.042.672 I print_info: n_expert         = 0
0.00.042.672 I print_info: n_expert_used    = 0
0.00.042.673 I print_info: causal attn      = 1
0.00.042.674 I print_info: pooling type     = 0
0.00.042.676 I print_info: rope type        = 2
0.00.042.678 I print_info: rope scaling     = linear
0.00.042.678 I print_info: freq_base_train  = 10000.0
0.00.042.678 I print_info: freq_scale_train = 1
0.00.042.678 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.679 I print_info: rope_finetuned   = unknown
0.00.042.679 I print_info: ssm_d_conv       = 0
0.00.042.679 I print_info: ssm_d_inner      = 0
0.00.042.679 I print_info: ssm_d_state      = 0
0.00.042.679 I print_info: ssm_dt_rank      = 0
0.00.042.679 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.680 I print_info: model type       = 1.4B
0.00.042.680 I print_info: model params     = 1.41 B
0.00.042.680 I print_info: general.name     = 1.4B
0.00.042.681 I print_info: vocab type       = BPE
0.00.042.681 I print_info: n_vocab          = 50304
0.00.042.681 I print_info: n_merges         = 50009
0.00.042.681 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.682 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.682 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.686 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.686 I print_info: LF token         = 187 ''
0.00.042.686 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.686 I print_info: max token length = 1024
0.00.042.687 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.622.751 I load_tensors: offloading 24 repeating layers to GPU
0.00.622.766 I load_tensors: offloading output layer to GPU
0.00.622.767 I load_tensors: offloaded 25/25 layers to GPU
0.00.622.818 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.622.821 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.624.342 I llama_init_from_model: n_seq_max     = 1
0.00.624.347 I llama_init_from_model: n_ctx         = 2048
0.00.624.348 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.624.348 I llama_init_from_model: n_batch       = 2048
0.00.624.348 I llama_init_from_model: n_ubatch      = 512
0.00.624.349 I llama_init_from_model: flash_attn    = 0
0.00.624.351 I llama_init_from_model: freq_base     = 10000.0
0.00.624.351 I llama_init_from_model: freq_scale    = 1
0.00.624.353 I ggml_metal_init: allocating
0.00.624.424 I ggml_metal_init: found device: Apple M4
0.00.624.437 I ggml_metal_init: picking default device: Apple M4
0.00.626.399 I ggml_metal_init: using embedded metal library
0.00.632.982 I ggml_metal_init: GPU name:   Apple M4
0.00.632.986 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.632.987 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.632.987 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.632.988 I ggml_metal_init: simdgroup reduction   = true
0.00.632.988 I ggml_metal_init: simdgroup matrix mul. = true
0.00.632.988 I ggml_metal_init: has residency sets    = true
0.00.632.989 I ggml_metal_init: has bfloat            = true
0.00.632.989 I ggml_metal_init: use bfloat            = true
0.00.632.990 I ggml_metal_init: hasUnifiedMemory      = true
0.00.632.992 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.650.723 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.704.613 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.704.620 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.704.666 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.709.350 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.709.352 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.709.352 I llama_init_from_model: graph nodes  = 967
0.00.709.353 I llama_init_from_model: graph splits = 2
0.00.709.359 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.709.467 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.709.468 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.993 I main: llama threadpool init, n_threads = 4
0.00.769.041 I 
0.00.769.068 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.070 I 
0.00.769.243 I sampler seed: 1234
0.00.769.248 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.267 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.267 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.268 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.557.044 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52906.11 tokens per second)
0.01.557.045 I llama_perf_context_print:        load time =     755.65 ms
0.01.557.046 I llama_perf_context_print: prompt eval time =      52.51 ms /     7 tokens (    7.50 ms per token,   133.30 tokens per second)
0.01.557.046 I llama_perf_context_print:        eval time =     732.44 ms /    63 runs   (   11.63 ms per token,    86.01 tokens per second)
0.01.557.047 I llama_perf_context_print:       total time =     788.78 ms /    70 tokens
0.01.557.307 I ggml_metal_free: deallocating

real	0m1.576s
user	0m0.109s
sys	0m0.217s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.826 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.057 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.062 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.064 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.069 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.070 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.070 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.070 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.071 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.072 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.072 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.072 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.073 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.075 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.075 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.077 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.077 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.077 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.770 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.795 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.466 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.468 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.468 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.469 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.469 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.469 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.470 I llama_model_loader: - type  f32:  194 tensors
0.00.025.470 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.471 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.471 I print_info: file format = GGUF V3 (latest)
0.00.025.475 I print_info: file type   = Q5_0
0.00.025.476 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.486 I load: special tokens cache size = 25
0.00.039.501 I load: token to piece cache size = 0.2984 MB
0.00.039.505 I print_info: arch             = gptneox
0.00.039.505 I print_info: vocab_only       = 0
0.00.039.506 I print_info: n_ctx_train      = 2048
0.00.039.506 I print_info: n_embd           = 2048
0.00.039.506 I print_info: n_layer          = 24
0.00.039.510 I print_info: n_head           = 16
0.00.039.511 I print_info: n_head_kv        = 16
0.00.039.511 I print_info: n_rot            = 32
0.00.039.511 I print_info: n_swa            = 0
0.00.039.512 I print_info: n_embd_head_k    = 128
0.00.039.514 I print_info: n_embd_head_v    = 128
0.00.039.514 I print_info: n_gqa            = 1
0.00.039.515 I print_info: n_embd_k_gqa     = 2048
0.00.039.516 I print_info: n_embd_v_gqa     = 2048
0.00.039.516 I print_info: f_norm_eps       = 1.0e-05
0.00.039.518 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.518 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.518 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.518 I print_info: f_logit_scale    = 0.0e+00
0.00.039.519 I print_info: n_ff             = 8192
0.00.039.519 I print_info: n_expert         = 0
0.00.039.519 I print_info: n_expert_used    = 0
0.00.039.520 I print_info: causal attn      = 1
0.00.039.520 I print_info: pooling type     = 0
0.00.039.520 I print_info: rope type        = 2
0.00.039.520 I print_info: rope scaling     = linear
0.00.039.520 I print_info: freq_base_train  = 10000.0
0.00.039.522 I print_info: freq_scale_train = 1
0.00.039.522 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.522 I print_info: rope_finetuned   = unknown
0.00.039.522 I print_info: ssm_d_conv       = 0
0.00.039.522 I print_info: ssm_d_inner      = 0
0.00.039.523 I print_info: ssm_d_state      = 0
0.00.039.523 I print_info: ssm_dt_rank      = 0
0.00.039.523 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.523 I print_info: model type       = 1.4B
0.00.039.523 I print_info: model params     = 1.41 B
0.00.039.523 I print_info: general.name     = 1.4B
0.00.039.524 I print_info: vocab type       = BPE
0.00.039.524 I print_info: n_vocab          = 50304
0.00.039.524 I print_info: n_merges         = 50009
0.00.039.524 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.525 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.525 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.525 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.525 I print_info: LF token         = 187 ''
0.00.039.529 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.529 I print_info: max token length = 1024
0.00.039.530 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.651.110 I load_tensors: offloading 24 repeating layers to GPU
0.00.651.125 I load_tensors: offloading output layer to GPU
0.00.651.125 I load_tensors: offloaded 25/25 layers to GPU
0.00.651.162 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.651.164 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.652.881 I llama_init_from_model: n_seq_max     = 1
0.00.652.884 I llama_init_from_model: n_ctx         = 128
0.00.652.884 I llama_init_from_model: n_ctx_per_seq = 128
0.00.652.885 I llama_init_from_model: n_batch       = 128
0.00.652.885 I llama_init_from_model: n_ubatch      = 128
0.00.652.885 I llama_init_from_model: flash_attn    = 0
0.00.652.888 I llama_init_from_model: freq_base     = 10000.0
0.00.652.889 I llama_init_from_model: freq_scale    = 1
0.00.652.889 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.652.892 I ggml_metal_init: allocating
0.00.653.003 I ggml_metal_init: found device: Apple M4
0.00.653.016 I ggml_metal_init: picking default device: Apple M4
0.00.654.996 I ggml_metal_init: using embedded metal library
0.00.661.627 I ggml_metal_init: GPU name:   Apple M4
0.00.661.632 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.661.633 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.661.634 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.661.634 I ggml_metal_init: simdgroup reduction   = true
0.00.661.635 I ggml_metal_init: simdgroup matrix mul. = true
0.00.661.635 I ggml_metal_init: has residency sets    = true
0.00.661.635 I ggml_metal_init: has bfloat            = true
0.00.661.635 I ggml_metal_init: use bfloat            = true
0.00.661.636 I ggml_metal_init: hasUnifiedMemory      = true
0.00.661.640 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.679.836 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.683.417 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.683.424 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.683.482 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.686.672 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.686.674 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.686.674 I llama_init_from_model: graph nodes  = 967
0.00.686.675 I llama_init_from_model: graph splits = 2
0.00.686.678 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.686.680 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.721.262 I 
0.00.721.343 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.721.352 I perplexity: tokenizing the input ..
0.00.728.336 I perplexity: tokenization took 6.98 ms
0.00.728.350 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.877.263 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.878.593 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.878.617 I llama_perf_context_print:        load time =     711.43 ms
0.00.878.617 I llama_perf_context_print: prompt eval time =     147.93 ms /   128 tokens (    1.16 ms per token,   865.25 tokens per second)
0.00.878.618 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.878.618 I llama_perf_context_print:       total time =     157.36 ms /   129 tokens
0.00.878.966 I ggml_metal_free: deallocating

real	0m0.898s
user	0m0.080s
sys	0m0.163s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.517 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.883 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.888 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.889 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.889 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.890 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.890 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.890 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.891 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.891 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.892 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.892 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.893 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.893 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.894 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.895 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.896 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.896 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.639 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.763 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.525 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.527 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.527 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.527 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.527 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.528 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.528 I llama_model_loader: - type  f32:  194 tensors
0.00.025.529 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.529 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.530 I print_info: file format = GGUF V3 (latest)
0.00.025.530 I print_info: file type   = Q5_1
0.00.025.531 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.245 I load: special tokens cache size = 25
0.00.039.050 I load: token to piece cache size = 0.2984 MB
0.00.039.052 I print_info: arch             = gptneox
0.00.039.053 I print_info: vocab_only       = 0
0.00.039.053 I print_info: n_ctx_train      = 2048
0.00.039.053 I print_info: n_embd           = 2048
0.00.039.053 I print_info: n_layer          = 24
0.00.039.056 I print_info: n_head           = 16
0.00.039.057 I print_info: n_head_kv        = 16
0.00.039.057 I print_info: n_rot            = 32
0.00.039.057 I print_info: n_swa            = 0
0.00.039.058 I print_info: n_embd_head_k    = 128
0.00.039.059 I print_info: n_embd_head_v    = 128
0.00.039.061 I print_info: n_gqa            = 1
0.00.039.061 I print_info: n_embd_k_gqa     = 2048
0.00.039.062 I print_info: n_embd_v_gqa     = 2048
0.00.039.063 I print_info: f_norm_eps       = 1.0e-05
0.00.039.064 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.069 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.070 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.070 I print_info: f_logit_scale    = 0.0e+00
0.00.039.072 I print_info: n_ff             = 8192
0.00.039.074 I print_info: n_expert         = 0
0.00.039.074 I print_info: n_expert_used    = 0
0.00.039.074 I print_info: causal attn      = 1
0.00.039.074 I print_info: pooling type     = 0
0.00.039.075 I print_info: rope type        = 2
0.00.039.075 I print_info: rope scaling     = linear
0.00.039.075 I print_info: freq_base_train  = 10000.0
0.00.039.076 I print_info: freq_scale_train = 1
0.00.039.076 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.076 I print_info: rope_finetuned   = unknown
0.00.039.076 I print_info: ssm_d_conv       = 0
0.00.039.076 I print_info: ssm_d_inner      = 0
0.00.039.076 I print_info: ssm_d_state      = 0
0.00.039.076 I print_info: ssm_dt_rank      = 0
0.00.039.076 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.077 I print_info: model type       = 1.4B
0.00.039.077 I print_info: model params     = 1.41 B
0.00.039.077 I print_info: general.name     = 1.4B
0.00.039.078 I print_info: vocab type       = BPE
0.00.039.078 I print_info: n_vocab          = 50304
0.00.039.078 I print_info: n_merges         = 50009
0.00.039.078 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.078 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.078 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.079 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.079 I print_info: LF token         = 187 ''
0.00.039.080 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.080 I print_info: max token length = 1024
0.00.039.080 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.716.888 I load_tensors: offloading 24 repeating layers to GPU
0.00.716.893 I load_tensors: offloading output layer to GPU
0.00.716.894 I load_tensors: offloaded 25/25 layers to GPU
0.00.716.918 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.716.919 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.718.358 I llama_init_from_model: n_seq_max     = 1
0.00.718.360 I llama_init_from_model: n_ctx         = 2048
0.00.718.360 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.718.361 I llama_init_from_model: n_batch       = 2048
0.00.718.361 I llama_init_from_model: n_ubatch      = 512
0.00.718.362 I llama_init_from_model: flash_attn    = 0
0.00.718.363 I llama_init_from_model: freq_base     = 10000.0
0.00.718.363 I llama_init_from_model: freq_scale    = 1
0.00.718.365 I ggml_metal_init: allocating
0.00.718.382 I ggml_metal_init: found device: Apple M4
0.00.718.391 I ggml_metal_init: picking default device: Apple M4
0.00.719.909 I ggml_metal_init: using embedded metal library
0.00.726.064 I ggml_metal_init: GPU name:   Apple M4
0.00.726.068 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.726.069 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.726.070 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.726.071 I ggml_metal_init: simdgroup reduction   = true
0.00.726.071 I ggml_metal_init: simdgroup matrix mul. = true
0.00.726.071 I ggml_metal_init: has residency sets    = true
0.00.726.072 I ggml_metal_init: has bfloat            = true
0.00.726.072 I ggml_metal_init: use bfloat            = true
0.00.726.073 I ggml_metal_init: hasUnifiedMemory      = true
0.00.726.074 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.743.756 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.797.091 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.797.099 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.797.133 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.801.687 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.801.689 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.801.689 I llama_init_from_model: graph nodes  = 967
0.00.801.689 I llama_init_from_model: graph splits = 2
0.00.801.695 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.801.826 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.801.826 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.861.404 I main: llama threadpool init, n_threads = 4
0.00.861.449 I 
0.00.861.474 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.861.474 I 
0.00.861.626 I sampler seed: 1234
0.00.861.631 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.861.651 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.861.651 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.861.651 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.698.892 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53024.65 tokens per second)
0.01.698.892 I llama_perf_context_print:        load time =     852.17 ms
0.01.698.893 I llama_perf_context_print: prompt eval time =      51.92 ms /     7 tokens (    7.42 ms per token,   134.83 tokens per second)
0.01.698.894 I llama_perf_context_print:        eval time =     782.44 ms /    63 runs   (   12.42 ms per token,    80.52 tokens per second)
0.01.698.895 I llama_perf_context_print:       total time =     838.21 ms /    70 tokens
0.01.699.141 I ggml_metal_free: deallocating

real	0m1.717s
user	0m0.108s
sys	0m0.240s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.961 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.236 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.241 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.243 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.250 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.250 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.250 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.251 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.252 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.252 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.253 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.253 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.253 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.255 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.255 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.256 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.257 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.257 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.944 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.108 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.827 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.828 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.828 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.829 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.829 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.829 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.830 I llama_model_loader: - type  f32:  194 tensors
0.00.024.830 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.831 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.831 I print_info: file format = GGUF V3 (latest)
0.00.024.832 I print_info: file type   = Q5_1
0.00.024.833 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.925 I load: special tokens cache size = 25
0.00.038.902 I load: token to piece cache size = 0.2984 MB
0.00.038.906 I print_info: arch             = gptneox
0.00.038.906 I print_info: vocab_only       = 0
0.00.038.907 I print_info: n_ctx_train      = 2048
0.00.038.907 I print_info: n_embd           = 2048
0.00.038.907 I print_info: n_layer          = 24
0.00.038.911 I print_info: n_head           = 16
0.00.038.912 I print_info: n_head_kv        = 16
0.00.038.912 I print_info: n_rot            = 32
0.00.038.912 I print_info: n_swa            = 0
0.00.038.913 I print_info: n_embd_head_k    = 128
0.00.038.913 I print_info: n_embd_head_v    = 128
0.00.038.914 I print_info: n_gqa            = 1
0.00.038.917 I print_info: n_embd_k_gqa     = 2048
0.00.038.917 I print_info: n_embd_v_gqa     = 2048
0.00.038.918 I print_info: f_norm_eps       = 1.0e-05
0.00.038.918 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.918 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.918 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.918 I print_info: f_logit_scale    = 0.0e+00
0.00.038.919 I print_info: n_ff             = 8192
0.00.038.919 I print_info: n_expert         = 0
0.00.038.919 I print_info: n_expert_used    = 0
0.00.038.919 I print_info: causal attn      = 1
0.00.038.920 I print_info: pooling type     = 0
0.00.038.921 I print_info: rope type        = 2
0.00.038.921 I print_info: rope scaling     = linear
0.00.038.921 I print_info: freq_base_train  = 10000.0
0.00.038.921 I print_info: freq_scale_train = 1
0.00.038.922 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.922 I print_info: rope_finetuned   = unknown
0.00.038.922 I print_info: ssm_d_conv       = 0
0.00.038.922 I print_info: ssm_d_inner      = 0
0.00.038.922 I print_info: ssm_d_state      = 0
0.00.038.922 I print_info: ssm_dt_rank      = 0
0.00.038.922 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.923 I print_info: model type       = 1.4B
0.00.038.923 I print_info: model params     = 1.41 B
0.00.038.923 I print_info: general.name     = 1.4B
0.00.038.925 I print_info: vocab type       = BPE
0.00.038.925 I print_info: n_vocab          = 50304
0.00.038.925 I print_info: n_merges         = 50009
0.00.038.925 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.925 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.926 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.926 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.926 I print_info: LF token         = 187 ''
0.00.038.926 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.927 I print_info: max token length = 1024
0.00.038.927 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.681.611 I load_tensors: offloading 24 repeating layers to GPU
0.00.681.626 I load_tensors: offloading output layer to GPU
0.00.681.626 I load_tensors: offloaded 25/25 layers to GPU
0.00.681.662 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.681.663 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.683.422 I llama_init_from_model: n_seq_max     = 1
0.00.683.425 I llama_init_from_model: n_ctx         = 128
0.00.683.425 I llama_init_from_model: n_ctx_per_seq = 128
0.00.683.426 I llama_init_from_model: n_batch       = 128
0.00.683.426 I llama_init_from_model: n_ubatch      = 128
0.00.683.427 I llama_init_from_model: flash_attn    = 0
0.00.683.429 I llama_init_from_model: freq_base     = 10000.0
0.00.683.429 I llama_init_from_model: freq_scale    = 1
0.00.683.430 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.683.432 I ggml_metal_init: allocating
0.00.683.522 I ggml_metal_init: found device: Apple M4
0.00.683.536 I ggml_metal_init: picking default device: Apple M4
0.00.685.361 I ggml_metal_init: using embedded metal library
0.00.692.192 I ggml_metal_init: GPU name:   Apple M4
0.00.692.198 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.692.198 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.692.199 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.692.202 I ggml_metal_init: simdgroup reduction   = true
0.00.692.203 I ggml_metal_init: simdgroup matrix mul. = true
0.00.692.203 I ggml_metal_init: has residency sets    = true
0.00.692.203 I ggml_metal_init: has bfloat            = true
0.00.692.204 I ggml_metal_init: use bfloat            = true
0.00.692.205 I ggml_metal_init: hasUnifiedMemory      = true
0.00.692.212 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.709.636 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.713.131 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.713.135 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.713.184 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.716.472 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.716.474 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.716.474 I llama_init_from_model: graph nodes  = 967
0.00.716.475 I llama_init_from_model: graph splits = 2
0.00.716.477 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.716.478 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.195 I 
0.00.747.271 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.277 I perplexity: tokenizing the input ..
0.00.755.041 I perplexity: tokenization took 7.761 ms
0.00.755.055 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.900.108 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.901.440 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.901.461 I llama_perf_context_print:        load time =     738.22 ms
0.00.901.462 I llama_perf_context_print: prompt eval time =     144.17 ms /   128 tokens (    1.13 ms per token,   887.82 tokens per second)
0.00.901.462 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.901.463 I llama_perf_context_print:       total time =     154.27 ms /   129 tokens
0.00.901.780 I ggml_metal_free: deallocating

real	0m0.916s
user	0m0.080s
sys	0m0.145s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.010.130 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.783 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.789 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.791 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.791 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.792 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.792 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.794 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.795 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.795 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.796 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.796 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.800 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.801 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.801 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.803 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.803 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.803 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.457 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.494 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.155 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.157 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.157 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.157 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.157 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.158 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.158 I llama_model_loader: - type  f32:  194 tensors
0.00.025.159 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.159 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.159 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.160 I print_info: file format = GGUF V3 (latest)
0.00.025.160 I print_info: file type   = Q2_K - Medium
0.00.025.161 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.230 I load: special tokens cache size = 25
0.00.039.161 I load: token to piece cache size = 0.2984 MB
0.00.039.163 I print_info: arch             = gptneox
0.00.039.164 I print_info: vocab_only       = 0
0.00.039.164 I print_info: n_ctx_train      = 2048
0.00.039.164 I print_info: n_embd           = 2048
0.00.039.164 I print_info: n_layer          = 24
0.00.039.167 I print_info: n_head           = 16
0.00.039.167 I print_info: n_head_kv        = 16
0.00.039.168 I print_info: n_rot            = 32
0.00.039.168 I print_info: n_swa            = 0
0.00.039.168 I print_info: n_embd_head_k    = 128
0.00.039.168 I print_info: n_embd_head_v    = 128
0.00.039.171 I print_info: n_gqa            = 1
0.00.039.171 I print_info: n_embd_k_gqa     = 2048
0.00.039.172 I print_info: n_embd_v_gqa     = 2048
0.00.039.173 I print_info: f_norm_eps       = 1.0e-05
0.00.039.173 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.173 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.173 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.173 I print_info: f_logit_scale    = 0.0e+00
0.00.039.174 I print_info: n_ff             = 8192
0.00.039.174 I print_info: n_expert         = 0
0.00.039.176 I print_info: n_expert_used    = 0
0.00.039.176 I print_info: causal attn      = 1
0.00.039.176 I print_info: pooling type     = 0
0.00.039.176 I print_info: rope type        = 2
0.00.039.177 I print_info: rope scaling     = linear
0.00.039.177 I print_info: freq_base_train  = 10000.0
0.00.039.177 I print_info: freq_scale_train = 1
0.00.039.178 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.178 I print_info: rope_finetuned   = unknown
0.00.039.178 I print_info: ssm_d_conv       = 0
0.00.039.178 I print_info: ssm_d_inner      = 0
0.00.039.178 I print_info: ssm_d_state      = 0
0.00.039.178 I print_info: ssm_dt_rank      = 0
0.00.039.178 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.179 I print_info: model type       = 1.4B
0.00.039.179 I print_info: model params     = 1.41 B
0.00.039.180 I print_info: general.name     = 1.4B
0.00.039.180 I print_info: vocab type       = BPE
0.00.039.180 I print_info: n_vocab          = 50304
0.00.039.180 I print_info: n_merges         = 50009
0.00.039.185 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.185 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.185 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.185 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.185 I print_info: LF token         = 187 ''
0.00.039.186 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.186 I print_info: max token length = 1024
0.00.039.186 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.365.617 I load_tensors: offloading 24 repeating layers to GPU
0.00.365.625 I load_tensors: offloading output layer to GPU
0.00.365.625 I load_tensors: offloaded 25/25 layers to GPU
0.00.365.661 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.365.662 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.367.399 I llama_init_from_model: n_seq_max     = 1
0.00.367.403 I llama_init_from_model: n_ctx         = 2048
0.00.367.403 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.367.404 I llama_init_from_model: n_batch       = 2048
0.00.367.404 I llama_init_from_model: n_ubatch      = 512
0.00.367.405 I llama_init_from_model: flash_attn    = 0
0.00.367.407 I llama_init_from_model: freq_base     = 10000.0
0.00.367.407 I llama_init_from_model: freq_scale    = 1
0.00.367.409 I ggml_metal_init: allocating
0.00.367.491 I ggml_metal_init: found device: Apple M4
0.00.367.510 I ggml_metal_init: picking default device: Apple M4
0.00.369.448 I ggml_metal_init: using embedded metal library
0.00.375.083 I ggml_metal_init: GPU name:   Apple M4
0.00.375.097 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.375.098 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.375.099 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.375.100 I ggml_metal_init: simdgroup reduction   = true
0.00.375.100 I ggml_metal_init: simdgroup matrix mul. = true
0.00.375.100 I ggml_metal_init: has residency sets    = true
0.00.375.101 I ggml_metal_init: has bfloat            = true
0.00.375.101 I ggml_metal_init: use bfloat            = true
0.00.375.103 I ggml_metal_init: hasUnifiedMemory      = true
0.00.375.108 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.397.095 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.456.597 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.456.605 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.456.639 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.461.130 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.461.132 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.461.132 I llama_init_from_model: graph nodes  = 967
0.00.461.132 I llama_init_from_model: graph splits = 2
0.00.461.138 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.461.261 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.461.261 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.524.105 I main: llama threadpool init, n_threads = 4
0.00.524.146 I 
0.00.524.169 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.524.170 I 
0.00.524.336 I sampler seed: 1234
0.00.524.341 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.524.352 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.524.353 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.524.353 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.203.918 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51862.67 tokens per second)
0.01.203.919 I llama_perf_context_print:        load time =     513.27 ms
0.01.203.920 I llama_perf_context_print: prompt eval time =      43.20 ms /     7 tokens (    6.17 ms per token,   162.06 tokens per second)
0.01.203.921 I llama_perf_context_print:        eval time =     633.54 ms /    63 runs   (   10.06 ms per token,    99.44 tokens per second)
0.01.203.921 I llama_perf_context_print:       total time =     680.52 ms /    70 tokens
0.01.204.117 I ggml_metal_free: deallocating

real	0m1.222s
user	0m0.112s
sys	0m0.182s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.114 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.849 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.927 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.933 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.935 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.935 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.936 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.936 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.936 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.937 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.938 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.938 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.938 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.939 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.939 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.940 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.941 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.942 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.942 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.739 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.827 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.654 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.656 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.656 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.656 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.657 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.657 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.658 I llama_model_loader: - type  f32:  194 tensors
0.00.025.658 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.658 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.658 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.659 I print_info: file format = GGUF V3 (latest)
0.00.025.660 I print_info: file type   = Q2_K - Medium
0.00.025.661 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.883 I load: special tokens cache size = 25
0.00.039.929 I load: token to piece cache size = 0.2984 MB
0.00.039.933 I print_info: arch             = gptneox
0.00.039.934 I print_info: vocab_only       = 0
0.00.039.934 I print_info: n_ctx_train      = 2048
0.00.039.934 I print_info: n_embd           = 2048
0.00.039.934 I print_info: n_layer          = 24
0.00.039.939 I print_info: n_head           = 16
0.00.039.940 I print_info: n_head_kv        = 16
0.00.039.940 I print_info: n_rot            = 32
0.00.039.940 I print_info: n_swa            = 0
0.00.039.940 I print_info: n_embd_head_k    = 128
0.00.039.942 I print_info: n_embd_head_v    = 128
0.00.039.942 I print_info: n_gqa            = 1
0.00.039.943 I print_info: n_embd_k_gqa     = 2048
0.00.039.943 I print_info: n_embd_v_gqa     = 2048
0.00.039.944 I print_info: f_norm_eps       = 1.0e-05
0.00.039.944 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.945 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.946 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.947 I print_info: f_logit_scale    = 0.0e+00
0.00.039.947 I print_info: n_ff             = 8192
0.00.039.947 I print_info: n_expert         = 0
0.00.039.947 I print_info: n_expert_used    = 0
0.00.039.948 I print_info: causal attn      = 1
0.00.039.948 I print_info: pooling type     = 0
0.00.039.948 I print_info: rope type        = 2
0.00.039.948 I print_info: rope scaling     = linear
0.00.039.950 I print_info: freq_base_train  = 10000.0
0.00.039.950 I print_info: freq_scale_train = 1
0.00.039.950 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.951 I print_info: rope_finetuned   = unknown
0.00.039.951 I print_info: ssm_d_conv       = 0
0.00.039.951 I print_info: ssm_d_inner      = 0
0.00.039.951 I print_info: ssm_d_state      = 0
0.00.039.951 I print_info: ssm_dt_rank      = 0
0.00.039.951 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.951 I print_info: model type       = 1.4B
0.00.039.958 I print_info: model params     = 1.41 B
0.00.039.962 I print_info: general.name     = 1.4B
0.00.039.962 I print_info: vocab type       = BPE
0.00.039.963 I print_info: n_vocab          = 50304
0.00.039.963 I print_info: n_merges         = 50009
0.00.039.963 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.963 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.963 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.964 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.964 I print_info: LF token         = 187 ''
0.00.039.964 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.964 I print_info: max token length = 1024
0.00.039.965 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.358.395 I load_tensors: offloading 24 repeating layers to GPU
0.00.358.410 I load_tensors: offloading output layer to GPU
0.00.358.411 I load_tensors: offloaded 25/25 layers to GPU
0.00.358.442 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.358.443 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.360.180 I llama_init_from_model: n_seq_max     = 1
0.00.360.186 I llama_init_from_model: n_ctx         = 128
0.00.360.186 I llama_init_from_model: n_ctx_per_seq = 128
0.00.360.187 I llama_init_from_model: n_batch       = 128
0.00.360.187 I llama_init_from_model: n_ubatch      = 128
0.00.360.187 I llama_init_from_model: flash_attn    = 0
0.00.360.189 I llama_init_from_model: freq_base     = 10000.0
0.00.360.190 I llama_init_from_model: freq_scale    = 1
0.00.360.191 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.360.192 I ggml_metal_init: allocating
0.00.360.277 I ggml_metal_init: found device: Apple M4
0.00.360.291 I ggml_metal_init: picking default device: Apple M4
0.00.362.076 I ggml_metal_init: using embedded metal library
0.00.367.474 I ggml_metal_init: GPU name:   Apple M4
0.00.367.486 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.367.487 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.367.487 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.367.488 I ggml_metal_init: simdgroup reduction   = true
0.00.367.489 I ggml_metal_init: simdgroup matrix mul. = true
0.00.367.489 I ggml_metal_init: has residency sets    = true
0.00.367.489 I ggml_metal_init: has bfloat            = true
0.00.367.490 I ggml_metal_init: use bfloat            = true
0.00.367.492 I ggml_metal_init: hasUnifiedMemory      = true
0.00.367.504 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.389.280 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.392.894 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.392.901 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.392.953 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.396.193 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.396.195 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.396.195 I llama_init_from_model: graph nodes  = 967
0.00.396.196 I llama_init_from_model: graph splits = 2
0.00.396.199 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.396.199 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.427.927 I 
0.00.428.024 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.428.033 I perplexity: tokenizing the input ..
0.00.434.579 I perplexity: tokenization took 6.545 ms
0.00.434.585 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.576.149 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.577.476 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.577.497 I llama_perf_context_print:        load time =     418.07 ms
0.00.577.501 I llama_perf_context_print: prompt eval time =     141.18 ms /   128 tokens (    1.10 ms per token,   906.64 tokens per second)
0.00.577.502 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.577.503 I llama_perf_context_print:       total time =     149.57 ms /   129 tokens
0.00.577.866 I ggml_metal_free: deallocating

real	0m0.593s
user	0m0.081s
sys	0m0.098s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.740 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.775 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.780 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.782 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.782 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.783 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.783 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.783 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.784 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.784 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.786 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.788 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.788 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.788 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.789 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.791 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.792 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.792 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.473 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.507 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.136 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.137 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.138 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.138 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.138 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.139 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.139 I llama_model_loader: - type  f32:  194 tensors
0.00.025.140 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.140 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.140 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.140 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.141 I print_info: file format = GGUF V3 (latest)
0.00.025.141 I print_info: file type   = Q3_K - Medium
0.00.025.142 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.986 I load: special tokens cache size = 25
0.00.038.959 I load: token to piece cache size = 0.2984 MB
0.00.038.962 I print_info: arch             = gptneox
0.00.038.962 I print_info: vocab_only       = 0
0.00.038.962 I print_info: n_ctx_train      = 2048
0.00.038.962 I print_info: n_embd           = 2048
0.00.038.963 I print_info: n_layer          = 24
0.00.038.970 I print_info: n_head           = 16
0.00.038.971 I print_info: n_head_kv        = 16
0.00.038.971 I print_info: n_rot            = 32
0.00.038.972 I print_info: n_swa            = 0
0.00.038.972 I print_info: n_embd_head_k    = 128
0.00.038.973 I print_info: n_embd_head_v    = 128
0.00.038.974 I print_info: n_gqa            = 1
0.00.038.975 I print_info: n_embd_k_gqa     = 2048
0.00.038.975 I print_info: n_embd_v_gqa     = 2048
0.00.038.976 I print_info: f_norm_eps       = 1.0e-05
0.00.038.979 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.980 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.980 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.982 I print_info: f_logit_scale    = 0.0e+00
0.00.038.983 I print_info: n_ff             = 8192
0.00.038.983 I print_info: n_expert         = 0
0.00.038.983 I print_info: n_expert_used    = 0
0.00.038.985 I print_info: causal attn      = 1
0.00.038.986 I print_info: pooling type     = 0
0.00.038.986 I print_info: rope type        = 2
0.00.038.987 I print_info: rope scaling     = linear
0.00.038.987 I print_info: freq_base_train  = 10000.0
0.00.038.987 I print_info: freq_scale_train = 1
0.00.038.987 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.990 I print_info: rope_finetuned   = unknown
0.00.038.990 I print_info: ssm_d_conv       = 0
0.00.038.990 I print_info: ssm_d_inner      = 0
0.00.038.990 I print_info: ssm_d_state      = 0
0.00.038.990 I print_info: ssm_dt_rank      = 0
0.00.038.990 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.990 I print_info: model type       = 1.4B
0.00.038.991 I print_info: model params     = 1.41 B
0.00.038.991 I print_info: general.name     = 1.4B
0.00.038.991 I print_info: vocab type       = BPE
0.00.038.992 I print_info: n_vocab          = 50304
0.00.038.992 I print_info: n_merges         = 50009
0.00.038.992 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.995 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.995 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.995 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.995 I print_info: LF token         = 187 ''
0.00.038.995 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.997 I print_info: max token length = 1024
0.00.038.997 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.440.400 I load_tensors: offloading 24 repeating layers to GPU
0.00.440.416 I load_tensors: offloading output layer to GPU
0.00.440.417 I load_tensors: offloaded 25/25 layers to GPU
0.00.440.448 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.440.449 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.442.102 I llama_init_from_model: n_seq_max     = 1
0.00.442.106 I llama_init_from_model: n_ctx         = 2048
0.00.442.107 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.442.108 I llama_init_from_model: n_batch       = 2048
0.00.442.108 I llama_init_from_model: n_ubatch      = 512
0.00.442.109 I llama_init_from_model: flash_attn    = 0
0.00.442.111 I llama_init_from_model: freq_base     = 10000.0
0.00.442.111 I llama_init_from_model: freq_scale    = 1
0.00.442.113 I ggml_metal_init: allocating
0.00.442.194 I ggml_metal_init: found device: Apple M4
0.00.442.206 I ggml_metal_init: picking default device: Apple M4
0.00.444.050 I ggml_metal_init: using embedded metal library
0.00.449.579 I ggml_metal_init: GPU name:   Apple M4
0.00.449.598 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.449.599 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.449.600 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.449.601 I ggml_metal_init: simdgroup reduction   = true
0.00.449.601 I ggml_metal_init: simdgroup matrix mul. = true
0.00.449.601 I ggml_metal_init: has residency sets    = true
0.00.449.601 I ggml_metal_init: has bfloat            = true
0.00.449.602 I ggml_metal_init: use bfloat            = true
0.00.449.604 I ggml_metal_init: hasUnifiedMemory      = true
0.00.449.608 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.470.728 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.525.763 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.525.770 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.525.805 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.529.828 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.529.830 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.529.830 I llama_init_from_model: graph nodes  = 967
0.00.529.831 I llama_init_from_model: graph splits = 2
0.00.529.841 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.529.956 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.529.956 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.587.091 I main: llama threadpool init, n_threads = 4
0.00.587.137 I 
0.00.587.161 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.587.161 I 
0.00.587.321 I sampler seed: 1234
0.00.587.326 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.587.349 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.587.350 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.587.350 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.334.142 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50247.70 tokens per second)
0.01.334.142 I llama_perf_context_print:        load time =     577.63 ms
0.01.334.143 I llama_perf_context_print: prompt eval time =      45.03 ms /     7 tokens (    6.43 ms per token,   155.45 tokens per second)
0.01.334.144 I llama_perf_context_print:        eval time =     698.81 ms /    63 runs   (   11.09 ms per token,    90.15 tokens per second)
0.01.334.144 I llama_perf_context_print:       total time =     747.77 ms /    70 tokens
0.01.334.378 I ggml_metal_free: deallocating

real	0m1.351s
user	0m0.112s
sys	0m0.176s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.748 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.787 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.794 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.796 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.796 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.796 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.797 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.797 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.798 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.798 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.799 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.799 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.799 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.800 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.800 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.802 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.802 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.803 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.536 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.634 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.348 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.350 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.350 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.351 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.351 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.352 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.352 I llama_model_loader: - type  f32:  194 tensors
0.00.024.353 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.353 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.353 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.353 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.356 I print_info: file format = GGUF V3 (latest)
0.00.024.357 I print_info: file type   = Q3_K - Medium
0.00.024.364 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.470 I load: special tokens cache size = 25
0.00.038.363 I load: token to piece cache size = 0.2984 MB
0.00.038.368 I print_info: arch             = gptneox
0.00.038.368 I print_info: vocab_only       = 0
0.00.038.368 I print_info: n_ctx_train      = 2048
0.00.038.368 I print_info: n_embd           = 2048
0.00.038.368 I print_info: n_layer          = 24
0.00.038.372 I print_info: n_head           = 16
0.00.038.373 I print_info: n_head_kv        = 16
0.00.038.373 I print_info: n_rot            = 32
0.00.038.373 I print_info: n_swa            = 0
0.00.038.373 I print_info: n_embd_head_k    = 128
0.00.038.373 I print_info: n_embd_head_v    = 128
0.00.038.374 I print_info: n_gqa            = 1
0.00.038.375 I print_info: n_embd_k_gqa     = 2048
0.00.038.375 I print_info: n_embd_v_gqa     = 2048
0.00.038.376 I print_info: f_norm_eps       = 1.0e-05
0.00.038.377 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.377 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.377 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.377 I print_info: f_logit_scale    = 0.0e+00
0.00.038.378 I print_info: n_ff             = 8192
0.00.038.378 I print_info: n_expert         = 0
0.00.038.378 I print_info: n_expert_used    = 0
0.00.038.378 I print_info: causal attn      = 1
0.00.038.378 I print_info: pooling type     = 0
0.00.038.378 I print_info: rope type        = 2
0.00.038.379 I print_info: rope scaling     = linear
0.00.038.379 I print_info: freq_base_train  = 10000.0
0.00.038.379 I print_info: freq_scale_train = 1
0.00.038.379 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.380 I print_info: rope_finetuned   = unknown
0.00.038.380 I print_info: ssm_d_conv       = 0
0.00.038.380 I print_info: ssm_d_inner      = 0
0.00.038.380 I print_info: ssm_d_state      = 0
0.00.038.380 I print_info: ssm_dt_rank      = 0
0.00.038.383 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.383 I print_info: model type       = 1.4B
0.00.038.383 I print_info: model params     = 1.41 B
0.00.038.383 I print_info: general.name     = 1.4B
0.00.038.384 I print_info: vocab type       = BPE
0.00.038.385 I print_info: n_vocab          = 50304
0.00.038.385 I print_info: n_merges         = 50009
0.00.038.385 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.385 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.387 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.387 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.387 I print_info: LF token         = 187 ''
0.00.038.387 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.387 I print_info: max token length = 1024
0.00.038.388 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.455.799 I load_tensors: offloading 24 repeating layers to GPU
0.00.455.807 I load_tensors: offloading output layer to GPU
0.00.455.808 I load_tensors: offloaded 25/25 layers to GPU
0.00.455.839 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.455.840 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.457.419 I llama_init_from_model: n_seq_max     = 1
0.00.457.423 I llama_init_from_model: n_ctx         = 128
0.00.457.423 I llama_init_from_model: n_ctx_per_seq = 128
0.00.457.424 I llama_init_from_model: n_batch       = 128
0.00.457.424 I llama_init_from_model: n_ubatch      = 128
0.00.457.424 I llama_init_from_model: flash_attn    = 0
0.00.457.426 I llama_init_from_model: freq_base     = 10000.0
0.00.457.427 I llama_init_from_model: freq_scale    = 1
0.00.457.427 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.457.429 I ggml_metal_init: allocating
0.00.457.530 I ggml_metal_init: found device: Apple M4
0.00.457.544 I ggml_metal_init: picking default device: Apple M4
0.00.459.395 I ggml_metal_init: using embedded metal library
0.00.464.996 I ggml_metal_init: GPU name:   Apple M4
0.00.465.011 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.465.012 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.465.013 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.465.013 I ggml_metal_init: simdgroup reduction   = true
0.00.465.013 I ggml_metal_init: simdgroup matrix mul. = true
0.00.465.014 I ggml_metal_init: has residency sets    = true
0.00.465.014 I ggml_metal_init: has bfloat            = true
0.00.465.014 I ggml_metal_init: use bfloat            = true
0.00.465.016 I ggml_metal_init: hasUnifiedMemory      = true
0.00.465.022 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.485.903 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.489.621 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.489.625 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.489.664 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.493.089 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.493.092 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.493.092 I llama_init_from_model: graph nodes  = 967
0.00.493.093 I llama_init_from_model: graph splits = 2
0.00.493.096 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.493.096 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.521.415 I 
0.00.521.498 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.521.504 I perplexity: tokenizing the input ..
0.00.527.947 I perplexity: tokenization took 6.441 ms
0.00.527.953 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.663.812 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.665.171 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.665.196 I llama_perf_context_print:        load time =     512.66 ms
0.00.665.197 I llama_perf_context_print: prompt eval time =     135.45 ms /   128 tokens (    1.06 ms per token,   945.00 tokens per second)
0.00.665.198 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.665.198 I llama_perf_context_print:       total time =     143.78 ms /   129 tokens
0.00.665.538 I ggml_metal_free: deallocating

real	0m0.679s
user	0m0.079s
sys	0m0.121s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.646 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.750 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.756 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.762 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.763 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.763 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.763 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.764 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.766 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.766 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.766 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.767 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.767 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.767 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.768 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.770 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.770 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.770 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.502 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.675 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.352 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.353 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.353 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.353 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.354 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.354 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.355 I llama_model_loader: - type  f32:  194 tensors
0.00.025.355 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.355 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.355 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.356 I print_info: file format = GGUF V3 (latest)
0.00.025.356 I print_info: file type   = Q4_K - Medium
0.00.025.357 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.166 I load: special tokens cache size = 25
0.00.039.112 I load: token to piece cache size = 0.2984 MB
0.00.039.114 I print_info: arch             = gptneox
0.00.039.115 I print_info: vocab_only       = 0
0.00.039.115 I print_info: n_ctx_train      = 2048
0.00.039.115 I print_info: n_embd           = 2048
0.00.039.115 I print_info: n_layer          = 24
0.00.039.118 I print_info: n_head           = 16
0.00.039.118 I print_info: n_head_kv        = 16
0.00.039.118 I print_info: n_rot            = 32
0.00.039.119 I print_info: n_swa            = 0
0.00.039.119 I print_info: n_embd_head_k    = 128
0.00.039.119 I print_info: n_embd_head_v    = 128
0.00.039.120 I print_info: n_gqa            = 1
0.00.039.121 I print_info: n_embd_k_gqa     = 2048
0.00.039.121 I print_info: n_embd_v_gqa     = 2048
0.00.039.122 I print_info: f_norm_eps       = 1.0e-05
0.00.039.122 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.122 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.122 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.123 I print_info: f_logit_scale    = 0.0e+00
0.00.039.123 I print_info: n_ff             = 8192
0.00.039.123 I print_info: n_expert         = 0
0.00.039.124 I print_info: n_expert_used    = 0
0.00.039.124 I print_info: causal attn      = 1
0.00.039.124 I print_info: pooling type     = 0
0.00.039.124 I print_info: rope type        = 2
0.00.039.124 I print_info: rope scaling     = linear
0.00.039.125 I print_info: freq_base_train  = 10000.0
0.00.039.125 I print_info: freq_scale_train = 1
0.00.039.125 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.125 I print_info: rope_finetuned   = unknown
0.00.039.125 I print_info: ssm_d_conv       = 0
0.00.039.125 I print_info: ssm_d_inner      = 0
0.00.039.126 I print_info: ssm_d_state      = 0
0.00.039.126 I print_info: ssm_dt_rank      = 0
0.00.039.127 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.128 I print_info: model type       = 1.4B
0.00.039.128 I print_info: model params     = 1.41 B
0.00.039.128 I print_info: general.name     = 1.4B
0.00.039.128 I print_info: vocab type       = BPE
0.00.039.130 I print_info: n_vocab          = 50304
0.00.039.130 I print_info: n_merges         = 50009
0.00.039.131 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.131 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.131 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.131 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.131 I print_info: LF token         = 187 ''
0.00.039.132 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.132 I print_info: max token length = 1024
0.00.039.132 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.528.118 I load_tensors: offloading 24 repeating layers to GPU
0.00.528.134 I load_tensors: offloading output layer to GPU
0.00.528.134 I load_tensors: offloaded 25/25 layers to GPU
0.00.528.167 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.528.168 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.529.638 I llama_init_from_model: n_seq_max     = 1
0.00.529.642 I llama_init_from_model: n_ctx         = 2048
0.00.529.642 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.529.643 I llama_init_from_model: n_batch       = 2048
0.00.529.643 I llama_init_from_model: n_ubatch      = 512
0.00.529.644 I llama_init_from_model: flash_attn    = 0
0.00.529.646 I llama_init_from_model: freq_base     = 10000.0
0.00.529.646 I llama_init_from_model: freq_scale    = 1
0.00.529.648 I ggml_metal_init: allocating
0.00.529.719 I ggml_metal_init: found device: Apple M4
0.00.529.734 I ggml_metal_init: picking default device: Apple M4
0.00.531.592 I ggml_metal_init: using embedded metal library
0.00.538.279 I ggml_metal_init: GPU name:   Apple M4
0.00.538.284 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.538.284 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.538.285 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.538.286 I ggml_metal_init: simdgroup reduction   = true
0.00.538.286 I ggml_metal_init: simdgroup matrix mul. = true
0.00.538.286 I ggml_metal_init: has residency sets    = true
0.00.538.287 I ggml_metal_init: has bfloat            = true
0.00.538.287 I ggml_metal_init: use bfloat            = true
0.00.538.288 I ggml_metal_init: hasUnifiedMemory      = true
0.00.538.289 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.556.668 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.616.553 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.616.559 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.616.595 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.620.786 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.620.787 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.620.788 I llama_init_from_model: graph nodes  = 967
0.00.620.788 I llama_init_from_model: graph splits = 2
0.00.620.793 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.620.922 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.620.922 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.675.500 I main: llama threadpool init, n_threads = 4
0.00.675.541 I 
0.00.675.564 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.675.564 I 
0.00.675.716 I sampler seed: 1234
0.00.675.721 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.675.760 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.675.764 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.675.764 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.431.085 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 48005.41 tokens per second)
0.01.431.085 I llama_perf_context_print:        load time =     666.10 ms
0.01.431.087 I llama_perf_context_print: prompt eval time =      47.49 ms /     7 tokens (    6.78 ms per token,   147.38 tokens per second)
0.01.431.088 I llama_perf_context_print:        eval time =     704.80 ms /    63 runs   (   11.19 ms per token,    89.39 tokens per second)
0.01.431.088 I llama_perf_context_print:       total time =     756.34 ms /    70 tokens
0.01.431.354 I ggml_metal_free: deallocating

real	0m1.448s
user	0m0.111s
sys	0m0.203s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.885 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.054 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.060 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.067 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.068 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.068 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.069 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.069 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.070 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.070 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.071 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.071 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.071 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.072 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.072 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.074 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.075 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.075 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.877 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.029 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.790 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.791 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.792 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.792 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.792 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.793 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.794 I llama_model_loader: - type  f32:  194 tensors
0.00.024.794 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.794 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.795 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.799 I print_info: file format = GGUF V3 (latest)
0.00.024.800 I print_info: file type   = Q4_K - Medium
0.00.024.801 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.885 I load: special tokens cache size = 25
0.00.038.913 I load: token to piece cache size = 0.2984 MB
0.00.038.917 I print_info: arch             = gptneox
0.00.038.917 I print_info: vocab_only       = 0
0.00.038.917 I print_info: n_ctx_train      = 2048
0.00.038.918 I print_info: n_embd           = 2048
0.00.038.918 I print_info: n_layer          = 24
0.00.038.923 I print_info: n_head           = 16
0.00.038.923 I print_info: n_head_kv        = 16
0.00.038.924 I print_info: n_rot            = 32
0.00.038.924 I print_info: n_swa            = 0
0.00.038.924 I print_info: n_embd_head_k    = 128
0.00.038.924 I print_info: n_embd_head_v    = 128
0.00.038.925 I print_info: n_gqa            = 1
0.00.038.926 I print_info: n_embd_k_gqa     = 2048
0.00.038.926 I print_info: n_embd_v_gqa     = 2048
0.00.038.927 I print_info: f_norm_eps       = 1.0e-05
0.00.038.927 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.927 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.928 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.928 I print_info: f_logit_scale    = 0.0e+00
0.00.038.928 I print_info: n_ff             = 8192
0.00.038.928 I print_info: n_expert         = 0
0.00.038.929 I print_info: n_expert_used    = 0
0.00.038.929 I print_info: causal attn      = 1
0.00.038.929 I print_info: pooling type     = 0
0.00.038.929 I print_info: rope type        = 2
0.00.038.929 I print_info: rope scaling     = linear
0.00.038.929 I print_info: freq_base_train  = 10000.0
0.00.038.930 I print_info: freq_scale_train = 1
0.00.038.930 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.930 I print_info: rope_finetuned   = unknown
0.00.038.930 I print_info: ssm_d_conv       = 0
0.00.038.932 I print_info: ssm_d_inner      = 0
0.00.038.932 I print_info: ssm_d_state      = 0
0.00.038.932 I print_info: ssm_dt_rank      = 0
0.00.038.933 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.933 I print_info: model type       = 1.4B
0.00.038.933 I print_info: model params     = 1.41 B
0.00.038.933 I print_info: general.name     = 1.4B
0.00.038.934 I print_info: vocab type       = BPE
0.00.038.934 I print_info: n_vocab          = 50304
0.00.038.934 I print_info: n_merges         = 50009
0.00.038.934 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.935 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.935 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.937 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.937 I print_info: LF token         = 187 ''
0.00.038.937 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.937 I print_info: max token length = 1024
0.00.038.938 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.523.542 I load_tensors: offloading 24 repeating layers to GPU
0.00.523.553 I load_tensors: offloading output layer to GPU
0.00.523.554 I load_tensors: offloaded 25/25 layers to GPU
0.00.523.586 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.523.587 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.525.174 I llama_init_from_model: n_seq_max     = 1
0.00.525.179 I llama_init_from_model: n_ctx         = 128
0.00.525.180 I llama_init_from_model: n_ctx_per_seq = 128
0.00.525.180 I llama_init_from_model: n_batch       = 128
0.00.525.181 I llama_init_from_model: n_ubatch      = 128
0.00.525.181 I llama_init_from_model: flash_attn    = 0
0.00.525.184 I llama_init_from_model: freq_base     = 10000.0
0.00.525.184 I llama_init_from_model: freq_scale    = 1
0.00.525.185 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.525.187 I ggml_metal_init: allocating
0.00.525.269 I ggml_metal_init: found device: Apple M4
0.00.525.284 I ggml_metal_init: picking default device: Apple M4
0.00.527.026 I ggml_metal_init: using embedded metal library
0.00.533.879 I ggml_metal_init: GPU name:   Apple M4
0.00.533.889 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.533.890 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.533.890 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.533.891 I ggml_metal_init: simdgroup reduction   = true
0.00.533.891 I ggml_metal_init: simdgroup matrix mul. = true
0.00.533.891 I ggml_metal_init: has residency sets    = true
0.00.533.891 I ggml_metal_init: has bfloat            = true
0.00.533.892 I ggml_metal_init: use bfloat            = true
0.00.533.893 I ggml_metal_init: hasUnifiedMemory      = true
0.00.533.905 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.552.943 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.556.433 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.556.443 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.556.511 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.559.763 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.559.764 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.559.765 I llama_init_from_model: graph nodes  = 967
0.00.559.765 I llama_init_from_model: graph splits = 2
0.00.559.768 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.559.768 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.589.366 I 
0.00.589.451 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.589.459 I perplexity: tokenizing the input ..
0.00.596.628 I perplexity: tokenization took 7.165 ms
0.00.596.635 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.739.193 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.740.538 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.740.559 I llama_perf_context_print:        load time =     580.47 ms
0.00.740.560 I llama_perf_context_print: prompt eval time =     141.61 ms /   128 tokens (    1.11 ms per token,   903.88 tokens per second)
0.00.740.561 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.740.561 I llama_perf_context_print:       total time =     151.20 ms /   129 tokens
0.00.740.944 I ggml_metal_free: deallocating

real	0m0.755s
user	0m0.081s
sys	0m0.125s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.826 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.066 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.077 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.078 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.079 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.079 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.080 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.080 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.081 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.082 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.082 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.083 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.083 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.083 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.084 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.085 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.085 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.086 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.899 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.974 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.708 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.710 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.710 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.710 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.711 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.711 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.711 I llama_model_loader: - type  f32:  194 tensors
0.00.026.712 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.712 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.713 I print_info: file format = GGUF V3 (latest)
0.00.026.713 I print_info: file type   = Q5_K - Medium
0.00.026.714 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.011 I load: special tokens cache size = 25
0.00.040.784 I load: token to piece cache size = 0.2984 MB
0.00.040.787 I print_info: arch             = gptneox
0.00.040.787 I print_info: vocab_only       = 0
0.00.040.787 I print_info: n_ctx_train      = 2048
0.00.040.787 I print_info: n_embd           = 2048
0.00.040.788 I print_info: n_layer          = 24
0.00.040.790 I print_info: n_head           = 16
0.00.040.791 I print_info: n_head_kv        = 16
0.00.040.791 I print_info: n_rot            = 32
0.00.040.791 I print_info: n_swa            = 0
0.00.040.791 I print_info: n_embd_head_k    = 128
0.00.040.792 I print_info: n_embd_head_v    = 128
0.00.040.792 I print_info: n_gqa            = 1
0.00.040.793 I print_info: n_embd_k_gqa     = 2048
0.00.040.794 I print_info: n_embd_v_gqa     = 2048
0.00.040.794 I print_info: f_norm_eps       = 1.0e-05
0.00.040.795 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.795 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.795 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.795 I print_info: f_logit_scale    = 0.0e+00
0.00.040.796 I print_info: n_ff             = 8192
0.00.040.796 I print_info: n_expert         = 0
0.00.040.796 I print_info: n_expert_used    = 0
0.00.040.796 I print_info: causal attn      = 1
0.00.040.796 I print_info: pooling type     = 0
0.00.040.797 I print_info: rope type        = 2
0.00.040.797 I print_info: rope scaling     = linear
0.00.040.797 I print_info: freq_base_train  = 10000.0
0.00.040.797 I print_info: freq_scale_train = 1
0.00.040.798 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.798 I print_info: rope_finetuned   = unknown
0.00.040.798 I print_info: ssm_d_conv       = 0
0.00.040.798 I print_info: ssm_d_inner      = 0
0.00.040.798 I print_info: ssm_d_state      = 0
0.00.040.798 I print_info: ssm_dt_rank      = 0
0.00.040.799 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.799 I print_info: model type       = 1.4B
0.00.040.799 I print_info: model params     = 1.41 B
0.00.040.799 I print_info: general.name     = 1.4B
0.00.040.800 I print_info: vocab type       = BPE
0.00.040.800 I print_info: n_vocab          = 50304
0.00.040.800 I print_info: n_merges         = 50009
0.00.040.801 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.801 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.801 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.801 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.802 I print_info: LF token         = 187 ''
0.00.040.802 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.802 I print_info: max token length = 1024
0.00.040.802 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.614.243 I load_tensors: offloading 24 repeating layers to GPU
0.00.614.258 I load_tensors: offloading output layer to GPU
0.00.614.259 I load_tensors: offloaded 25/25 layers to GPU
0.00.614.292 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.614.293 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.615.615 I llama_init_from_model: n_seq_max     = 1
0.00.615.627 I llama_init_from_model: n_ctx         = 2048
0.00.615.627 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.615.628 I llama_init_from_model: n_batch       = 2048
0.00.615.629 I llama_init_from_model: n_ubatch      = 512
0.00.615.629 I llama_init_from_model: flash_attn    = 0
0.00.615.631 I llama_init_from_model: freq_base     = 10000.0
0.00.615.632 I llama_init_from_model: freq_scale    = 1
0.00.615.634 I ggml_metal_init: allocating
0.00.615.712 I ggml_metal_init: found device: Apple M4
0.00.615.725 I ggml_metal_init: picking default device: Apple M4
0.00.617.500 I ggml_metal_init: using embedded metal library
0.00.621.802 I ggml_metal_init: GPU name:   Apple M4
0.00.621.806 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.621.806 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.621.807 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.621.807 I ggml_metal_init: simdgroup reduction   = true
0.00.621.807 I ggml_metal_init: simdgroup matrix mul. = true
0.00.621.807 I ggml_metal_init: has residency sets    = true
0.00.621.808 I ggml_metal_init: has bfloat            = true
0.00.621.808 I ggml_metal_init: use bfloat            = true
0.00.621.808 I ggml_metal_init: hasUnifiedMemory      = true
0.00.621.810 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.632.979 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.666.090 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.666.096 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.666.130 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.670.809 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.670.810 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.670.811 I llama_init_from_model: graph nodes  = 967
0.00.670.811 I llama_init_from_model: graph splits = 2
0.00.670.817 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.670.947 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.670.948 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.331 I main: llama threadpool init, n_threads = 4
0.00.730.373 I 
0.00.730.395 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.395 I 
0.00.730.545 I sampler seed: 1234
0.00.730.550 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.730.594 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.730.597 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.730.598 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.568.492 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49477.35 tokens per second)
0.01.568.493 I llama_perf_context_print:        load time =     719.75 ms
0.01.568.494 I llama_perf_context_print: prompt eval time =      52.86 ms /     7 tokens (    7.55 ms per token,   132.42 tokens per second)
0.01.568.494 I llama_perf_context_print:        eval time =     782.59 ms /    63 runs   (   12.42 ms per token,    80.50 tokens per second)
0.01.568.495 I llama_perf_context_print:       total time =     838.92 ms /    70 tokens
0.01.568.756 I ggml_metal_free: deallocating

real	0m1.585s
user	0m0.101s
sys	0m0.193s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.868 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.904 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.910 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.911 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.912 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.912 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.913 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.913 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.914 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.914 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.914 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.915 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.915 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.915 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.916 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.918 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.918 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.918 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.638 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.696 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.393 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.395 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.395 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.395 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.396 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.396 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.397 I llama_model_loader: - type  f32:  194 tensors
0.00.025.397 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.397 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.398 I print_info: file format = GGUF V3 (latest)
0.00.025.398 I print_info: file type   = Q5_K - Medium
0.00.025.400 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.406 I load: special tokens cache size = 25
0.00.039.569 I load: token to piece cache size = 0.2984 MB
0.00.039.573 I print_info: arch             = gptneox
0.00.039.573 I print_info: vocab_only       = 0
0.00.039.573 I print_info: n_ctx_train      = 2048
0.00.039.573 I print_info: n_embd           = 2048
0.00.039.574 I print_info: n_layer          = 24
0.00.039.578 I print_info: n_head           = 16
0.00.039.579 I print_info: n_head_kv        = 16
0.00.039.579 I print_info: n_rot            = 32
0.00.039.579 I print_info: n_swa            = 0
0.00.039.579 I print_info: n_embd_head_k    = 128
0.00.039.580 I print_info: n_embd_head_v    = 128
0.00.039.580 I print_info: n_gqa            = 1
0.00.039.581 I print_info: n_embd_k_gqa     = 2048
0.00.039.582 I print_info: n_embd_v_gqa     = 2048
0.00.039.582 I print_info: f_norm_eps       = 1.0e-05
0.00.039.583 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.583 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.583 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.583 I print_info: f_logit_scale    = 0.0e+00
0.00.039.584 I print_info: n_ff             = 8192
0.00.039.584 I print_info: n_expert         = 0
0.00.039.584 I print_info: n_expert_used    = 0
0.00.039.584 I print_info: causal attn      = 1
0.00.039.584 I print_info: pooling type     = 0
0.00.039.585 I print_info: rope type        = 2
0.00.039.585 I print_info: rope scaling     = linear
0.00.039.585 I print_info: freq_base_train  = 10000.0
0.00.039.585 I print_info: freq_scale_train = 1
0.00.039.585 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.586 I print_info: rope_finetuned   = unknown
0.00.039.586 I print_info: ssm_d_conv       = 0
0.00.039.586 I print_info: ssm_d_inner      = 0
0.00.039.586 I print_info: ssm_d_state      = 0
0.00.039.586 I print_info: ssm_dt_rank      = 0
0.00.039.586 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.587 I print_info: model type       = 1.4B
0.00.039.587 I print_info: model params     = 1.41 B
0.00.039.587 I print_info: general.name     = 1.4B
0.00.039.588 I print_info: vocab type       = BPE
0.00.039.588 I print_info: n_vocab          = 50304
0.00.039.588 I print_info: n_merges         = 50009
0.00.039.588 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.588 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.589 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.589 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.589 I print_info: LF token         = 187 ''
0.00.039.589 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.589 I print_info: max token length = 1024
0.00.039.590 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.590.116 I load_tensors: offloading 24 repeating layers to GPU
0.00.590.131 I load_tensors: offloading output layer to GPU
0.00.590.132 I load_tensors: offloaded 25/25 layers to GPU
0.00.590.165 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.590.167 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.591.937 I llama_init_from_model: n_seq_max     = 1
0.00.591.944 I llama_init_from_model: n_ctx         = 128
0.00.591.945 I llama_init_from_model: n_ctx_per_seq = 128
0.00.591.945 I llama_init_from_model: n_batch       = 128
0.00.591.945 I llama_init_from_model: n_ubatch      = 128
0.00.591.946 I llama_init_from_model: flash_attn    = 0
0.00.591.948 I llama_init_from_model: freq_base     = 10000.0
0.00.591.949 I llama_init_from_model: freq_scale    = 1
0.00.591.949 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.591.951 I ggml_metal_init: allocating
0.00.592.038 I ggml_metal_init: found device: Apple M4
0.00.592.053 I ggml_metal_init: picking default device: Apple M4
0.00.593.886 I ggml_metal_init: using embedded metal library
0.00.600.512 I ggml_metal_init: GPU name:   Apple M4
0.00.600.517 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.600.518 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.600.518 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.600.519 I ggml_metal_init: simdgroup reduction   = true
0.00.600.519 I ggml_metal_init: simdgroup matrix mul. = true
0.00.600.520 I ggml_metal_init: has residency sets    = true
0.00.600.520 I ggml_metal_init: has bfloat            = true
0.00.600.520 I ggml_metal_init: use bfloat            = true
0.00.600.521 I ggml_metal_init: hasUnifiedMemory      = true
0.00.600.528 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.619.112 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.622.687 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.622.695 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.622.762 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.626.041 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.626.043 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.626.043 I llama_init_from_model: graph nodes  = 967
0.00.626.044 I llama_init_from_model: graph splits = 2
0.00.626.047 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.626.048 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.656.417 I 
0.00.656.503 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.656.510 I perplexity: tokenizing the input ..
0.00.662.554 I perplexity: tokenization took 6.04 ms
0.00.662.561 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.060 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.800.396 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.800.417 I llama_perf_context_print:        load time =     646.54 ms
0.00.800.418 I llama_perf_context_print: prompt eval time =     135.86 ms /   128 tokens (    1.06 ms per token,   942.12 tokens per second)
0.00.800.419 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.419 I llama_perf_context_print:       total time =     144.00 ms /   129 tokens
0.00.800.785 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.078s
sys	0m0.132s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.010.703 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.523 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.528 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.530 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.530 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.531 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.531 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.531 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.533 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.533 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.533 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.534 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.534 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.534 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.535 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.537 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.537 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.537 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.378 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.470 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.315 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.317 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.317 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.318 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.318 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.318 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.319 I llama_model_loader: - type  f32:  194 tensors
0.00.027.319 I llama_model_loader: - type q6_K:   98 tensors
0.00.027.319 I print_info: file format = GGUF V3 (latest)
0.00.027.320 I print_info: file type   = Q6_K
0.00.027.321 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.035.709 I load: special tokens cache size = 25
0.00.041.811 I load: token to piece cache size = 0.2984 MB
0.00.041.815 I print_info: arch             = gptneox
0.00.041.816 I print_info: vocab_only       = 0
0.00.041.816 I print_info: n_ctx_train      = 2048
0.00.041.816 I print_info: n_embd           = 2048
0.00.041.816 I print_info: n_layer          = 24
0.00.041.820 I print_info: n_head           = 16
0.00.041.821 I print_info: n_head_kv        = 16
0.00.041.821 I print_info: n_rot            = 32
0.00.041.821 I print_info: n_swa            = 0
0.00.041.821 I print_info: n_embd_head_k    = 128
0.00.041.821 I print_info: n_embd_head_v    = 128
0.00.041.822 I print_info: n_gqa            = 1
0.00.041.822 I print_info: n_embd_k_gqa     = 2048
0.00.041.823 I print_info: n_embd_v_gqa     = 2048
0.00.041.823 I print_info: f_norm_eps       = 1.0e-05
0.00.041.824 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.824 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.824 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.824 I print_info: f_logit_scale    = 0.0e+00
0.00.041.825 I print_info: n_ff             = 8192
0.00.041.825 I print_info: n_expert         = 0
0.00.041.825 I print_info: n_expert_used    = 0
0.00.041.825 I print_info: causal attn      = 1
0.00.041.825 I print_info: pooling type     = 0
0.00.041.825 I print_info: rope type        = 2
0.00.041.826 I print_info: rope scaling     = linear
0.00.041.826 I print_info: freq_base_train  = 10000.0
0.00.041.830 I print_info: freq_scale_train = 1
0.00.041.830 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.830 I print_info: rope_finetuned   = unknown
0.00.041.830 I print_info: ssm_d_conv       = 0
0.00.041.830 I print_info: ssm_d_inner      = 0
0.00.041.830 I print_info: ssm_d_state      = 0
0.00.041.830 I print_info: ssm_dt_rank      = 0
0.00.041.830 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.831 I print_info: model type       = 1.4B
0.00.041.831 I print_info: model params     = 1.41 B
0.00.041.831 I print_info: general.name     = 1.4B
0.00.041.832 I print_info: vocab type       = BPE
0.00.041.832 I print_info: n_vocab          = 50304
0.00.041.832 I print_info: n_merges         = 50009
0.00.041.832 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.832 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.832 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.833 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.833 I print_info: LF token         = 187 ''
0.00.041.833 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.833 I print_info: max token length = 1024
0.00.041.834 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.745.513 I load_tensors: offloading 24 repeating layers to GPU
0.00.745.521 I load_tensors: offloading output layer to GPU
0.00.745.522 I load_tensors: offloaded 25/25 layers to GPU
0.00.745.548 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.745.550 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.746.708 I llama_init_from_model: n_seq_max     = 1
0.00.746.710 I llama_init_from_model: n_ctx         = 2048
0.00.746.711 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.746.711 I llama_init_from_model: n_batch       = 2048
0.00.746.712 I llama_init_from_model: n_ubatch      = 512
0.00.746.712 I llama_init_from_model: flash_attn    = 0
0.00.746.713 I llama_init_from_model: freq_base     = 10000.0
0.00.746.714 I llama_init_from_model: freq_scale    = 1
0.00.746.715 I ggml_metal_init: allocating
0.00.746.777 I ggml_metal_init: found device: Apple M4
0.00.746.800 I ggml_metal_init: picking default device: Apple M4
0.00.748.284 I ggml_metal_init: using embedded metal library
0.00.754.275 I ggml_metal_init: GPU name:   Apple M4
0.00.754.278 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.754.279 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.754.280 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.754.280 I ggml_metal_init: simdgroup reduction   = true
0.00.754.281 I ggml_metal_init: simdgroup matrix mul. = true
0.00.754.281 I ggml_metal_init: has residency sets    = true
0.00.754.281 I ggml_metal_init: has bfloat            = true
0.00.754.281 I ggml_metal_init: use bfloat            = true
0.00.754.282 I ggml_metal_init: hasUnifiedMemory      = true
0.00.754.284 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.771.152 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.828.234 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.828.245 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.828.288 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.833.131 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.833.133 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.833.133 I llama_init_from_model: graph nodes  = 967
0.00.833.134 I llama_init_from_model: graph splits = 2
0.00.833.138 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.833.256 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.833.257 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.896.580 I main: llama threadpool init, n_threads = 4
0.00.896.625 I 
0.00.896.649 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.896.652 I 
0.00.896.831 I sampler seed: 1234
0.00.896.835 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.896.880 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.896.884 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.896.884 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.776.815 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53747.16 tokens per second)
0.01.776.816 I llama_perf_context_print:        load time =     885.15 ms
0.01.776.816 I llama_perf_context_print: prompt eval time =      57.80 ms /     7 tokens (    8.26 ms per token,   121.11 tokens per second)
0.01.776.818 I llama_perf_context_print:        eval time =     819.22 ms /    63 runs   (   13.00 ms per token,    76.90 tokens per second)
0.01.776.819 I llama_perf_context_print:       total time =     880.96 ms /    70 tokens
0.01.777.045 I ggml_metal_free: deallocating

real	0m1.795s
user	0m0.111s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4783 (a800ae46) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.790 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.994 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.000 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.007 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.007 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.008 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.008 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.008 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.009 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.010 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.010 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.010 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.011 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.011 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.012 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.013 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.013 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.014 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.739 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.775 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.540 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.542 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.542 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.542 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.543 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.543 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.544 I llama_model_loader: - type  f32:  194 tensors
0.00.024.544 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.545 I print_info: file format = GGUF V3 (latest)
0.00.024.545 I print_info: file type   = Q6_K
0.00.024.546 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.474 I load: special tokens cache size = 25
0.00.038.465 I load: token to piece cache size = 0.2984 MB
0.00.038.469 I print_info: arch             = gptneox
0.00.038.469 I print_info: vocab_only       = 0
0.00.038.470 I print_info: n_ctx_train      = 2048
0.00.038.470 I print_info: n_embd           = 2048
0.00.038.470 I print_info: n_layer          = 24
0.00.038.474 I print_info: n_head           = 16
0.00.038.475 I print_info: n_head_kv        = 16
0.00.038.475 I print_info: n_rot            = 32
0.00.038.475 I print_info: n_swa            = 0
0.00.038.476 I print_info: n_embd_head_k    = 128
0.00.038.476 I print_info: n_embd_head_v    = 128
0.00.038.479 I print_info: n_gqa            = 1
0.00.038.480 I print_info: n_embd_k_gqa     = 2048
0.00.038.480 I print_info: n_embd_v_gqa     = 2048
0.00.038.481 I print_info: f_norm_eps       = 1.0e-05
0.00.038.481 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.482 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.482 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.482 I print_info: f_logit_scale    = 0.0e+00
0.00.038.483 I print_info: n_ff             = 8192
0.00.038.483 I print_info: n_expert         = 0
0.00.038.483 I print_info: n_expert_used    = 0
0.00.038.483 I print_info: causal attn      = 1
0.00.038.484 I print_info: pooling type     = 0
0.00.038.484 I print_info: rope type        = 2
0.00.038.484 I print_info: rope scaling     = linear
0.00.038.485 I print_info: freq_base_train  = 10000.0
0.00.038.485 I print_info: freq_scale_train = 1
0.00.038.486 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.486 I print_info: rope_finetuned   = unknown
0.00.038.487 I print_info: ssm_d_conv       = 0
0.00.038.487 I print_info: ssm_d_inner      = 0
0.00.038.487 I print_info: ssm_d_state      = 0
0.00.038.487 I print_info: ssm_dt_rank      = 0
0.00.038.487 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.487 I print_info: model type       = 1.4B
0.00.038.488 I print_info: model params     = 1.41 B
0.00.038.488 I print_info: general.name     = 1.4B
0.00.038.488 I print_info: vocab type       = BPE
0.00.038.488 I print_info: n_vocab          = 50304
0.00.038.489 I print_info: n_merges         = 50009
0.00.038.489 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.489 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.489 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.489 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.489 I print_info: LF token         = 187 ''
0.00.038.490 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.490 I print_info: max token length = 1024
0.00.038.494 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.635.998 I load_tensors: offloading 24 repeating layers to GPU
0.00.636.011 I load_tensors: offloading output layer to GPU
0.00.636.011 I load_tensors: offloaded 25/25 layers to GPU
0.00.636.046 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.636.058 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.637.659 I llama_init_from_model: n_seq_max     = 1
0.00.637.662 I llama_init_from_model: n_ctx         = 128
0.00.637.663 I llama_init_from_model: n_ctx_per_seq = 128
0.00.637.663 I llama_init_from_model: n_batch       = 128
0.00.637.664 I llama_init_from_model: n_ubatch      = 128
0.00.637.664 I llama_init_from_model: flash_attn    = 0
0.00.637.666 I llama_init_from_model: freq_base     = 10000.0
0.00.637.666 I llama_init_from_model: freq_scale    = 1
0.00.637.667 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.637.670 I ggml_metal_init: allocating
0.00.637.756 I ggml_metal_init: found device: Apple M4
0.00.637.771 I ggml_metal_init: picking default device: Apple M4
0.00.639.291 I ggml_metal_init: using embedded metal library
0.00.645.811 I ggml_metal_init: GPU name:   Apple M4
0.00.645.817 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.645.818 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.645.819 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.645.819 I ggml_metal_init: simdgroup reduction   = true
0.00.645.820 I ggml_metal_init: simdgroup matrix mul. = true
0.00.645.820 I ggml_metal_init: has residency sets    = true
0.00.645.820 I ggml_metal_init: has bfloat            = true
0.00.645.820 I ggml_metal_init: use bfloat            = true
0.00.645.821 I ggml_metal_init: hasUnifiedMemory      = true
0.00.645.825 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.663.779 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.667.353 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.667.358 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.667.410 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.670.680 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.670.681 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.670.682 I llama_init_from_model: graph nodes  = 967
0.00.670.682 I llama_init_from_model: graph splits = 2
0.00.670.685 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.670.686 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.706.195 I 
0.00.706.276 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.706.283 I perplexity: tokenizing the input ..
0.00.713.078 I perplexity: tokenization took 6.793 ms
0.00.713.086 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.843.890 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.845.238 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.845.264 I llama_perf_context_print:        load time =     697.39 ms
0.00.845.265 I llama_perf_context_print: prompt eval time =     130.57 ms /   128 tokens (    1.02 ms per token,   980.29 tokens per second)
0.00.845.266 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.845.266 I llama_perf_context_print:       total time =     139.07 ms /   129 tokens
0.00.845.612 I ggml_metal_free: deallocating

real	0m0.860s
user	0m0.078s
sys	0m0.157s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4783 (a800ae46)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c408090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c4087a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c408d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c409300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c4098b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c409e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c40a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c40a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c40af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c40b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c40b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c40be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c40c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c40d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c40d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c40e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c40e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c40eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c40f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c40fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c4104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c410be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c411300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c411ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c4122c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c412580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c412b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c413800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c413d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c414000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c4144a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c414760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c414ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c415530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c4157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c415c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c416130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c4165d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c416a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c416f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c4173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c417850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c417cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c418190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c418450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c418a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c419070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c419990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c419fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c41a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c41abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c41b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c41b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c41bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c41c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c41ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c41cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c41d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c41d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c41dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c41e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c41e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c41ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c41f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c41f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c41f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c41fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c420300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c4207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c420c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c4210e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c421580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c421a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14c421f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14c4224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14c422a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14c422f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14c4234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14c423a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14c423f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14c4244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14c4249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14c424f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14c425490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14c4259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14c425f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14c426480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14c4269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14c426f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14c427470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14c4279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14c427f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14c428460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14c4289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14c428f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14c429450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14c4299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14c419680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14c429e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14c42a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14c42ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14c42b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14c42b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14c42bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14c42c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14c42c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14c42caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14c42d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14c42d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14c42dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14c42e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14c42e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14c42ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c42ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c42f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c42f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c42fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c4301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c430690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c430b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c430fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c431470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c431910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c431db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c432250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c4326f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c432b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c433030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c4334d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c433970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c433e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c4342b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c434750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c434bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c435090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c435530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c4359d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c435e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c436310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c4367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c436c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c4370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c437590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c437a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c437ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c438370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c438810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c438cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c439150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c4395f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c439a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c439f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c43a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c43a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c43ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c43b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c43b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c43baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c43bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c43c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c43c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c43cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c43d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c43d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c43db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c43dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c43e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c43e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c43edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c43f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c43f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c43fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c440050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c4404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c440990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c440e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c4412d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c441770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c441c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c4420b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c442550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c4429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c442e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c443330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c4437d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c443c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c444110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c4445b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c444a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c444ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c445390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c445830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c445cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c446220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c446770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c446cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c447210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c4474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c447ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c4480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c448700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14c448ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14c449390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c449650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c449c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14c44a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c44aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c44af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c44b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c44b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c44bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c44c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c44ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c44cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c44d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c44da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c44dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c44e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c44ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c44efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c44f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c44fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c44ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c450500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c450a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c450fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c4514f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c451a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c451f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c4524e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c452a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c452f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c4534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c453a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c453f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c4544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c454a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c454f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c4554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c455a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c455f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c4564a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c4569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c456f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c457490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c4579e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c457f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c458480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c4589d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c458f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c459470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c4599c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c459f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c45a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c45a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c45af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c45b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c45b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c45bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c45c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c45c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c45cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c45d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c45d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c45ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c45e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c45e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14c45ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14c45f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c45f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c45fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c460090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c460530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c4609d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c460e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c461310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c4617b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c461c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c4620f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c462590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c462a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c304410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14c304880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x14c304cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x14c305160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x14c3055d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x14c305a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x14c305eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x14c306320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x14c306790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x14c306c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x14c307070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c3074e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c308000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c308720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c308e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c309560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c309820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14c309ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c309f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c30a3c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.725.525 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.725.529 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c447790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c447da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c4483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c41b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c41ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c41d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c449f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c412840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c419330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c419c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c41a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c418d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c41baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c411840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c41dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c42a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c414a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c414ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c44a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c4489c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c412e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c413110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c4133d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c462cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c462fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c463270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c463530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c4637f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c463ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c463d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c464030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c4642f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c4645b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c464870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c464b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c464df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c4650b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c465370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c465630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c4658f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c465bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c465e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c466130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c4663f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c4666b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c466970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c466c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c466ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c4671b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c467470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c467730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c4679f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c467cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c467f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c468230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c4684f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c4687b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c468a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c468d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c468ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c4692b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c469570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c469830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c469af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c469db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c46a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c46a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c46a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c46a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c46ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c46ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c46b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c46b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14c46b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14c46b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14c46bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14c46beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14c46c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14c46c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14c46c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14c46c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14c46cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14c46cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14c46d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14c46d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14c46d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14c46da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14c46dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14c46dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14c46e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14c46e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14c46e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14c46eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14c46ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14c46f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14c46f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14c46f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14c46f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14c46fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14c46fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14c4700b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14c470370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14c470630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14c4708f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14c470bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14c470e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14c471130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14c4713f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14c4716b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14c471970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14c471c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14c471ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14c4721b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c472470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c472730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c4729f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c472cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c472f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c473230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c4734f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c4737b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c473a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c473d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c473ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c4742b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c474570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c474830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c474af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c474db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c475070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c475330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c4755f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c4758b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c475b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c475e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c4760f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c4763b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c476670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c476930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c476bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c476eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c477170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c477430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c4776f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c4779b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c477c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c477f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c4781f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c4784b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c478770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c478a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c478cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c478fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c479270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c479530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c4797f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c479ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c479d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c47a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c47a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c47a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c47a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c47ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c47adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c47b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c47b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c47b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c47b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c47bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c47be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c47c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c47c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c47c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c47c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c47cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c47cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c47d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c47d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c47d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c47d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c47dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c47df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c47e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c47e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c47e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c47ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c47ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c47eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c47f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c47f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c47f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c47faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c47fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c480070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c480330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c4805f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c4808b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c480b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c480e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c4810f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c4813b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14c481670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14c481b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c481dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c482090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14c482500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c482970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c482e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c483380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c483890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c484400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c4846c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c484c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c485240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c485800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c485dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c486380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c486940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c486f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c4874c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c487a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c488040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c488600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c488bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c489180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c489740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c489d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c48a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c48a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c48ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c48b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c48b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c48bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c48c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c48cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c48d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c48d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c48dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c48e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c48e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c48ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c48f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c48f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c48fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c490480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c490a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c491000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c4915c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c491b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c492140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c492700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c492cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c493280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c493840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c493e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c4943c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c494980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c494f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c495500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c495ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c496080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c496640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c496c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c4971c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c497780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c497d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c498300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14c4988c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14c498dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c4992c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c4997c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c499cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c49a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c49a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c49abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c49b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c49b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c49bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c49bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c49c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c49c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c49cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14c49d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x14c49d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x14c49ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x14c49e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x14c49e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x14c49ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x14c49f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x14c49f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x14c49fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x14c4a00c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c4a05c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c4a0fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c4a16f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c4a1e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c4a2530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c4a27f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14c4a2fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c4a32a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c4a38b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14c30ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14c30adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14c30b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14c30b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14c30b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14c30b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14c30bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14c30c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14c30c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14c30c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14c30c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14c30cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14c30d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14c30d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14c30ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14c30e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14c30ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14c30f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14c30fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14c310190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14c3108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14c310fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14c3116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14c311e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14c312530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14c3127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14c312ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14c312f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14c313390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14c313800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14c313c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14c3141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14c314610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14c3148d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14c314d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14c3151b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14c315620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14c315a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14c315f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14c316370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14c3167e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14c316c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14c3170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14c317530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14c3179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14c317e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14c318280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14c3186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14c318b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14c318fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14c319440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14c3198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14c319d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14c31a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14c31a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14c31aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14c31afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14c31b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14c31b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14c31bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14c31c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14c31c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14c31cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14c31cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14c31d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14c31d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14c31dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14c31e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14c31e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14c31ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14c31ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14c31f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14c31f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14c31fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14c320050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14c3204c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14c320930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14c320da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14c321210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14c321680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14c321af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14c321f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14c3223d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14c322840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14c322cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14c323120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14c323590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14c323a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14c323e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14c3242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14c324750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14c324bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14c325030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14c3254a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14c325910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14c325d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14c3261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14c326660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14c326ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14c3273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14c327990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14c327f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14c3284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14c328aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14c329050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14c329600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14c329bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14c32a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14c32a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14c32acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14c32b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14c32b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14c32bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14c32c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14c32c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14c32cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14c32d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14c32d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14c32dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14c32e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14c32e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14c32eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14c32f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14c32f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14c32fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14c32ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14c330480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14c330980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14c330e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14c331380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14c331880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14c331d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14c332280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14c332780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14c332c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14c333180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14c333680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14c333b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14c334080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14c334580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14c334a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14c334f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14c335480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14c335980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14c335e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14c336380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14c336880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14c336d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14c337280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14c337780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14c337c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14c338180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14c338680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14c338b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14c339080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14c339580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14c339a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14c339f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14c33a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14c33a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14c33ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14c33b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14c33b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14c33bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14c33c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14c33c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14c33cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14c33d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14c33d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14c33db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14c33e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14c33e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14c33ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14c33ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14c33f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14c33f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14c33fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14c340380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14c340880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14c340d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14c341280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14c341780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14c341c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14c342180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14c342680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14c342b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14c343080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14c343580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14c343a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14c343f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14c344480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14c344980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14c344e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14c345380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14c345930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14c345ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14c346490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14c346a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14c347050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14c347660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14c347c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14c348460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14c348900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14c348bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14c3491d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14c3497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14c349fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14c34a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14c34a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14c34adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14c34b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14c34bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14c34c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14c34c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14c34caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14c34cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14c34d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14c34da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14c34dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14c34e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14c34ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14c34efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14c34f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14c34fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14c34ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14c350510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14c350a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14c350fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14c351500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14c351a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14c351fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14c3524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14c352a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14c352f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14c3534e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14c353a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14c353f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14c3544d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14c354a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14c354f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14c3554c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14c355a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14c355f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14c3564b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14c356a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14c356f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14c3574a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14c3579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14c357f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14c358490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14c3589e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14c358f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14c359480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14c3599d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14c359f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14c35a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14c35a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14c35af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14c35b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14c35b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14c35bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14c35c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14c35c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14c35cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14c35d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14c35d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14c35dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14c35e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14c35e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14c35ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14c35f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14c35f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14c35faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14c35ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14c3603e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14c360880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14c360d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14c3611c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14c361660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14c361b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14c361fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14c362440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14c3628e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x14c362d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x14c363220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x14c3636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x14c363b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x14c364000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x14c3644a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x14c364940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x14c364de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x14c365280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14c3657d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14c365ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14c366610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14c366d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14c367450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14c367710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14c367f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14c3681c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14c3687d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.776s
user	0m0.279s
sys	0m0.315s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4783 (a800ae46)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e70b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e70b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e70be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e70c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e70c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e70cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e70d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e70dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e70e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e70e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e70ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e70ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e70fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e710260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e710a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e711190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e7118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e711fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e7126f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e712ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e7135e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e713d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e714420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e714cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e7153e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e7156a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e715cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e716920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e716e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e717120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e7175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e717880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e718110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e718650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e718910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e718db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e719250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e7196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e719b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e71a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e71a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e71a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e71ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e71b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e71b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e71bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e71c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e71cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e71d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e71d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e71dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e71e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e71e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e71ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e71f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e71fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e720040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e720300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e720910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e721100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e7213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e721860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e721d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e7221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e722640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e722ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e722f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e723420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e7238c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e723d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e724200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e7246a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e724b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e725090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e7255e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e725b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e726080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e7265d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e726b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e727070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e7275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e727b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e728060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e7285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e728b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e729050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e7295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e729af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e72a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e72a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e72aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e72b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e72b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e72bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e72c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e72c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e72cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e71c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e72cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e72d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e72dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e72e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e72e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e72ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e72f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e72f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e72fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e730160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e7306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e730c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e731150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e7316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e731bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e732090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e732530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e7329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e732e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e733310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e7337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e733c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e7340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e734590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e734a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e734ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e735370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e735810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e735cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e736150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e7365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e736a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e736f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e7373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e737870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e737d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e7381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e738650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e738af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e738f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e739430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e7398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e739d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e73a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e73a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e73ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e73aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e73b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e73b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e73bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e73c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e73c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e73cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e73d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e73d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e73d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e73de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e73e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e73e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e73ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e73f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e73f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e73f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e73fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e740330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e7407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e740c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e741110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e7415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e741a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e741ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e742390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e742830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e742cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e743170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e743610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e743ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e743f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e7443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e744890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e744d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e7451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e745670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e745b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e745fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e746450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e7468f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e746d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e747230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e7476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e747b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e748010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e7484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e748950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e748df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e749340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e749890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e749de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e74a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e74a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e74ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e74b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e74b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e74c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e74c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e74c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e74cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e74d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e74db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e74e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e74e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e74e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e74f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e74f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e74fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e750100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e750650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e750ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e7510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e751640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e751b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e7520e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e752630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e752b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e7530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e753620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e753b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e7540c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e754610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e754b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e7550b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e755600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e755b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e7560a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e7565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e756b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e757090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e7575e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e757b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e758080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e7585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e758b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e759070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e7595c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e759b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e75a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e75a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e75ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e75b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e75b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e75baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e75c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e75c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e75cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e75d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e75d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e75dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e75e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e75e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e75eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e75f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e75f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e75fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e760000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e760550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e760aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e760ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e761540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e761a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e761f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e7623d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e762870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e762d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e7631b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e763650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e763af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e763f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e764430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e7648d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e764d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e765210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e7656b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e765b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e765ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11e766490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11e766930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11e766dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11e767270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11e767710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11e767bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11e768050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11e7684f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11e768990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11e768e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e769380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e769aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e76a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e76a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e76b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e76b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e76bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e76bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e76c380 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.860 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.871 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e60b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e60b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e60b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e60be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e60c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e60c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e60cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e60d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e60d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e60d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e60dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e60e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e60ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e60f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e60ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e610660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e610d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e6114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e611bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e612390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e612ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e6131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e6138f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e614010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e614730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e6149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e614cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e615120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e615590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e615a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e615f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e616410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e616880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e616b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e616fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e617420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e617980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e617e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e618380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e618880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e618d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e619280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e619780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e619c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e61a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e61a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e61aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e61aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e61b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e61b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e61bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e61c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e61c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e61c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e61cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e61d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e61da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e61dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e61e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e61eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e61efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e61f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e61f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e61fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e620230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e6206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e620b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e621010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e6214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e621950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e621df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e622290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e622730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e622c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e6231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e623720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e623c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e6241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e624710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e624c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e6251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e625700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e625c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e6261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e6266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e626c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e627190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e6276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e627c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e628180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e6286d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e628c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e629170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e6296c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e629c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e62a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e62a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e62ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e62b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e62b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e62bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e62c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e62c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e62cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e62d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e62d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e62dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e62e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e62e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e62ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e62f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e62f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e62fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e630050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e6304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e630990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e630e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e6312d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e631770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e631c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e6320b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e632550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e6329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e632e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e633330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e6337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e633c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e634110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e6345b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e634a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e634ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e635390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e635830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e635cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e636170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e636610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e636ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e636f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e6373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e637890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e637d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e6381d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e638670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e638b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e638fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e639450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e6398f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e639d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e63a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e63a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e63ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e63b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e63b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e63b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e63bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e63c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e63c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e63cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e63d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e63d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e63d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e63de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e63e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e63e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e63ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e63f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e63f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e63fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e63feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e640350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e6407f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e640c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e641130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e6415d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e641a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e641f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e6423b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e642850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e642cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e643190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e643630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e643ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e643f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e644410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e6448b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e644d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e6451f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e645690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e645b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e645fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e646470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e646910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e646db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e647300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e647850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e647da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e6482f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e6485b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e648bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e6491d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e6497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e649fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e64a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e64a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e64ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e64b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e64bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e64bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e64c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e64c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e64d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e64d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e64db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e64e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e64e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e64eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e64f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e64f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e64fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e6500a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e6505f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e650b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e651090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e6515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e651b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e652080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e6525d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e652b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e653070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e6535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e653b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e654060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e6545b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e654b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e655050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e6555a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e655af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e656040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e656590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e656ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e657030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e657580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e657ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e658020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e658570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e658ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e659010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e659560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e659ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e65a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e65a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e65aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e65aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e65b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e65ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e65bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e65c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e65ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e65cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e65d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e65da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e65dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e65e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e65ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e65efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e65f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e65fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e65fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e660390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e660830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e660cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e661170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e661610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e661ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e661f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e6623f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e662890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e662d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e6631d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e663670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e663b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e663fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11e664450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11e6648f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11e664d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11e665230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11e6656d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11e665b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11e666010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11e6664b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11e666950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11e666df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e667340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e667a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e668180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e6688a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e668fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e669280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e669a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e669d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e66a340 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e76c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e74aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e74a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e74b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e71e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e71dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e7205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e74d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e715960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e71c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e71cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e71d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e71b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e71d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e714960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e70a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e71f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e720bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e72d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e76b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e717b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e717e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e74d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e74bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e715f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e716230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e7164f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e76c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e76caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e76cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e76d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e76d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e76d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e76d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e76db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e76dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e76e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e76e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e76e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e76e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e76eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e76ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e76f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e76f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e76f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e76f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e76fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e76fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e7701a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e770460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e770720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e7709e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e770ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e770f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e771220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e7714e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e7717a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e771a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e771d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e771fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e7722a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e772560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e772820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e772ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e772da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e773060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e773320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e7735e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e7738a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e773b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e773e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e7740e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e7743a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e774660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e774920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e774be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e774ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e775160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e775420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e7756e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e7759a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e775c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e775f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e7761e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e7764a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e776760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e776a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e776ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e776fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e777260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e777520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e7777e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e777aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e777d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e778020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e7782e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e7785a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e778860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e778b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e778de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e7790a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e779360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e779620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e7798e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e779ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e779e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e77a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e77a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e77a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e77a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e77ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e77aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e77b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e77b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e77b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e77b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e77bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e77bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e77c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e77c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e77c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e77ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e77cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e77cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e77d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e77d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e77d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e77dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e77dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e77e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e77e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e77e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e77e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e77eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e77ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e77f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e77f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e77f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e77f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e77fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e77fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e780160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e780420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e7806e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e7809a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e780c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e780f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e7811e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e7814a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e781760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e781a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e781ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e781fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e782260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e782520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e7827e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e782aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e782d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e783020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e7832e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e7835a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e783860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e783b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e783de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e7840a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e784360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e784620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e7848e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e784ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e784e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e785120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e7853e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e7856a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e785960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e785c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e785ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e7861a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e786460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e786720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e7869e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e786ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e786f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e787220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e7874e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e7877a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e787a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e787d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e787fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e7882a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e788560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e788820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e788ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e788da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e789060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e789320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e7895e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e7898a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e789b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e789e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e78a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e78a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11a907100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11a9075d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11a907890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11a907b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11a907fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11a908430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11a908940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11a908e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11a9092e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11a909ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11a909f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11a90a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11a90aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11a90b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11a90b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11a90bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11a90c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11a90c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11a90cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11a90d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11a90d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11a90ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11a90e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11a90e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11a90ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11a90f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11a90fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11a910010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11a9105c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11a910b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11a911120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11a9116d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11a911c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11a912230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11a9127e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11a912d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11a913340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11a9138f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11a913ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11a914450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11a914a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11a914fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11a915560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11a915b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11a9160c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11a916670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11a916c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11a9171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11a917780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11a917d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11a9182e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11a918890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11a918e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11a9193f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11a9199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11a919f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11a91a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11a91aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11a91b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11a91b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11a91bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11a91c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11a91c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11a91ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11a91d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11a91d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11a91dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11a91e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11a91e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11a91ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11a91f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11a91f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11a91fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11a9200e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11a9205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11a920ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11a920fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11a9214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11a9219e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11a921ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11a9223e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11a9228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11a922de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11a9232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11a9237e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11a923ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11a9241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11a9246e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11a924be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11a9250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11a9255e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11a925ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11a9264f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11a926c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11a927330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11a927a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11a927d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11a928500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11a9287c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11a928dd0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.956s
user	0m0.231s
sys	0m0.192s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.44 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.39 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.83 sec*proc (2 tests)

Total Test time (real) =   1.84 sec
        1.87 real         0.51 user         0.23 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.23 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.31 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.55 real         0.12 user         0.08 sys
```
