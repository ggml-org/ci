### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.23 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.71 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.66 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.35 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.95 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.25 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.24 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.10 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.84 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  181.82 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.88 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.65 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.34 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 225.60 sec*proc (28 tests)

Total Test time (real) = 225.61 sec

real	3m45.707s
user	7m41.998s
sys	0m6.656s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.18 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.93 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.18 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.19 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.22 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.36 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.60 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.36 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.27 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.96 sec*proc (28 tests)

Total Test time (real) =  51.97 sec

real	0m51.984s
user	1m12.232s
sys	0m5.778s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.079 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.003 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.454 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.460 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.463 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.463 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.464 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.465 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.465 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.467 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.467 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.468 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.468 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.469 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.471 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.472 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.473 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.473 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.474 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.475 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.475 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.024.838 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.025.867 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.869 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.025.870 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.025.870 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.025.871 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.025.871 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.025.872 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.025.872 I llama_model_loader: - type  f32:  124 tensors
0.00.025.873 I llama_model_loader: - type  f16:   73 tensors
0.00.025.873 I print_info: file format = GGUF V3 (latest)
0.00.025.887 I print_info: file type   = F16
0.00.025.891 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.029.861 I load: special tokens cache size = 5
0.00.031.826 I load: token to piece cache size = 0.2032 MB
0.00.031.854 I print_info: arch             = bert
0.00.031.856 I print_info: vocab_only       = 0
0.00.031.856 I print_info: n_ctx_train      = 512
0.00.031.856 I print_info: n_embd           = 384
0.00.031.857 I print_info: n_layer          = 12
0.00.031.860 I print_info: n_head           = 12
0.00.031.861 I print_info: n_head_kv        = 12
0.00.031.861 I print_info: n_rot            = 32
0.00.031.861 I print_info: n_swa            = 0
0.00.031.863 I print_info: n_embd_head_k    = 32
0.00.031.863 I print_info: n_embd_head_v    = 32
0.00.031.866 I print_info: n_gqa            = 1
0.00.031.867 I print_info: n_embd_k_gqa     = 384
0.00.031.868 I print_info: n_embd_v_gqa     = 384
0.00.031.868 I print_info: f_norm_eps       = 1.0e-12
0.00.031.869 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.031.869 I print_info: f_clamp_kqv      = 0.0e+00
0.00.031.870 I print_info: f_max_alibi_bias = 0.0e+00
0.00.031.870 I print_info: f_logit_scale    = 0.0e+00
0.00.031.871 I print_info: n_ff             = 1536
0.00.031.871 I print_info: n_expert         = 0
0.00.031.871 I print_info: n_expert_used    = 0
0.00.031.872 I print_info: causal attn      = 0
0.00.031.872 I print_info: pooling type     = 2
0.00.031.872 I print_info: rope type        = 2
0.00.031.873 I print_info: rope scaling     = linear
0.00.031.873 I print_info: freq_base_train  = 10000.0
0.00.031.873 I print_info: freq_scale_train = 1
0.00.031.876 I print_info: n_ctx_orig_yarn  = 512
0.00.031.876 I print_info: rope_finetuned   = unknown
0.00.031.876 I print_info: ssm_d_conv       = 0
0.00.031.876 I print_info: ssm_d_inner      = 0
0.00.031.876 I print_info: ssm_d_state      = 0
0.00.031.877 I print_info: ssm_dt_rank      = 0
0.00.031.877 I print_info: ssm_dt_b_c_rms   = 0
0.00.031.877 I print_info: model type       = 33M
0.00.031.888 I print_info: model params     = 33.21 M
0.00.031.892 I print_info: general.name     = Bge Small
0.00.031.892 I print_info: vocab type       = WPM
0.00.031.894 I print_info: n_vocab          = 30522
0.00.031.894 I print_info: n_merges         = 0
0.00.031.895 I print_info: UNK token        = 100 '[UNK]'
0.00.031.895 I print_info: SEP token        = 102 '[SEP]'
0.00.031.895 I print_info: PAD token        = 0 '[PAD]'
0.00.031.896 I print_info: CLS token        = 101 '[CLS]'
0.00.031.896 I print_info: MASK token       = 103 '[MASK]'
0.00.031.896 I print_info: LF token         = 0 '[PAD]'
0.00.031.896 I print_info: max token length = 21
0.00.033.933 I load_tensors: offloading 12 repeating layers to GPU
0.00.033.933 I load_tensors: offloading output layer to GPU
0.00.033.936 I load_tensors: offloaded 13/13 layers to GPU
0.00.033.961 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.033.964 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.034.182 I llama_new_context_with_model: n_seq_max     = 1
0.00.034.183 I llama_new_context_with_model: n_ctx         = 512
0.00.034.183 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.034.184 I llama_new_context_with_model: n_batch       = 2048
0.00.034.184 I llama_new_context_with_model: n_ubatch      = 2048
0.00.034.184 I llama_new_context_with_model: flash_attn    = 0
0.00.034.185 I llama_new_context_with_model: freq_base     = 10000.0
0.00.034.185 I llama_new_context_with_model: freq_scale    = 1
0.00.034.186 I ggml_metal_init: allocating
0.00.034.190 I ggml_metal_init: found device: Apple M4
0.00.034.192 I ggml_metal_init: picking default device: Apple M4
0.00.034.973 I ggml_metal_init: using embedded metal library
0.00.038.990 I ggml_metal_init: GPU name:   Apple M4
0.00.038.993 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.038.993 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.038.994 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.038.994 I ggml_metal_init: simdgroup reduction   = true
0.00.038.994 I ggml_metal_init: simdgroup matrix mul. = true
0.00.038.994 I ggml_metal_init: has bfloat            = true
0.00.038.994 I ggml_metal_init: use bfloat            = true
0.00.038.995 I ggml_metal_init: hasUnifiedMemory      = true
0.00.038.996 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.050.949 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.051.529 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.051.531 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.051.533 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.052.288 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.052.290 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.052.290 I llama_new_context_with_model: graph nodes  = 429
0.00.052.291 I llama_new_context_with_model: graph splits = 2
0.00.052.292 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.052.292 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.059.088 I 
0.00.059.104 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.059.746 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.064.835 I llama_perf_context_print:        load time =      44.08 ms
0.00.064.836 I llama_perf_context_print: prompt eval time =       4.95 ms /     9 tokens (    0.55 ms per token,  1818.18 tokens per second)
0.00.064.837 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.064.837 I llama_perf_context_print:       total time =       5.75 ms /    10 tokens
0.00.064.991 I ggml_metal_free: deallocating

real	0m0.242s
user	0m0.047s
sys	0m0.028s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.047 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.100 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.815 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.819 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.820 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.821 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.821 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.823 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.824 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.824 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.825 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.825 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.825 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.826 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.828 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.828 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.828 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.829 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.829 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.829 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.356 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.032 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.033 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.033 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.034 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.034 I llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101
0.00.015.035 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.035 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.015.035 I llama_model_loader: - kv  24:                          general.file_type u32              = 7
0.00.015.035 I llama_model_loader: - type  f32:  124 tensors
0.00.015.036 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.036 I print_info: file format = GGUF V3 (latest)
0.00.015.043 I print_info: file type   = Q8_0
0.00.015.044 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.514 I load: special tokens cache size = 5
0.00.018.796 I load: token to piece cache size = 0.2032 MB
0.00.018.805 I print_info: arch             = bert
0.00.018.807 I print_info: vocab_only       = 0
0.00.018.807 I print_info: n_ctx_train      = 512
0.00.018.807 I print_info: n_embd           = 384
0.00.018.807 I print_info: n_layer          = 12
0.00.018.810 I print_info: n_head           = 12
0.00.018.811 I print_info: n_head_kv        = 12
0.00.018.811 I print_info: n_rot            = 32
0.00.018.811 I print_info: n_swa            = 0
0.00.018.811 I print_info: n_embd_head_k    = 32
0.00.018.812 I print_info: n_embd_head_v    = 32
0.00.018.812 I print_info: n_gqa            = 1
0.00.018.813 I print_info: n_embd_k_gqa     = 384
0.00.018.813 I print_info: n_embd_v_gqa     = 384
0.00.018.814 I print_info: f_norm_eps       = 1.0e-12
0.00.018.814 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.814 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.814 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.815 I print_info: f_logit_scale    = 0.0e+00
0.00.018.815 I print_info: n_ff             = 1536
0.00.018.815 I print_info: n_expert         = 0
0.00.018.815 I print_info: n_expert_used    = 0
0.00.018.816 I print_info: causal attn      = 0
0.00.018.816 I print_info: pooling type     = 2
0.00.018.816 I print_info: rope type        = 2
0.00.018.816 I print_info: rope scaling     = linear
0.00.018.817 I print_info: freq_base_train  = 10000.0
0.00.018.818 I print_info: freq_scale_train = 1
0.00.018.818 I print_info: n_ctx_orig_yarn  = 512
0.00.018.818 I print_info: rope_finetuned   = unknown
0.00.018.818 I print_info: ssm_d_conv       = 0
0.00.018.818 I print_info: ssm_d_inner      = 0
0.00.018.818 I print_info: ssm_d_state      = 0
0.00.018.818 I print_info: ssm_dt_rank      = 0
0.00.018.819 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.821 I print_info: model type       = 33M
0.00.018.821 I print_info: model params     = 33.21 M
0.00.018.821 I print_info: general.name     = Bge Small
0.00.018.822 I print_info: vocab type       = WPM
0.00.018.822 I print_info: n_vocab          = 30522
0.00.018.822 I print_info: n_merges         = 0
0.00.018.822 I print_info: UNK token        = 100 '[UNK]'
0.00.018.823 I print_info: SEP token        = 102 '[SEP]'
0.00.018.823 I print_info: PAD token        = 0 '[PAD]'
0.00.018.823 I print_info: CLS token        = 101 '[CLS]'
0.00.018.823 I print_info: MASK token       = 103 '[MASK]'
0.00.018.823 I print_info: LF token         = 0 '[PAD]'
0.00.018.824 I print_info: max token length = 21
0.00.020.084 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.084 I load_tensors: offloading output layer to GPU
0.00.020.084 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.092 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.093 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.020.235 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.236 I llama_new_context_with_model: n_ctx         = 512
0.00.020.236 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.236 I llama_new_context_with_model: n_batch       = 2048
0.00.020.237 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.237 I llama_new_context_with_model: flash_attn    = 0
0.00.020.237 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.237 I llama_new_context_with_model: freq_scale    = 1
0.00.020.238 I ggml_metal_init: allocating
0.00.020.241 I ggml_metal_init: found device: Apple M4
0.00.020.244 I ggml_metal_init: picking default device: Apple M4
0.00.020.834 I ggml_metal_init: using embedded metal library
0.00.023.349 I ggml_metal_init: GPU name:   Apple M4
0.00.023.351 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.352 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.352 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.352 I ggml_metal_init: simdgroup reduction   = true
0.00.023.353 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.353 I ggml_metal_init: has bfloat            = true
0.00.023.353 I ggml_metal_init: use bfloat            = true
0.00.023.353 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.354 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.562 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.041 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.043 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.046 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.712 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.714 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.714 I llama_new_context_with_model: graph nodes  = 429
0.00.034.714 I llama_new_context_with_model: graph splits = 2
0.00.034.715 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.716 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.919 I 
0.00.039.939 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.534 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.092 I llama_perf_context_print:        load time =      30.82 ms
0.00.045.093 I llama_perf_context_print: prompt eval time =       4.42 ms /     9 tokens (    0.49 ms per token,  2033.90 tokens per second)
0.00.045.094 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.094 I llama_perf_context_print:       total time =       5.17 ms /    10 tokens
0.00.045.279 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.213 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.696 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.856 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.861 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.863 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.864 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.865 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.865 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.866 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.867 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.868 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.868 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.869 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.869 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.872 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.873 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.874 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.874 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.875 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.162 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.172 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.544 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.546 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.546 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.547 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.547 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.548 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.548 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.048.548 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.549 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.549 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.549 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.550 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.048.550 I llama_model_loader: - type  f32:   40 tensors
0.00.048.551 I llama_model_loader: - type  f16:   30 tensors
0.00.048.553 I print_info: file format = GGUF V3 (latest)
0.00.048.571 I print_info: file type   = F16
0.00.048.572 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.064.965 W load: empty token at index 5
0.00.069.336 W load: model vocab missing newline token, using special_pad_id instead
0.00.070.693 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.070.715 I load: special tokens cache size = 5
0.00.337.267 I load: token to piece cache size = 1.5060 MB
0.00.337.296 I print_info: arch             = jina-bert-v2
0.00.337.297 I print_info: vocab_only       = 0
0.00.337.297 I print_info: n_ctx_train      = 8192
0.00.337.297 I print_info: n_embd           = 384
0.00.337.298 I print_info: n_layer          = 4
0.00.337.303 I print_info: n_head           = 12
0.00.337.303 I print_info: n_head_kv        = 12
0.00.337.303 I print_info: n_rot            = 32
0.00.337.303 I print_info: n_swa            = 0
0.00.337.304 I print_info: n_embd_head_k    = 32
0.00.337.304 I print_info: n_embd_head_v    = 32
0.00.337.304 I print_info: n_gqa            = 1
0.00.337.305 I print_info: n_embd_k_gqa     = 384
0.00.337.305 I print_info: n_embd_v_gqa     = 384
0.00.337.307 I print_info: f_norm_eps       = 1.0e-12
0.00.337.308 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.337.309 I print_info: f_clamp_kqv      = 0.0e+00
0.00.337.309 I print_info: f_max_alibi_bias = 8.0e+00
0.00.337.309 I print_info: f_logit_scale    = 0.0e+00
0.00.337.310 I print_info: n_ff             = 1536
0.00.337.310 I print_info: n_expert         = 0
0.00.337.310 I print_info: n_expert_used    = 0
0.00.337.310 I print_info: causal attn      = 0
0.00.337.311 I print_info: pooling type     = -1
0.00.337.311 I print_info: rope type        = -1
0.00.337.311 I print_info: rope scaling     = linear
0.00.337.312 I print_info: freq_base_train  = 10000.0
0.00.337.312 I print_info: freq_scale_train = 1
0.00.337.312 I print_info: n_ctx_orig_yarn  = 8192
0.00.337.314 I print_info: rope_finetuned   = unknown
0.00.337.315 I print_info: ssm_d_conv       = 0
0.00.337.315 I print_info: ssm_d_inner      = 0
0.00.337.315 I print_info: ssm_d_state      = 0
0.00.337.315 I print_info: ssm_dt_rank      = 0
0.00.337.315 I print_info: ssm_dt_b_c_rms   = 0
0.00.337.315 I print_info: model type       = 33M
0.00.337.316 I print_info: model params     = 32.90 M
0.00.337.316 I print_info: general.name     = Jina Bert Implementation
0.00.337.316 I print_info: vocab type       = BPE
0.00.337.317 I print_info: n_vocab          = 61056
0.00.337.317 I print_info: n_merges         = 39382
0.00.337.317 I print_info: BOS token        = 0 '<s>'
0.00.337.317 I print_info: EOS token        = 2 '</s>'
0.00.337.317 I print_info: UNK token        = 3 '<unk>'
0.00.337.318 I print_info: SEP token        = 2 '</s>'
0.00.337.318 I print_info: PAD token        = 1 '<pad>'
0.00.337.322 I print_info: CLS token        = 0 '<s>'
0.00.337.322 I print_info: MASK token       = 4 '<mask>'
0.00.337.323 I print_info: EOG token        = 2 '</s>'
0.00.337.323 I print_info: max token length = 45
0.00.338.301 I load_tensors: offloading 4 repeating layers to GPU
0.00.338.301 I load_tensors: offloading output layer to GPU
0.00.338.301 I load_tensors: offloaded 5/5 layers to GPU
0.00.338.326 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.338.327 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.338.524 I llama_new_context_with_model: n_seq_max     = 1
0.00.338.525 I llama_new_context_with_model: n_ctx         = 8192
0.00.338.525 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.338.526 I llama_new_context_with_model: n_batch       = 2048
0.00.338.526 I llama_new_context_with_model: n_ubatch      = 2048
0.00.338.526 I llama_new_context_with_model: flash_attn    = 0
0.00.338.526 I llama_new_context_with_model: freq_base     = 10000.0
0.00.338.526 I llama_new_context_with_model: freq_scale    = 1
0.00.338.527 I ggml_metal_init: allocating
0.00.338.530 I ggml_metal_init: found device: Apple M4
0.00.338.532 I ggml_metal_init: picking default device: Apple M4
0.00.339.288 I ggml_metal_init: using embedded metal library
0.00.341.990 I ggml_metal_init: GPU name:   Apple M4
0.00.341.992 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.341.992 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.341.992 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.341.993 I ggml_metal_init: simdgroup reduction   = true
0.00.341.993 I ggml_metal_init: simdgroup matrix mul. = true
0.00.341.993 I ggml_metal_init: has bfloat            = true
0.00.341.993 I ggml_metal_init: use bfloat            = true
0.00.341.994 I ggml_metal_init: hasUnifiedMemory      = true
0.00.341.994 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.351.754 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.354.402 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.354.406 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.354.408 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.355.059 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.355.060 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.355.060 I llama_new_context_with_model: graph nodes  = 154
0.00.355.060 I llama_new_context_with_model: graph splits = 2
0.00.355.061 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.355.062 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.375.567 I 
0.00.375.590 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.375.740 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.375.741 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.375.755 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.375.755 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.375.762 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.375.762 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.376.272 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.379.817 I llama_perf_context_print:        load time =     352.86 ms
0.00.379.818 I llama_perf_context_print: prompt eval time =       3.54 ms /    62 tokens (    0.06 ms per token, 17538.90 tokens per second)
0.00.379.819 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.379.821 I llama_perf_context_print:       total time =       4.25 ms /    63 tokens
0.00.380.077 I ggml_metal_free: deallocating

real	0m1.124s
user	0m0.368s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.172 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.281 I main: llama backend init
0.00.000.286 I main: load the model and apply lora adapter, if any
0.00.033.292 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.045.575 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.045.588 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.045.593 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.045.593 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.045.594 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.045.600 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.045.601 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.045.604 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.045.605 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.045.606 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.045.607 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.045.607 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.045.607 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.045.608 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.045.615 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.045.616 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.045.616 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.054.267 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.056.821 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.065.416 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.065.420 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.065.420 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.065.421 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.065.421 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.065.422 I llama_model_loader: - type  f32:  194 tensors
0.00.065.423 I llama_model_loader: - type  f16:   98 tensors
0.00.065.424 I print_info: file format = GGUF V3 (latest)
0.00.065.444 I print_info: file type   = all F32 (guessed)
0.00.065.445 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.095.358 I load: special tokens cache size = 25
0.00.102.272 I load: token to piece cache size = 0.2984 MB
0.00.102.294 I print_info: arch             = gptneox
0.00.102.295 I print_info: vocab_only       = 0
0.00.102.295 I print_info: n_ctx_train      = 2048
0.00.102.295 I print_info: n_embd           = 2048
0.00.102.296 I print_info: n_layer          = 24
0.00.102.298 I print_info: n_head           = 16
0.00.102.299 I print_info: n_head_kv        = 16
0.00.102.299 I print_info: n_rot            = 32
0.00.102.299 I print_info: n_swa            = 0
0.00.102.300 I print_info: n_embd_head_k    = 128
0.00.102.300 I print_info: n_embd_head_v    = 128
0.00.102.300 I print_info: n_gqa            = 1
0.00.102.301 I print_info: n_embd_k_gqa     = 2048
0.00.102.302 I print_info: n_embd_v_gqa     = 2048
0.00.102.302 I print_info: f_norm_eps       = 1.0e-05
0.00.102.303 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.102.303 I print_info: f_clamp_kqv      = 0.0e+00
0.00.102.305 I print_info: f_max_alibi_bias = 0.0e+00
0.00.102.305 I print_info: f_logit_scale    = 0.0e+00
0.00.102.306 I print_info: n_ff             = 8192
0.00.102.306 I print_info: n_expert         = 0
0.00.102.310 I print_info: n_expert_used    = 0
0.00.102.310 I print_info: causal attn      = 1
0.00.102.310 I print_info: pooling type     = 0
0.00.102.310 I print_info: rope type        = 2
0.00.102.311 I print_info: rope scaling     = linear
0.00.102.311 I print_info: freq_base_train  = 10000.0
0.00.102.311 I print_info: freq_scale_train = 1
0.00.102.312 I print_info: n_ctx_orig_yarn  = 2048
0.00.102.312 I print_info: rope_finetuned   = unknown
0.00.102.312 I print_info: ssm_d_conv       = 0
0.00.102.312 I print_info: ssm_d_inner      = 0
0.00.102.312 I print_info: ssm_d_state      = 0
0.00.102.312 I print_info: ssm_dt_rank      = 0
0.00.102.312 I print_info: ssm_dt_b_c_rms   = 0
0.00.102.312 I print_info: model type       = 1.4B
0.00.102.313 I print_info: model params     = 1.41 B
0.00.102.313 I print_info: general.name     = 1.4B
0.00.102.313 I print_info: vocab type       = BPE
0.00.102.313 I print_info: n_vocab          = 50304
0.00.102.314 I print_info: n_merges         = 50009
0.00.102.314 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.102.314 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.102.314 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.102.314 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.102.315 I print_info: LF token         = 128 ''
0.00.102.315 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.102.315 I print_info: max token length = 1024
0.00.104.955 I load_tensors: offloading 24 repeating layers to GPU
0.00.104.955 I load_tensors: offloading output layer to GPU
0.00.104.955 I load_tensors: offloaded 25/25 layers to GPU
0.00.104.973 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.104.974 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.105.268 I llama_new_context_with_model: n_seq_max     = 1
0.00.105.269 I llama_new_context_with_model: n_ctx         = 2048
0.00.105.269 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.105.269 I llama_new_context_with_model: n_batch       = 2048
0.00.105.270 I llama_new_context_with_model: n_ubatch      = 512
0.00.105.270 I llama_new_context_with_model: flash_attn    = 0
0.00.105.270 I llama_new_context_with_model: freq_base     = 10000.0
0.00.105.270 I llama_new_context_with_model: freq_scale    = 1
0.00.105.271 I ggml_metal_init: allocating
0.00.105.274 I ggml_metal_init: found device: Apple M4
0.00.105.276 I ggml_metal_init: picking default device: Apple M4
0.00.105.943 I ggml_metal_init: using embedded metal library
0.00.116.381 I ggml_metal_init: GPU name:   Apple M4
0.00.116.383 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.116.383 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.116.383 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.116.384 I ggml_metal_init: simdgroup reduction   = true
0.00.116.384 I ggml_metal_init: simdgroup matrix mul. = true
0.00.116.384 I ggml_metal_init: has bfloat            = true
0.00.116.384 I ggml_metal_init: use bfloat            = true
0.00.116.384 I ggml_metal_init: hasUnifiedMemory      = true
0.00.116.385 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.140.081 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.161.295 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.161.302 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.161.325 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.162.279 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.162.281 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.162.282 I llama_new_context_with_model: graph nodes  = 967
0.00.162.282 I llama_new_context_with_model: graph splits = 2
0.00.162.286 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.162.414 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.162.414 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.246.033 I main: llama threadpool init, n_threads = 4
0.00.246.075 I 
0.00.246.094 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.246.096 I 
0.00.246.167 I sampler seed: 1234
0.00.246.172 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.246.207 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.246.209 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.246.209 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.089.983 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58292.28 tokens per second)
0.02.089.984 I llama_perf_context_print:        load time =     212.73 ms
0.02.089.985 I llama_perf_context_print: prompt eval time =      43.52 ms /     7 tokens (    6.22 ms per token,   160.86 tokens per second)
0.02.089.986 I llama_perf_context_print:        eval time =    1797.39 ms /    63 runs   (   28.53 ms per token,    35.05 tokens per second)
0.02.089.987 I llama_perf_context_print:       total time =    1843.95 ms /    70 tokens
0.02.090.248 I ggml_metal_free: deallocating

real	0m2.381s
user	0m0.145s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.332 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.693 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.040.654 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.671 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.680 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.681 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.682 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.682 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.683 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.686 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.686 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.687 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.688 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.688 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.689 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.690 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.695 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.696 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.696 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.161 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.423 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.232 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.059.239 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.240 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.240 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.241 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.242 I llama_model_loader: - type  f32:  194 tensors
0.00.059.243 I llama_model_loader: - type  f16:   98 tensors
0.00.059.244 I print_info: file format = GGUF V3 (latest)
0.00.059.264 I print_info: file type   = all F32 (guessed)
0.00.059.266 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.092.811 I load: special tokens cache size = 25
0.00.100.282 I load: token to piece cache size = 0.2984 MB
0.00.100.297 I print_info: arch             = gptneox
0.00.100.298 I print_info: vocab_only       = 0
0.00.100.298 I print_info: n_ctx_train      = 2048
0.00.100.298 I print_info: n_embd           = 2048
0.00.100.298 I print_info: n_layer          = 24
0.00.100.302 I print_info: n_head           = 16
0.00.100.303 I print_info: n_head_kv        = 16
0.00.100.303 I print_info: n_rot            = 32
0.00.100.303 I print_info: n_swa            = 0
0.00.100.303 I print_info: n_embd_head_k    = 128
0.00.100.303 I print_info: n_embd_head_v    = 128
0.00.100.304 I print_info: n_gqa            = 1
0.00.100.305 I print_info: n_embd_k_gqa     = 2048
0.00.100.308 I print_info: n_embd_v_gqa     = 2048
0.00.100.309 I print_info: f_norm_eps       = 1.0e-05
0.00.100.309 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.100.309 I print_info: f_clamp_kqv      = 0.0e+00
0.00.100.310 I print_info: f_max_alibi_bias = 0.0e+00
0.00.100.310 I print_info: f_logit_scale    = 0.0e+00
0.00.100.310 I print_info: n_ff             = 8192
0.00.100.311 I print_info: n_expert         = 0
0.00.100.311 I print_info: n_expert_used    = 0
0.00.100.311 I print_info: causal attn      = 1
0.00.100.311 I print_info: pooling type     = 0
0.00.100.311 I print_info: rope type        = 2
0.00.100.312 I print_info: rope scaling     = linear
0.00.100.312 I print_info: freq_base_train  = 10000.0
0.00.100.312 I print_info: freq_scale_train = 1
0.00.100.312 I print_info: n_ctx_orig_yarn  = 2048
0.00.100.313 I print_info: rope_finetuned   = unknown
0.00.100.314 I print_info: ssm_d_conv       = 0
0.00.100.314 I print_info: ssm_d_inner      = 0
0.00.100.314 I print_info: ssm_d_state      = 0
0.00.100.314 I print_info: ssm_dt_rank      = 0
0.00.100.315 I print_info: ssm_dt_b_c_rms   = 0
0.00.100.315 I print_info: model type       = 1.4B
0.00.100.315 I print_info: model params     = 1.41 B
0.00.100.315 I print_info: general.name     = 1.4B
0.00.100.316 I print_info: vocab type       = BPE
0.00.100.316 I print_info: n_vocab          = 50304
0.00.100.316 I print_info: n_merges         = 50009
0.00.100.316 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.100.317 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.100.323 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.100.325 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.100.326 I print_info: LF token         = 128 ''
0.00.100.328 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.100.328 I print_info: max token length = 1024
0.00.103.108 I load_tensors: offloading 24 repeating layers to GPU
0.00.103.108 I load_tensors: offloading output layer to GPU
0.00.103.108 I load_tensors: offloaded 25/25 layers to GPU
0.00.103.119 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.103.120 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.103.425 I llama_new_context_with_model: n_seq_max     = 1
0.00.103.426 I llama_new_context_with_model: n_ctx         = 128
0.00.103.426 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.103.426 I llama_new_context_with_model: n_batch       = 128
0.00.103.427 I llama_new_context_with_model: n_ubatch      = 128
0.00.103.427 I llama_new_context_with_model: flash_attn    = 0
0.00.103.427 I llama_new_context_with_model: freq_base     = 10000.0
0.00.103.427 I llama_new_context_with_model: freq_scale    = 1
0.00.103.428 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.103.428 I ggml_metal_init: allocating
0.00.103.431 I ggml_metal_init: found device: Apple M4
0.00.103.433 I ggml_metal_init: picking default device: Apple M4
0.00.104.104 I ggml_metal_init: using embedded metal library
0.00.106.977 I ggml_metal_init: GPU name:   Apple M4
0.00.106.979 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.106.979 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.106.979 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.106.980 I ggml_metal_init: simdgroup reduction   = true
0.00.106.980 I ggml_metal_init: simdgroup matrix mul. = true
0.00.106.980 I ggml_metal_init: has bfloat            = true
0.00.106.980 I ggml_metal_init: use bfloat            = true
0.00.106.981 I ggml_metal_init: hasUnifiedMemory      = true
0.00.106.981 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.116.815 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.118.119 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.118.124 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.118.139 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.119.087 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.119.088 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.119.088 I llama_new_context_with_model: graph nodes  = 967
0.00.119.088 I llama_new_context_with_model: graph splits = 2
0.00.119.089 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.119.089 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.104.745 I 
0.01.104.805 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.104.846 I perplexity: tokenizing the input ..
0.01.119.766 I perplexity: tokenization took 14.917 ms
0.01.119.787 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.242.591 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.244.449 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.244.509 I llama_perf_context_print:        load time =    1079.04 ms
0.01.244.511 I llama_perf_context_print: prompt eval time =     121.83 ms /   128 tokens (    0.95 ms per token,  1050.65 tokens per second)
0.01.244.512 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.244.516 I llama_perf_context_print:       total time =     139.77 ms /   129 tokens
0.01.245.279 I ggml_metal_free: deallocating

real	0m1.443s
user	0m0.131s
sys	0m0.204s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.656 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.468 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.473 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.474 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.475 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.475 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.475 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.476 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.477 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.477 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.478 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.478 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.478 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.479 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.482 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.484 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.484 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.484 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.556 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.586 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.656 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.657 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.658 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.658 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.658 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.659 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.030.660 I llama_model_loader: - type  f32:  194 tensors
0.00.030.660 I llama_model_loader: - type q8_0:   98 tensors
0.00.030.661 I print_info: file format = GGUF V3 (latest)
0.00.030.676 I print_info: file type   = Q8_0
0.00.030.677 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.051.098 I load: special tokens cache size = 25
0.00.057.098 I load: token to piece cache size = 0.2984 MB
0.00.057.115 I print_info: arch             = gptneox
0.00.057.116 I print_info: vocab_only       = 0
0.00.057.116 I print_info: n_ctx_train      = 2048
0.00.057.116 I print_info: n_embd           = 2048
0.00.057.117 I print_info: n_layer          = 24
0.00.057.122 I print_info: n_head           = 16
0.00.057.123 I print_info: n_head_kv        = 16
0.00.057.123 I print_info: n_rot            = 32
0.00.057.123 I print_info: n_swa            = 0
0.00.057.123 I print_info: n_embd_head_k    = 128
0.00.057.124 I print_info: n_embd_head_v    = 128
0.00.057.124 I print_info: n_gqa            = 1
0.00.057.125 I print_info: n_embd_k_gqa     = 2048
0.00.057.126 I print_info: n_embd_v_gqa     = 2048
0.00.057.126 I print_info: f_norm_eps       = 1.0e-05
0.00.057.127 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.127 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.128 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.128 I print_info: f_logit_scale    = 0.0e+00
0.00.057.128 I print_info: n_ff             = 8192
0.00.057.129 I print_info: n_expert         = 0
0.00.057.129 I print_info: n_expert_used    = 0
0.00.057.129 I print_info: causal attn      = 1
0.00.057.129 I print_info: pooling type     = 0
0.00.057.129 I print_info: rope type        = 2
0.00.057.130 I print_info: rope scaling     = linear
0.00.057.130 I print_info: freq_base_train  = 10000.0
0.00.057.130 I print_info: freq_scale_train = 1
0.00.057.131 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.131 I print_info: rope_finetuned   = unknown
0.00.057.131 I print_info: ssm_d_conv       = 0
0.00.057.131 I print_info: ssm_d_inner      = 0
0.00.057.131 I print_info: ssm_d_state      = 0
0.00.057.131 I print_info: ssm_dt_rank      = 0
0.00.057.133 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.133 I print_info: model type       = 1.4B
0.00.057.133 I print_info: model params     = 1.41 B
0.00.057.134 I print_info: general.name     = 1.4B
0.00.057.134 I print_info: vocab type       = BPE
0.00.057.135 I print_info: n_vocab          = 50304
0.00.057.135 I print_info: n_merges         = 50009
0.00.057.135 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.135 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.135 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.135 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.136 I print_info: LF token         = 128 ''
0.00.057.136 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.136 I print_info: max token length = 1024
0.00.059.577 I load_tensors: offloading 24 repeating layers to GPU
0.00.059.577 I load_tensors: offloading output layer to GPU
0.00.059.577 I load_tensors: offloaded 25/25 layers to GPU
0.00.059.589 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.059.590 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.059.919 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.920 I llama_new_context_with_model: n_ctx         = 2048
0.00.059.920 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.059.921 I llama_new_context_with_model: n_batch       = 2048
0.00.059.921 I llama_new_context_with_model: n_ubatch      = 512
0.00.059.921 I llama_new_context_with_model: flash_attn    = 0
0.00.059.921 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.921 I llama_new_context_with_model: freq_scale    = 1
0.00.059.922 I ggml_metal_init: allocating
0.00.059.926 I ggml_metal_init: found device: Apple M4
0.00.059.928 I ggml_metal_init: picking default device: Apple M4
0.00.060.681 I ggml_metal_init: using embedded metal library
0.00.063.421 I ggml_metal_init: GPU name:   Apple M4
0.00.063.423 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.423 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.424 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.424 I ggml_metal_init: simdgroup reduction   = true
0.00.063.424 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.424 I ggml_metal_init: has bfloat            = true
0.00.063.424 I ggml_metal_init: use bfloat            = true
0.00.063.425 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.425 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.060 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.097.828 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.839 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.878 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.986 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.098.988 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.098.988 I llama_new_context_with_model: graph nodes  = 967
0.00.098.989 I llama_new_context_with_model: graph splits = 2
0.00.098.994 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.099.109 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.110 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.283.622 I main: llama threadpool init, n_threads = 4
0.01.283.688 I 
0.01.283.740 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.283.743 I 
0.01.284.258 I sampler seed: 1234
0.01.284.266 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.284.297 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.284.299 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.284.299 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.399.581 I llama_perf_sampler_print:    sampling time =       1.59 ms /    71 runs   (    0.02 ms per token, 44626.02 tokens per second)
0.02.399.582 I llama_perf_context_print:        load time =    1273.96 ms
0.02.399.584 I llama_perf_context_print: prompt eval time =      50.30 ms /     7 tokens (    7.19 ms per token,   139.16 tokens per second)
0.02.399.585 I llama_perf_context_print:        eval time =    1062.20 ms /    63 runs   (   16.86 ms per token,    59.31 tokens per second)
0.02.399.585 I llama_perf_context_print:       total time =    1115.96 ms /    70 tokens
0.02.399.822 I ggml_metal_free: deallocating

real	0m2.416s
user	0m0.124s
sys	0m0.223s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.130 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.375 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.164 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.173 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.175 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.176 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.176 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.177 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.177 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.178 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.179 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.181 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.186 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.186 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.187 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.187 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.190 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.190 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.190 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.441 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.113 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.358 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.360 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.361 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.361 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.361 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.362 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.363 I llama_model_loader: - type  f32:  194 tensors
0.00.039.363 I llama_model_loader: - type q8_0:   98 tensors
0.00.039.364 I print_info: file format = GGUF V3 (latest)
0.00.039.378 I print_info: file type   = Q8_0
0.00.039.379 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.063.794 I load: special tokens cache size = 25
0.00.069.953 I load: token to piece cache size = 0.2984 MB
0.00.069.970 I print_info: arch             = gptneox
0.00.069.971 I print_info: vocab_only       = 0
0.00.069.971 I print_info: n_ctx_train      = 2048
0.00.069.971 I print_info: n_embd           = 2048
0.00.069.972 I print_info: n_layer          = 24
0.00.069.976 I print_info: n_head           = 16
0.00.069.978 I print_info: n_head_kv        = 16
0.00.069.979 I print_info: n_rot            = 32
0.00.069.979 I print_info: n_swa            = 0
0.00.069.979 I print_info: n_embd_head_k    = 128
0.00.069.979 I print_info: n_embd_head_v    = 128
0.00.069.980 I print_info: n_gqa            = 1
0.00.069.981 I print_info: n_embd_k_gqa     = 2048
0.00.069.981 I print_info: n_embd_v_gqa     = 2048
0.00.069.982 I print_info: f_norm_eps       = 1.0e-05
0.00.069.983 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.983 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.983 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.983 I print_info: f_logit_scale    = 0.0e+00
0.00.069.984 I print_info: n_ff             = 8192
0.00.069.984 I print_info: n_expert         = 0
0.00.069.984 I print_info: n_expert_used    = 0
0.00.069.984 I print_info: causal attn      = 1
0.00.069.984 I print_info: pooling type     = 0
0.00.069.985 I print_info: rope type        = 2
0.00.069.985 I print_info: rope scaling     = linear
0.00.069.985 I print_info: freq_base_train  = 10000.0
0.00.069.985 I print_info: freq_scale_train = 1
0.00.069.986 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.986 I print_info: rope_finetuned   = unknown
0.00.069.986 I print_info: ssm_d_conv       = 0
0.00.069.986 I print_info: ssm_d_inner      = 0
0.00.069.986 I print_info: ssm_d_state      = 0
0.00.069.987 I print_info: ssm_dt_rank      = 0
0.00.069.987 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.987 I print_info: model type       = 1.4B
0.00.069.988 I print_info: model params     = 1.41 B
0.00.069.988 I print_info: general.name     = 1.4B
0.00.069.989 I print_info: vocab type       = BPE
0.00.069.989 I print_info: n_vocab          = 50304
0.00.069.989 I print_info: n_merges         = 50009
0.00.069.989 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.989 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.989 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.990 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.990 I print_info: LF token         = 128 ''
0.00.069.990 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.990 I print_info: max token length = 1024
0.00.072.363 I load_tensors: offloading 24 repeating layers to GPU
0.00.072.364 I load_tensors: offloading output layer to GPU
0.00.072.364 I load_tensors: offloaded 25/25 layers to GPU
0.00.072.375 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.072.376 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.072.643 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.644 I llama_new_context_with_model: n_ctx         = 128
0.00.072.644 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.072.644 I llama_new_context_with_model: n_batch       = 128
0.00.072.645 I llama_new_context_with_model: n_ubatch      = 128
0.00.072.645 I llama_new_context_with_model: flash_attn    = 0
0.00.072.645 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.647 I llama_new_context_with_model: freq_scale    = 1
0.00.072.648 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.072.648 I ggml_metal_init: allocating
0.00.072.651 I ggml_metal_init: found device: Apple M4
0.00.072.653 I ggml_metal_init: picking default device: Apple M4
0.00.073.277 I ggml_metal_init: using embedded metal library
0.00.075.921 I ggml_metal_init: GPU name:   Apple M4
0.00.075.922 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.923 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.923 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.923 I ggml_metal_init: simdgroup reduction   = true
0.00.075.923 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.924 I ggml_metal_init: has bfloat            = true
0.00.075.924 I ggml_metal_init: use bfloat            = true
0.00.075.924 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.925 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.941 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.245 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.086.248 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.086.263 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.097 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.087.099 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.087.099 I llama_new_context_with_model: graph nodes  = 967
0.00.087.099 I llama_new_context_with_model: graph splits = 2
0.00.087.100 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.087.101 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.339.858 I 
0.01.339.927 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.339.957 I perplexity: tokenizing the input ..
0.01.356.867 I perplexity: tokenization took 16.906 ms
0.01.356.873 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.498.765 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.500.422 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.500.459 I llama_perf_context_print:        load time =    1328.48 ms
0.01.500.460 I llama_perf_context_print: prompt eval time =     141.01 ms /   128 tokens (    1.10 ms per token,   907.75 tokens per second)
0.01.500.461 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.500.462 I llama_perf_context_print:       total time =     160.60 ms /   129 tokens
0.01.501.305 I ggml_metal_free: deallocating

real	0m1.523s
user	0m0.109s
sys	0m0.165s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.060 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.097 I main: llama backend init
0.00.000.100 I main: load the model and apply lora adapter, if any
0.00.018.061 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.529 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.036.536 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.538 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.538 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.539 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.539 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.539 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.540 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.543 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.543 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.543 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.544 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.544 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.545 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.548 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.548 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.549 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.959 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.204 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.251 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.046.252 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.252 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.253 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.253 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.253 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.046.254 I llama_model_loader: - type  f32:  194 tensors
0.00.046.254 I llama_model_loader: - type q4_0:   97 tensors
0.00.046.255 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.255 I print_info: file format = GGUF V3 (latest)
0.00.046.268 I print_info: file type   = Q4_0
0.00.046.269 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.068.489 I load: special tokens cache size = 25
0.00.076.493 I load: token to piece cache size = 0.2984 MB
0.00.076.501 I print_info: arch             = gptneox
0.00.076.501 I print_info: vocab_only       = 0
0.00.076.501 I print_info: n_ctx_train      = 2048
0.00.076.501 I print_info: n_embd           = 2048
0.00.076.502 I print_info: n_layer          = 24
0.00.076.507 I print_info: n_head           = 16
0.00.076.507 I print_info: n_head_kv        = 16
0.00.076.507 I print_info: n_rot            = 32
0.00.076.510 I print_info: n_swa            = 0
0.00.076.510 I print_info: n_embd_head_k    = 128
0.00.076.510 I print_info: n_embd_head_v    = 128
0.00.076.511 I print_info: n_gqa            = 1
0.00.076.511 I print_info: n_embd_k_gqa     = 2048
0.00.076.512 I print_info: n_embd_v_gqa     = 2048
0.00.076.512 I print_info: f_norm_eps       = 1.0e-05
0.00.076.515 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.516 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.516 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.516 I print_info: f_logit_scale    = 0.0e+00
0.00.076.517 I print_info: n_ff             = 8192
0.00.076.517 I print_info: n_expert         = 0
0.00.076.517 I print_info: n_expert_used    = 0
0.00.076.517 I print_info: causal attn      = 1
0.00.076.518 I print_info: pooling type     = 0
0.00.076.518 I print_info: rope type        = 2
0.00.076.518 I print_info: rope scaling     = linear
0.00.076.518 I print_info: freq_base_train  = 10000.0
0.00.076.519 I print_info: freq_scale_train = 1
0.00.076.519 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.519 I print_info: rope_finetuned   = unknown
0.00.076.519 I print_info: ssm_d_conv       = 0
0.00.076.519 I print_info: ssm_d_inner      = 0
0.00.076.519 I print_info: ssm_d_state      = 0
0.00.076.519 I print_info: ssm_dt_rank      = 0
0.00.076.519 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.520 I print_info: model type       = 1.4B
0.00.076.520 I print_info: model params     = 1.41 B
0.00.076.520 I print_info: general.name     = 1.4B
0.00.076.521 I print_info: vocab type       = BPE
0.00.076.521 I print_info: n_vocab          = 50304
0.00.076.521 I print_info: n_merges         = 50009
0.00.076.521 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.521 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.522 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.522 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.522 I print_info: LF token         = 128 ''
0.00.076.522 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.522 I print_info: max token length = 1024
0.00.078.536 I load_tensors: offloading 24 repeating layers to GPU
0.00.078.536 I load_tensors: offloading output layer to GPU
0.00.078.536 I load_tensors: offloaded 25/25 layers to GPU
0.00.078.542 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.078.543 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.079.019 I llama_new_context_with_model: n_seq_max     = 1
0.00.079.020 I llama_new_context_with_model: n_ctx         = 2048
0.00.079.020 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.079.020 I llama_new_context_with_model: n_batch       = 2048
0.00.079.020 I llama_new_context_with_model: n_ubatch      = 512
0.00.079.021 I llama_new_context_with_model: flash_attn    = 0
0.00.079.021 I llama_new_context_with_model: freq_base     = 10000.0
0.00.079.021 I llama_new_context_with_model: freq_scale    = 1
0.00.079.022 I ggml_metal_init: allocating
0.00.079.025 I ggml_metal_init: found device: Apple M4
0.00.079.027 I ggml_metal_init: picking default device: Apple M4
0.00.079.667 I ggml_metal_init: using embedded metal library
0.00.082.547 I ggml_metal_init: GPU name:   Apple M4
0.00.082.549 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.082.550 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.082.550 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.082.550 I ggml_metal_init: simdgroup reduction   = true
0.00.082.551 I ggml_metal_init: simdgroup matrix mul. = true
0.00.082.551 I ggml_metal_init: has bfloat            = true
0.00.082.551 I ggml_metal_init: use bfloat            = true
0.00.082.551 I ggml_metal_init: hasUnifiedMemory      = true
0.00.082.553 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.295 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.114.188 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.114.197 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.114.217 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.115.276 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.115.278 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.115.278 I llama_new_context_with_model: graph nodes  = 967
0.00.115.279 I llama_new_context_with_model: graph splits = 2
0.00.115.283 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.115.416 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.115.417 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.114 I main: llama threadpool init, n_threads = 4
0.00.657.158 I 
0.00.657.181 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.181 I 
0.00.657.492 I sampler seed: 1234
0.00.657.497 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.657.540 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.657.540 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.657.540 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.337.768 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60580.20 tokens per second)
0.01.337.770 I llama_perf_context_print:        load time =     639.04 ms
0.01.337.770 I llama_perf_context_print: prompt eval time =      45.06 ms /     7 tokens (    6.44 ms per token,   155.34 tokens per second)
0.01.337.771 I llama_perf_context_print:        eval time =     632.28 ms /    63 runs   (   10.04 ms per token,    99.64 tokens per second)
0.01.337.772 I llama_perf_context_print:       total time =     680.66 ms /    70 tokens
0.01.338.043 I ggml_metal_free: deallocating

real	0m1.358s
user	0m0.121s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.278 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.026.716 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.040.533 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.040.547 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.556 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.556 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.557 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.558 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.558 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.560 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.561 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.561 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.562 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.563 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.563 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.564 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.569 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.570 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.571 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.413 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.606 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.645 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.059.650 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.651 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.652 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.652 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.653 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.059.654 I llama_model_loader: - type  f32:  194 tensors
0.00.059.654 I llama_model_loader: - type q4_0:   97 tensors
0.00.059.661 I llama_model_loader: - type q6_K:    1 tensors
0.00.059.663 I print_info: file format = GGUF V3 (latest)
0.00.059.679 I print_info: file type   = Q4_0
0.00.059.681 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.091.015 I load: special tokens cache size = 25
0.00.098.268 I load: token to piece cache size = 0.2984 MB
0.00.098.283 I print_info: arch             = gptneox
0.00.098.284 I print_info: vocab_only       = 0
0.00.098.284 I print_info: n_ctx_train      = 2048
0.00.098.285 I print_info: n_embd           = 2048
0.00.098.285 I print_info: n_layer          = 24
0.00.098.288 I print_info: n_head           = 16
0.00.098.288 I print_info: n_head_kv        = 16
0.00.098.289 I print_info: n_rot            = 32
0.00.098.290 I print_info: n_swa            = 0
0.00.098.290 I print_info: n_embd_head_k    = 128
0.00.098.290 I print_info: n_embd_head_v    = 128
0.00.098.290 I print_info: n_gqa            = 1
0.00.098.291 I print_info: n_embd_k_gqa     = 2048
0.00.098.292 I print_info: n_embd_v_gqa     = 2048
0.00.098.292 I print_info: f_norm_eps       = 1.0e-05
0.00.098.293 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.098.293 I print_info: f_clamp_kqv      = 0.0e+00
0.00.098.293 I print_info: f_max_alibi_bias = 0.0e+00
0.00.098.293 I print_info: f_logit_scale    = 0.0e+00
0.00.098.294 I print_info: n_ff             = 8192
0.00.098.299 I print_info: n_expert         = 0
0.00.098.301 I print_info: n_expert_used    = 0
0.00.098.301 I print_info: causal attn      = 1
0.00.098.301 I print_info: pooling type     = 0
0.00.098.301 I print_info: rope type        = 2
0.00.098.302 I print_info: rope scaling     = linear
0.00.098.302 I print_info: freq_base_train  = 10000.0
0.00.098.302 I print_info: freq_scale_train = 1
0.00.098.303 I print_info: n_ctx_orig_yarn  = 2048
0.00.098.303 I print_info: rope_finetuned   = unknown
0.00.098.303 I print_info: ssm_d_conv       = 0
0.00.098.303 I print_info: ssm_d_inner      = 0
0.00.098.303 I print_info: ssm_d_state      = 0
0.00.098.305 I print_info: ssm_dt_rank      = 0
0.00.098.305 I print_info: ssm_dt_b_c_rms   = 0
0.00.098.305 I print_info: model type       = 1.4B
0.00.098.305 I print_info: model params     = 1.41 B
0.00.098.307 I print_info: general.name     = 1.4B
0.00.098.307 I print_info: vocab type       = BPE
0.00.098.307 I print_info: n_vocab          = 50304
0.00.098.307 I print_info: n_merges         = 50009
0.00.098.308 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.098.308 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.098.308 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.098.308 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.098.309 I print_info: LF token         = 128 ''
0.00.098.309 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.098.310 I print_info: max token length = 1024
0.00.100.499 I load_tensors: offloading 24 repeating layers to GPU
0.00.100.499 I load_tensors: offloading output layer to GPU
0.00.100.500 I load_tensors: offloaded 25/25 layers to GPU
0.00.100.510 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.100.512 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.100.852 I llama_new_context_with_model: n_seq_max     = 1
0.00.100.853 I llama_new_context_with_model: n_ctx         = 128
0.00.100.854 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.100.854 I llama_new_context_with_model: n_batch       = 128
0.00.100.854 I llama_new_context_with_model: n_ubatch      = 128
0.00.100.854 I llama_new_context_with_model: flash_attn    = 0
0.00.100.855 I llama_new_context_with_model: freq_base     = 10000.0
0.00.100.855 I llama_new_context_with_model: freq_scale    = 1
0.00.100.855 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.100.856 I ggml_metal_init: allocating
0.00.100.859 I ggml_metal_init: found device: Apple M4
0.00.100.861 I ggml_metal_init: picking default device: Apple M4
0.00.101.515 I ggml_metal_init: using embedded metal library
0.00.104.300 I ggml_metal_init: GPU name:   Apple M4
0.00.104.302 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.104.302 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.104.303 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.104.303 I ggml_metal_init: simdgroup reduction   = true
0.00.104.303 I ggml_metal_init: simdgroup matrix mul. = true
0.00.104.303 I ggml_metal_init: has bfloat            = true
0.00.104.303 I ggml_metal_init: use bfloat            = true
0.00.104.304 I ggml_metal_init: hasUnifiedMemory      = true
0.00.104.304 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.114.273 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.115.621 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.115.624 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.115.636 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.116.533 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.116.534 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.116.535 I llama_new_context_with_model: graph nodes  = 967
0.00.116.535 I llama_new_context_with_model: graph splits = 2
0.00.116.536 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.116.536 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.761.975 I 
0.00.762.097 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.762.139 I perplexity: tokenizing the input ..
0.00.778.652 I perplexity: tokenization took 16.502 ms
0.00.778.693 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.918.513 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.919.670 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.919.703 I llama_perf_context_print:        load time =     735.23 ms
0.00.919.704 I llama_perf_context_print: prompt eval time =     138.90 ms /   128 tokens (    1.09 ms per token,   921.51 tokens per second)
0.00.919.705 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.919.705 I llama_perf_context_print:       total time =     157.74 ms /   129 tokens
0.00.920.222 I ggml_metal_free: deallocating

real	0m0.961s
user	0m0.124s
sys	0m0.120s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.069 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.099 I main: llama backend init
0.00.000.101 I main: load the model and apply lora adapter, if any
0.00.015.673 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.304 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.038.310 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.318 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.319 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.320 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.320 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.321 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.322 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.323 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.323 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.324 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.326 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.327 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.327 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.329 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.330 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.330 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.828 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.426 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.746 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.759 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.759 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.760 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.761 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.761 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.055.762 I llama_model_loader: - type  f32:  194 tensors
0.00.055.763 I llama_model_loader: - type q4_1:   97 tensors
0.00.055.764 I llama_model_loader: - type q6_K:    1 tensors
0.00.055.765 I print_info: file format = GGUF V3 (latest)
0.00.055.794 I print_info: file type   = Q4_1
0.00.055.795 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.093.919 I load: special tokens cache size = 25
0.00.102.274 I load: token to piece cache size = 0.2984 MB
0.00.102.290 I print_info: arch             = gptneox
0.00.102.291 I print_info: vocab_only       = 0
0.00.102.292 I print_info: n_ctx_train      = 2048
0.00.102.292 I print_info: n_embd           = 2048
0.00.102.292 I print_info: n_layer          = 24
0.00.102.296 I print_info: n_head           = 16
0.00.102.296 I print_info: n_head_kv        = 16
0.00.102.297 I print_info: n_rot            = 32
0.00.102.297 I print_info: n_swa            = 0
0.00.102.297 I print_info: n_embd_head_k    = 128
0.00.102.297 I print_info: n_embd_head_v    = 128
0.00.102.298 I print_info: n_gqa            = 1
0.00.102.299 I print_info: n_embd_k_gqa     = 2048
0.00.102.300 I print_info: n_embd_v_gqa     = 2048
0.00.102.300 I print_info: f_norm_eps       = 1.0e-05
0.00.102.301 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.102.301 I print_info: f_clamp_kqv      = 0.0e+00
0.00.102.301 I print_info: f_max_alibi_bias = 0.0e+00
0.00.102.301 I print_info: f_logit_scale    = 0.0e+00
0.00.102.303 I print_info: n_ff             = 8192
0.00.102.303 I print_info: n_expert         = 0
0.00.102.303 I print_info: n_expert_used    = 0
0.00.102.303 I print_info: causal attn      = 1
0.00.102.303 I print_info: pooling type     = 0
0.00.102.304 I print_info: rope type        = 2
0.00.102.304 I print_info: rope scaling     = linear
0.00.102.306 I print_info: freq_base_train  = 10000.0
0.00.102.306 I print_info: freq_scale_train = 1
0.00.102.307 I print_info: n_ctx_orig_yarn  = 2048
0.00.102.307 I print_info: rope_finetuned   = unknown
0.00.102.307 I print_info: ssm_d_conv       = 0
0.00.102.307 I print_info: ssm_d_inner      = 0
0.00.102.307 I print_info: ssm_d_state      = 0
0.00.102.307 I print_info: ssm_dt_rank      = 0
0.00.102.308 I print_info: ssm_dt_b_c_rms   = 0
0.00.102.308 I print_info: model type       = 1.4B
0.00.102.308 I print_info: model params     = 1.41 B
0.00.102.308 I print_info: general.name     = 1.4B
0.00.102.309 I print_info: vocab type       = BPE
0.00.102.309 I print_info: n_vocab          = 50304
0.00.102.310 I print_info: n_merges         = 50009
0.00.102.310 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.102.315 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.102.315 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.102.315 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.102.315 I print_info: LF token         = 128 ''
0.00.102.316 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.102.316 I print_info: max token length = 1024
0.00.104.833 I load_tensors: offloading 24 repeating layers to GPU
0.00.104.833 I load_tensors: offloading output layer to GPU
0.00.104.833 I load_tensors: offloaded 25/25 layers to GPU
0.00.104.844 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.104.845 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.105.265 I llama_new_context_with_model: n_seq_max     = 1
0.00.105.266 I llama_new_context_with_model: n_ctx         = 2048
0.00.105.266 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.105.267 I llama_new_context_with_model: n_batch       = 2048
0.00.105.267 I llama_new_context_with_model: n_ubatch      = 512
0.00.105.267 I llama_new_context_with_model: flash_attn    = 0
0.00.105.268 I llama_new_context_with_model: freq_base     = 10000.0
0.00.105.268 I llama_new_context_with_model: freq_scale    = 1
0.00.105.269 I ggml_metal_init: allocating
0.00.105.272 I ggml_metal_init: found device: Apple M4
0.00.105.274 I ggml_metal_init: picking default device: Apple M4
0.00.106.049 I ggml_metal_init: using embedded metal library
0.00.109.244 I ggml_metal_init: GPU name:   Apple M4
0.00.109.245 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.109.246 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.109.246 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.109.247 I ggml_metal_init: simdgroup reduction   = true
0.00.109.247 I ggml_metal_init: simdgroup matrix mul. = true
0.00.109.247 I ggml_metal_init: has bfloat            = true
0.00.109.247 I ggml_metal_init: use bfloat            = true
0.00.109.248 I ggml_metal_init: hasUnifiedMemory      = true
0.00.109.248 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.119.449 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.141.265 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.141.271 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.141.294 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.142.285 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.142.286 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.142.287 I llama_new_context_with_model: graph nodes  = 967
0.00.142.287 I llama_new_context_with_model: graph splits = 2
0.00.142.290 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.142.419 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.142.419 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.834.709 I main: llama threadpool init, n_threads = 4
0.00.834.806 I 
0.00.834.873 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.834.878 I 
0.00.835.375 I sampler seed: 1234
0.00.835.388 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.835.435 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.835.438 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.835.438 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.576.608 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62445.03 tokens per second)
0.01.576.609 I llama_perf_context_print:        load time =     819.02 ms
0.01.576.610 I llama_perf_context_print: prompt eval time =      50.52 ms /     7 tokens (    7.22 ms per token,   138.56 tokens per second)
0.01.576.611 I llama_perf_context_print:        eval time =     687.88 ms /    63 runs   (   10.92 ms per token,    91.59 tokens per second)
0.01.576.616 I llama_perf_context_print:       total time =     741.91 ms /    70 tokens
0.01.576.859 I ggml_metal_free: deallocating

real	0m1.632s
user	0m0.156s
sys	0m0.185s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.749 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.910 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.020.914 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.916 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.916 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.917 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.917 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.917 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.918 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.920 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.920 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.920 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.921 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.922 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.922 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.924 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.924 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.924 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.828 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.886 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.771 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.772 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.772 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.773 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.773 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.773 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.029.774 I llama_model_loader: - type  f32:  194 tensors
0.00.029.774 I llama_model_loader: - type q4_1:   97 tensors
0.00.029.774 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.775 I print_info: file format = GGUF V3 (latest)
0.00.029.786 I print_info: file type   = Q4_1
0.00.029.788 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.048.747 I load: special tokens cache size = 25
0.00.054.648 I load: token to piece cache size = 0.2984 MB
0.00.054.663 I print_info: arch             = gptneox
0.00.054.664 I print_info: vocab_only       = 0
0.00.054.664 I print_info: n_ctx_train      = 2048
0.00.054.664 I print_info: n_embd           = 2048
0.00.054.664 I print_info: n_layer          = 24
0.00.054.667 I print_info: n_head           = 16
0.00.054.668 I print_info: n_head_kv        = 16
0.00.054.668 I print_info: n_rot            = 32
0.00.054.669 I print_info: n_swa            = 0
0.00.054.670 I print_info: n_embd_head_k    = 128
0.00.054.670 I print_info: n_embd_head_v    = 128
0.00.054.670 I print_info: n_gqa            = 1
0.00.054.671 I print_info: n_embd_k_gqa     = 2048
0.00.054.674 I print_info: n_embd_v_gqa     = 2048
0.00.054.674 I print_info: f_norm_eps       = 1.0e-05
0.00.054.675 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.675 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.675 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.675 I print_info: f_logit_scale    = 0.0e+00
0.00.054.679 I print_info: n_ff             = 8192
0.00.054.679 I print_info: n_expert         = 0
0.00.054.679 I print_info: n_expert_used    = 0
0.00.054.679 I print_info: causal attn      = 1
0.00.054.679 I print_info: pooling type     = 0
0.00.054.681 I print_info: rope type        = 2
0.00.054.681 I print_info: rope scaling     = linear
0.00.054.681 I print_info: freq_base_train  = 10000.0
0.00.054.681 I print_info: freq_scale_train = 1
0.00.054.682 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.682 I print_info: rope_finetuned   = unknown
0.00.054.682 I print_info: ssm_d_conv       = 0
0.00.054.682 I print_info: ssm_d_inner      = 0
0.00.054.682 I print_info: ssm_d_state      = 0
0.00.054.682 I print_info: ssm_dt_rank      = 0
0.00.054.682 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.683 I print_info: model type       = 1.4B
0.00.054.686 I print_info: model params     = 1.41 B
0.00.054.687 I print_info: general.name     = 1.4B
0.00.054.687 I print_info: vocab type       = BPE
0.00.054.688 I print_info: n_vocab          = 50304
0.00.054.688 I print_info: n_merges         = 50009
0.00.054.688 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.691 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.691 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.691 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.691 I print_info: LF token         = 128 ''
0.00.054.693 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.693 I print_info: max token length = 1024
0.00.056.579 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.580 I load_tensors: offloading output layer to GPU
0.00.056.580 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.590 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.056.592 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.056.885 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.885 I llama_new_context_with_model: n_ctx         = 128
0.00.056.886 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.886 I llama_new_context_with_model: n_batch       = 128
0.00.056.886 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.886 I llama_new_context_with_model: flash_attn    = 0
0.00.056.886 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.887 I llama_new_context_with_model: freq_scale    = 1
0.00.056.887 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.887 I ggml_metal_init: allocating
0.00.056.890 I ggml_metal_init: found device: Apple M4
0.00.056.892 I ggml_metal_init: picking default device: Apple M4
0.00.057.451 I ggml_metal_init: using embedded metal library
0.00.059.811 I ggml_metal_init: GPU name:   Apple M4
0.00.059.813 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.813 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.813 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.813 I ggml_metal_init: simdgroup reduction   = true
0.00.059.814 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.814 I ggml_metal_init: has bfloat            = true
0.00.059.814 I ggml_metal_init: use bfloat            = true
0.00.059.814 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.815 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.564 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.070.947 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.951 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.966 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.071.865 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.071.866 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.071.866 I llama_new_context_with_model: graph nodes  = 967
0.00.071.866 I llama_new_context_with_model: graph splits = 2
0.00.071.867 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.868 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.690 I 
0.00.667.735 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.761 I perplexity: tokenizing the input ..
0.00.675.609 I perplexity: tokenization took 7.847 ms
0.00.675.613 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.668 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.798.874 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.798.900 I llama_perf_context_print:        load time =     658.93 ms
0.00.798.901 I llama_perf_context_print: prompt eval time =     121.83 ms /   128 tokens (    0.95 ms per token,  1050.65 tokens per second)
0.00.798.902 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.798.903 I llama_perf_context_print:       total time =     131.21 ms /   129 tokens
0.00.799.425 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.077s
sys	0m0.093s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.532 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.651 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.656 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.658 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.659 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.659 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.659 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.660 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.661 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.661 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.661 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.662 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.662 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.662 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.665 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.666 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.666 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.667 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.690 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.710 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.724 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.725 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.725 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.726 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.726 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.726 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.727 I llama_model_loader: - type  f32:  194 tensors
0.00.027.727 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.727 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.728 I print_info: file format = GGUF V3 (latest)
0.00.027.733 I print_info: file type   = Q5_0
0.00.027.734 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.047.466 I load: special tokens cache size = 25
0.00.053.396 I load: token to piece cache size = 0.2984 MB
0.00.053.411 I print_info: arch             = gptneox
0.00.053.412 I print_info: vocab_only       = 0
0.00.053.412 I print_info: n_ctx_train      = 2048
0.00.053.413 I print_info: n_embd           = 2048
0.00.053.413 I print_info: n_layer          = 24
0.00.053.416 I print_info: n_head           = 16
0.00.053.417 I print_info: n_head_kv        = 16
0.00.053.417 I print_info: n_rot            = 32
0.00.053.418 I print_info: n_swa            = 0
0.00.053.418 I print_info: n_embd_head_k    = 128
0.00.053.418 I print_info: n_embd_head_v    = 128
0.00.053.419 I print_info: n_gqa            = 1
0.00.053.419 I print_info: n_embd_k_gqa     = 2048
0.00.053.420 I print_info: n_embd_v_gqa     = 2048
0.00.053.421 I print_info: f_norm_eps       = 1.0e-05
0.00.053.421 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.421 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.421 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.421 I print_info: f_logit_scale    = 0.0e+00
0.00.053.422 I print_info: n_ff             = 8192
0.00.053.422 I print_info: n_expert         = 0
0.00.053.422 I print_info: n_expert_used    = 0
0.00.053.422 I print_info: causal attn      = 1
0.00.053.422 I print_info: pooling type     = 0
0.00.053.423 I print_info: rope type        = 2
0.00.053.425 I print_info: rope scaling     = linear
0.00.053.425 I print_info: freq_base_train  = 10000.0
0.00.053.425 I print_info: freq_scale_train = 1
0.00.053.425 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.426 I print_info: rope_finetuned   = unknown
0.00.053.426 I print_info: ssm_d_conv       = 0
0.00.053.426 I print_info: ssm_d_inner      = 0
0.00.053.426 I print_info: ssm_d_state      = 0
0.00.053.426 I print_info: ssm_dt_rank      = 0
0.00.053.426 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.427 I print_info: model type       = 1.4B
0.00.053.427 I print_info: model params     = 1.41 B
0.00.053.427 I print_info: general.name     = 1.4B
0.00.053.428 I print_info: vocab type       = BPE
0.00.053.428 I print_info: n_vocab          = 50304
0.00.053.428 I print_info: n_merges         = 50009
0.00.053.428 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.428 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.428 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.429 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.429 I print_info: LF token         = 128 ''
0.00.053.429 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.429 I print_info: max token length = 1024
0.00.055.473 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.474 I load_tensors: offloading output layer to GPU
0.00.055.474 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.484 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.055.486 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.055.772 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.773 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.773 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.773 I llama_new_context_with_model: n_batch       = 2048
0.00.055.773 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.774 I llama_new_context_with_model: flash_attn    = 0
0.00.055.774 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.774 I llama_new_context_with_model: freq_scale    = 1
0.00.055.775 I ggml_metal_init: allocating
0.00.055.778 I ggml_metal_init: found device: Apple M4
0.00.055.780 I ggml_metal_init: picking default device: Apple M4
0.00.056.374 I ggml_metal_init: using embedded metal library
0.00.058.821 I ggml_metal_init: GPU name:   Apple M4
0.00.058.823 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.823 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.824 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.824 I ggml_metal_init: simdgroup reduction   = true
0.00.058.824 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.824 I ggml_metal_init: has bfloat            = true
0.00.058.824 I ggml_metal_init: use bfloat            = true
0.00.058.825 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.825 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.901 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.994 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.003 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.035 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.122 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.124 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.124 I llama_new_context_with_model: graph nodes  = 967
0.00.089.124 I llama_new_context_with_model: graph splits = 2
0.00.089.127 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.257 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.258 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.802.899 I main: llama threadpool init, n_threads = 4
0.00.802.943 I 
0.00.802.968 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.802.970 I 
0.00.803.203 I sampler seed: 1234
0.00.803.208 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.803.223 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.803.223 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.803.225 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.596.292 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55686.27 tokens per second)
0.01.596.293 I llama_perf_context_print:        load time =     793.36 ms
0.01.596.294 I llama_perf_context_print: prompt eval time =      43.24 ms /     7 tokens (    6.18 ms per token,   161.89 tokens per second)
0.01.596.295 I llama_perf_context_print:        eval time =     746.72 ms /    63 runs   (   11.85 ms per token,    84.37 tokens per second)
0.01.596.295 I llama_perf_context_print:       total time =     793.40 ms /    70 tokens
0.01.596.498 I ggml_metal_free: deallocating

real	0m1.613s
user	0m0.111s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.836 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.653 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.031.659 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.661 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.661 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.661 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.662 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.662 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.663 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.663 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.664 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.664 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.664 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.665 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.665 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.666 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.667 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.667 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.773 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.104 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.242 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.043.243 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.243 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.244 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.244 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.245 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.043.245 I llama_model_loader: - type  f32:  194 tensors
0.00.043.245 I llama_model_loader: - type q5_0:   97 tensors
0.00.043.246 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.246 I print_info: file format = GGUF V3 (latest)
0.00.043.258 I print_info: file type   = Q5_0
0.00.043.259 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.072.197 I load: special tokens cache size = 25
0.00.081.842 I load: token to piece cache size = 0.2984 MB
0.00.081.854 I print_info: arch             = gptneox
0.00.081.855 I print_info: vocab_only       = 0
0.00.081.855 I print_info: n_ctx_train      = 2048
0.00.081.855 I print_info: n_embd           = 2048
0.00.081.856 I print_info: n_layer          = 24
0.00.081.859 I print_info: n_head           = 16
0.00.081.860 I print_info: n_head_kv        = 16
0.00.081.860 I print_info: n_rot            = 32
0.00.081.860 I print_info: n_swa            = 0
0.00.081.860 I print_info: n_embd_head_k    = 128
0.00.081.866 I print_info: n_embd_head_v    = 128
0.00.081.867 I print_info: n_gqa            = 1
0.00.081.868 I print_info: n_embd_k_gqa     = 2048
0.00.081.869 I print_info: n_embd_v_gqa     = 2048
0.00.081.870 I print_info: f_norm_eps       = 1.0e-05
0.00.081.870 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.081.871 I print_info: f_clamp_kqv      = 0.0e+00
0.00.081.871 I print_info: f_max_alibi_bias = 0.0e+00
0.00.081.871 I print_info: f_logit_scale    = 0.0e+00
0.00.081.872 I print_info: n_ff             = 8192
0.00.081.874 I print_info: n_expert         = 0
0.00.081.874 I print_info: n_expert_used    = 0
0.00.081.874 I print_info: causal attn      = 1
0.00.081.875 I print_info: pooling type     = 0
0.00.081.875 I print_info: rope type        = 2
0.00.081.875 I print_info: rope scaling     = linear
0.00.081.876 I print_info: freq_base_train  = 10000.0
0.00.081.876 I print_info: freq_scale_train = 1
0.00.081.876 I print_info: n_ctx_orig_yarn  = 2048
0.00.081.876 I print_info: rope_finetuned   = unknown
0.00.081.876 I print_info: ssm_d_conv       = 0
0.00.081.877 I print_info: ssm_d_inner      = 0
0.00.081.877 I print_info: ssm_d_state      = 0
0.00.081.879 I print_info: ssm_dt_rank      = 0
0.00.081.879 I print_info: ssm_dt_b_c_rms   = 0
0.00.081.879 I print_info: model type       = 1.4B
0.00.081.880 I print_info: model params     = 1.41 B
0.00.081.880 I print_info: general.name     = 1.4B
0.00.081.880 I print_info: vocab type       = BPE
0.00.081.885 I print_info: n_vocab          = 50304
0.00.081.885 I print_info: n_merges         = 50009
0.00.081.885 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.081.886 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.081.886 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.081.886 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.081.886 I print_info: LF token         = 128 ''
0.00.081.887 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.081.887 I print_info: max token length = 1024
0.00.084.372 I load_tensors: offloading 24 repeating layers to GPU
0.00.084.372 I load_tensors: offloading output layer to GPU
0.00.084.373 I load_tensors: offloaded 25/25 layers to GPU
0.00.084.379 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.084.380 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.084.988 I llama_new_context_with_model: n_seq_max     = 1
0.00.084.989 I llama_new_context_with_model: n_ctx         = 128
0.00.084.990 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.084.990 I llama_new_context_with_model: n_batch       = 128
0.00.084.990 I llama_new_context_with_model: n_ubatch      = 128
0.00.084.990 I llama_new_context_with_model: flash_attn    = 0
0.00.084.991 I llama_new_context_with_model: freq_base     = 10000.0
0.00.084.991 I llama_new_context_with_model: freq_scale    = 1
0.00.084.992 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.084.992 I ggml_metal_init: allocating
0.00.084.997 I ggml_metal_init: found device: Apple M4
0.00.084.999 I ggml_metal_init: picking default device: Apple M4
0.00.085.786 I ggml_metal_init: using embedded metal library
0.00.089.275 I ggml_metal_init: GPU name:   Apple M4
0.00.089.278 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.089.278 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.089.279 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.089.279 I ggml_metal_init: simdgroup reduction   = true
0.00.089.279 I ggml_metal_init: simdgroup matrix mul. = true
0.00.089.279 I ggml_metal_init: has bfloat            = true
0.00.089.279 I ggml_metal_init: use bfloat            = true
0.00.089.280 I ggml_metal_init: hasUnifiedMemory      = true
0.00.089.281 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.683 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.102.266 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.102.268 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.282 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.380 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.103.382 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.103.382 I llama_new_context_with_model: graph nodes  = 967
0.00.103.382 I llama_new_context_with_model: graph splits = 2
0.00.103.384 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.103.384 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.493 I 
0.00.752.550 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.578 I perplexity: tokenizing the input ..
0.00.766.413 I perplexity: tokenization took 13.833 ms
0.00.766.420 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.903.729 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.904.876 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.904.905 I llama_perf_context_print:        load time =     735.64 ms
0.00.904.906 I llama_perf_context_print: prompt eval time =     136.37 ms /   128 tokens (    1.07 ms per token,   938.61 tokens per second)
0.00.904.906 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.904.907 I llama_perf_context_print:       total time =     152.43 ms /   129 tokens
0.00.905.380 I ggml_metal_free: deallocating

real	0m0.933s
user	0m0.106s
sys	0m0.112s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.010.961 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.200 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.021.205 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.206 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.211 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.212 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.212 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.212 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.213 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.213 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.214 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.216 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.216 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.217 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.217 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.219 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.220 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.220 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.350 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.476 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.546 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.547 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.547 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.548 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.548 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.548 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.030.549 I llama_model_loader: - type  f32:  194 tensors
0.00.030.549 I llama_model_loader: - type q5_1:   97 tensors
0.00.030.549 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.550 I print_info: file format = GGUF V3 (latest)
0.00.030.562 I print_info: file type   = Q5_1
0.00.030.563 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.050.324 I load: special tokens cache size = 25
0.00.056.288 I load: token to piece cache size = 0.2984 MB
0.00.056.303 I print_info: arch             = gptneox
0.00.056.304 I print_info: vocab_only       = 0
0.00.056.304 I print_info: n_ctx_train      = 2048
0.00.056.304 I print_info: n_embd           = 2048
0.00.056.305 I print_info: n_layer          = 24
0.00.056.307 I print_info: n_head           = 16
0.00.056.308 I print_info: n_head_kv        = 16
0.00.056.308 I print_info: n_rot            = 32
0.00.056.309 I print_info: n_swa            = 0
0.00.056.309 I print_info: n_embd_head_k    = 128
0.00.056.311 I print_info: n_embd_head_v    = 128
0.00.056.311 I print_info: n_gqa            = 1
0.00.056.312 I print_info: n_embd_k_gqa     = 2048
0.00.056.313 I print_info: n_embd_v_gqa     = 2048
0.00.056.314 I print_info: f_norm_eps       = 1.0e-05
0.00.056.314 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.314 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.314 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.314 I print_info: f_logit_scale    = 0.0e+00
0.00.056.315 I print_info: n_ff             = 8192
0.00.056.315 I print_info: n_expert         = 0
0.00.056.315 I print_info: n_expert_used    = 0
0.00.056.315 I print_info: causal attn      = 1
0.00.056.315 I print_info: pooling type     = 0
0.00.056.315 I print_info: rope type        = 2
0.00.056.316 I print_info: rope scaling     = linear
0.00.056.316 I print_info: freq_base_train  = 10000.0
0.00.056.316 I print_info: freq_scale_train = 1
0.00.056.316 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.317 I print_info: rope_finetuned   = unknown
0.00.056.317 I print_info: ssm_d_conv       = 0
0.00.056.317 I print_info: ssm_d_inner      = 0
0.00.056.317 I print_info: ssm_d_state      = 0
0.00.056.317 I print_info: ssm_dt_rank      = 0
0.00.056.317 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.317 I print_info: model type       = 1.4B
0.00.056.318 I print_info: model params     = 1.41 B
0.00.056.318 I print_info: general.name     = 1.4B
0.00.056.318 I print_info: vocab type       = BPE
0.00.056.318 I print_info: n_vocab          = 50304
0.00.056.319 I print_info: n_merges         = 50009
0.00.056.319 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.319 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.319 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.319 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.319 I print_info: LF token         = 128 ''
0.00.056.320 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.320 I print_info: max token length = 1024
0.00.058.350 I load_tensors: offloading 24 repeating layers to GPU
0.00.058.351 I load_tensors: offloading output layer to GPU
0.00.058.351 I load_tensors: offloaded 25/25 layers to GPU
0.00.058.362 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.058.363 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.058.634 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.635 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.635 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.635 I llama_new_context_with_model: n_batch       = 2048
0.00.058.635 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.635 I llama_new_context_with_model: flash_attn    = 0
0.00.058.636 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.636 I llama_new_context_with_model: freq_scale    = 1
0.00.058.637 I ggml_metal_init: allocating
0.00.058.640 I ggml_metal_init: found device: Apple M4
0.00.058.642 I ggml_metal_init: picking default device: Apple M4
0.00.059.256 I ggml_metal_init: using embedded metal library
0.00.061.698 I ggml_metal_init: GPU name:   Apple M4
0.00.061.699 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.700 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.700 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.701 I ggml_metal_init: simdgroup reduction   = true
0.00.061.701 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.701 I ggml_metal_init: has bfloat            = true
0.00.061.701 I ggml_metal_init: use bfloat            = true
0.00.061.702 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.702 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.751 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.092.565 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.571 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.589 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.799 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.093.801 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.093.801 I llama_new_context_with_model: graph nodes  = 967
0.00.093.802 I llama_new_context_with_model: graph splits = 2
0.00.093.804 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.093.958 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.093.959 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.825.683 I main: llama threadpool init, n_threads = 4
0.00.825.720 I 
0.00.825.740 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.825.741 I 
0.00.825.892 I sampler seed: 1234
0.00.825.897 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.825.942 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.825.945 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.825.946 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.673.695 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56037.88 tokens per second)
0.01.673.696 I llama_perf_context_print:        load time =     814.72 ms
0.01.673.696 I llama_perf_context_print: prompt eval time =      42.22 ms /     7 tokens (    6.03 ms per token,   165.80 tokens per second)
0.01.673.697 I llama_perf_context_print:        eval time =     802.50 ms /    63 runs   (   12.74 ms per token,    78.50 tokens per second)
0.01.673.697 I llama_perf_context_print:       total time =     848.02 ms /    70 tokens
0.01.673.974 I ggml_metal_free: deallocating

real	0m1.699s
user	0m0.112s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.733 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.558 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.563 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.565 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.565 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.566 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.566 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.566 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.567 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.567 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.568 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.568 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.571 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.571 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.572 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.574 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.575 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.575 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.713 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.835 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.932 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.933 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.933 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.933 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.934 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.934 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.935 I llama_model_loader: - type  f32:  194 tensors
0.00.027.935 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.935 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.936 I print_info: file format = GGUF V3 (latest)
0.00.027.943 I print_info: file type   = Q5_1
0.00.027.945 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.046.928 I load: special tokens cache size = 25
0.00.052.916 I load: token to piece cache size = 0.2984 MB
0.00.052.931 I print_info: arch             = gptneox
0.00.052.932 I print_info: vocab_only       = 0
0.00.052.932 I print_info: n_ctx_train      = 2048
0.00.052.932 I print_info: n_embd           = 2048
0.00.052.932 I print_info: n_layer          = 24
0.00.052.935 I print_info: n_head           = 16
0.00.052.936 I print_info: n_head_kv        = 16
0.00.052.936 I print_info: n_rot            = 32
0.00.052.936 I print_info: n_swa            = 0
0.00.052.937 I print_info: n_embd_head_k    = 128
0.00.052.938 I print_info: n_embd_head_v    = 128
0.00.052.939 I print_info: n_gqa            = 1
0.00.052.940 I print_info: n_embd_k_gqa     = 2048
0.00.052.941 I print_info: n_embd_v_gqa     = 2048
0.00.052.941 I print_info: f_norm_eps       = 1.0e-05
0.00.052.942 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.942 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.942 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.942 I print_info: f_logit_scale    = 0.0e+00
0.00.052.943 I print_info: n_ff             = 8192
0.00.052.943 I print_info: n_expert         = 0
0.00.052.943 I print_info: n_expert_used    = 0
0.00.052.943 I print_info: causal attn      = 1
0.00.052.943 I print_info: pooling type     = 0
0.00.052.943 I print_info: rope type        = 2
0.00.052.947 I print_info: rope scaling     = linear
0.00.052.947 I print_info: freq_base_train  = 10000.0
0.00.052.948 I print_info: freq_scale_train = 1
0.00.052.948 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.948 I print_info: rope_finetuned   = unknown
0.00.052.948 I print_info: ssm_d_conv       = 0
0.00.052.948 I print_info: ssm_d_inner      = 0
0.00.052.948 I print_info: ssm_d_state      = 0
0.00.052.948 I print_info: ssm_dt_rank      = 0
0.00.052.948 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.949 I print_info: model type       = 1.4B
0.00.052.949 I print_info: model params     = 1.41 B
0.00.052.949 I print_info: general.name     = 1.4B
0.00.052.950 I print_info: vocab type       = BPE
0.00.052.951 I print_info: n_vocab          = 50304
0.00.052.951 I print_info: n_merges         = 50009
0.00.052.951 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.951 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.951 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.951 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.951 I print_info: LF token         = 128 ''
0.00.052.952 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.952 I print_info: max token length = 1024
0.00.054.933 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.933 I load_tensors: offloading output layer to GPU
0.00.054.933 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.944 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.945 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.055.218 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.218 I llama_new_context_with_model: n_ctx         = 128
0.00.055.218 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.219 I llama_new_context_with_model: n_batch       = 128
0.00.055.219 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.219 I llama_new_context_with_model: flash_attn    = 0
0.00.055.219 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.219 I llama_new_context_with_model: freq_scale    = 1
0.00.055.220 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.220 I ggml_metal_init: allocating
0.00.055.223 I ggml_metal_init: found device: Apple M4
0.00.055.225 I ggml_metal_init: picking default device: Apple M4
0.00.055.786 I ggml_metal_init: using embedded metal library
0.00.058.171 I ggml_metal_init: GPU name:   Apple M4
0.00.058.172 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.172 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.173 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.173 I ggml_metal_init: simdgroup reduction   = true
0.00.058.173 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.173 I ggml_metal_init: has bfloat            = true
0.00.058.174 I ggml_metal_init: use bfloat            = true
0.00.058.174 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.174 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.935 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.161 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.163 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.176 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.109 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.110 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.110 I llama_new_context_with_model: graph nodes  = 967
0.00.070.110 I llama_new_context_with_model: graph splits = 2
0.00.070.111 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.112 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.757.825 I 
0.00.757.878 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.757.904 I perplexity: tokenizing the input ..
0.00.765.587 I perplexity: tokenization took 7.682 ms
0.00.765.591 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.900.655 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.901.892 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.901.930 I llama_perf_context_print:        load time =     749.08 ms
0.00.901.931 I llama_perf_context_print: prompt eval time =     134.83 ms /   128 tokens (    1.05 ms per token,   949.36 tokens per second)
0.00.901.932 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.901.932 I llama_perf_context_print:       total time =     144.11 ms /   129 tokens
0.00.902.513 I ggml_metal_free: deallocating

real	0m0.919s
user	0m0.079s
sys	0m0.108s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.015.587 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.893 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.024.899 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.900 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.901 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.901 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.902 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.902 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.903 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.903 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.904 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.904 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.904 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.905 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.905 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.906 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.907 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.907 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.902 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.361 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.518 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.520 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.520 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.521 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.521 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.522 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.036.522 I llama_model_loader: - type  f32:  194 tensors
0.00.036.522 I llama_model_loader: - type q2_K:   49 tensors
0.00.036.523 I llama_model_loader: - type q3_K:   48 tensors
0.00.036.523 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.523 I print_info: file format = GGUF V3 (latest)
0.00.036.535 I print_info: file type   = Q2_K - Medium
0.00.036.536 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.065.993 I load: special tokens cache size = 25
0.00.076.541 I load: token to piece cache size = 0.2984 MB
0.00.076.558 I print_info: arch             = gptneox
0.00.076.560 I print_info: vocab_only       = 0
0.00.076.560 I print_info: n_ctx_train      = 2048
0.00.076.560 I print_info: n_embd           = 2048
0.00.076.561 I print_info: n_layer          = 24
0.00.076.565 I print_info: n_head           = 16
0.00.076.566 I print_info: n_head_kv        = 16
0.00.076.566 I print_info: n_rot            = 32
0.00.076.566 I print_info: n_swa            = 0
0.00.076.567 I print_info: n_embd_head_k    = 128
0.00.076.567 I print_info: n_embd_head_v    = 128
0.00.076.568 I print_info: n_gqa            = 1
0.00.076.569 I print_info: n_embd_k_gqa     = 2048
0.00.076.570 I print_info: n_embd_v_gqa     = 2048
0.00.076.571 I print_info: f_norm_eps       = 1.0e-05
0.00.076.573 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.573 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.573 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.574 I print_info: f_logit_scale    = 0.0e+00
0.00.076.574 I print_info: n_ff             = 8192
0.00.076.575 I print_info: n_expert         = 0
0.00.076.575 I print_info: n_expert_used    = 0
0.00.076.575 I print_info: causal attn      = 1
0.00.076.576 I print_info: pooling type     = 0
0.00.076.576 I print_info: rope type        = 2
0.00.076.576 I print_info: rope scaling     = linear
0.00.076.577 I print_info: freq_base_train  = 10000.0
0.00.076.577 I print_info: freq_scale_train = 1
0.00.076.577 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.578 I print_info: rope_finetuned   = unknown
0.00.076.578 I print_info: ssm_d_conv       = 0
0.00.076.578 I print_info: ssm_d_inner      = 0
0.00.076.578 I print_info: ssm_d_state      = 0
0.00.076.579 I print_info: ssm_dt_rank      = 0
0.00.076.579 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.579 I print_info: model type       = 1.4B
0.00.076.580 I print_info: model params     = 1.41 B
0.00.076.580 I print_info: general.name     = 1.4B
0.00.076.581 I print_info: vocab type       = BPE
0.00.076.581 I print_info: n_vocab          = 50304
0.00.076.582 I print_info: n_merges         = 50009
0.00.076.582 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.582 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.585 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.586 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.586 I print_info: LF token         = 128 ''
0.00.076.586 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.587 I print_info: max token length = 1024
0.00.079.237 I load_tensors: offloading 24 repeating layers to GPU
0.00.079.238 I load_tensors: offloading output layer to GPU
0.00.079.238 I load_tensors: offloaded 25/25 layers to GPU
0.00.079.249 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.079.251 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.079.640 I llama_new_context_with_model: n_seq_max     = 1
0.00.079.641 I llama_new_context_with_model: n_ctx         = 2048
0.00.079.641 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.079.641 I llama_new_context_with_model: n_batch       = 2048
0.00.079.642 I llama_new_context_with_model: n_ubatch      = 512
0.00.079.642 I llama_new_context_with_model: flash_attn    = 0
0.00.079.643 I llama_new_context_with_model: freq_base     = 10000.0
0.00.079.643 I llama_new_context_with_model: freq_scale    = 1
0.00.079.644 I ggml_metal_init: allocating
0.00.079.648 I ggml_metal_init: found device: Apple M4
0.00.079.650 I ggml_metal_init: picking default device: Apple M4
0.00.080.461 I ggml_metal_init: using embedded metal library
0.00.084.124 I ggml_metal_init: GPU name:   Apple M4
0.00.084.126 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.084.127 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.084.127 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.084.128 I ggml_metal_init: simdgroup reduction   = true
0.00.084.128 I ggml_metal_init: simdgroup matrix mul. = true
0.00.084.128 I ggml_metal_init: has bfloat            = true
0.00.084.128 I ggml_metal_init: use bfloat            = true
0.00.084.129 I ggml_metal_init: hasUnifiedMemory      = true
0.00.084.129 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.095.532 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.120.185 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.120.199 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.120.221 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.121.180 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.121.182 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.121.183 I llama_new_context_with_model: graph nodes  = 967
0.00.121.183 I llama_new_context_with_model: graph splits = 2
0.00.121.187 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.121.309 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.121.309 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.521.862 I main: llama threadpool init, n_threads = 4
0.00.521.932 I 
0.00.521.963 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.521.963 I 
0.00.522.362 I sampler seed: 1234
0.00.522.369 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.522.449 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.522.455 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.522.455 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.199.310 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56936.65 tokens per second)
0.01.199.311 I llama_perf_context_print:        load time =     506.27 ms
0.01.199.312 I llama_perf_context_print: prompt eval time =      36.12 ms /     7 tokens (    5.16 ms per token,   193.80 tokens per second)
0.01.199.312 I llama_perf_context_print:        eval time =     637.72 ms /    63 runs   (   10.12 ms per token,    98.79 tokens per second)
0.01.199.313 I llama_perf_context_print:       total time =     677.45 ms /    70 tokens
0.01.199.591 I ggml_metal_free: deallocating

real	0m1.236s
user	0m0.136s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.935 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.700 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.705 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.707 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.707 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.707 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.708 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.708 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.709 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.709 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.710 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.710 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.710 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.713 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.713 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.716 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.716 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.717 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.697 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.722 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.643 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.644 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.644 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.645 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.645 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.645 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.646 I llama_model_loader: - type  f32:  194 tensors
0.00.026.646 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.646 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.646 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.647 I print_info: file format = GGUF V3 (latest)
0.00.026.654 I print_info: file type   = Q2_K - Medium
0.00.026.655 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.046.202 I load: special tokens cache size = 25
0.00.052.181 I load: token to piece cache size = 0.2984 MB
0.00.052.195 I print_info: arch             = gptneox
0.00.052.197 I print_info: vocab_only       = 0
0.00.052.197 I print_info: n_ctx_train      = 2048
0.00.052.197 I print_info: n_embd           = 2048
0.00.052.197 I print_info: n_layer          = 24
0.00.052.200 I print_info: n_head           = 16
0.00.052.201 I print_info: n_head_kv        = 16
0.00.052.201 I print_info: n_rot            = 32
0.00.052.201 I print_info: n_swa            = 0
0.00.052.201 I print_info: n_embd_head_k    = 128
0.00.052.201 I print_info: n_embd_head_v    = 128
0.00.052.202 I print_info: n_gqa            = 1
0.00.052.203 I print_info: n_embd_k_gqa     = 2048
0.00.052.204 I print_info: n_embd_v_gqa     = 2048
0.00.052.204 I print_info: f_norm_eps       = 1.0e-05
0.00.052.205 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.205 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.205 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.205 I print_info: f_logit_scale    = 0.0e+00
0.00.052.206 I print_info: n_ff             = 8192
0.00.052.206 I print_info: n_expert         = 0
0.00.052.206 I print_info: n_expert_used    = 0
0.00.052.206 I print_info: causal attn      = 1
0.00.052.206 I print_info: pooling type     = 0
0.00.052.206 I print_info: rope type        = 2
0.00.052.207 I print_info: rope scaling     = linear
0.00.052.207 I print_info: freq_base_train  = 10000.0
0.00.052.207 I print_info: freq_scale_train = 1
0.00.052.207 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.207 I print_info: rope_finetuned   = unknown
0.00.052.208 I print_info: ssm_d_conv       = 0
0.00.052.208 I print_info: ssm_d_inner      = 0
0.00.052.208 I print_info: ssm_d_state      = 0
0.00.052.208 I print_info: ssm_dt_rank      = 0
0.00.052.209 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.209 I print_info: model type       = 1.4B
0.00.052.209 I print_info: model params     = 1.41 B
0.00.052.209 I print_info: general.name     = 1.4B
0.00.052.210 I print_info: vocab type       = BPE
0.00.052.211 I print_info: n_vocab          = 50304
0.00.052.211 I print_info: n_merges         = 50009
0.00.052.211 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.212 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.212 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.212 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.212 I print_info: LF token         = 128 ''
0.00.052.212 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.212 I print_info: max token length = 1024
0.00.054.102 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.103 I load_tensors: offloading output layer to GPU
0.00.054.103 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.114 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.115 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.054.392 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.393 I llama_new_context_with_model: n_ctx         = 128
0.00.054.393 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.393 I llama_new_context_with_model: n_batch       = 128
0.00.054.393 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.394 I llama_new_context_with_model: flash_attn    = 0
0.00.054.394 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.394 I llama_new_context_with_model: freq_scale    = 1
0.00.054.395 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.395 I ggml_metal_init: allocating
0.00.054.398 I ggml_metal_init: found device: Apple M4
0.00.054.400 I ggml_metal_init: picking default device: Apple M4
0.00.054.986 I ggml_metal_init: using embedded metal library
0.00.057.395 I ggml_metal_init: GPU name:   Apple M4
0.00.057.397 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.398 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.398 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.398 I ggml_metal_init: simdgroup reduction   = true
0.00.057.398 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.399 I ggml_metal_init: has bfloat            = true
0.00.057.399 I ggml_metal_init: use bfloat            = true
0.00.057.399 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.400 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.334 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.665 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.667 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.681 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.617 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.618 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.619 I llama_new_context_with_model: graph nodes  = 967
0.00.069.619 I llama_new_context_with_model: graph splits = 2
0.00.069.620 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.620 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.393.351 I 
0.00.393.383 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.393.397 I perplexity: tokenizing the input ..
0.00.401.151 I perplexity: tokenization took 7.752 ms
0.00.401.154 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.533.722 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.534.890 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.534.919 I llama_perf_context_print:        load time =     382.41 ms
0.00.534.920 I llama_perf_context_print: prompt eval time =     132.34 ms /   128 tokens (    1.03 ms per token,   967.19 tokens per second)
0.00.534.921 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.534.921 I llama_perf_context_print:       total time =     141.57 ms /   129 tokens
0.00.535.460 I ggml_metal_free: deallocating

real	0m0.551s
user	0m0.079s
sys	0m0.071s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.513 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.762 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.022.767 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.769 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.771 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.771 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.771 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.772 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.774 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.775 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.775 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.777 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.777 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.777 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.778 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.781 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.781 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.781 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.776 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.896 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.788 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.789 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.790 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.790 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.790 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.791 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.031.791 I llama_model_loader: - type  f32:  194 tensors
0.00.031.791 I llama_model_loader: - type q3_K:   25 tensors
0.00.031.792 I llama_model_loader: - type q4_K:   71 tensors
0.00.031.792 I llama_model_loader: - type q5_K:    1 tensors
0.00.031.792 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.793 I print_info: file format = GGUF V3 (latest)
0.00.031.804 I print_info: file type   = Q3_K - Medium
0.00.031.805 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.051.059 I load: special tokens cache size = 25
0.00.057.077 I load: token to piece cache size = 0.2984 MB
0.00.057.092 I print_info: arch             = gptneox
0.00.057.093 I print_info: vocab_only       = 0
0.00.057.093 I print_info: n_ctx_train      = 2048
0.00.057.093 I print_info: n_embd           = 2048
0.00.057.093 I print_info: n_layer          = 24
0.00.057.096 I print_info: n_head           = 16
0.00.057.097 I print_info: n_head_kv        = 16
0.00.057.097 I print_info: n_rot            = 32
0.00.057.097 I print_info: n_swa            = 0
0.00.057.098 I print_info: n_embd_head_k    = 128
0.00.057.098 I print_info: n_embd_head_v    = 128
0.00.057.099 I print_info: n_gqa            = 1
0.00.057.099 I print_info: n_embd_k_gqa     = 2048
0.00.057.101 I print_info: n_embd_v_gqa     = 2048
0.00.057.102 I print_info: f_norm_eps       = 1.0e-05
0.00.057.102 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.104 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.104 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.104 I print_info: f_logit_scale    = 0.0e+00
0.00.057.104 I print_info: n_ff             = 8192
0.00.057.106 I print_info: n_expert         = 0
0.00.057.106 I print_info: n_expert_used    = 0
0.00.057.106 I print_info: causal attn      = 1
0.00.057.106 I print_info: pooling type     = 0
0.00.057.106 I print_info: rope type        = 2
0.00.057.107 I print_info: rope scaling     = linear
0.00.057.107 I print_info: freq_base_train  = 10000.0
0.00.057.107 I print_info: freq_scale_train = 1
0.00.057.107 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.107 I print_info: rope_finetuned   = unknown
0.00.057.108 I print_info: ssm_d_conv       = 0
0.00.057.108 I print_info: ssm_d_inner      = 0
0.00.057.108 I print_info: ssm_d_state      = 0
0.00.057.108 I print_info: ssm_dt_rank      = 0
0.00.057.108 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.108 I print_info: model type       = 1.4B
0.00.057.108 I print_info: model params     = 1.41 B
0.00.057.108 I print_info: general.name     = 1.4B
0.00.057.109 I print_info: vocab type       = BPE
0.00.057.109 I print_info: n_vocab          = 50304
0.00.057.109 I print_info: n_merges         = 50009
0.00.057.109 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.110 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.110 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.110 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.110 I print_info: LF token         = 128 ''
0.00.057.111 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.111 I print_info: max token length = 1024
0.00.059.033 I load_tensors: offloading 24 repeating layers to GPU
0.00.059.033 I load_tensors: offloading output layer to GPU
0.00.059.033 I load_tensors: offloaded 25/25 layers to GPU
0.00.059.044 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.059.045 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.059.334 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.335 I llama_new_context_with_model: n_ctx         = 2048
0.00.059.335 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.059.335 I llama_new_context_with_model: n_batch       = 2048
0.00.059.335 I llama_new_context_with_model: n_ubatch      = 512
0.00.059.335 I llama_new_context_with_model: flash_attn    = 0
0.00.059.336 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.336 I llama_new_context_with_model: freq_scale    = 1
0.00.059.337 I ggml_metal_init: allocating
0.00.059.340 I ggml_metal_init: found device: Apple M4
0.00.059.342 I ggml_metal_init: picking default device: Apple M4
0.00.059.940 I ggml_metal_init: using embedded metal library
0.00.062.393 I ggml_metal_init: GPU name:   Apple M4
0.00.062.395 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.395 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.396 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.396 I ggml_metal_init: simdgroup reduction   = true
0.00.062.396 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.396 I ggml_metal_init: has bfloat            = true
0.00.062.396 I ggml_metal_init: use bfloat            = true
0.00.062.397 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.397 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.611 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.094.227 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.234 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.252 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.250 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.252 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.252 I llama_new_context_with_model: graph nodes  = 967
0.00.095.253 I llama_new_context_with_model: graph splits = 2
0.00.095.255 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.095.371 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.372 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.347 I main: llama threadpool init, n_threads = 4
0.00.629.385 I 
0.00.629.409 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.410 I 
0.00.629.635 I sampler seed: 1234
0.00.629.641 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.629.656 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.629.657 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.629.657 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.373.716 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57119.87 tokens per second)
0.01.373.716 I llama_perf_context_print:        load time =     620.83 ms
0.01.373.717 I llama_perf_context_print: prompt eval time =      40.52 ms /     7 tokens (    5.79 ms per token,   172.75 tokens per second)
0.01.373.718 I llama_perf_context_print:        eval time =     700.47 ms /    63 runs   (   11.12 ms per token,    89.94 tokens per second)
0.01.373.718 I llama_perf_context_print:       total time =     744.37 ms /    70 tokens
0.01.373.990 I ggml_metal_free: deallocating

real	0m1.391s
user	0m0.111s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.782 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.012 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.017 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.019 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.019 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.020 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.020 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.020 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.021 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.021 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.022 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.022 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.023 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.023 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.023 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.025 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.025 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.026 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.992 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.026 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.026 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.027 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.027 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.028 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.028 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.028 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.029 I llama_model_loader: - type  f32:  194 tensors
0.00.025.029 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.029 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.030 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.030 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.030 I print_info: file format = GGUF V3 (latest)
0.00.025.037 I print_info: file type   = Q3_K - Medium
0.00.025.038 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.134 I load: special tokens cache size = 25
0.00.050.026 I load: token to piece cache size = 0.2984 MB
0.00.050.041 I print_info: arch             = gptneox
0.00.050.042 I print_info: vocab_only       = 0
0.00.050.042 I print_info: n_ctx_train      = 2048
0.00.050.042 I print_info: n_embd           = 2048
0.00.050.043 I print_info: n_layer          = 24
0.00.050.046 I print_info: n_head           = 16
0.00.050.046 I print_info: n_head_kv        = 16
0.00.050.046 I print_info: n_rot            = 32
0.00.050.047 I print_info: n_swa            = 0
0.00.050.047 I print_info: n_embd_head_k    = 128
0.00.050.047 I print_info: n_embd_head_v    = 128
0.00.050.048 I print_info: n_gqa            = 1
0.00.050.048 I print_info: n_embd_k_gqa     = 2048
0.00.050.049 I print_info: n_embd_v_gqa     = 2048
0.00.050.050 I print_info: f_norm_eps       = 1.0e-05
0.00.050.051 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.053 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.053 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.053 I print_info: f_logit_scale    = 0.0e+00
0.00.050.053 I print_info: n_ff             = 8192
0.00.050.054 I print_info: n_expert         = 0
0.00.050.054 I print_info: n_expert_used    = 0
0.00.050.054 I print_info: causal attn      = 1
0.00.050.054 I print_info: pooling type     = 0
0.00.050.054 I print_info: rope type        = 2
0.00.050.054 I print_info: rope scaling     = linear
0.00.050.056 I print_info: freq_base_train  = 10000.0
0.00.050.056 I print_info: freq_scale_train = 1
0.00.050.056 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.056 I print_info: rope_finetuned   = unknown
0.00.050.056 I print_info: ssm_d_conv       = 0
0.00.050.056 I print_info: ssm_d_inner      = 0
0.00.050.056 I print_info: ssm_d_state      = 0
0.00.050.057 I print_info: ssm_dt_rank      = 0
0.00.050.057 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.057 I print_info: model type       = 1.4B
0.00.050.057 I print_info: model params     = 1.41 B
0.00.050.057 I print_info: general.name     = 1.4B
0.00.050.058 I print_info: vocab type       = BPE
0.00.050.058 I print_info: n_vocab          = 50304
0.00.050.058 I print_info: n_merges         = 50009
0.00.050.058 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.058 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.059 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.059 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.060 I print_info: LF token         = 128 ''
0.00.050.060 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.060 I print_info: max token length = 1024
0.00.052.006 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.007 I load_tensors: offloading output layer to GPU
0.00.052.007 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.017 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.018 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.295 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.296 I llama_new_context_with_model: n_ctx         = 128
0.00.052.296 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.296 I llama_new_context_with_model: n_batch       = 128
0.00.052.296 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.297 I llama_new_context_with_model: flash_attn    = 0
0.00.052.297 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.297 I llama_new_context_with_model: freq_scale    = 1
0.00.052.298 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.298 I ggml_metal_init: allocating
0.00.052.301 I ggml_metal_init: found device: Apple M4
0.00.052.303 I ggml_metal_init: picking default device: Apple M4
0.00.052.876 I ggml_metal_init: using embedded metal library
0.00.055.228 I ggml_metal_init: GPU name:   Apple M4
0.00.055.229 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.230 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.230 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.230 I ggml_metal_init: simdgroup reduction   = true
0.00.055.231 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.231 I ggml_metal_init: has bfloat            = true
0.00.055.231 I ggml_metal_init: use bfloat            = true
0.00.055.231 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.232 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.004 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.248 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.250 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.263 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.133 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.134 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.135 I llama_new_context_with_model: graph nodes  = 967
0.00.067.135 I llama_new_context_with_model: graph splits = 2
0.00.067.136 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.136 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.470.103 I 
0.00.470.137 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.470.149 I perplexity: tokenizing the input ..
0.00.477.790 I perplexity: tokenization took 7.64 ms
0.00.477.793 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.610.018 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.611.245 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.611.273 I llama_perf_context_print:        load time =     461.31 ms
0.00.611.274 I llama_perf_context_print: prompt eval time =     131.98 ms /   128 tokens (    1.03 ms per token,   969.87 tokens per second)
0.00.611.274 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.611.275 I llama_perf_context_print:       total time =     141.17 ms /   129 tokens
0.00.611.778 I ggml_metal_free: deallocating

real	0m0.625s
user	0m0.078s
sys	0m0.078s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.011.921 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.958 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.027.969 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.970 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.971 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.972 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.972 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.972 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.973 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.973 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.976 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.976 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.977 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.977 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.977 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.979 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.979 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.979 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.004 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.032 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.015 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.016 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.016 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.017 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.017 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.017 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.037.018 I llama_model_loader: - type  f32:  194 tensors
0.00.037.018 I llama_model_loader: - type q4_K:   61 tensors
0.00.037.018 I llama_model_loader: - type q5_K:   24 tensors
0.00.037.018 I llama_model_loader: - type q6_K:   13 tensors
0.00.037.019 I print_info: file format = GGUF V3 (latest)
0.00.037.026 I print_info: file type   = Q4_K - Medium
0.00.037.026 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.058.428 I load: special tokens cache size = 25
0.00.064.453 I load: token to piece cache size = 0.2984 MB
0.00.064.467 I print_info: arch             = gptneox
0.00.064.468 I print_info: vocab_only       = 0
0.00.064.469 I print_info: n_ctx_train      = 2048
0.00.064.469 I print_info: n_embd           = 2048
0.00.064.469 I print_info: n_layer          = 24
0.00.064.472 I print_info: n_head           = 16
0.00.064.473 I print_info: n_head_kv        = 16
0.00.064.473 I print_info: n_rot            = 32
0.00.064.473 I print_info: n_swa            = 0
0.00.064.474 I print_info: n_embd_head_k    = 128
0.00.064.474 I print_info: n_embd_head_v    = 128
0.00.064.474 I print_info: n_gqa            = 1
0.00.064.475 I print_info: n_embd_k_gqa     = 2048
0.00.064.476 I print_info: n_embd_v_gqa     = 2048
0.00.064.476 I print_info: f_norm_eps       = 1.0e-05
0.00.064.476 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.476 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.478 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.478 I print_info: f_logit_scale    = 0.0e+00
0.00.064.479 I print_info: n_ff             = 8192
0.00.064.479 I print_info: n_expert         = 0
0.00.064.479 I print_info: n_expert_used    = 0
0.00.064.479 I print_info: causal attn      = 1
0.00.064.479 I print_info: pooling type     = 0
0.00.064.479 I print_info: rope type        = 2
0.00.064.480 I print_info: rope scaling     = linear
0.00.064.480 I print_info: freq_base_train  = 10000.0
0.00.064.480 I print_info: freq_scale_train = 1
0.00.064.480 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.480 I print_info: rope_finetuned   = unknown
0.00.064.481 I print_info: ssm_d_conv       = 0
0.00.064.481 I print_info: ssm_d_inner      = 0
0.00.064.481 I print_info: ssm_d_state      = 0
0.00.064.481 I print_info: ssm_dt_rank      = 0
0.00.064.481 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.481 I print_info: model type       = 1.4B
0.00.064.481 I print_info: model params     = 1.41 B
0.00.064.481 I print_info: general.name     = 1.4B
0.00.064.482 I print_info: vocab type       = BPE
0.00.064.482 I print_info: n_vocab          = 50304
0.00.064.482 I print_info: n_merges         = 50009
0.00.064.483 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.483 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.484 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.484 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.484 I print_info: LF token         = 128 ''
0.00.064.484 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.484 I print_info: max token length = 1024
0.00.066.521 I load_tensors: offloading 24 repeating layers to GPU
0.00.066.522 I load_tensors: offloading output layer to GPU
0.00.066.522 I load_tensors: offloaded 25/25 layers to GPU
0.00.066.533 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.066.534 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.066.819 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.820 I llama_new_context_with_model: n_ctx         = 2048
0.00.066.820 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.066.820 I llama_new_context_with_model: n_batch       = 2048
0.00.066.820 I llama_new_context_with_model: n_ubatch      = 512
0.00.066.820 I llama_new_context_with_model: flash_attn    = 0
0.00.066.821 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.821 I llama_new_context_with_model: freq_scale    = 1
0.00.066.821 I ggml_metal_init: allocating
0.00.066.824 I ggml_metal_init: found device: Apple M4
0.00.066.826 I ggml_metal_init: picking default device: Apple M4
0.00.067.424 I ggml_metal_init: using embedded metal library
0.00.069.788 I ggml_metal_init: GPU name:   Apple M4
0.00.069.789 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.790 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.790 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.790 I ggml_metal_init: simdgroup reduction   = true
0.00.069.791 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.791 I ggml_metal_init: has bfloat            = true
0.00.069.791 I ggml_metal_init: use bfloat            = true
0.00.069.791 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.792 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.448 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.096 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.103.101 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.103.119 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.246 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.104.247 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.104.248 I llama_new_context_with_model: graph nodes  = 967
0.00.104.248 I llama_new_context_with_model: graph splits = 2
0.00.104.251 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.104.385 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.386 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.715.932 I main: llama threadpool init, n_threads = 4
0.00.715.967 I 
0.00.715.988 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.715.988 I 
0.00.716.206 I sampler seed: 1234
0.00.716.212 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.716.255 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.716.257 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.716.257 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.476.371 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57211.93 tokens per second)
0.01.476.371 I llama_perf_context_print:        load time =     704.01 ms
0.01.476.372 I llama_perf_context_print: prompt eval time =      47.13 ms /     7 tokens (    6.73 ms per token,   148.53 tokens per second)
0.01.476.374 I llama_perf_context_print:        eval time =     710.00 ms /    63 runs   (   11.27 ms per token,    88.73 tokens per second)
0.01.476.374 I llama_perf_context_print:       total time =     760.44 ms /    70 tokens
0.01.476.608 I ggml_metal_free: deallocating

real	0m1.494s
user	0m0.113s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.967 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.155 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.160 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.162 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.162 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.162 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.163 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.163 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.164 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.164 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.165 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.167 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.168 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.168 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.169 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.170 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.172 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.172 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.264 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.378 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.410 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.411 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.411 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.412 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.412 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.412 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.413 I llama_model_loader: - type  f32:  194 tensors
0.00.026.413 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.413 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.414 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.414 I print_info: file format = GGUF V3 (latest)
0.00.026.421 I print_info: file type   = Q4_K - Medium
0.00.026.423 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.515 I load: special tokens cache size = 25
0.00.051.479 I load: token to piece cache size = 0.2984 MB
0.00.051.493 I print_info: arch             = gptneox
0.00.051.494 I print_info: vocab_only       = 0
0.00.051.494 I print_info: n_ctx_train      = 2048
0.00.051.495 I print_info: n_embd           = 2048
0.00.051.495 I print_info: n_layer          = 24
0.00.051.498 I print_info: n_head           = 16
0.00.051.498 I print_info: n_head_kv        = 16
0.00.051.499 I print_info: n_rot            = 32
0.00.051.499 I print_info: n_swa            = 0
0.00.051.499 I print_info: n_embd_head_k    = 128
0.00.051.499 I print_info: n_embd_head_v    = 128
0.00.051.500 I print_info: n_gqa            = 1
0.00.051.501 I print_info: n_embd_k_gqa     = 2048
0.00.051.501 I print_info: n_embd_v_gqa     = 2048
0.00.051.502 I print_info: f_norm_eps       = 1.0e-05
0.00.051.502 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.502 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.503 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.504 I print_info: f_logit_scale    = 0.0e+00
0.00.051.504 I print_info: n_ff             = 8192
0.00.051.504 I print_info: n_expert         = 0
0.00.051.505 I print_info: n_expert_used    = 0
0.00.051.505 I print_info: causal attn      = 1
0.00.051.505 I print_info: pooling type     = 0
0.00.051.505 I print_info: rope type        = 2
0.00.051.505 I print_info: rope scaling     = linear
0.00.051.506 I print_info: freq_base_train  = 10000.0
0.00.051.506 I print_info: freq_scale_train = 1
0.00.051.506 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.506 I print_info: rope_finetuned   = unknown
0.00.051.507 I print_info: ssm_d_conv       = 0
0.00.051.508 I print_info: ssm_d_inner      = 0
0.00.051.508 I print_info: ssm_d_state      = 0
0.00.051.508 I print_info: ssm_dt_rank      = 0
0.00.051.508 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.508 I print_info: model type       = 1.4B
0.00.051.508 I print_info: model params     = 1.41 B
0.00.051.508 I print_info: general.name     = 1.4B
0.00.051.510 I print_info: vocab type       = BPE
0.00.051.510 I print_info: n_vocab          = 50304
0.00.051.510 I print_info: n_merges         = 50009
0.00.051.510 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.510 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.510 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.511 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.511 I print_info: LF token         = 128 ''
0.00.051.511 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.514 I print_info: max token length = 1024
0.00.053.502 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.502 I load_tensors: offloading output layer to GPU
0.00.053.503 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.513 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.515 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.798 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.799 I llama_new_context_with_model: n_ctx         = 128
0.00.053.799 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.799 I llama_new_context_with_model: n_batch       = 128
0.00.053.799 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.799 I llama_new_context_with_model: flash_attn    = 0
0.00.053.800 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.800 I llama_new_context_with_model: freq_scale    = 1
0.00.053.800 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.801 I ggml_metal_init: allocating
0.00.053.804 I ggml_metal_init: found device: Apple M4
0.00.053.805 I ggml_metal_init: picking default device: Apple M4
0.00.054.378 I ggml_metal_init: using embedded metal library
0.00.056.757 I ggml_metal_init: GPU name:   Apple M4
0.00.056.759 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.759 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.759 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.760 I ggml_metal_init: simdgroup reduction   = true
0.00.056.760 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.760 I ggml_metal_init: has bfloat            = true
0.00.056.760 I ggml_metal_init: use bfloat            = true
0.00.056.761 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.761 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.518 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.867 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.869 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.882 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.848 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.849 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.849 I llama_new_context_with_model: graph nodes  = 967
0.00.068.850 I llama_new_context_with_model: graph splits = 2
0.00.068.851 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.851 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.550.573 I 
0.00.550.605 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.550.623 I perplexity: tokenizing the input ..
0.00.558.529 I perplexity: tokenization took 7.903 ms
0.00.558.533 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.692.831 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.694.001 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.694.029 I llama_perf_context_print:        load time =     540.60 ms
0.00.694.029 I llama_perf_context_print: prompt eval time =     134.07 ms /   128 tokens (    1.05 ms per token,   954.71 tokens per second)
0.00.694.030 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.694.031 I llama_perf_context_print:       total time =     143.46 ms /   129 tokens
0.00.694.548 I ggml_metal_free: deallocating

real	0m0.710s
user	0m0.079s
sys	0m0.097s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.697 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.373 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.025.378 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.380 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.380 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.381 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.381 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.383 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.387 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.390 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.391 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.391 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.391 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.392 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.392 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.395 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.395 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.395 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.374 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.502 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.486 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.487 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.488 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.488 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.488 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.489 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.034.489 I llama_model_loader: - type  f32:  194 tensors
0.00.034.489 I llama_model_loader: - type q5_K:   61 tensors
0.00.034.490 I llama_model_loader: - type q6_K:   37 tensors
0.00.034.490 I print_info: file format = GGUF V3 (latest)
0.00.034.502 I print_info: file type   = Q5_K - Medium
0.00.034.503 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.056.355 I load: special tokens cache size = 25
0.00.062.230 I load: token to piece cache size = 0.2984 MB
0.00.062.245 I print_info: arch             = gptneox
0.00.062.246 I print_info: vocab_only       = 0
0.00.062.246 I print_info: n_ctx_train      = 2048
0.00.062.246 I print_info: n_embd           = 2048
0.00.062.246 I print_info: n_layer          = 24
0.00.062.249 I print_info: n_head           = 16
0.00.062.250 I print_info: n_head_kv        = 16
0.00.062.250 I print_info: n_rot            = 32
0.00.062.250 I print_info: n_swa            = 0
0.00.062.251 I print_info: n_embd_head_k    = 128
0.00.062.251 I print_info: n_embd_head_v    = 128
0.00.062.251 I print_info: n_gqa            = 1
0.00.062.252 I print_info: n_embd_k_gqa     = 2048
0.00.062.253 I print_info: n_embd_v_gqa     = 2048
0.00.062.253 I print_info: f_norm_eps       = 1.0e-05
0.00.062.254 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.254 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.254 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.254 I print_info: f_logit_scale    = 0.0e+00
0.00.062.255 I print_info: n_ff             = 8192
0.00.062.257 I print_info: n_expert         = 0
0.00.062.257 I print_info: n_expert_used    = 0
0.00.062.259 I print_info: causal attn      = 1
0.00.062.263 I print_info: pooling type     = 0
0.00.062.263 I print_info: rope type        = 2
0.00.062.263 I print_info: rope scaling     = linear
0.00.062.264 I print_info: freq_base_train  = 10000.0
0.00.062.265 I print_info: freq_scale_train = 1
0.00.062.265 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.265 I print_info: rope_finetuned   = unknown
0.00.062.265 I print_info: ssm_d_conv       = 0
0.00.062.265 I print_info: ssm_d_inner      = 0
0.00.062.265 I print_info: ssm_d_state      = 0
0.00.062.265 I print_info: ssm_dt_rank      = 0
0.00.062.265 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.266 I print_info: model type       = 1.4B
0.00.062.266 I print_info: model params     = 1.41 B
0.00.062.268 I print_info: general.name     = 1.4B
0.00.062.269 I print_info: vocab type       = BPE
0.00.062.269 I print_info: n_vocab          = 50304
0.00.062.269 I print_info: n_merges         = 50009
0.00.062.269 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.271 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.271 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.271 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.271 I print_info: LF token         = 128 ''
0.00.062.271 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.271 I print_info: max token length = 1024
0.00.064.280 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.281 I load_tensors: offloading output layer to GPU
0.00.064.281 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.292 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.064.293 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.064.577 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.578 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.578 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.578 I llama_new_context_with_model: n_batch       = 2048
0.00.064.578 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.578 I llama_new_context_with_model: flash_attn    = 0
0.00.064.579 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.579 I llama_new_context_with_model: freq_scale    = 1
0.00.064.580 I ggml_metal_init: allocating
0.00.064.583 I ggml_metal_init: found device: Apple M4
0.00.064.585 I ggml_metal_init: picking default device: Apple M4
0.00.065.182 I ggml_metal_init: using embedded metal library
0.00.067.620 I ggml_metal_init: GPU name:   Apple M4
0.00.067.622 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.622 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.623 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.623 I ggml_metal_init: simdgroup reduction   = true
0.00.067.623 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.623 I ggml_metal_init: has bfloat            = true
0.00.067.623 I ggml_metal_init: use bfloat            = true
0.00.067.624 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.624 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.433 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.097.184 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.189 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.208 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.259 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.098.260 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.098.261 I llama_new_context_with_model: graph nodes  = 967
0.00.098.261 I llama_new_context_with_model: graph splits = 2
0.00.098.264 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.098.398 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.399 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.744.529 I main: llama threadpool init, n_threads = 4
0.00.744.580 I 
0.00.744.606 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.744.606 I 
0.00.744.876 I sampler seed: 1234
0.00.744.880 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.744.924 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.744.924 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.744.925 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.602.000 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 63906.39 tokens per second)
0.01.602.002 I llama_perf_context_print:        load time =     735.83 ms
0.01.602.002 I llama_perf_context_print: prompt eval time =      51.63 ms /     7 tokens (    7.38 ms per token,   135.58 tokens per second)
0.01.602.003 I llama_perf_context_print:        eval time =     802.59 ms /    63 runs   (   12.74 ms per token,    78.50 tokens per second)
0.01.602.003 I llama_perf_context_print:       total time =     857.48 ms /    70 tokens
0.01.602.219 I ggml_metal_free: deallocating

real	0m1.617s
user	0m0.113s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.821 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.074 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.079 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.081 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.081 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.082 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.082 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.082 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.083 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.083 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.084 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.084 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.084 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.085 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.085 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.087 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.087 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.087 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.146 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.271 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.322 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.324 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.324 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.324 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.325 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.325 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.325 I llama_model_loader: - type  f32:  194 tensors
0.00.025.326 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.326 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.326 I print_info: file format = GGUF V3 (latest)
0.00.025.333 I print_info: file type   = Q5_K - Medium
0.00.025.334 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.498 I load: special tokens cache size = 25
0.00.050.431 I load: token to piece cache size = 0.2984 MB
0.00.050.446 I print_info: arch             = gptneox
0.00.050.447 I print_info: vocab_only       = 0
0.00.050.447 I print_info: n_ctx_train      = 2048
0.00.050.448 I print_info: n_embd           = 2048
0.00.050.448 I print_info: n_layer          = 24
0.00.050.451 I print_info: n_head           = 16
0.00.050.451 I print_info: n_head_kv        = 16
0.00.050.452 I print_info: n_rot            = 32
0.00.050.452 I print_info: n_swa            = 0
0.00.050.452 I print_info: n_embd_head_k    = 128
0.00.050.452 I print_info: n_embd_head_v    = 128
0.00.050.453 I print_info: n_gqa            = 1
0.00.050.456 I print_info: n_embd_k_gqa     = 2048
0.00.050.457 I print_info: n_embd_v_gqa     = 2048
0.00.050.458 I print_info: f_norm_eps       = 1.0e-05
0.00.050.458 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.459 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.459 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.459 I print_info: f_logit_scale    = 0.0e+00
0.00.050.460 I print_info: n_ff             = 8192
0.00.050.460 I print_info: n_expert         = 0
0.00.050.460 I print_info: n_expert_used    = 0
0.00.050.460 I print_info: causal attn      = 1
0.00.050.460 I print_info: pooling type     = 0
0.00.050.460 I print_info: rope type        = 2
0.00.050.460 I print_info: rope scaling     = linear
0.00.050.461 I print_info: freq_base_train  = 10000.0
0.00.050.462 I print_info: freq_scale_train = 1
0.00.050.462 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.462 I print_info: rope_finetuned   = unknown
0.00.050.462 I print_info: ssm_d_conv       = 0
0.00.050.462 I print_info: ssm_d_inner      = 0
0.00.050.462 I print_info: ssm_d_state      = 0
0.00.050.462 I print_info: ssm_dt_rank      = 0
0.00.050.462 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.463 I print_info: model type       = 1.4B
0.00.050.463 I print_info: model params     = 1.41 B
0.00.050.463 I print_info: general.name     = 1.4B
0.00.050.464 I print_info: vocab type       = BPE
0.00.050.464 I print_info: n_vocab          = 50304
0.00.050.464 I print_info: n_merges         = 50009
0.00.050.464 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.464 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.465 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.465 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.465 I print_info: LF token         = 128 ''
0.00.050.465 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.465 I print_info: max token length = 1024
0.00.052.500 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.500 I load_tensors: offloading output layer to GPU
0.00.052.500 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.511 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.512 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.815 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.816 I llama_new_context_with_model: n_ctx         = 128
0.00.052.816 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.816 I llama_new_context_with_model: n_batch       = 128
0.00.052.816 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.817 I llama_new_context_with_model: flash_attn    = 0
0.00.052.817 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.817 I llama_new_context_with_model: freq_scale    = 1
0.00.052.818 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.818 I ggml_metal_init: allocating
0.00.052.821 I ggml_metal_init: found device: Apple M4
0.00.052.823 I ggml_metal_init: picking default device: Apple M4
0.00.053.374 I ggml_metal_init: using embedded metal library
0.00.055.701 I ggml_metal_init: GPU name:   Apple M4
0.00.055.702 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.703 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.703 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.703 I ggml_metal_init: simdgroup reduction   = true
0.00.055.704 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.704 I ggml_metal_init: has bfloat            = true
0.00.055.704 I ggml_metal_init: use bfloat            = true
0.00.055.704 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.705 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.522 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.755 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.757 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.773 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.677 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.677 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.678 I llama_new_context_with_model: graph nodes  = 967
0.00.067.678 I llama_new_context_with_model: graph splits = 2
0.00.067.679 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.679 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.638.509 I 
0.00.638.545 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.638.559 I perplexity: tokenizing the input ..
0.00.646.247 I perplexity: tokenization took 7.687 ms
0.00.646.250 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.787.369 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.788.561 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.788.588 I llama_perf_context_print:        load time =     629.68 ms
0.00.788.588 I llama_perf_context_print: prompt eval time =     140.87 ms /   128 tokens (    1.10 ms per token,   908.61 tokens per second)
0.00.788.589 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.788.590 I llama_perf_context_print:       total time =     150.08 ms /   129 tokens
0.00.789.135 I ggml_metal_free: deallocating

real	0m0.803s
user	0m0.079s
sys	0m0.113s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.015.006 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.649 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.028.654 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.655 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.656 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.656 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.656 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.657 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.659 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.659 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.660 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.660 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.662 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.662 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.663 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.666 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.667 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.667 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.326 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.513 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.325 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.327 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.327 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.327 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.328 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.328 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.039.328 I llama_model_loader: - type  f32:  194 tensors
0.00.039.329 I llama_model_loader: - type q6_K:   98 tensors
0.00.039.329 I print_info: file format = GGUF V3 (latest)
0.00.039.341 I print_info: file type   = Q6_K
0.00.039.342 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.068.155 I load: special tokens cache size = 25
0.00.078.332 I load: token to piece cache size = 0.2984 MB
0.00.078.348 I print_info: arch             = gptneox
0.00.078.350 I print_info: vocab_only       = 0
0.00.078.350 I print_info: n_ctx_train      = 2048
0.00.078.350 I print_info: n_embd           = 2048
0.00.078.351 I print_info: n_layer          = 24
0.00.078.354 I print_info: n_head           = 16
0.00.078.355 I print_info: n_head_kv        = 16
0.00.078.355 I print_info: n_rot            = 32
0.00.078.356 I print_info: n_swa            = 0
0.00.078.356 I print_info: n_embd_head_k    = 128
0.00.078.356 I print_info: n_embd_head_v    = 128
0.00.078.357 I print_info: n_gqa            = 1
0.00.078.358 I print_info: n_embd_k_gqa     = 2048
0.00.078.359 I print_info: n_embd_v_gqa     = 2048
0.00.078.359 I print_info: f_norm_eps       = 1.0e-05
0.00.078.360 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.360 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.360 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.360 I print_info: f_logit_scale    = 0.0e+00
0.00.078.361 I print_info: n_ff             = 8192
0.00.078.361 I print_info: n_expert         = 0
0.00.078.361 I print_info: n_expert_used    = 0
0.00.078.362 I print_info: causal attn      = 1
0.00.078.363 I print_info: pooling type     = 0
0.00.078.364 I print_info: rope type        = 2
0.00.078.364 I print_info: rope scaling     = linear
0.00.078.364 I print_info: freq_base_train  = 10000.0
0.00.078.365 I print_info: freq_scale_train = 1
0.00.078.366 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.366 I print_info: rope_finetuned   = unknown
0.00.078.367 I print_info: ssm_d_conv       = 0
0.00.078.367 I print_info: ssm_d_inner      = 0
0.00.078.368 I print_info: ssm_d_state      = 0
0.00.078.369 I print_info: ssm_dt_rank      = 0
0.00.078.370 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.370 I print_info: model type       = 1.4B
0.00.078.370 I print_info: model params     = 1.41 B
0.00.078.370 I print_info: general.name     = 1.4B
0.00.078.371 I print_info: vocab type       = BPE
0.00.078.371 I print_info: n_vocab          = 50304
0.00.078.371 I print_info: n_merges         = 50009
0.00.078.372 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.372 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.372 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.372 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.373 I print_info: LF token         = 128 ''
0.00.078.373 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.373 I print_info: max token length = 1024
0.00.081.170 I load_tensors: offloading 24 repeating layers to GPU
0.00.081.170 I load_tensors: offloading output layer to GPU
0.00.081.171 I load_tensors: offloaded 25/25 layers to GPU
0.00.081.182 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.081.184 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.081.586 I llama_new_context_with_model: n_seq_max     = 1
0.00.081.587 I llama_new_context_with_model: n_ctx         = 2048
0.00.081.587 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.081.587 I llama_new_context_with_model: n_batch       = 2048
0.00.081.587 I llama_new_context_with_model: n_ubatch      = 512
0.00.081.588 I llama_new_context_with_model: flash_attn    = 0
0.00.081.588 I llama_new_context_with_model: freq_base     = 10000.0
0.00.081.589 I llama_new_context_with_model: freq_scale    = 1
0.00.081.589 I ggml_metal_init: allocating
0.00.081.593 I ggml_metal_init: found device: Apple M4
0.00.081.596 I ggml_metal_init: picking default device: Apple M4
0.00.082.426 I ggml_metal_init: using embedded metal library
0.00.086.101 I ggml_metal_init: GPU name:   Apple M4
0.00.086.104 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.086.104 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.086.105 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.086.105 I ggml_metal_init: simdgroup reduction   = true
0.00.086.105 I ggml_metal_init: simdgroup matrix mul. = true
0.00.086.105 I ggml_metal_init: has bfloat            = true
0.00.086.106 I ggml_metal_init: use bfloat            = true
0.00.086.106 I ggml_metal_init: hasUnifiedMemory      = true
0.00.086.107 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.097.784 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.118.829 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.118.835 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.118.855 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.119.899 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.119.900 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.119.900 I llama_new_context_with_model: graph nodes  = 967
0.00.119.901 I llama_new_context_with_model: graph splits = 2
0.00.119.904 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.120.032 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.120.033 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.879.415 I main: llama threadpool init, n_threads = 4
0.00.879.459 I 
0.00.879.485 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.879.485 I 
0.00.879.814 I sampler seed: 1234
0.00.879.819 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.879.874 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.879.876 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.879.876 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.763.834 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60374.15 tokens per second)
0.01.763.835 I llama_perf_context_print:        load time =     864.40 ms
0.01.763.836 I llama_perf_context_print: prompt eval time =      54.63 ms /     7 tokens (    7.80 ms per token,   128.12 tokens per second)
0.01.763.837 I llama_perf_context_print:        eval time =     826.41 ms /    63 runs   (   13.12 ms per token,    76.23 tokens per second)
0.01.763.837 I llama_perf_context_print:       total time =     884.42 ms /    70 tokens
0.01.764.096 I ggml_metal_free: deallocating

real	0m1.793s
user	0m0.133s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4462 (a857dc50) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.774 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.736 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.740 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.746 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.746 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.748 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.749 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.749 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.750 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.750 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.754 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.754 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.754 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.755 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.755 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.756 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.757 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.757 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.929 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.008 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.086 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.088 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.088 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.088 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.088 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.089 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.089 I llama_model_loader: - type  f32:  194 tensors
0.00.026.090 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.090 I print_info: file format = GGUF V3 (latest)
0.00.026.102 I print_info: file type   = Q6_K
0.00.026.103 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.116 I load: special tokens cache size = 25
0.00.051.123 I load: token to piece cache size = 0.2984 MB
0.00.051.138 I print_info: arch             = gptneox
0.00.051.139 I print_info: vocab_only       = 0
0.00.051.140 I print_info: n_ctx_train      = 2048
0.00.051.140 I print_info: n_embd           = 2048
0.00.051.140 I print_info: n_layer          = 24
0.00.051.142 I print_info: n_head           = 16
0.00.051.143 I print_info: n_head_kv        = 16
0.00.051.143 I print_info: n_rot            = 32
0.00.051.143 I print_info: n_swa            = 0
0.00.051.144 I print_info: n_embd_head_k    = 128
0.00.051.145 I print_info: n_embd_head_v    = 128
0.00.051.146 I print_info: n_gqa            = 1
0.00.051.147 I print_info: n_embd_k_gqa     = 2048
0.00.051.147 I print_info: n_embd_v_gqa     = 2048
0.00.051.148 I print_info: f_norm_eps       = 1.0e-05
0.00.051.148 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.149 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.149 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.149 I print_info: f_logit_scale    = 0.0e+00
0.00.051.149 I print_info: n_ff             = 8192
0.00.051.150 I print_info: n_expert         = 0
0.00.051.150 I print_info: n_expert_used    = 0
0.00.051.150 I print_info: causal attn      = 1
0.00.051.150 I print_info: pooling type     = 0
0.00.051.150 I print_info: rope type        = 2
0.00.051.151 I print_info: rope scaling     = linear
0.00.051.152 I print_info: freq_base_train  = 10000.0
0.00.051.152 I print_info: freq_scale_train = 1
0.00.051.152 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.152 I print_info: rope_finetuned   = unknown
0.00.051.153 I print_info: ssm_d_conv       = 0
0.00.051.154 I print_info: ssm_d_inner      = 0
0.00.051.154 I print_info: ssm_d_state      = 0
0.00.051.154 I print_info: ssm_dt_rank      = 0
0.00.051.154 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.155 I print_info: model type       = 1.4B
0.00.051.155 I print_info: model params     = 1.41 B
0.00.051.156 I print_info: general.name     = 1.4B
0.00.051.156 I print_info: vocab type       = BPE
0.00.051.156 I print_info: n_vocab          = 50304
0.00.051.160 I print_info: n_merges         = 50009
0.00.051.160 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.160 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.160 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.161 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.162 I print_info: LF token         = 128 ''
0.00.051.162 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.162 I print_info: max token length = 1024
0.00.053.202 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.202 I load_tensors: offloading output layer to GPU
0.00.053.202 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.213 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.214 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.494 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.495 I llama_new_context_with_model: n_ctx         = 128
0.00.053.495 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.496 I llama_new_context_with_model: n_batch       = 128
0.00.053.496 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.496 I llama_new_context_with_model: flash_attn    = 0
0.00.053.496 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.496 I llama_new_context_with_model: freq_scale    = 1
0.00.053.497 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.497 I ggml_metal_init: allocating
0.00.053.500 I ggml_metal_init: found device: Apple M4
0.00.053.502 I ggml_metal_init: picking default device: Apple M4
0.00.054.052 I ggml_metal_init: using embedded metal library
0.00.056.404 I ggml_metal_init: GPU name:   Apple M4
0.00.056.406 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.406 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.407 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.407 I ggml_metal_init: simdgroup reduction   = true
0.00.056.407 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.407 I ggml_metal_init: has bfloat            = true
0.00.056.407 I ggml_metal_init: use bfloat            = true
0.00.056.408 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.408 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.145 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.520 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.522 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.535 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.395 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.396 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.397 I llama_new_context_with_model: graph nodes  = 967
0.00.068.397 I llama_new_context_with_model: graph splits = 2
0.00.068.398 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.398 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.316.058 I 
0.00.316.089 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.316.100 I perplexity: tokenizing the input ..
0.00.323.674 I perplexity: tokenization took 7.571 ms
0.00.323.679 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.463.812 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.464.985 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.465.015 I llama_perf_context_print:        load time =     306.28 ms
0.00.465.016 I llama_perf_context_print: prompt eval time =     139.91 ms /   128 tokens (    1.09 ms per token,   914.89 tokens per second)
0.00.465.017 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.465.017 I llama_perf_context_print:       total time =     148.96 ms /   129 tokens
0.00.465.433 I ggml_metal_free: deallocating

real	0m0.481s
user	0m0.078s
sys	0m0.066s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4462 (a857dc50)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12960a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12960aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12960aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12960b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12960bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12960c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12960c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12960cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12960d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12960d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12960dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12960e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12960ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12960f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12960fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x129610310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x129610a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x129611150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x129611870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x129612040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x129612760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x129612e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1296135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x129613e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x129614560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x129614820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x129614e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x129615aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x129615fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1296162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x129616740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x129616a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x129617290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1296177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x129617a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129617f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1296183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x129618870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x129618d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1296191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x129619650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x129619af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x129619f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12961a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12961a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12961ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12961b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12961bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12961c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12961c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12961ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12961d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12961da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12961e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12961e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12961ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12961f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12961f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12961fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x129620280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x129620540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1296209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x129620e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x129621320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1296217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x129621c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x129622100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1296225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x129622a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x129622ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x129623380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x129623820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129623cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x129624210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x129624760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x129624cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x129625200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x129625750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x129625ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1296261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x129626740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129626c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1296271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x129627730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x129627c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1296281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x129628720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x129628c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1296291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x129629710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x129629c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12962a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12962a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12962ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12962b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12962b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12962bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12961b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12962c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12962c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12962cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12962d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12962d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12962dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12962e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12962e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12962ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12962f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12962f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12962fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1296302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x129630820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x129630d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x129631210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1296316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x129631b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x129631ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129632490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129632930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129632dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129633270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x129633710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129633bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x129634050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1296344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129634990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129634e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1296352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x129635770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x129635c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1296360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x129636550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1296369f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x129636e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x129637330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1296377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x129637c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x129638110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1296385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x129638a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x129638ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x129639390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x129639830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x129639cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12963a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12963a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12963aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12963af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12963b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12963b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12963bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12963c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12963c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12963cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12963cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12963d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12963d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12963dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12963e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12963e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12963eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12963f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12963f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12963f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12963fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x129640290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x129640730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x129640bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129641070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x129641510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1296419b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x129641e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1296422f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129642790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129642c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1296430d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129643570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129643a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129643eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x129644350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1296447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129644c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129645130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1296455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129645a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129645f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1296463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129646850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129646cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x129647190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x129647630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x129647ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x129647f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1296484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129648a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x129648f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1296494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x129649770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x129649d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12964a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12964a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12964b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12964b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12964b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12964bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12964c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12964cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12964d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12964d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12964dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12964e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12964e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12964ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12964f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12964f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12964fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x129650270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1296507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x129650d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x129651260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1296517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x129651d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x129652250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1296527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x129652cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x129653240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x129653790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x129653ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x129654230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x129654780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x129654cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x129655220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x129655770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x129655cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x129656210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x129656760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x129656cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x129657200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x129657750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x129657ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1296581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x129658740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x129658c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1296591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x129659730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x129659c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12965a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12965a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12965ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12965b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12965b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12965bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12965c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12965c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12965cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12965d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12965d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12965dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12965e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12965e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12965ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12965f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12965f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12965fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x129660170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1296606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x129660c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1296610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x129661550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1296619f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x129661e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x129662330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1296627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129662c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x129663110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1296635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x129663a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129663ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x129664390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x129664830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x129664cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x129665170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1296656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x129665de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x129666500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x129666c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x129667340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x129667600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x129667df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1296680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1296686c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.120.786 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.120.790 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x129668370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12964a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x129649a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12964a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12961d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12961d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12961f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12964c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x129614ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12961b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12961bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12961c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12961a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12961cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x129613ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x129609960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12961e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12961fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12962c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1296678c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x129616cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x129616f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12964c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12964ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1296150f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1296153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x129615670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x129668b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x129668de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1296690a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x129669360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x129669620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1296698e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x129669ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x129669e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12966a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12966a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12966a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12966a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12966ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12966aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12966b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12966b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12966b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12966b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12966bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12966bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12966c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12966c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12966c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12966ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12966cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12966cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12966d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12966d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12966d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12966dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12966dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12966e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12966e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12966e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12966e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12966eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12966ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12966f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12966f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12966f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12966f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12966fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12966fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x129670160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x129670420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1296706e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1296709a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x129670c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x129670f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1296711e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1296714a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x129671760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x129671a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x129671ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129671fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x129672260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x129672520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1296727e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x129672aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x129672d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x129673020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1296732e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1296735a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x129673860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x129673b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x129673de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1296740a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x129674360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x129674620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1296748e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x129674ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x129674e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x129675120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1296753e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1296756a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x129675960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x129675c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x129675ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1296761a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x129676460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x129676720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1296769e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x129676ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x129676f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x129677220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1296774e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1296777a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x129677a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x129677d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x129677fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1296782a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129678560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129678820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129678ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x129678da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129679060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x129679320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1296795e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1296798a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129679b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x129679e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12967a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12967a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12967a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12967a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12967abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12967aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12967b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12967b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12967b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12967b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12967bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12967bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12967c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12967c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12967c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12967ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12967cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12967cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12967d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12967d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12967d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12967daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12967dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12967e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12967e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12967e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12967e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12967eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12967ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12967f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12967f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12967f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12967f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12967fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12967fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x129680120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1296803e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1296806a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x129680960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x129680c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129680ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1296811a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x129681460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x129681720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1296819e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129681ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129681f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x129682220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1296824e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1296827a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129682a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x129682d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x129682fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1296832a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129683560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x129683820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129683ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129683da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x129684060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129684320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1296845e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1296848a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x129684b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x129684e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1296850e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1296853a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129685660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x129685920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x129685be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x129685ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x129686160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x129686420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1296866e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1296869a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x129686c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x129686f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1296871e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1296874a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x129687760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x129687a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x129687ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x129687fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x129688570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x129688830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x129688af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x129688db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x129689070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x129689330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1296895f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1296898b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x129689b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x129689e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12968a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12968a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12968a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12968a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12968abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12968aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12968b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12968b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12968b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12968b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12968bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12968bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12968c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12968c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12968c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12968ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12968ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12968cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12968d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12968d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12968d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12968dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12968e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12968e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12968eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12968eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12968f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12968fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12968ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x129690530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x129690a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x129690fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x129691520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x129691a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x129691fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x129692510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x129692a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x129692fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x129693500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x129693a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x129693fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1296944f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x129694a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x129694f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1296954e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x129695a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x129695f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x129696240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x129696500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x129696a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x129696f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x129697400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x129697900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129697e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x129698300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x129698800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x129698d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129699200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x129699700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x129699c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12969a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12969a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12969ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12969b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12969bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12969c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12969ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12969cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12969d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12969d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12969ddf0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12969daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12964bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12969cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12969e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12969e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12969e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12969ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12969ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12969f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12969f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12969f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12969f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12969fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1296a03f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1296a0a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1296a0ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1296a0fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1296a1260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1296a1520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1296a17e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1296a1aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1296a1d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1296a2020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1296a22e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1296a25a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1296a2860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1296a2b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1296a2de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1296a30a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1296a3360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1296a3620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1296a38e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1296a3ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1296a3e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1296a4120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1296a43e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1296a46a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1296a4960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1296a4c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1296a4ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1296a51a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1296a5460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1296a5720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1296a59e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1296a5ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1296a5f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1296a6220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1296a64e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1296a67a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1296a6a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1296a6d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1296a6fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1296a72a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1296a7560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1296a7820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1296a7ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1296a7da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1296a8060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1296a8320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1296a85e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1296a88a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1296a8b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1296a8e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1296a90e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1296a93a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1296a9660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1296a9920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1296a9be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1296a9ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1296aa160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1296aa420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1296aa6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1296aa9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1296aac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1296aaf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1296ab1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1296ab4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1296ab760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1296aba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1296abce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1296abfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1296ac260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1296ac520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1296ac7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1296acaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1296acd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1296ad020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1296ad2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1296ad5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1296ad860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1296adb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1296adde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1296ae0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1296ae360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1296ae620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1296ae8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1296aeba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1296aee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1296af120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1296af3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1296af6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1296af960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1296afc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1296afee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1296b01a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1296b0460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1296b0720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1296b09e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1296b0ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1296b0f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1296b1220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1296b14e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1296b17a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1296b1a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1296b1d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1296b1fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1296b22a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1296b2560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1296b2820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1296b2ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1296b2da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1296b3060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1296b3320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1296b35e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1296b38a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1296b3b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1296b3e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1296b40e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1296b43a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1296b4660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1296b4920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1296b4be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1296b4ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1296b5160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1296b5420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1296b56e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1296b59a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1296b5c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1296b5f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1296b61e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1296b64a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1296b6760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1296b6a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1296b6ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1296b6fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1296b7260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1296b7520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1296b77e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1296b7aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1296b7d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1296b8020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1296b82e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1296b85a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1296b8860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1296b8b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1296b8de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1296b90a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1296b9360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1296b9620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1296b98e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1296b9ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1296b9e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1296ba120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1296ba3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1296ba6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1296ba960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1296bac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1296baee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1296bb1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1296bb460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1296bb720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1296bb9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1296bbca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1296bbf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1296bc220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1296bc4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1296bc7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1296bca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1296bcd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1296bcfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1296bd2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1296bd560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1296bd820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1296bdae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1296bdda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1296be060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1296be320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1296be5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1296be8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1296beb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1296bee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1296bf0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1296bf3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1296bf660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1296bf920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1296bfbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1296bfea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1296c0160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1296c0420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1296c06e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1296c09a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1296c0c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1296c0f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1296c11e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1296c14a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1296c1760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1296c1a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1296c1ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1296c1fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1296c2260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1296c2830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1296c2af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1296c2db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1296c3070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1296c3330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1296c35f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1296c38b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1296c3b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1296c3e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1296c40f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1296c43b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1296c4670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1296c4930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1296c4bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1296c4eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1296c5170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1296c5430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1296c56f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1296c59b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1296c5c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1296c5f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1296c61f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1296c64b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1296c6770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1296c6a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1296c6cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1296c6fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1296c7270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1296c7530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1296c77f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1296c7ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1296c7d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1296c8030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1296c82f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1296c85b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1296c8870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1296c8b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1296c8df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1296c90b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1296c9370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1296c9630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1296c98f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1296c9bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1296c9e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1296ca130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1296ca3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1296ca6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1296ca970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1296cac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1296caef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1296cb1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1296cb470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1296cb730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1296cb9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1296cbcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1296cbf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1296cc230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1296cc4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1296cc7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1296cca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1296ccd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1296ccff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1296cd3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1296cd6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1296cd970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1296cdde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1296ce250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1296ce6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1296ceb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1296cefa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1296cf410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1296cf880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1296cfcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1296d0860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1296d0f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1296d16a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1296d1dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1296d2080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1296d2340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1296d2870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1296d2ce0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.762s
user	0m0.280s
sys	0m0.287s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4462 (a857dc50)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15760ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15760d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15760d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15760df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15760e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15760ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15760f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15760f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15760fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1576100a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1576105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157610aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1576115c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x157611d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157612580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x157612ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1576133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x157613ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x157614200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1576149d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1576150f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x157615810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157615f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1576167d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157616ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1576171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1576177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x157618430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x157618970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157618c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1576190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x157619390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157619c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15761a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15761a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15761a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15761ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15761b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15761b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15761bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15761bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15761c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15761c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15761cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15761d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15761d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15761dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15761e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15761ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15761f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15761f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15761fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157620410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157620a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157621210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1576216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157621b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157621e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157622420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157622c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157622ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157623370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157623810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157623cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157624150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1576245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157624a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x157624f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1576253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157625870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x157625d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1576261b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x157626650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x157626ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1576270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x157627640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x157627b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1576280e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x157628630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x157628b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1576290d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x157629620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x157629b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15762a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15762a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15762ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15762b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15762b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15762bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15762c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15762c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15762cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15762d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15762d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15762db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15762e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15762e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15761e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15762ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15762f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15762f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15762fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1576301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157630730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x157630c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1576311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157631720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157631c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1576321c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157632710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x157632c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1576331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157633700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157633ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157634040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1576344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157634980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157634e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1576352c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157635760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157635c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1576360a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157636540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1576369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157636e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157637320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1576377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x157637c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x157638100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1576385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x157638a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x157638ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157639380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157639820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x157639cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15763a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15763a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15763aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15763af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15763b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15763b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15763bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15763c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15763c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15763cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15763cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15763d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15763d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15763dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15763e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15763e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15763eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15763f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15763f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15763f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15763fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157640280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157640720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157640bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157641060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157641500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1576419a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157641e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1576422e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157642780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157642c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1576430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157643560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157643a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157643ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157644340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1576447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157644c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157645120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1576455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157645a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157645f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1576463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157646840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157646ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157647180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157647620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157647ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x157647f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x157648400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1576488a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x157648d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1576491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x157649680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157649b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x157649fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15764a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15764a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15764ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15764b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15764b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15764be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15764c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15764c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15764cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15764d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15764db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15764dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15764e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15764e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15764eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15764f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15764fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15764ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157650470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157650c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157651170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1576516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157651c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157652160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1576526b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157652c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157653150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1576536a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157653bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157654140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157654690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157654be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157655130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157655680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157655bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157656120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x157656670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157656bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157657110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157657660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157657bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157658100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157658650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x157658ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1576590f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157659640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x157659b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15765a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15765a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15765ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15765b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15765b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15765bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15765c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15765c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15765cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15765d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15765d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15765db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15765e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15765e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15765eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15765f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15765f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15765fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x157660080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1576605d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157660b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x157661070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1576615c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157661b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157662060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1576625b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157662b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157663050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1576635a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157663a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157663ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157664380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157664820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157664cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157665160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157665600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157665aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157665f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1576663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157666880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157666d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1576671c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x157667660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157667b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x157668050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x157668770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x157668e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1576695b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157669cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x157669f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15766a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15766aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15766b050 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.085.965 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.968 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x159008810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x159008c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1590090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x159009560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1590099d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x159009e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15900a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15900a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15900ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15900b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15900b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15900bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15900c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15900ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15900d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15900dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15900e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15900eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15900f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15900f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x159010110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x159010830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x159010f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x159011670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x159011d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x159012050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x159012310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x159012780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x159012bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x159013060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x159013560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x159013a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x159013ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1590141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x159014610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x159014a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x159014fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1590154e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1590159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x159015ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1590163e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1590168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x159016de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1590172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1590177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x159017c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1590180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x159018530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1590189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x159018e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x159019280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1590196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x159019b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x159019fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15901a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15901ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15901b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15901b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15901b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15901c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15901c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15901cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15901cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15901d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15901d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15901dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15901e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15901e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15901eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15901efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15901f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15901f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15901fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1590202e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x159020830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x159020d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1590212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x159021820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x159021d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1590222c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x159022810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x159022d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1590232b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x159023800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x159023d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1590242a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1590247f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x159024d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x159025290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1590257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x159025d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x159026280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1590267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x159026d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x159027270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1590277c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x159027d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x159028260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1590287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x159028d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x159029250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1590297a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x159029cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15902a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15902a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15902ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15902b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15902b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15902bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15902c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15902c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15902ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15902d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15902d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15902db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15902dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15902e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15902e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15902edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15902f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15902f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15902fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x159030050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1590304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x159030990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x159030e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1590312d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x159031770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x159031c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1590320b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x159032550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1590329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x159032e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x159033330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1590337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x159033c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x159034110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1590345b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x159034a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x159034ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x159035390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x159035830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x159035cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x159036170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x159036610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x159036ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x159036f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1590373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x159037890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x159037d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1590381d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x159038670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x159038b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x159038fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x159039450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1590398f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x159039d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15903a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15903a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15903ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15903b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15903b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15903b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15903bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15903c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15903c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15903cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15903d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15903d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15903d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15903de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15903e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15903e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15903ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15903f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15903f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15903fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15903feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x159040350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1590407f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x159040c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x159041130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1590415d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x159041a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x159041f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1590423b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x159042850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x159042cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x159043190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x159043630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x159043ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x159043f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x159044410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x159044960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x159044eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x159045400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x159045950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x159045c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x159046220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x159046830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x159046e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x159047630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x159047ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x159047d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1590483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1590489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1590491a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x159049640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x159049ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x159049f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15904a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15904ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15904b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15904b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15904bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15904c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15904c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15904cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15904d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15904d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15904dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15904e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15904e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15904ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15904f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15904f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15904fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x159050180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1590506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x159050c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x159051170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1590516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x159051c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x159052160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1590526b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x159052c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x159053150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1590536a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x159053bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x159054140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x159054690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x159054be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x159055130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x159055680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x159055bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x159056120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x159056670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x159056bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x159057110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x159057660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x159057bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x159058100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x159058650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x159058ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1590590f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x159059640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x159059b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15905a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15905a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15905ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15905b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15905b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15905bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15905c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15905c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15905cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15905d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15905d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15905d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15905de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15905e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15905e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15905ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15905f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15905f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15905fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15905fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x159060390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x159060830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x159060cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x159061170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x159061610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x159061b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x159062280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1590629a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1590630c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1590637e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x159063aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x159064290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x159064550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x159064b60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15766ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15764c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15764c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15764cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1576200c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15761fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1576220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15764eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157617470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15761df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15761e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15761ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15761d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15761f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157616470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15760c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x157620ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1576226e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15762ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15766a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x157619650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x157619910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15764f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15764d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157617a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157617d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x157618000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15766b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15766b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15766ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15766bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15766bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15766c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15766c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15766c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15766cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15766cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15766d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15766d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15766d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15766d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15766db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15766ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15766e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15766e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15766e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15766e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15766ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15766ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15766f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15766f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15766f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15766f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15766fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15766fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1576701b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157670470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157670730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1576709f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157670cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157670f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157671230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1576714f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1576717b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157671a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157671d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157671ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1576722b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157672570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157672830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x157672af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x157672db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x157673070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x157673330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1576735f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1576738b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x157673b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x157673e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1576740f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1576743b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x157674670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x157674930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x157674bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x157674eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x157675170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x157675430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1576756f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1576759b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x157675c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x157675f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1576761f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1576764b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157676770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157676a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157676cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157676fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157677270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157677530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1576777f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157677ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157677d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157678030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1576782f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1576785b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x157678870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x157678b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157678df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1576790b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x157679370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157679630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1576798f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x157679bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157679e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15767a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15767a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15767a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15767a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15767ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15767aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15767b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15767b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15767b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15767b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15767bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15767bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15767c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15767c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15767c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15767ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15767cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15767cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15767d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15767d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15767d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15767daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15767ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15767e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15767e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15767e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15767e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15767eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15767ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15767f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15767f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15767f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15767f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15767fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15767feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157680170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157680430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1576806f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1576809b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157680c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157680f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1576811f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1576814b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157681770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157681a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157681cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157681fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157682270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157682530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1576827f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157682ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157682d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157683030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1576832f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1576835b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157683870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157683b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157683df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1576840b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157684370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157684630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1576848f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157684bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157684e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157685130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1576853f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1576856b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157685970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157685c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157685ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1576861b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x157686470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x157686730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1576869f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x157686cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x157686f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157687230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1576874f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1576877b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x157687a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x157687d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157687ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1576882b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x157688570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x157688830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x157688af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x157688db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x157689070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x157689330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1576895f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1576898b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157689b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157689e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15768a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15768a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15768a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15768a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15768af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15768b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15768b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15768b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15768ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15768bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15768bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15768c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15768c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15768c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15768ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15768cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15768d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15768d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15768d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15768d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15768db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15768ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15768e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15768e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15768e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15768e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15768eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15768ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15768f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15768f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15768f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15768f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15768fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15768fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157690180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x157690440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x157690700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1576909c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x157690c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x157690f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x157691200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157691750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x157691ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1576921f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157692740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157692c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1576931e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x157693730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157693c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1576941d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x157694720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157694c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1576951c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x157695710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157695c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1576961b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157696700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157696c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1576971a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1576976f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x157697c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1576980e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157698580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157698a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157698ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157699360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157699800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157699ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15769a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15769a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15769aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15769af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15769b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15769b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15769bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15769c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15769c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15769ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15769d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15769dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15769e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15769e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15769ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15769f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15769f6f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.921s
user	0m0.243s
sys	0m0.136s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.52 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.56 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.08 sec*proc (2 tests)

Total Test time (real) =   1.09 sec
        1.11 real         0.69 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.24 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.15 user         0.04 sys
```
