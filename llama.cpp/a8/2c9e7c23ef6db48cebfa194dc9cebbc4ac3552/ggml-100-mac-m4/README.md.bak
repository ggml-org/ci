### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.23 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.21 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.59 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.21 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.21 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.17 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.25 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.51 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.25 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.07 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.21 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.27 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.90 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.95 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  103.28 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.84 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.74 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 163.81 sec*proc (29 tests)

Total Test time (real) = 163.82 sec

real	2m43.954s
user	4m35.235s
sys	0m5.681s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.20 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.22 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.85 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.16 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.41 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.48 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.27 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.08 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.36 sec*proc (29 tests)

Total Test time (real) =  48.37 sec

real	0m48.384s
user	0m54.817s
sys	0m5.235s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.104 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.297 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.076 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.022.085 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.088 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.022.089 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.090 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.022.090 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.022.091 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.022.097 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.022.097 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.022.098 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.022.104 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.022.105 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.022.108 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.022.109 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.022.110 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.022.111 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.022.112 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.022.112 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.022.113 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.026.275 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.027.606 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.609 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.027.610 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.027.610 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.027.611 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.027.611 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.027.612 I llama_model_loader: - type  f32:  124 tensors
0.00.027.613 I llama_model_loader: - type  f16:   73 tensors
0.00.027.614 I print_info: file format = GGUF V3 (latest)
0.00.027.615 I print_info: file type   = F16
0.00.027.617 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.030.134 I load: special tokens cache size = 5
0.00.031.494 I load: token to piece cache size = 0.2032 MB
0.00.031.498 I print_info: arch             = bert
0.00.031.498 I print_info: vocab_only       = 0
0.00.031.499 I print_info: n_ctx_train      = 512
0.00.031.499 I print_info: n_embd           = 384
0.00.031.499 I print_info: n_layer          = 12
0.00.031.504 I print_info: n_head           = 12
0.00.031.505 I print_info: n_head_kv        = 12
0.00.031.505 I print_info: n_rot            = 32
0.00.031.505 I print_info: n_swa            = 0
0.00.031.505 I print_info: n_embd_head_k    = 32
0.00.031.505 I print_info: n_embd_head_v    = 32
0.00.031.506 I print_info: n_gqa            = 1
0.00.031.507 I print_info: n_embd_k_gqa     = 384
0.00.031.507 I print_info: n_embd_v_gqa     = 384
0.00.031.508 I print_info: f_norm_eps       = 1.0e-12
0.00.031.508 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.031.508 I print_info: f_clamp_kqv      = 0.0e+00
0.00.031.510 I print_info: f_max_alibi_bias = 0.0e+00
0.00.031.510 I print_info: f_logit_scale    = 0.0e+00
0.00.031.511 I print_info: n_ff             = 1536
0.00.031.511 I print_info: n_expert         = 0
0.00.031.511 I print_info: n_expert_used    = 0
0.00.031.511 I print_info: causal attn      = 0
0.00.031.511 I print_info: pooling type     = 2
0.00.031.511 I print_info: rope type        = 2
0.00.031.512 I print_info: rope scaling     = linear
0.00.031.512 I print_info: freq_base_train  = 10000.0
0.00.031.513 I print_info: freq_scale_train = 1
0.00.031.513 I print_info: n_ctx_orig_yarn  = 512
0.00.031.513 I print_info: rope_finetuned   = unknown
0.00.031.513 I print_info: ssm_d_conv       = 0
0.00.031.513 I print_info: ssm_d_inner      = 0
0.00.031.513 I print_info: ssm_d_state      = 0
0.00.031.514 I print_info: ssm_dt_rank      = 0
0.00.031.515 I print_info: ssm_dt_b_c_rms   = 0
0.00.031.516 I print_info: model type       = 33M
0.00.031.516 I print_info: model params     = 33.21 M
0.00.031.516 I print_info: general.name     = Bge Small
0.00.031.517 I print_info: vocab type       = WPM
0.00.031.517 I print_info: n_vocab          = 30522
0.00.031.517 I print_info: n_merges         = 0
0.00.031.517 I print_info: BOS token        = 101 '[CLS]'
0.00.031.517 I print_info: UNK token        = 100 '[UNK]'
0.00.031.518 I print_info: SEP token        = 102 '[SEP]'
0.00.031.518 I print_info: PAD token        = 0 '[PAD]'
0.00.031.518 I print_info: MASK token       = 103 '[MASK]'
0.00.031.518 I print_info: LF token         = 0 '[PAD]'
0.00.031.519 I print_info: max token length = 21
0.00.031.519 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.033.543 I load_tensors: offloading 12 repeating layers to GPU
0.00.033.544 I load_tensors: offloading output layer to GPU
0.00.033.544 I load_tensors: offloaded 13/13 layers to GPU
0.00.033.564 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.033.565 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.033.747 I llama_init_from_model: n_seq_max     = 1
0.00.033.748 I llama_init_from_model: n_ctx         = 512
0.00.033.748 I llama_init_from_model: n_ctx_per_seq = 512
0.00.033.748 I llama_init_from_model: n_batch       = 2048
0.00.033.749 I llama_init_from_model: n_ubatch      = 2048
0.00.033.749 I llama_init_from_model: flash_attn    = 0
0.00.033.749 I llama_init_from_model: freq_base     = 10000.0
0.00.033.749 I llama_init_from_model: freq_scale    = 1
0.00.033.750 I ggml_metal_init: allocating
0.00.033.754 I ggml_metal_init: found device: Apple M4
0.00.033.757 I ggml_metal_init: picking default device: Apple M4
0.00.034.299 I ggml_metal_init: using embedded metal library
0.00.036.858 I ggml_metal_init: GPU name:   Apple M4
0.00.036.860 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.036.860 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.036.861 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.036.861 I ggml_metal_init: simdgroup reduction   = true
0.00.036.861 I ggml_metal_init: simdgroup matrix mul. = true
0.00.036.861 I ggml_metal_init: has residency sets    = true
0.00.036.862 I ggml_metal_init: has bfloat            = true
0.00.036.862 I ggml_metal_init: use bfloat            = true
0.00.036.862 I ggml_metal_init: hasUnifiedMemory      = true
0.00.036.863 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.046.685 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.047.383 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.047.386 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.047.413 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.048.645 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.048.646 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.048.647 I llama_init_from_model: graph nodes  = 429
0.00.048.647 I llama_init_from_model: graph splits = 2
0.00.048.648 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.048.648 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.055.758 I 
0.00.055.801 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.056.390 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.060.926 I llama_perf_context_print:        load time =      38.45 ms
0.00.060.927 I llama_perf_context_print: prompt eval time =       4.41 ms /     9 tokens (    0.49 ms per token,  2041.74 tokens per second)
0.00.060.927 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.060.928 I llama_perf_context_print:       total time =       5.17 ms /    10 tokens
0.00.061.139 I ggml_metal_free: deallocating

real	0m0.237s
user	0m0.046s
sys	0m0.025s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.049 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.105 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.743 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.748 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.750 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.750 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.750 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.751 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.751 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.752 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.752 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.752 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.753 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.753 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.755 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.756 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.756 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.757 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.758 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.758 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.015.110 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.831 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.832 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.832 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.833 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.833 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.833 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.833 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.834 I llama_model_loader: - type  f32:  124 tensors
0.00.015.834 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.835 I print_info: file format = GGUF V3 (latest)
0.00.015.835 I print_info: file type   = Q8_0
0.00.015.837 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.018.389 I load: special tokens cache size = 5
0.00.019.590 I load: token to piece cache size = 0.2032 MB
0.00.019.593 I print_info: arch             = bert
0.00.019.594 I print_info: vocab_only       = 0
0.00.019.594 I print_info: n_ctx_train      = 512
0.00.019.594 I print_info: n_embd           = 384
0.00.019.594 I print_info: n_layer          = 12
0.00.019.599 I print_info: n_head           = 12
0.00.019.599 I print_info: n_head_kv        = 12
0.00.019.599 I print_info: n_rot            = 32
0.00.019.599 I print_info: n_swa            = 0
0.00.019.601 I print_info: n_embd_head_k    = 32
0.00.019.601 I print_info: n_embd_head_v    = 32
0.00.019.601 I print_info: n_gqa            = 1
0.00.019.602 I print_info: n_embd_k_gqa     = 384
0.00.019.602 I print_info: n_embd_v_gqa     = 384
0.00.019.603 I print_info: f_norm_eps       = 1.0e-12
0.00.019.603 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.604 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.606 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.606 I print_info: f_logit_scale    = 0.0e+00
0.00.019.607 I print_info: n_ff             = 1536
0.00.019.607 I print_info: n_expert         = 0
0.00.019.607 I print_info: n_expert_used    = 0
0.00.019.607 I print_info: causal attn      = 0
0.00.019.608 I print_info: pooling type     = 2
0.00.019.608 I print_info: rope type        = 2
0.00.019.608 I print_info: rope scaling     = linear
0.00.019.608 I print_info: freq_base_train  = 10000.0
0.00.019.609 I print_info: freq_scale_train = 1
0.00.019.609 I print_info: n_ctx_orig_yarn  = 512
0.00.019.609 I print_info: rope_finetuned   = unknown
0.00.019.609 I print_info: ssm_d_conv       = 0
0.00.019.609 I print_info: ssm_d_inner      = 0
0.00.019.609 I print_info: ssm_d_state      = 0
0.00.019.609 I print_info: ssm_dt_rank      = 0
0.00.019.610 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.610 I print_info: model type       = 33M
0.00.019.610 I print_info: model params     = 33.21 M
0.00.019.610 I print_info: general.name     = Bge Small
0.00.019.611 I print_info: vocab type       = WPM
0.00.019.611 I print_info: n_vocab          = 30522
0.00.019.611 I print_info: n_merges         = 0
0.00.019.611 I print_info: BOS token        = 101 '[CLS]'
0.00.019.612 I print_info: UNK token        = 100 '[UNK]'
0.00.019.612 I print_info: SEP token        = 102 '[SEP]'
0.00.019.612 I print_info: PAD token        = 0 '[PAD]'
0.00.019.612 I print_info: MASK token       = 103 '[MASK]'
0.00.019.612 I print_info: LF token         = 0 '[PAD]'
0.00.019.613 I print_info: max token length = 21
0.00.019.613 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.021.441 I load_tensors: offloading 12 repeating layers to GPU
0.00.021.442 I load_tensors: offloading output layer to GPU
0.00.021.442 I load_tensors: offloaded 13/13 layers to GPU
0.00.021.448 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.021.449 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.641 I llama_init_from_model: n_seq_max     = 1
0.00.021.642 I llama_init_from_model: n_ctx         = 512
0.00.021.642 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.642 I llama_init_from_model: n_batch       = 2048
0.00.021.642 I llama_init_from_model: n_ubatch      = 2048
0.00.021.643 I llama_init_from_model: flash_attn    = 0
0.00.021.643 I llama_init_from_model: freq_base     = 10000.0
0.00.021.643 I llama_init_from_model: freq_scale    = 1
0.00.021.643 I ggml_metal_init: allocating
0.00.021.647 I ggml_metal_init: found device: Apple M4
0.00.021.655 I ggml_metal_init: picking default device: Apple M4
0.00.022.251 I ggml_metal_init: using embedded metal library
0.00.024.874 I ggml_metal_init: GPU name:   Apple M4
0.00.024.877 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.877 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.878 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.878 I ggml_metal_init: simdgroup reduction   = true
0.00.024.878 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.878 I ggml_metal_init: has residency sets    = true
0.00.024.878 I ggml_metal_init: has bfloat            = true
0.00.024.878 I ggml_metal_init: use bfloat            = true
0.00.024.879 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.880 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.728 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.340 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.342 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.355 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.036.331 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.036.333 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.036.333 I llama_init_from_model: graph nodes  = 429
0.00.036.333 I llama_init_from_model: graph splits = 2
0.00.036.334 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.334 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.298 I 
0.00.040.318 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.860 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.160 I llama_perf_context_print:        load time =      30.19 ms
0.00.045.161 I llama_perf_context_print: prompt eval time =       4.19 ms /     9 tokens (    0.47 ms per token,  2147.97 tokens per second)
0.00.045.162 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.162 I llama_perf_context_print:       total time =       4.86 ms /    10 tokens
0.00.045.288 I ggml_metal_free: deallocating

real	0m0.059s
user	0m0.030s
sys	0m0.018s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.280 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.480 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.374 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.379 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.381 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.382 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.386 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.387 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.387 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.390 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.397 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.398 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.401 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.401 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.405 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.409 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.410 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.411 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.411 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.043.431 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.518 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.982 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.984 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.985 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.985 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.985 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.986 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.986 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.986 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.987 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.987 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.987 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.049.988 I llama_model_loader: - type  f32:   40 tensors
0.00.049.992 I llama_model_loader: - type  f16:   30 tensors
0.00.049.993 I print_info: file format = GGUF V3 (latest)
0.00.049.994 I print_info: file type   = F16
0.00.049.995 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.054.357 W load: empty token at index 5
0.00.059.486 W load: model vocab missing newline token, using special_pad_id instead
0.00.061.002 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.061.036 I load: special tokens cache size = 5
0.00.320.115 I load: token to piece cache size = 1.5060 MB
0.00.320.121 I print_info: arch             = jina-bert-v2
0.00.320.121 I print_info: vocab_only       = 0
0.00.320.122 I print_info: n_ctx_train      = 8192
0.00.320.122 I print_info: n_embd           = 384
0.00.320.122 I print_info: n_layer          = 4
0.00.320.128 I print_info: n_head           = 12
0.00.320.129 I print_info: n_head_kv        = 12
0.00.320.129 I print_info: n_rot            = 32
0.00.320.136 I print_info: n_swa            = 0
0.00.320.136 I print_info: n_embd_head_k    = 32
0.00.320.136 I print_info: n_embd_head_v    = 32
0.00.320.137 I print_info: n_gqa            = 1
0.00.320.137 I print_info: n_embd_k_gqa     = 384
0.00.320.138 I print_info: n_embd_v_gqa     = 384
0.00.320.139 I print_info: f_norm_eps       = 1.0e-12
0.00.320.140 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.320.141 I print_info: f_clamp_kqv      = 0.0e+00
0.00.320.141 I print_info: f_max_alibi_bias = 8.0e+00
0.00.320.142 I print_info: f_logit_scale    = 0.0e+00
0.00.320.142 I print_info: n_ff             = 1536
0.00.320.142 I print_info: n_expert         = 0
0.00.320.142 I print_info: n_expert_used    = 0
0.00.320.143 I print_info: causal attn      = 0
0.00.320.143 I print_info: pooling type     = -1
0.00.320.143 I print_info: rope type        = -1
0.00.320.143 I print_info: rope scaling     = linear
0.00.320.143 I print_info: freq_base_train  = 10000.0
0.00.320.144 I print_info: freq_scale_train = 1
0.00.320.144 I print_info: n_ctx_orig_yarn  = 8192
0.00.320.144 I print_info: rope_finetuned   = unknown
0.00.320.144 I print_info: ssm_d_conv       = 0
0.00.320.145 I print_info: ssm_d_inner      = 0
0.00.320.145 I print_info: ssm_d_state      = 0
0.00.320.150 I print_info: ssm_dt_rank      = 0
0.00.320.150 I print_info: ssm_dt_b_c_rms   = 0
0.00.320.150 I print_info: model type       = 33M
0.00.320.150 I print_info: model params     = 32.90 M
0.00.320.151 I print_info: general.name     = Jina Bert Implementation
0.00.320.152 I print_info: vocab type       = BPE
0.00.320.152 I print_info: n_vocab          = 61056
0.00.320.154 I print_info: n_merges         = 39382
0.00.320.154 I print_info: BOS token        = 0 '<s>'
0.00.320.154 I print_info: EOS token        = 2 '</s>'
0.00.320.154 I print_info: UNK token        = 3 '<unk>'
0.00.320.155 I print_info: SEP token        = 2 '</s>'
0.00.320.155 I print_info: PAD token        = 1 '<pad>'
0.00.320.155 I print_info: MASK token       = 4 '<mask>'
0.00.320.156 I print_info: EOG token        = 2 '</s>'
0.00.320.156 I print_info: max token length = 45
0.00.320.156 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.322.185 I load_tensors: offloading 4 repeating layers to GPU
0.00.322.186 I load_tensors: offloading output layer to GPU
0.00.322.186 I load_tensors: offloaded 5/5 layers to GPU
0.00.322.211 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.322.213 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.322.489 I llama_init_from_model: n_seq_max     = 1
0.00.322.490 I llama_init_from_model: n_ctx         = 8192
0.00.322.490 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.322.490 I llama_init_from_model: n_batch       = 2048
0.00.322.490 I llama_init_from_model: n_ubatch      = 2048
0.00.322.490 I llama_init_from_model: flash_attn    = 0
0.00.322.491 I llama_init_from_model: freq_base     = 10000.0
0.00.322.491 I llama_init_from_model: freq_scale    = 1
0.00.322.492 I ggml_metal_init: allocating
0.00.322.495 I ggml_metal_init: found device: Apple M4
0.00.322.500 I ggml_metal_init: picking default device: Apple M4
0.00.323.398 I ggml_metal_init: using embedded metal library
0.00.326.252 I ggml_metal_init: GPU name:   Apple M4
0.00.326.253 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.326.254 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.326.254 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.326.254 I ggml_metal_init: simdgroup reduction   = true
0.00.326.254 I ggml_metal_init: simdgroup matrix mul. = true
0.00.326.255 I ggml_metal_init: has residency sets    = true
0.00.326.255 I ggml_metal_init: has bfloat            = true
0.00.326.255 I ggml_metal_init: use bfloat            = true
0.00.326.255 I ggml_metal_init: hasUnifiedMemory      = true
0.00.326.256 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.336.329 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.339.397 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.339.399 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.339.419 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.345.295 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.345.297 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.345.297 I llama_init_from_model: graph nodes  = 154
0.00.345.297 I llama_init_from_model: graph splits = 2
0.00.345.298 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.345.299 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.352.788 I 
0.00.352.819 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.352.914 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.352.915 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.352.918 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.352.919 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.352.921 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.352.921 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.353.414 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.356.861 I llama_perf_context_print:        load time =     329.30 ms
0.00.356.862 I llama_perf_context_print: prompt eval time =       3.44 ms /    62 tokens (    0.06 ms per token, 18023.26 tokens per second)
0.00.356.863 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.356.864 I llama_perf_context_print:       total time =       4.07 ms /    63 tokens
0.00.357.059 I ggml_metal_free: deallocating

real	0m1.058s
user	0m0.326s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.127 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.274 I main: llama backend init
0.00.000.279 I main: load the model and apply lora adapter, if any
0.00.105.485 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.117.513 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.117.524 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.117.527 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.117.528 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.117.528 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.117.529 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.117.529 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.117.531 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.117.535 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.117.536 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.117.536 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.117.537 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.117.537 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.117.538 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.117.541 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.117.542 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.117.542 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.124.426 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.126.550 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.133.336 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.133.342 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.133.342 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.133.343 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.133.344 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.133.345 I llama_model_loader: - type  f32:  194 tensors
0.00.133.345 I llama_model_loader: - type  f16:   98 tensors
0.00.133.346 I print_info: file format = GGUF V3 (latest)
0.00.133.347 I print_info: file type   = all F32 (guessed)
0.00.133.350 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.143.534 I load: special tokens cache size = 25
0.00.149.298 I load: token to piece cache size = 0.2984 MB
0.00.149.303 I print_info: arch             = gptneox
0.00.149.304 I print_info: vocab_only       = 0
0.00.149.304 I print_info: n_ctx_train      = 2048
0.00.149.304 I print_info: n_embd           = 2048
0.00.149.304 I print_info: n_layer          = 24
0.00.149.309 I print_info: n_head           = 16
0.00.149.310 I print_info: n_head_kv        = 16
0.00.149.310 I print_info: n_rot            = 32
0.00.149.310 I print_info: n_swa            = 0
0.00.149.311 I print_info: n_embd_head_k    = 128
0.00.149.311 I print_info: n_embd_head_v    = 128
0.00.149.312 I print_info: n_gqa            = 1
0.00.149.312 I print_info: n_embd_k_gqa     = 2048
0.00.149.313 I print_info: n_embd_v_gqa     = 2048
0.00.149.314 I print_info: f_norm_eps       = 1.0e-05
0.00.149.314 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.149.314 I print_info: f_clamp_kqv      = 0.0e+00
0.00.149.314 I print_info: f_max_alibi_bias = 0.0e+00
0.00.149.314 I print_info: f_logit_scale    = 0.0e+00
0.00.149.315 I print_info: n_ff             = 8192
0.00.149.315 I print_info: n_expert         = 0
0.00.149.315 I print_info: n_expert_used    = 0
0.00.149.315 I print_info: causal attn      = 1
0.00.149.316 I print_info: pooling type     = 0
0.00.149.316 I print_info: rope type        = 2
0.00.149.316 I print_info: rope scaling     = linear
0.00.149.316 I print_info: freq_base_train  = 10000.0
0.00.149.317 I print_info: freq_scale_train = 1
0.00.149.317 I print_info: n_ctx_orig_yarn  = 2048
0.00.149.317 I print_info: rope_finetuned   = unknown
0.00.149.317 I print_info: ssm_d_conv       = 0
0.00.149.317 I print_info: ssm_d_inner      = 0
0.00.149.317 I print_info: ssm_d_state      = 0
0.00.149.318 I print_info: ssm_dt_rank      = 0
0.00.149.318 I print_info: ssm_dt_b_c_rms   = 0
0.00.149.318 I print_info: model type       = 1.4B
0.00.149.318 I print_info: model params     = 1.41 B
0.00.149.319 I print_info: general.name     = 1.4B
0.00.149.319 I print_info: vocab type       = BPE
0.00.149.319 I print_info: n_vocab          = 50304
0.00.149.320 I print_info: n_merges         = 50009
0.00.149.320 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.149.320 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.149.320 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.149.320 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.149.321 I print_info: LF token         = 187 ''
0.00.149.321 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.149.321 I print_info: max token length = 1024
0.00.149.322 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.191.976 I load_tensors: offloading 24 repeating layers to GPU
0.00.191.979 I load_tensors: offloading output layer to GPU
0.00.191.979 I load_tensors: offloaded 25/25 layers to GPU
0.00.192.007 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.192.008 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.192.621 I llama_init_from_model: n_seq_max     = 1
0.00.192.622 I llama_init_from_model: n_ctx         = 2048
0.00.192.623 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.192.623 I llama_init_from_model: n_batch       = 2048
0.00.192.623 I llama_init_from_model: n_ubatch      = 512
0.00.192.623 I llama_init_from_model: flash_attn    = 0
0.00.192.624 I llama_init_from_model: freq_base     = 10000.0
0.00.192.624 I llama_init_from_model: freq_scale    = 1
0.00.192.625 I ggml_metal_init: allocating
0.00.192.659 I ggml_metal_init: found device: Apple M4
0.00.192.664 I ggml_metal_init: picking default device: Apple M4
0.00.193.333 I ggml_metal_init: using embedded metal library
0.00.217.169 I ggml_metal_init: GPU name:   Apple M4
0.00.217.173 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.217.173 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.217.174 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.217.174 I ggml_metal_init: simdgroup reduction   = true
0.00.217.174 I ggml_metal_init: simdgroup matrix mul. = true
0.00.217.174 I ggml_metal_init: has residency sets    = true
0.00.217.174 I ggml_metal_init: has bfloat            = true
0.00.217.174 I ggml_metal_init: use bfloat            = true
0.00.217.175 I ggml_metal_init: hasUnifiedMemory      = true
0.00.217.177 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.244.311 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.273.026 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.273.033 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.273.119 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.276.848 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.276.849 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.276.850 I llama_init_from_model: graph nodes  = 967
0.00.276.850 I llama_init_from_model: graph splits = 2
0.00.276.855 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.276.978 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.276.978 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.343.273 I main: llama threadpool init, n_threads = 4
0.00.343.316 I 
0.00.343.349 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.343.350 I 
0.00.343.472 I sampler seed: 1234
0.00.343.477 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.343.502 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.343.503 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.343.503 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.172.838 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58292.28 tokens per second)
0.02.172.838 I llama_perf_context_print:        load time =     236.89 ms
0.02.172.839 I llama_perf_context_print: prompt eval time =      43.63 ms /     7 tokens (    6.23 ms per token,   160.45 tokens per second)
0.02.172.840 I llama_perf_context_print:        eval time =    1782.79 ms /    63 runs   (   28.30 ms per token,    35.34 tokens per second)
0.02.172.840 I llama_perf_context_print:       total time =    1830.45 ms /    70 tokens
0.02.173.033 I ggml_metal_free: deallocating

real	0m2.528s
user	0m0.120s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.546 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.616 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.230 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.235 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.237 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.238 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.244 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.244 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.245 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.246 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.247 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.247 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.248 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.248 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.249 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.251 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.254 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.255 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.256 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.342 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.588 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.878 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.880 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.881 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.882 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.882 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.883 I llama_model_loader: - type  f32:  194 tensors
0.00.055.883 I llama_model_loader: - type  f16:   98 tensors
0.00.055.884 I print_info: file format = GGUF V3 (latest)
0.00.055.884 I print_info: file type   = all F32 (guessed)
0.00.055.886 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.922 I load: special tokens cache size = 25
0.00.076.796 I load: token to piece cache size = 0.2984 MB
0.00.076.799 I print_info: arch             = gptneox
0.00.076.799 I print_info: vocab_only       = 0
0.00.076.799 I print_info: n_ctx_train      = 2048
0.00.076.800 I print_info: n_embd           = 2048
0.00.076.800 I print_info: n_layer          = 24
0.00.076.803 I print_info: n_head           = 16
0.00.076.804 I print_info: n_head_kv        = 16
0.00.076.804 I print_info: n_rot            = 32
0.00.076.804 I print_info: n_swa            = 0
0.00.076.805 I print_info: n_embd_head_k    = 128
0.00.076.805 I print_info: n_embd_head_v    = 128
0.00.076.806 I print_info: n_gqa            = 1
0.00.076.806 I print_info: n_embd_k_gqa     = 2048
0.00.076.807 I print_info: n_embd_v_gqa     = 2048
0.00.076.808 I print_info: f_norm_eps       = 1.0e-05
0.00.076.808 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.808 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.808 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.809 I print_info: f_logit_scale    = 0.0e+00
0.00.076.809 I print_info: n_ff             = 8192
0.00.076.809 I print_info: n_expert         = 0
0.00.076.810 I print_info: n_expert_used    = 0
0.00.076.810 I print_info: causal attn      = 1
0.00.076.810 I print_info: pooling type     = 0
0.00.076.810 I print_info: rope type        = 2
0.00.076.819 I print_info: rope scaling     = linear
0.00.076.820 I print_info: freq_base_train  = 10000.0
0.00.076.820 I print_info: freq_scale_train = 1
0.00.076.820 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.821 I print_info: rope_finetuned   = unknown
0.00.076.821 I print_info: ssm_d_conv       = 0
0.00.076.821 I print_info: ssm_d_inner      = 0
0.00.076.821 I print_info: ssm_d_state      = 0
0.00.076.821 I print_info: ssm_dt_rank      = 0
0.00.076.822 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.822 I print_info: model type       = 1.4B
0.00.076.822 I print_info: model params     = 1.41 B
0.00.076.823 I print_info: general.name     = 1.4B
0.00.076.823 I print_info: vocab type       = BPE
0.00.076.823 I print_info: n_vocab          = 50304
0.00.076.823 I print_info: n_merges         = 50009
0.00.076.824 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.824 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.824 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.824 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.825 I print_info: LF token         = 187 ''
0.00.076.827 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.827 I print_info: max token length = 1024
0.00.076.827 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.418.898 I load_tensors: offloading 24 repeating layers to GPU
0.01.418.902 I load_tensors: offloading output layer to GPU
0.01.418.902 I load_tensors: offloaded 25/25 layers to GPU
0.01.418.926 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.418.928 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.419.769 I llama_init_from_model: n_seq_max     = 1
0.01.419.770 I llama_init_from_model: n_ctx         = 128
0.01.419.771 I llama_init_from_model: n_ctx_per_seq = 128
0.01.419.771 I llama_init_from_model: n_batch       = 128
0.01.419.771 I llama_init_from_model: n_ubatch      = 128
0.01.419.771 I llama_init_from_model: flash_attn    = 0
0.01.419.772 I llama_init_from_model: freq_base     = 10000.0
0.01.419.772 I llama_init_from_model: freq_scale    = 1
0.01.419.772 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.419.773 I ggml_metal_init: allocating
0.01.419.838 I ggml_metal_init: found device: Apple M4
0.01.419.843 I ggml_metal_init: picking default device: Apple M4
0.01.420.971 I ggml_metal_init: using embedded metal library
0.01.424.790 I ggml_metal_init: GPU name:   Apple M4
0.01.424.792 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.424.793 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.424.793 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.424.793 I ggml_metal_init: simdgroup reduction   = true
0.01.424.793 I ggml_metal_init: simdgroup matrix mul. = true
0.01.424.794 I ggml_metal_init: has residency sets    = true
0.01.424.794 I ggml_metal_init: has bfloat            = true
0.01.424.794 I ggml_metal_init: use bfloat            = true
0.01.424.794 I ggml_metal_init: hasUnifiedMemory      = true
0.01.424.797 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.435.541 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.437.211 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.437.213 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.437.238 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.438.873 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.438.874 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.438.874 I llama_init_from_model: graph nodes  = 967
0.01.438.875 I llama_init_from_model: graph splits = 2
0.01.438.876 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.438.876 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.473.194 I 
0.01.473.232 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.473.235 I perplexity: tokenizing the input ..
0.01.478.259 I perplexity: tokenization took 5.022 ms
0.01.478.263 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.596.995 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.599.941 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.599.994 I llama_perf_context_print:        load time =    1449.57 ms
0.01.599.996 I llama_perf_context_print: prompt eval time =     118.45 ms /   128 tokens (    0.93 ms per token,  1080.58 tokens per second)
0.01.599.997 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.599.998 I llama_perf_context_print:       total time =     126.80 ms /   129 tokens
0.01.600.642 I ggml_metal_free: deallocating

real	0m1.789s
user	0m0.106s
sys	0m0.254s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.009.994 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.449 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.454 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.456 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.457 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.457 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.458 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.458 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.459 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.460 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.460 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.460 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.461 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.461 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.462 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.464 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.464 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.464 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.280 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.456 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.208 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.209 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.209 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.210 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.210 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.210 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.211 I llama_model_loader: - type  f32:  194 tensors
0.00.033.211 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.212 I print_info: file format = GGUF V3 (latest)
0.00.033.213 I print_info: file type   = Q8_0
0.00.033.214 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.041.765 I load: special tokens cache size = 25
0.00.047.721 I load: token to piece cache size = 0.2984 MB
0.00.047.726 I print_info: arch             = gptneox
0.00.047.726 I print_info: vocab_only       = 0
0.00.047.727 I print_info: n_ctx_train      = 2048
0.00.047.730 I print_info: n_embd           = 2048
0.00.047.731 I print_info: n_layer          = 24
0.00.047.737 I print_info: n_head           = 16
0.00.047.738 I print_info: n_head_kv        = 16
0.00.047.738 I print_info: n_rot            = 32
0.00.047.738 I print_info: n_swa            = 0
0.00.047.738 I print_info: n_embd_head_k    = 128
0.00.047.738 I print_info: n_embd_head_v    = 128
0.00.047.739 I print_info: n_gqa            = 1
0.00.047.740 I print_info: n_embd_k_gqa     = 2048
0.00.047.740 I print_info: n_embd_v_gqa     = 2048
0.00.047.741 I print_info: f_norm_eps       = 1.0e-05
0.00.047.742 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.742 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.742 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.742 I print_info: f_logit_scale    = 0.0e+00
0.00.047.743 I print_info: n_ff             = 8192
0.00.047.743 I print_info: n_expert         = 0
0.00.047.743 I print_info: n_expert_used    = 0
0.00.047.743 I print_info: causal attn      = 1
0.00.047.744 I print_info: pooling type     = 0
0.00.047.744 I print_info: rope type        = 2
0.00.047.744 I print_info: rope scaling     = linear
0.00.047.745 I print_info: freq_base_train  = 10000.0
0.00.047.745 I print_info: freq_scale_train = 1
0.00.047.745 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.747 I print_info: rope_finetuned   = unknown
0.00.047.747 I print_info: ssm_d_conv       = 0
0.00.047.747 I print_info: ssm_d_inner      = 0
0.00.047.747 I print_info: ssm_d_state      = 0
0.00.047.747 I print_info: ssm_dt_rank      = 0
0.00.047.747 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.747 I print_info: model type       = 1.4B
0.00.047.748 I print_info: model params     = 1.41 B
0.00.047.748 I print_info: general.name     = 1.4B
0.00.047.748 I print_info: vocab type       = BPE
0.00.047.749 I print_info: n_vocab          = 50304
0.00.047.750 I print_info: n_merges         = 50009
0.00.047.751 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.751 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.751 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.751 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.751 I print_info: LF token         = 187 ''
0.00.047.752 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.752 I print_info: max token length = 1024
0.00.047.752 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.217.235 I load_tensors: offloading 24 repeating layers to GPU
0.01.217.241 I load_tensors: offloading output layer to GPU
0.01.217.242 I load_tensors: offloaded 25/25 layers to GPU
0.01.217.267 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.217.269 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.218.059 I llama_init_from_model: n_seq_max     = 1
0.01.218.060 I llama_init_from_model: n_ctx         = 2048
0.01.218.061 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.218.061 I llama_init_from_model: n_batch       = 2048
0.01.218.061 I llama_init_from_model: n_ubatch      = 512
0.01.218.062 I llama_init_from_model: flash_attn    = 0
0.01.218.062 I llama_init_from_model: freq_base     = 10000.0
0.01.218.063 I llama_init_from_model: freq_scale    = 1
0.01.218.064 I ggml_metal_init: allocating
0.01.218.078 I ggml_metal_init: found device: Apple M4
0.01.218.085 I ggml_metal_init: picking default device: Apple M4
0.01.219.287 I ggml_metal_init: using embedded metal library
0.01.224.610 I ggml_metal_init: GPU name:   Apple M4
0.01.224.613 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.224.614 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.224.615 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.224.615 I ggml_metal_init: simdgroup reduction   = true
0.01.224.615 I ggml_metal_init: simdgroup matrix mul. = true
0.01.224.615 I ggml_metal_init: has residency sets    = true
0.01.224.616 I ggml_metal_init: has bfloat            = true
0.01.224.616 I ggml_metal_init: use bfloat            = true
0.01.224.617 I ggml_metal_init: hasUnifiedMemory      = true
0.01.224.618 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.240.122 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.293.253 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.293.262 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.293.304 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.297.925 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.297.927 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.297.927 I llama_init_from_model: graph nodes  = 967
0.01.297.928 I llama_init_from_model: graph splits = 2
0.01.297.932 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.298.045 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.298.045 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.355.944 I main: llama threadpool init, n_threads = 4
0.01.355.983 I 
0.01.356.005 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.356.005 I 
0.01.356.159 I sampler seed: 1234
0.01.356.163 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.356.203 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.356.204 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.356.204 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.443.146 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56483.69 tokens per second)
0.02.443.147 I llama_perf_context_print:        load time =    1345.24 ms
0.02.443.148 I llama_perf_context_print: prompt eval time =      49.57 ms /     7 tokens (    7.08 ms per token,   141.21 tokens per second)
0.02.443.148 I llama_perf_context_print:        eval time =    1034.50 ms /    63 runs   (   16.42 ms per token,    60.90 tokens per second)
0.02.443.149 I llama_perf_context_print:       total time =    1087.91 ms /    70 tokens
0.02.443.405 I ggml_metal_free: deallocating

real	0m2.464s
user	0m0.107s
sys	0m0.300s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.325 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.481 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.487 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.488 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.489 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.494 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.494 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.494 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.496 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.496 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.496 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.497 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.497 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.497 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.498 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.501 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.501 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.501 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.283 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.379 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.175 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.177 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.177 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.178 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.178 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.178 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.179 I llama_model_loader: - type  f32:  194 tensors
0.00.025.179 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.180 I print_info: file format = GGUF V3 (latest)
0.00.025.180 I print_info: file type   = Q8_0
0.00.025.181 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.107 I load: special tokens cache size = 25
0.00.039.037 I load: token to piece cache size = 0.2984 MB
0.00.039.041 I print_info: arch             = gptneox
0.00.039.041 I print_info: vocab_only       = 0
0.00.039.042 I print_info: n_ctx_train      = 2048
0.00.039.042 I print_info: n_embd           = 2048
0.00.039.042 I print_info: n_layer          = 24
0.00.039.046 I print_info: n_head           = 16
0.00.039.047 I print_info: n_head_kv        = 16
0.00.039.047 I print_info: n_rot            = 32
0.00.039.047 I print_info: n_swa            = 0
0.00.039.048 I print_info: n_embd_head_k    = 128
0.00.039.048 I print_info: n_embd_head_v    = 128
0.00.039.050 I print_info: n_gqa            = 1
0.00.039.051 I print_info: n_embd_k_gqa     = 2048
0.00.039.051 I print_info: n_embd_v_gqa     = 2048
0.00.039.052 I print_info: f_norm_eps       = 1.0e-05
0.00.039.052 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.052 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.053 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.054 I print_info: f_logit_scale    = 0.0e+00
0.00.039.055 I print_info: n_ff             = 8192
0.00.039.055 I print_info: n_expert         = 0
0.00.039.055 I print_info: n_expert_used    = 0
0.00.039.055 I print_info: causal attn      = 1
0.00.039.057 I print_info: pooling type     = 0
0.00.039.057 I print_info: rope type        = 2
0.00.039.057 I print_info: rope scaling     = linear
0.00.039.057 I print_info: freq_base_train  = 10000.0
0.00.039.058 I print_info: freq_scale_train = 1
0.00.039.059 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.059 I print_info: rope_finetuned   = unknown
0.00.039.059 I print_info: ssm_d_conv       = 0
0.00.039.059 I print_info: ssm_d_inner      = 0
0.00.039.059 I print_info: ssm_d_state      = 0
0.00.039.060 I print_info: ssm_dt_rank      = 0
0.00.039.061 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.061 I print_info: model type       = 1.4B
0.00.039.061 I print_info: model params     = 1.41 B
0.00.039.061 I print_info: general.name     = 1.4B
0.00.039.062 I print_info: vocab type       = BPE
0.00.039.062 I print_info: n_vocab          = 50304
0.00.039.062 I print_info: n_merges         = 50009
0.00.039.062 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.062 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.062 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.063 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.063 I print_info: LF token         = 187 ''
0.00.039.063 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.063 I print_info: max token length = 1024
0.00.039.064 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.935.185 I load_tensors: offloading 24 repeating layers to GPU
0.00.935.192 I load_tensors: offloading output layer to GPU
0.00.935.193 I load_tensors: offloaded 25/25 layers to GPU
0.00.935.222 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.935.224 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.936.566 I llama_init_from_model: n_seq_max     = 1
0.00.936.568 I llama_init_from_model: n_ctx         = 128
0.00.936.568 I llama_init_from_model: n_ctx_per_seq = 128
0.00.936.569 I llama_init_from_model: n_batch       = 128
0.00.936.569 I llama_init_from_model: n_ubatch      = 128
0.00.936.569 I llama_init_from_model: flash_attn    = 0
0.00.936.570 I llama_init_from_model: freq_base     = 10000.0
0.00.936.570 I llama_init_from_model: freq_scale    = 1
0.00.936.571 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.936.572 I ggml_metal_init: allocating
0.00.936.638 I ggml_metal_init: found device: Apple M4
0.00.936.649 I ggml_metal_init: picking default device: Apple M4
0.00.937.902 I ggml_metal_init: using embedded metal library
0.00.943.132 I ggml_metal_init: GPU name:   Apple M4
0.00.943.135 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.943.136 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.943.137 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.943.137 I ggml_metal_init: simdgroup reduction   = true
0.00.943.137 I ggml_metal_init: simdgroup matrix mul. = true
0.00.943.137 I ggml_metal_init: has residency sets    = true
0.00.943.138 I ggml_metal_init: has bfloat            = true
0.00.943.138 I ggml_metal_init: use bfloat            = true
0.00.943.139 I ggml_metal_init: hasUnifiedMemory      = true
0.00.943.140 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.957.916 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.960.669 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.960.676 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.960.733 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.963.729 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.963.730 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.963.731 I llama_init_from_model: graph nodes  = 967
0.00.963.731 I llama_init_from_model: graph splits = 2
0.00.963.734 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.963.734 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.989.645 I 
0.00.989.721 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.989.726 I perplexity: tokenizing the input ..
0.00.996.893 I perplexity: tokenization took 7.163 ms
0.00.996.904 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.121.746 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.123.080 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.123.102 I llama_perf_context_print:        load time =     980.31 ms
0.01.123.103 I llama_perf_context_print: prompt eval time =     123.87 ms /   128 tokens (    0.97 ms per token,  1033.32 tokens per second)
0.01.123.104 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.123.104 I llama_perf_context_print:       total time =     133.46 ms /   129 tokens
0.01.123.448 I ggml_metal_free: deallocating

real	0m1.139s
user	0m0.076s
sys	0m0.194s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.098 I main: llama backend init
0.00.000.101 I main: load the model and apply lora adapter, if any
0.00.015.854 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.277 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.037.282 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.283 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.284 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.285 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.285 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.285 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.286 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.287 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.289 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.289 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.290 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.290 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.291 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.292 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.293 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.293 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.227 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.705 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.770 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.048.772 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.773 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.773 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.773 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.774 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.048.774 I llama_model_loader: - type  f32:  194 tensors
0.00.048.775 I llama_model_loader: - type q4_0:   97 tensors
0.00.048.775 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.776 I print_info: file format = GGUF V3 (latest)
0.00.048.776 I print_info: file type   = Q4_0
0.00.048.779 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.060.967 I load: special tokens cache size = 25
0.00.071.324 I load: token to piece cache size = 0.2984 MB
0.00.071.329 I print_info: arch             = gptneox
0.00.071.330 I print_info: vocab_only       = 0
0.00.071.330 I print_info: n_ctx_train      = 2048
0.00.071.330 I print_info: n_embd           = 2048
0.00.071.331 I print_info: n_layer          = 24
0.00.071.336 I print_info: n_head           = 16
0.00.071.338 I print_info: n_head_kv        = 16
0.00.071.338 I print_info: n_rot            = 32
0.00.071.338 I print_info: n_swa            = 0
0.00.071.339 I print_info: n_embd_head_k    = 128
0.00.071.339 I print_info: n_embd_head_v    = 128
0.00.071.340 I print_info: n_gqa            = 1
0.00.071.342 I print_info: n_embd_k_gqa     = 2048
0.00.071.343 I print_info: n_embd_v_gqa     = 2048
0.00.071.344 I print_info: f_norm_eps       = 1.0e-05
0.00.071.344 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.071.345 I print_info: f_clamp_kqv      = 0.0e+00
0.00.071.346 I print_info: f_max_alibi_bias = 0.0e+00
0.00.071.349 I print_info: f_logit_scale    = 0.0e+00
0.00.071.350 I print_info: n_ff             = 8192
0.00.071.350 I print_info: n_expert         = 0
0.00.071.351 I print_info: n_expert_used    = 0
0.00.071.351 I print_info: causal attn      = 1
0.00.071.352 I print_info: pooling type     = 0
0.00.071.353 I print_info: rope type        = 2
0.00.071.353 I print_info: rope scaling     = linear
0.00.071.354 I print_info: freq_base_train  = 10000.0
0.00.071.354 I print_info: freq_scale_train = 1
0.00.071.354 I print_info: n_ctx_orig_yarn  = 2048
0.00.071.355 I print_info: rope_finetuned   = unknown
0.00.071.355 I print_info: ssm_d_conv       = 0
0.00.071.356 I print_info: ssm_d_inner      = 0
0.00.071.356 I print_info: ssm_d_state      = 0
0.00.071.356 I print_info: ssm_dt_rank      = 0
0.00.071.356 I print_info: ssm_dt_b_c_rms   = 0
0.00.071.357 I print_info: model type       = 1.4B
0.00.071.358 I print_info: model params     = 1.41 B
0.00.071.358 I print_info: general.name     = 1.4B
0.00.071.359 I print_info: vocab type       = BPE
0.00.071.359 I print_info: n_vocab          = 50304
0.00.071.360 I print_info: n_merges         = 50009
0.00.071.360 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.071.360 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.071.361 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.071.361 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.071.361 I print_info: LF token         = 187 ''
0.00.071.364 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.071.364 I print_info: max token length = 1024
0.00.071.365 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.638.942 I load_tensors: offloading 24 repeating layers to GPU
0.00.638.953 I load_tensors: offloading output layer to GPU
0.00.638.954 I load_tensors: offloaded 25/25 layers to GPU
0.00.638.989 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.638.994 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.640.628 I llama_init_from_model: n_seq_max     = 1
0.00.640.632 I llama_init_from_model: n_ctx         = 2048
0.00.640.632 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.640.633 I llama_init_from_model: n_batch       = 2048
0.00.640.633 I llama_init_from_model: n_ubatch      = 512
0.00.640.634 I llama_init_from_model: flash_attn    = 0
0.00.640.635 I llama_init_from_model: freq_base     = 10000.0
0.00.640.636 I llama_init_from_model: freq_scale    = 1
0.00.640.638 I ggml_metal_init: allocating
0.00.640.716 I ggml_metal_init: found device: Apple M4
0.00.640.730 I ggml_metal_init: picking default device: Apple M4
0.00.642.575 I ggml_metal_init: using embedded metal library
0.00.649.011 I ggml_metal_init: GPU name:   Apple M4
0.00.649.016 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.649.017 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.649.018 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.649.019 I ggml_metal_init: simdgroup reduction   = true
0.00.649.019 I ggml_metal_init: simdgroup matrix mul. = true
0.00.649.019 I ggml_metal_init: has residency sets    = true
0.00.649.020 I ggml_metal_init: has bfloat            = true
0.00.649.020 I ggml_metal_init: use bfloat            = true
0.00.649.021 I ggml_metal_init: hasUnifiedMemory      = true
0.00.649.023 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.668.331 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.720.679 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.720.685 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.720.720 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.725.461 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.725.464 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.725.464 I llama_init_from_model: graph nodes  = 967
0.00.725.464 I llama_init_from_model: graph splits = 2
0.00.725.469 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.725.597 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.725.598 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.782.168 I main: llama threadpool init, n_threads = 4
0.00.782.212 I 
0.00.782.236 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.782.237 I 
0.00.782.394 I sampler seed: 1234
0.00.782.398 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.782.419 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.782.420 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.782.420 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.464.609 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51598.84 tokens per second)
0.01.464.610 I llama_perf_context_print:        load time =     765.59 ms
0.01.464.611 I llama_perf_context_print: prompt eval time =      49.04 ms /     7 tokens (    7.01 ms per token,   142.73 tokens per second)
0.01.464.612 I llama_perf_context_print:        eval time =     630.29 ms /    63 runs   (   10.00 ms per token,    99.95 tokens per second)
0.01.464.613 I llama_perf_context_print:       total time =     683.16 ms /    70 tokens
0.01.464.850 I ggml_metal_free: deallocating

real	0m1.496s
user	0m0.124s
sys	0m0.203s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.836 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.033 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.039 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.041 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.041 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.042 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.042 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.042 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.043 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.043 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.044 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.044 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.044 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.045 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.045 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.047 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.047 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.048 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.757 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.889 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.611 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.613 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.613 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.614 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.614 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.614 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.615 I llama_model_loader: - type  f32:  194 tensors
0.00.025.615 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.615 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.616 I print_info: file format = GGUF V3 (latest)
0.00.025.617 I print_info: file type   = Q4_0
0.00.025.617 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.809 I load: special tokens cache size = 25
0.00.039.881 I load: token to piece cache size = 0.2984 MB
0.00.039.886 I print_info: arch             = gptneox
0.00.039.886 I print_info: vocab_only       = 0
0.00.039.887 I print_info: n_ctx_train      = 2048
0.00.039.887 I print_info: n_embd           = 2048
0.00.039.887 I print_info: n_layer          = 24
0.00.039.891 I print_info: n_head           = 16
0.00.039.892 I print_info: n_head_kv        = 16
0.00.039.892 I print_info: n_rot            = 32
0.00.039.895 I print_info: n_swa            = 0
0.00.039.895 I print_info: n_embd_head_k    = 128
0.00.039.895 I print_info: n_embd_head_v    = 128
0.00.039.896 I print_info: n_gqa            = 1
0.00.039.896 I print_info: n_embd_k_gqa     = 2048
0.00.039.897 I print_info: n_embd_v_gqa     = 2048
0.00.039.897 I print_info: f_norm_eps       = 1.0e-05
0.00.039.901 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.902 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.902 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.903 I print_info: f_logit_scale    = 0.0e+00
0.00.039.904 I print_info: n_ff             = 8192
0.00.039.904 I print_info: n_expert         = 0
0.00.039.904 I print_info: n_expert_used    = 0
0.00.039.904 I print_info: causal attn      = 1
0.00.039.905 I print_info: pooling type     = 0
0.00.039.905 I print_info: rope type        = 2
0.00.039.905 I print_info: rope scaling     = linear
0.00.039.905 I print_info: freq_base_train  = 10000.0
0.00.039.906 I print_info: freq_scale_train = 1
0.00.039.906 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.906 I print_info: rope_finetuned   = unknown
0.00.039.906 I print_info: ssm_d_conv       = 0
0.00.039.906 I print_info: ssm_d_inner      = 0
0.00.039.906 I print_info: ssm_d_state      = 0
0.00.039.906 I print_info: ssm_dt_rank      = 0
0.00.039.906 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.907 I print_info: model type       = 1.4B
0.00.039.907 I print_info: model params     = 1.41 B
0.00.039.907 I print_info: general.name     = 1.4B
0.00.039.908 I print_info: vocab type       = BPE
0.00.039.908 I print_info: n_vocab          = 50304
0.00.039.908 I print_info: n_merges         = 50009
0.00.039.908 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.908 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.908 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.908 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.909 I print_info: LF token         = 187 ''
0.00.039.909 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.910 I print_info: max token length = 1024
0.00.039.911 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.584.722 I load_tensors: offloading 24 repeating layers to GPU
0.00.584.737 I load_tensors: offloading output layer to GPU
0.00.584.738 I load_tensors: offloaded 25/25 layers to GPU
0.00.584.773 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.584.775 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.586.449 I llama_init_from_model: n_seq_max     = 1
0.00.586.452 I llama_init_from_model: n_ctx         = 128
0.00.586.453 I llama_init_from_model: n_ctx_per_seq = 128
0.00.586.453 I llama_init_from_model: n_batch       = 128
0.00.586.454 I llama_init_from_model: n_ubatch      = 128
0.00.586.454 I llama_init_from_model: flash_attn    = 0
0.00.586.456 I llama_init_from_model: freq_base     = 10000.0
0.00.586.457 I llama_init_from_model: freq_scale    = 1
0.00.586.458 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.586.460 I ggml_metal_init: allocating
0.00.586.545 I ggml_metal_init: found device: Apple M4
0.00.586.559 I ggml_metal_init: picking default device: Apple M4
0.00.588.327 I ggml_metal_init: using embedded metal library
0.00.595.156 I ggml_metal_init: GPU name:   Apple M4
0.00.595.167 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.595.168 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.595.168 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.595.169 I ggml_metal_init: simdgroup reduction   = true
0.00.595.170 I ggml_metal_init: simdgroup matrix mul. = true
0.00.595.170 I ggml_metal_init: has residency sets    = true
0.00.595.170 I ggml_metal_init: has bfloat            = true
0.00.595.170 I ggml_metal_init: use bfloat            = true
0.00.595.172 I ggml_metal_init: hasUnifiedMemory      = true
0.00.595.177 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.614.369 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.618.041 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.618.048 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.618.100 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.621.317 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.621.319 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.621.319 I llama_init_from_model: graph nodes  = 967
0.00.621.320 I llama_init_from_model: graph splits = 2
0.00.621.323 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.621.323 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.645.791 I 
0.00.645.866 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.645.874 I perplexity: tokenizing the input ..
0.00.652.522 I perplexity: tokenization took 6.646 ms
0.00.652.530 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.786.213 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.787.563 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.787.583 I llama_perf_context_print:        load time =     635.95 ms
0.00.787.584 I llama_perf_context_print: prompt eval time =     133.30 ms /   128 tokens (    1.04 ms per token,   960.24 tokens per second)
0.00.787.584 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.787.585 I llama_perf_context_print:       total time =     141.79 ms /   129 tokens
0.00.787.958 I ggml_metal_free: deallocating

real	0m0.804s
user	0m0.080s
sys	0m0.116s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.772 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.109 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.120 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.122 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.122 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.123 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.123 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.123 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.124 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.124 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.125 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.125 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.125 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.126 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.126 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.128 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.128 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.128 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.822 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.902 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.546 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.547 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.548 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.548 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.548 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.549 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.549 I llama_model_loader: - type  f32:  194 tensors
0.00.025.549 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.549 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.550 I print_info: file format = GGUF V3 (latest)
0.00.025.550 I print_info: file type   = Q4_1
0.00.025.551 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.346 I load: special tokens cache size = 25
0.00.039.228 I load: token to piece cache size = 0.2984 MB
0.00.039.231 I print_info: arch             = gptneox
0.00.039.231 I print_info: vocab_only       = 0
0.00.039.231 I print_info: n_ctx_train      = 2048
0.00.039.232 I print_info: n_embd           = 2048
0.00.039.232 I print_info: n_layer          = 24
0.00.039.235 I print_info: n_head           = 16
0.00.039.235 I print_info: n_head_kv        = 16
0.00.039.235 I print_info: n_rot            = 32
0.00.039.236 I print_info: n_swa            = 0
0.00.039.237 I print_info: n_embd_head_k    = 128
0.00.039.239 I print_info: n_embd_head_v    = 128
0.00.039.239 I print_info: n_gqa            = 1
0.00.039.240 I print_info: n_embd_k_gqa     = 2048
0.00.039.241 I print_info: n_embd_v_gqa     = 2048
0.00.039.241 I print_info: f_norm_eps       = 1.0e-05
0.00.039.242 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.247 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.247 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.247 I print_info: f_logit_scale    = 0.0e+00
0.00.039.248 I print_info: n_ff             = 8192
0.00.039.248 I print_info: n_expert         = 0
0.00.039.248 I print_info: n_expert_used    = 0
0.00.039.248 I print_info: causal attn      = 1
0.00.039.249 I print_info: pooling type     = 0
0.00.039.249 I print_info: rope type        = 2
0.00.039.249 I print_info: rope scaling     = linear
0.00.039.250 I print_info: freq_base_train  = 10000.0
0.00.039.250 I print_info: freq_scale_train = 1
0.00.039.250 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.250 I print_info: rope_finetuned   = unknown
0.00.039.250 I print_info: ssm_d_conv       = 0
0.00.039.254 I print_info: ssm_d_inner      = 0
0.00.039.254 I print_info: ssm_d_state      = 0
0.00.039.254 I print_info: ssm_dt_rank      = 0
0.00.039.254 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.254 I print_info: model type       = 1.4B
0.00.039.255 I print_info: model params     = 1.41 B
0.00.039.255 I print_info: general.name     = 1.4B
0.00.039.255 I print_info: vocab type       = BPE
0.00.039.255 I print_info: n_vocab          = 50304
0.00.039.255 I print_info: n_merges         = 50009
0.00.039.256 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.256 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.256 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.256 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.256 I print_info: LF token         = 187 ''
0.00.039.257 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.257 I print_info: max token length = 1024
0.00.039.257 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.657.218 I load_tensors: offloading 24 repeating layers to GPU
0.00.657.231 I load_tensors: offloading output layer to GPU
0.00.657.232 I load_tensors: offloaded 25/25 layers to GPU
0.00.657.260 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.657.262 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.658.754 I llama_init_from_model: n_seq_max     = 1
0.00.658.762 I llama_init_from_model: n_ctx         = 2048
0.00.658.763 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.658.764 I llama_init_from_model: n_batch       = 2048
0.00.658.764 I llama_init_from_model: n_ubatch      = 512
0.00.658.765 I llama_init_from_model: flash_attn    = 0
0.00.658.766 I llama_init_from_model: freq_base     = 10000.0
0.00.658.766 I llama_init_from_model: freq_scale    = 1
0.00.658.770 I ggml_metal_init: allocating
0.00.658.819 I ggml_metal_init: found device: Apple M4
0.00.658.832 I ggml_metal_init: picking default device: Apple M4
0.00.660.581 I ggml_metal_init: using embedded metal library
0.00.667.380 I ggml_metal_init: GPU name:   Apple M4
0.00.667.385 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.667.386 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.667.387 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.667.387 I ggml_metal_init: simdgroup reduction   = true
0.00.667.388 I ggml_metal_init: simdgroup matrix mul. = true
0.00.667.388 I ggml_metal_init: has residency sets    = true
0.00.667.388 I ggml_metal_init: has bfloat            = true
0.00.667.389 I ggml_metal_init: use bfloat            = true
0.00.667.390 I ggml_metal_init: hasUnifiedMemory      = true
0.00.667.391 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.686.101 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.742.407 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.742.413 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.742.445 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.746.519 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.746.521 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.746.521 I llama_init_from_model: graph nodes  = 967
0.00.746.521 I llama_init_from_model: graph splits = 2
0.00.746.527 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.746.652 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.746.653 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.802.946 I main: llama threadpool init, n_threads = 4
0.00.802.990 I 
0.00.803.015 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.803.016 I 
0.00.803.170 I sampler seed: 1234
0.00.803.175 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.803.205 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.803.206 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.803.207 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.530.597 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56618.82 tokens per second)
0.01.530.599 I llama_perf_context_print:        load time =     793.44 ms
0.01.530.599 I llama_perf_context_print: prompt eval time =      48.89 ms /     7 tokens (    6.98 ms per token,   143.18 tokens per second)
0.01.530.600 I llama_perf_context_print:        eval time =     675.78 ms /    63 runs   (   10.73 ms per token,    93.23 tokens per second)
0.01.530.600 I llama_perf_context_print:       total time =     728.39 ms /    70 tokens
0.01.530.903 I ggml_metal_free: deallocating

real	0m1.547s
user	0m0.108s
sys	0m0.209s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.930 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.115 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.121 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.127 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.128 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.128 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.128 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.129 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.130 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.130 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.130 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.131 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.131 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.133 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.133 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.135 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.135 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.135 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.906 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.119 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.861 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.863 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.863 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.864 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.864 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.864 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.865 I llama_model_loader: - type  f32:  194 tensors
0.00.024.865 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.866 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.866 I print_info: file format = GGUF V3 (latest)
0.00.024.867 I print_info: file type   = Q4_1
0.00.024.868 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.058 I load: special tokens cache size = 25
0.00.039.074 I load: token to piece cache size = 0.2984 MB
0.00.039.078 I print_info: arch             = gptneox
0.00.039.079 I print_info: vocab_only       = 0
0.00.039.079 I print_info: n_ctx_train      = 2048
0.00.039.079 I print_info: n_embd           = 2048
0.00.039.079 I print_info: n_layer          = 24
0.00.039.083 I print_info: n_head           = 16
0.00.039.084 I print_info: n_head_kv        = 16
0.00.039.084 I print_info: n_rot            = 32
0.00.039.084 I print_info: n_swa            = 0
0.00.039.085 I print_info: n_embd_head_k    = 128
0.00.039.085 I print_info: n_embd_head_v    = 128
0.00.039.086 I print_info: n_gqa            = 1
0.00.039.086 I print_info: n_embd_k_gqa     = 2048
0.00.039.087 I print_info: n_embd_v_gqa     = 2048
0.00.039.088 I print_info: f_norm_eps       = 1.0e-05
0.00.039.088 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.088 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.088 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.088 I print_info: f_logit_scale    = 0.0e+00
0.00.039.089 I print_info: n_ff             = 8192
0.00.039.089 I print_info: n_expert         = 0
0.00.039.089 I print_info: n_expert_used    = 0
0.00.039.090 I print_info: causal attn      = 1
0.00.039.090 I print_info: pooling type     = 0
0.00.039.090 I print_info: rope type        = 2
0.00.039.090 I print_info: rope scaling     = linear
0.00.039.090 I print_info: freq_base_train  = 10000.0
0.00.039.091 I print_info: freq_scale_train = 1
0.00.039.091 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.091 I print_info: rope_finetuned   = unknown
0.00.039.091 I print_info: ssm_d_conv       = 0
0.00.039.091 I print_info: ssm_d_inner      = 0
0.00.039.091 I print_info: ssm_d_state      = 0
0.00.039.092 I print_info: ssm_dt_rank      = 0
0.00.039.092 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.092 I print_info: model type       = 1.4B
0.00.039.093 I print_info: model params     = 1.41 B
0.00.039.093 I print_info: general.name     = 1.4B
0.00.039.093 I print_info: vocab type       = BPE
0.00.039.093 I print_info: n_vocab          = 50304
0.00.039.094 I print_info: n_merges         = 50009
0.00.039.094 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.094 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.094 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.095 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.095 I print_info: LF token         = 187 ''
0.00.039.095 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.095 I print_info: max token length = 1024
0.00.039.096 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.640.864 I load_tensors: offloading 24 repeating layers to GPU
0.00.640.879 I load_tensors: offloading output layer to GPU
0.00.640.880 I load_tensors: offloaded 25/25 layers to GPU
0.00.640.916 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.640.917 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.642.305 I llama_init_from_model: n_seq_max     = 1
0.00.642.312 I llama_init_from_model: n_ctx         = 128
0.00.642.312 I llama_init_from_model: n_ctx_per_seq = 128
0.00.642.313 I llama_init_from_model: n_batch       = 128
0.00.642.313 I llama_init_from_model: n_ubatch      = 128
0.00.642.313 I llama_init_from_model: flash_attn    = 0
0.00.642.315 I llama_init_from_model: freq_base     = 10000.0
0.00.642.316 I llama_init_from_model: freq_scale    = 1
0.00.642.316 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.642.321 I ggml_metal_init: allocating
0.00.642.388 I ggml_metal_init: found device: Apple M4
0.00.642.405 I ggml_metal_init: picking default device: Apple M4
0.00.644.589 I ggml_metal_init: using embedded metal library
0.00.651.711 I ggml_metal_init: GPU name:   Apple M4
0.00.651.720 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.651.721 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.651.721 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.651.722 I ggml_metal_init: simdgroup reduction   = true
0.00.651.722 I ggml_metal_init: simdgroup matrix mul. = true
0.00.651.723 I ggml_metal_init: has residency sets    = true
0.00.651.723 I ggml_metal_init: has bfloat            = true
0.00.651.723 I ggml_metal_init: use bfloat            = true
0.00.651.724 I ggml_metal_init: hasUnifiedMemory      = true
0.00.651.728 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.670.896 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.674.606 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.674.615 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.674.680 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.677.895 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.677.897 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.677.897 I llama_init_from_model: graph nodes  = 967
0.00.677.897 I llama_init_from_model: graph splits = 2
0.00.677.900 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.677.901 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.230 I 
0.00.705.313 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.325 I perplexity: tokenizing the input ..
0.00.712.908 I perplexity: tokenization took 7.58 ms
0.00.712.916 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.848.044 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.849.473 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.849.497 I llama_perf_context_print:        load time =     696.29 ms
0.00.849.498 I llama_perf_context_print: prompt eval time =     134.18 ms /   128 tokens (    1.05 ms per token,   953.97 tokens per second)
0.00.849.498 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.849.500 I llama_perf_context_print:       total time =     144.27 ms /   129 tokens
0.00.849.898 I ggml_metal_free: deallocating

real	0m0.864s
user	0m0.081s
sys	0m0.123s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.853 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.703 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.707 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.709 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.709 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.710 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.710 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.710 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.711 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.713 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.714 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.714 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.714 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.715 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.715 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.716 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.717 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.717 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.443 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.508 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.267 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.268 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.269 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.269 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.269 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.270 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.270 I llama_model_loader: - type  f32:  194 tensors
0.00.024.270 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.271 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.271 I print_info: file format = GGUF V3 (latest)
0.00.024.272 I print_info: file type   = Q5_0
0.00.024.273 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.031.987 I load: special tokens cache size = 25
0.00.037.753 I load: token to piece cache size = 0.2984 MB
0.00.037.756 I print_info: arch             = gptneox
0.00.037.756 I print_info: vocab_only       = 0
0.00.037.756 I print_info: n_ctx_train      = 2048
0.00.037.756 I print_info: n_embd           = 2048
0.00.037.756 I print_info: n_layer          = 24
0.00.037.759 I print_info: n_head           = 16
0.00.037.760 I print_info: n_head_kv        = 16
0.00.037.760 I print_info: n_rot            = 32
0.00.037.760 I print_info: n_swa            = 0
0.00.037.760 I print_info: n_embd_head_k    = 128
0.00.037.761 I print_info: n_embd_head_v    = 128
0.00.037.761 I print_info: n_gqa            = 1
0.00.037.762 I print_info: n_embd_k_gqa     = 2048
0.00.037.763 I print_info: n_embd_v_gqa     = 2048
0.00.037.763 I print_info: f_norm_eps       = 1.0e-05
0.00.037.764 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.764 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.764 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.764 I print_info: f_logit_scale    = 0.0e+00
0.00.037.765 I print_info: n_ff             = 8192
0.00.037.765 I print_info: n_expert         = 0
0.00.037.765 I print_info: n_expert_used    = 0
0.00.037.765 I print_info: causal attn      = 1
0.00.037.766 I print_info: pooling type     = 0
0.00.037.766 I print_info: rope type        = 2
0.00.037.766 I print_info: rope scaling     = linear
0.00.037.766 I print_info: freq_base_train  = 10000.0
0.00.037.768 I print_info: freq_scale_train = 1
0.00.037.768 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.768 I print_info: rope_finetuned   = unknown
0.00.037.768 I print_info: ssm_d_conv       = 0
0.00.037.769 I print_info: ssm_d_inner      = 0
0.00.037.769 I print_info: ssm_d_state      = 0
0.00.037.775 I print_info: ssm_dt_rank      = 0
0.00.037.775 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.776 I print_info: model type       = 1.4B
0.00.037.776 I print_info: model params     = 1.41 B
0.00.037.778 I print_info: general.name     = 1.4B
0.00.037.778 I print_info: vocab type       = BPE
0.00.037.779 I print_info: n_vocab          = 50304
0.00.037.779 I print_info: n_merges         = 50009
0.00.037.779 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.779 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.779 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.779 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.780 I print_info: LF token         = 187 ''
0.00.037.780 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.780 I print_info: max token length = 1024
0.00.037.781 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.714.867 I load_tensors: offloading 24 repeating layers to GPU
0.00.714.883 I load_tensors: offloading output layer to GPU
0.00.714.884 I load_tensors: offloaded 25/25 layers to GPU
0.00.714.918 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.714.920 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.716.392 I llama_init_from_model: n_seq_max     = 1
0.00.716.398 I llama_init_from_model: n_ctx         = 2048
0.00.716.399 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.716.399 I llama_init_from_model: n_batch       = 2048
0.00.716.400 I llama_init_from_model: n_ubatch      = 512
0.00.716.400 I llama_init_from_model: flash_attn    = 0
0.00.716.402 I llama_init_from_model: freq_base     = 10000.0
0.00.716.402 I llama_init_from_model: freq_scale    = 1
0.00.716.404 I ggml_metal_init: allocating
0.00.716.498 I ggml_metal_init: found device: Apple M4
0.00.716.513 I ggml_metal_init: picking default device: Apple M4
0.00.718.214 I ggml_metal_init: using embedded metal library
0.00.723.487 I ggml_metal_init: GPU name:   Apple M4
0.00.723.496 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.723.496 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.723.497 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.723.497 I ggml_metal_init: simdgroup reduction   = true
0.00.723.497 I ggml_metal_init: simdgroup matrix mul. = true
0.00.723.498 I ggml_metal_init: has residency sets    = true
0.00.723.498 I ggml_metal_init: has bfloat            = true
0.00.723.498 I ggml_metal_init: use bfloat            = true
0.00.723.499 I ggml_metal_init: hasUnifiedMemory      = true
0.00.723.509 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.735.597 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.767.764 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.767.771 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.767.813 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.772.290 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.772.292 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.772.292 I llama_init_from_model: graph nodes  = 967
0.00.772.293 I llama_init_from_model: graph splits = 2
0.00.772.303 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.772.426 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.772.426 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.827.606 I main: llama threadpool init, n_threads = 4
0.00.827.653 I 
0.00.827.679 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.827.680 I 
0.00.827.852 I sampler seed: 1234
0.00.827.856 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.827.877 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.827.877 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.827.877 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.617.685 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52090.98 tokens per second)
0.01.617.685 I llama_perf_context_print:        load time =     817.99 ms
0.01.617.686 I llama_perf_context_print: prompt eval time =      42.89 ms /     7 tokens (    6.13 ms per token,   163.19 tokens per second)
0.01.617.687 I llama_perf_context_print:        eval time =     744.00 ms /    63 runs   (   11.81 ms per token,    84.68 tokens per second)
0.01.617.687 I llama_perf_context_print:       total time =     790.84 ms /    70 tokens
0.01.617.950 I ggml_metal_free: deallocating

real	0m1.635s
user	0m0.101s
sys	0m0.191s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.114 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.964 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.809 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.814 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.816 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.816 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.817 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.817 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.817 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.818 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.819 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.819 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.820 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.820 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.820 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.821 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.823 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.823 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.823 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.502 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.637 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.333 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.334 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.335 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.335 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.335 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.336 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.336 I llama_model_loader: - type  f32:  194 tensors
0.00.024.337 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.337 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.337 I print_info: file format = GGUF V3 (latest)
0.00.024.338 I print_info: file type   = Q5_0
0.00.024.339 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.402 I load: special tokens cache size = 25
0.00.038.432 I load: token to piece cache size = 0.2984 MB
0.00.038.436 I print_info: arch             = gptneox
0.00.038.436 I print_info: vocab_only       = 0
0.00.038.437 I print_info: n_ctx_train      = 2048
0.00.038.437 I print_info: n_embd           = 2048
0.00.038.437 I print_info: n_layer          = 24
0.00.038.440 I print_info: n_head           = 16
0.00.038.441 I print_info: n_head_kv        = 16
0.00.038.441 I print_info: n_rot            = 32
0.00.038.441 I print_info: n_swa            = 0
0.00.038.441 I print_info: n_embd_head_k    = 128
0.00.038.442 I print_info: n_embd_head_v    = 128
0.00.038.442 I print_info: n_gqa            = 1
0.00.038.443 I print_info: n_embd_k_gqa     = 2048
0.00.038.444 I print_info: n_embd_v_gqa     = 2048
0.00.038.444 I print_info: f_norm_eps       = 1.0e-05
0.00.038.448 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.448 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.448 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.448 I print_info: f_logit_scale    = 0.0e+00
0.00.038.449 I print_info: n_ff             = 8192
0.00.038.449 I print_info: n_expert         = 0
0.00.038.449 I print_info: n_expert_used    = 0
0.00.038.449 I print_info: causal attn      = 1
0.00.038.449 I print_info: pooling type     = 0
0.00.038.449 I print_info: rope type        = 2
0.00.038.450 I print_info: rope scaling     = linear
0.00.038.450 I print_info: freq_base_train  = 10000.0
0.00.038.450 I print_info: freq_scale_train = 1
0.00.038.450 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.450 I print_info: rope_finetuned   = unknown
0.00.038.451 I print_info: ssm_d_conv       = 0
0.00.038.452 I print_info: ssm_d_inner      = 0
0.00.038.452 I print_info: ssm_d_state      = 0
0.00.038.452 I print_info: ssm_dt_rank      = 0
0.00.038.452 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.452 I print_info: model type       = 1.4B
0.00.038.453 I print_info: model params     = 1.41 B
0.00.038.453 I print_info: general.name     = 1.4B
0.00.038.453 I print_info: vocab type       = BPE
0.00.038.453 I print_info: n_vocab          = 50304
0.00.038.454 I print_info: n_merges         = 50009
0.00.038.454 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.454 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.454 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.454 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.454 I print_info: LF token         = 187 ''
0.00.038.455 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.456 I print_info: max token length = 1024
0.00.038.456 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.714.665 I load_tensors: offloading 24 repeating layers to GPU
0.00.714.673 I load_tensors: offloading output layer to GPU
0.00.714.674 I load_tensors: offloaded 25/25 layers to GPU
0.00.714.703 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.714.707 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.716.012 I llama_init_from_model: n_seq_max     = 1
0.00.716.020 I llama_init_from_model: n_ctx         = 128
0.00.716.021 I llama_init_from_model: n_ctx_per_seq = 128
0.00.716.021 I llama_init_from_model: n_batch       = 128
0.00.716.021 I llama_init_from_model: n_ubatch      = 128
0.00.716.022 I llama_init_from_model: flash_attn    = 0
0.00.716.025 I llama_init_from_model: freq_base     = 10000.0
0.00.716.025 I llama_init_from_model: freq_scale    = 1
0.00.716.026 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.716.029 I ggml_metal_init: allocating
0.00.716.113 I ggml_metal_init: found device: Apple M4
0.00.716.128 I ggml_metal_init: picking default device: Apple M4
0.00.717.763 I ggml_metal_init: using embedded metal library
0.00.723.279 I ggml_metal_init: GPU name:   Apple M4
0.00.723.285 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.723.286 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.723.286 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.723.287 I ggml_metal_init: simdgroup reduction   = true
0.00.723.287 I ggml_metal_init: simdgroup matrix mul. = true
0.00.723.287 I ggml_metal_init: has residency sets    = true
0.00.723.287 I ggml_metal_init: has bfloat            = true
0.00.723.287 I ggml_metal_init: use bfloat            = true
0.00.723.288 I ggml_metal_init: hasUnifiedMemory      = true
0.00.723.292 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.738.213 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.740.201 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.740.204 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.740.236 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.742.215 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.742.216 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.742.217 I llama_init_from_model: graph nodes  = 967
0.00.742.217 I llama_init_from_model: graph splits = 2
0.00.742.218 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.742.219 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.772.136 I 
0.00.772.176 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.772.182 I perplexity: tokenizing the input ..
0.00.776.037 I perplexity: tokenization took 3.853 ms
0.00.776.040 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.922.108 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.923.591 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.923.616 I llama_perf_context_print:        load time =     763.17 ms
0.00.923.617 I llama_perf_context_print: prompt eval time =     145.84 ms /   128 tokens (    1.14 ms per token,   877.70 tokens per second)
0.00.923.618 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.923.618 I llama_perf_context_print:       total time =     151.48 ms /   129 tokens
0.00.923.966 I ggml_metal_free: deallocating

real	0m0.938s
user	0m0.070s
sys	0m0.132s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.980 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.754 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.759 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.760 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.761 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.761 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.761 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.761 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.762 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.763 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.763 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.763 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.764 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.764 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.765 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.766 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.766 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.766 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.410 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.405 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.066 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.067 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.068 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.068 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.068 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.069 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.069 I llama_model_loader: - type  f32:  194 tensors
0.00.025.069 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.070 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.070 I print_info: file format = GGUF V3 (latest)
0.00.025.071 I print_info: file type   = Q5_1
0.00.025.071 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.826 I load: special tokens cache size = 25
0.00.038.814 I load: token to piece cache size = 0.2984 MB
0.00.038.817 I print_info: arch             = gptneox
0.00.038.817 I print_info: vocab_only       = 0
0.00.038.818 I print_info: n_ctx_train      = 2048
0.00.038.818 I print_info: n_embd           = 2048
0.00.038.818 I print_info: n_layer          = 24
0.00.038.821 I print_info: n_head           = 16
0.00.038.821 I print_info: n_head_kv        = 16
0.00.038.821 I print_info: n_rot            = 32
0.00.038.822 I print_info: n_swa            = 0
0.00.038.822 I print_info: n_embd_head_k    = 128
0.00.038.822 I print_info: n_embd_head_v    = 128
0.00.038.824 I print_info: n_gqa            = 1
0.00.038.825 I print_info: n_embd_k_gqa     = 2048
0.00.038.826 I print_info: n_embd_v_gqa     = 2048
0.00.038.826 I print_info: f_norm_eps       = 1.0e-05
0.00.038.827 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.827 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.827 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.827 I print_info: f_logit_scale    = 0.0e+00
0.00.038.828 I print_info: n_ff             = 8192
0.00.038.828 I print_info: n_expert         = 0
0.00.038.828 I print_info: n_expert_used    = 0
0.00.038.828 I print_info: causal attn      = 1
0.00.038.828 I print_info: pooling type     = 0
0.00.038.828 I print_info: rope type        = 2
0.00.038.829 I print_info: rope scaling     = linear
0.00.038.829 I print_info: freq_base_train  = 10000.0
0.00.038.831 I print_info: freq_scale_train = 1
0.00.038.831 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.831 I print_info: rope_finetuned   = unknown
0.00.038.831 I print_info: ssm_d_conv       = 0
0.00.038.832 I print_info: ssm_d_inner      = 0
0.00.038.833 I print_info: ssm_d_state      = 0
0.00.038.833 I print_info: ssm_dt_rank      = 0
0.00.038.833 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.833 I print_info: model type       = 1.4B
0.00.038.834 I print_info: model params     = 1.41 B
0.00.038.834 I print_info: general.name     = 1.4B
0.00.038.834 I print_info: vocab type       = BPE
0.00.038.835 I print_info: n_vocab          = 50304
0.00.038.835 I print_info: n_merges         = 50009
0.00.038.835 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.835 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.835 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.836 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.836 I print_info: LF token         = 187 ''
0.00.038.836 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.836 I print_info: max token length = 1024
0.00.038.840 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.639.914 I load_tensors: offloading 24 repeating layers to GPU
0.00.639.926 I load_tensors: offloading output layer to GPU
0.00.639.927 I load_tensors: offloaded 25/25 layers to GPU
0.00.639.964 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.639.965 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.641.629 I llama_init_from_model: n_seq_max     = 1
0.00.641.632 I llama_init_from_model: n_ctx         = 2048
0.00.641.632 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.641.633 I llama_init_from_model: n_batch       = 2048
0.00.641.633 I llama_init_from_model: n_ubatch      = 512
0.00.641.634 I llama_init_from_model: flash_attn    = 0
0.00.641.635 I llama_init_from_model: freq_base     = 10000.0
0.00.641.636 I llama_init_from_model: freq_scale    = 1
0.00.641.637 I ggml_metal_init: allocating
0.00.641.658 I ggml_metal_init: found device: Apple M4
0.00.641.666 I ggml_metal_init: picking default device: Apple M4
0.00.643.182 I ggml_metal_init: using embedded metal library
0.00.649.581 I ggml_metal_init: GPU name:   Apple M4
0.00.649.584 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.649.585 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.649.586 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.649.586 I ggml_metal_init: simdgroup reduction   = true
0.00.649.587 I ggml_metal_init: simdgroup matrix mul. = true
0.00.649.587 I ggml_metal_init: has residency sets    = true
0.00.649.587 I ggml_metal_init: has bfloat            = true
0.00.649.587 I ggml_metal_init: use bfloat            = true
0.00.649.588 I ggml_metal_init: hasUnifiedMemory      = true
0.00.649.589 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.666.621 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.720.259 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.720.267 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.720.302 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.724.553 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.724.555 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.724.555 I llama_init_from_model: graph nodes  = 967
0.00.724.555 I llama_init_from_model: graph splits = 2
0.00.724.563 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.724.696 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.724.696 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.783.768 I main: llama threadpool init, n_threads = 4
0.00.783.814 I 
0.00.783.841 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.783.841 I 
0.00.784.011 I sampler seed: 1234
0.00.784.016 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.784.028 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.784.028 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.784.028 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.636.540 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50000.00 tokens per second)
0.01.636.541 I llama_perf_context_print:        load time =     773.07 ms
0.01.636.542 I llama_perf_context_print: prompt eval time =      50.30 ms /     7 tokens (    7.19 ms per token,   139.15 tokens per second)
0.01.636.542 I llama_perf_context_print:        eval time =     799.15 ms /    63 runs   (   12.68 ms per token,    78.83 tokens per second)
0.01.636.543 I llama_perf_context_print:       total time =     853.49 ms /    70 tokens
0.01.636.768 I ggml_metal_free: deallocating

real	0m1.656s
user	0m0.109s
sys	0m0.226s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.862 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.737 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.743 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.750 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.750 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.751 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.751 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.752 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.752 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.753 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.753 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.754 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.754 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.754 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.755 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.757 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.757 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.757 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.570 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.603 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.226 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.227 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.228 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.228 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.228 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.229 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.229 I llama_model_loader: - type  f32:  194 tensors
0.00.025.230 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.230 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.231 I print_info: file format = GGUF V3 (latest)
0.00.025.231 I print_info: file type   = Q5_1
0.00.025.233 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.573 I load: special tokens cache size = 25
0.00.039.315 I load: token to piece cache size = 0.2984 MB
0.00.039.319 I print_info: arch             = gptneox
0.00.039.320 I print_info: vocab_only       = 0
0.00.039.320 I print_info: n_ctx_train      = 2048
0.00.039.320 I print_info: n_embd           = 2048
0.00.039.320 I print_info: n_layer          = 24
0.00.039.324 I print_info: n_head           = 16
0.00.039.325 I print_info: n_head_kv        = 16
0.00.039.325 I print_info: n_rot            = 32
0.00.039.325 I print_info: n_swa            = 0
0.00.039.329 I print_info: n_embd_head_k    = 128
0.00.039.329 I print_info: n_embd_head_v    = 128
0.00.039.341 I print_info: n_gqa            = 1
0.00.039.346 I print_info: n_embd_k_gqa     = 2048
0.00.039.347 I print_info: n_embd_v_gqa     = 2048
0.00.039.348 I print_info: f_norm_eps       = 1.0e-05
0.00.039.348 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.348 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.348 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.348 I print_info: f_logit_scale    = 0.0e+00
0.00.039.349 I print_info: n_ff             = 8192
0.00.039.349 I print_info: n_expert         = 0
0.00.039.349 I print_info: n_expert_used    = 0
0.00.039.349 I print_info: causal attn      = 1
0.00.039.350 I print_info: pooling type     = 0
0.00.039.350 I print_info: rope type        = 2
0.00.039.350 I print_info: rope scaling     = linear
0.00.039.350 I print_info: freq_base_train  = 10000.0
0.00.039.350 I print_info: freq_scale_train = 1
0.00.039.351 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.351 I print_info: rope_finetuned   = unknown
0.00.039.351 I print_info: ssm_d_conv       = 0
0.00.039.351 I print_info: ssm_d_inner      = 0
0.00.039.351 I print_info: ssm_d_state      = 0
0.00.039.351 I print_info: ssm_dt_rank      = 0
0.00.039.351 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.352 I print_info: model type       = 1.4B
0.00.039.352 I print_info: model params     = 1.41 B
0.00.039.352 I print_info: general.name     = 1.4B
0.00.039.353 I print_info: vocab type       = BPE
0.00.039.353 I print_info: n_vocab          = 50304
0.00.039.353 I print_info: n_merges         = 50009
0.00.039.353 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.353 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.354 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.354 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.354 I print_info: LF token         = 187 ''
0.00.039.354 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.354 I print_info: max token length = 1024
0.00.039.355 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.637.159 I load_tensors: offloading 24 repeating layers to GPU
0.00.637.171 I load_tensors: offloading output layer to GPU
0.00.637.172 I load_tensors: offloaded 25/25 layers to GPU
0.00.637.200 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.637.202 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.638.102 I llama_init_from_model: n_seq_max     = 1
0.00.638.106 I llama_init_from_model: n_ctx         = 128
0.00.638.107 I llama_init_from_model: n_ctx_per_seq = 128
0.00.638.107 I llama_init_from_model: n_batch       = 128
0.00.638.108 I llama_init_from_model: n_ubatch      = 128
0.00.638.108 I llama_init_from_model: flash_attn    = 0
0.00.638.109 I llama_init_from_model: freq_base     = 10000.0
0.00.638.110 I llama_init_from_model: freq_scale    = 1
0.00.638.110 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.638.112 I ggml_metal_init: allocating
0.00.638.177 I ggml_metal_init: found device: Apple M4
0.00.638.188 I ggml_metal_init: picking default device: Apple M4
0.00.639.538 I ggml_metal_init: using embedded metal library
0.00.642.499 I ggml_metal_init: GPU name:   Apple M4
0.00.642.501 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.642.502 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.642.502 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.642.502 I ggml_metal_init: simdgroup reduction   = true
0.00.642.503 I ggml_metal_init: simdgroup matrix mul. = true
0.00.642.503 I ggml_metal_init: has residency sets    = true
0.00.642.503 I ggml_metal_init: has bfloat            = true
0.00.642.503 I ggml_metal_init: use bfloat            = true
0.00.642.503 I ggml_metal_init: hasUnifiedMemory      = true
0.00.642.505 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.653.011 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.654.667 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.654.670 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.654.729 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.656.389 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.656.390 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.656.391 I llama_init_from_model: graph nodes  = 967
0.00.656.391 I llama_init_from_model: graph splits = 2
0.00.656.392 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.656.392 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.113 I 
0.00.684.149 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.152 I perplexity: tokenizing the input ..
0.00.688.090 I perplexity: tokenization took 3.936 ms
0.00.688.098 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.834.718 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.836.275 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.836.301 I llama_perf_context_print:        load time =     674.25 ms
0.00.836.302 I llama_perf_context_print: prompt eval time =     146.39 ms /   128 tokens (    1.14 ms per token,   874.40 tokens per second)
0.00.836.305 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.836.305 I llama_perf_context_print:       total time =     152.19 ms /   129 tokens
0.00.836.643 I ggml_metal_free: deallocating

real	0m0.853s
user	0m0.065s
sys	0m0.123s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.801 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.344 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.349 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.350 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.351 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.351 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.351 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.352 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.353 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.353 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.354 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.355 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.355 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.356 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.356 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.357 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.358 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.358 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.054 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.182 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.867 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.868 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.868 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.869 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.869 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.869 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.869 I llama_model_loader: - type  f32:  194 tensors
0.00.023.870 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.870 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.870 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.870 I print_info: file format = GGUF V3 (latest)
0.00.023.871 I print_info: file type   = Q2_K - Medium
0.00.023.872 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.031.665 I load: special tokens cache size = 25
0.00.037.352 I load: token to piece cache size = 0.2984 MB
0.00.037.355 I print_info: arch             = gptneox
0.00.037.355 I print_info: vocab_only       = 0
0.00.037.355 I print_info: n_ctx_train      = 2048
0.00.037.356 I print_info: n_embd           = 2048
0.00.037.356 I print_info: n_layer          = 24
0.00.037.359 I print_info: n_head           = 16
0.00.037.359 I print_info: n_head_kv        = 16
0.00.037.360 I print_info: n_rot            = 32
0.00.037.360 I print_info: n_swa            = 0
0.00.037.362 I print_info: n_embd_head_k    = 128
0.00.037.362 I print_info: n_embd_head_v    = 128
0.00.037.363 I print_info: n_gqa            = 1
0.00.037.363 I print_info: n_embd_k_gqa     = 2048
0.00.037.364 I print_info: n_embd_v_gqa     = 2048
0.00.037.365 I print_info: f_norm_eps       = 1.0e-05
0.00.037.365 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.365 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.365 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.366 I print_info: f_logit_scale    = 0.0e+00
0.00.037.366 I print_info: n_ff             = 8192
0.00.037.367 I print_info: n_expert         = 0
0.00.037.367 I print_info: n_expert_used    = 0
0.00.037.367 I print_info: causal attn      = 1
0.00.037.367 I print_info: pooling type     = 0
0.00.037.367 I print_info: rope type        = 2
0.00.037.367 I print_info: rope scaling     = linear
0.00.037.368 I print_info: freq_base_train  = 10000.0
0.00.037.368 I print_info: freq_scale_train = 1
0.00.037.368 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.369 I print_info: rope_finetuned   = unknown
0.00.037.369 I print_info: ssm_d_conv       = 0
0.00.037.369 I print_info: ssm_d_inner      = 0
0.00.037.369 I print_info: ssm_d_state      = 0
0.00.037.369 I print_info: ssm_dt_rank      = 0
0.00.037.369 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.370 I print_info: model type       = 1.4B
0.00.037.370 I print_info: model params     = 1.41 B
0.00.037.370 I print_info: general.name     = 1.4B
0.00.037.370 I print_info: vocab type       = BPE
0.00.037.371 I print_info: n_vocab          = 50304
0.00.037.371 I print_info: n_merges         = 50009
0.00.037.372 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.372 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.372 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.372 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.372 I print_info: LF token         = 187 ''
0.00.037.373 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.373 I print_info: max token length = 1024
0.00.037.373 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.345.656 I load_tensors: offloading 24 repeating layers to GPU
0.00.345.672 I load_tensors: offloading output layer to GPU
0.00.345.673 I load_tensors: offloaded 25/25 layers to GPU
0.00.345.707 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.345.709 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.347.356 I llama_init_from_model: n_seq_max     = 1
0.00.347.359 I llama_init_from_model: n_ctx         = 2048
0.00.347.360 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.347.360 I llama_init_from_model: n_batch       = 2048
0.00.347.361 I llama_init_from_model: n_ubatch      = 512
0.00.347.361 I llama_init_from_model: flash_attn    = 0
0.00.347.363 I llama_init_from_model: freq_base     = 10000.0
0.00.347.363 I llama_init_from_model: freq_scale    = 1
0.00.347.365 I ggml_metal_init: allocating
0.00.347.443 I ggml_metal_init: found device: Apple M4
0.00.347.456 I ggml_metal_init: picking default device: Apple M4
0.00.349.295 I ggml_metal_init: using embedded metal library
0.00.354.997 I ggml_metal_init: GPU name:   Apple M4
0.00.355.013 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.355.014 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.355.015 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.355.016 I ggml_metal_init: simdgroup reduction   = true
0.00.355.016 I ggml_metal_init: simdgroup matrix mul. = true
0.00.355.017 I ggml_metal_init: has residency sets    = true
0.00.355.017 I ggml_metal_init: has bfloat            = true
0.00.355.017 I ggml_metal_init: use bfloat            = true
0.00.355.019 I ggml_metal_init: hasUnifiedMemory      = true
0.00.355.024 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.376.685 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.437.063 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.437.071 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.437.113 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.441.443 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.441.444 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.441.445 I llama_init_from_model: graph nodes  = 967
0.00.441.445 I llama_init_from_model: graph splits = 2
0.00.441.449 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.441.567 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.441.568 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.500.025 I main: llama threadpool init, n_threads = 4
0.00.500.071 I 
0.00.500.095 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.500.095 I 
0.00.500.259 I sampler seed: 1234
0.00.500.264 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.500.307 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.500.311 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.500.311 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.170.383 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53463.86 tokens per second)
0.01.170.383 I llama_perf_context_print:        load time =     490.51 ms
0.01.170.384 I llama_perf_context_print: prompt eval time =      35.75 ms /     7 tokens (    5.11 ms per token,   195.78 tokens per second)
0.01.170.385 I llama_perf_context_print:        eval time =     631.54 ms /    63 runs   (   10.02 ms per token,    99.76 tokens per second)
0.01.170.387 I llama_perf_context_print:       total time =     671.07 ms /    70 tokens
0.01.170.619 I ggml_metal_free: deallocating

real	0m1.193s
user	0m0.112s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.116 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.130 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.127 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.133 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.135 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.135 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.136 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.136 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.136 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.137 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.138 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.138 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.138 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.139 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.139 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.139 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.141 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.141 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.142 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.841 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.925 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.710 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.711 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.712 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.712 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.712 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.713 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.713 I llama_model_loader: - type  f32:  194 tensors
0.00.024.714 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.714 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.714 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.715 I print_info: file format = GGUF V3 (latest)
0.00.024.715 I print_info: file type   = Q2_K - Medium
0.00.024.716 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.552 I load: special tokens cache size = 25
0.00.038.505 I load: token to piece cache size = 0.2984 MB
0.00.038.509 I print_info: arch             = gptneox
0.00.038.510 I print_info: vocab_only       = 0
0.00.038.510 I print_info: n_ctx_train      = 2048
0.00.038.510 I print_info: n_embd           = 2048
0.00.038.510 I print_info: n_layer          = 24
0.00.038.515 I print_info: n_head           = 16
0.00.038.516 I print_info: n_head_kv        = 16
0.00.038.516 I print_info: n_rot            = 32
0.00.038.516 I print_info: n_swa            = 0
0.00.038.516 I print_info: n_embd_head_k    = 128
0.00.038.516 I print_info: n_embd_head_v    = 128
0.00.038.520 I print_info: n_gqa            = 1
0.00.038.521 I print_info: n_embd_k_gqa     = 2048
0.00.038.521 I print_info: n_embd_v_gqa     = 2048
0.00.038.522 I print_info: f_norm_eps       = 1.0e-05
0.00.038.522 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.522 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.526 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.527 I print_info: f_logit_scale    = 0.0e+00
0.00.038.528 I print_info: n_ff             = 8192
0.00.038.528 I print_info: n_expert         = 0
0.00.038.528 I print_info: n_expert_used    = 0
0.00.038.528 I print_info: causal attn      = 1
0.00.038.528 I print_info: pooling type     = 0
0.00.038.528 I print_info: rope type        = 2
0.00.038.529 I print_info: rope scaling     = linear
0.00.038.529 I print_info: freq_base_train  = 10000.0
0.00.038.532 I print_info: freq_scale_train = 1
0.00.038.532 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.532 I print_info: rope_finetuned   = unknown
0.00.038.533 I print_info: ssm_d_conv       = 0
0.00.038.533 I print_info: ssm_d_inner      = 0
0.00.038.533 I print_info: ssm_d_state      = 0
0.00.038.533 I print_info: ssm_dt_rank      = 0
0.00.038.533 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.534 I print_info: model type       = 1.4B
0.00.038.534 I print_info: model params     = 1.41 B
0.00.038.534 I print_info: general.name     = 1.4B
0.00.038.535 I print_info: vocab type       = BPE
0.00.038.535 I print_info: n_vocab          = 50304
0.00.038.535 I print_info: n_merges         = 50009
0.00.038.535 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.535 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.536 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.537 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.537 I print_info: LF token         = 187 ''
0.00.038.537 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.537 I print_info: max token length = 1024
0.00.038.538 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.348.142 I load_tensors: offloading 24 repeating layers to GPU
0.00.348.150 I load_tensors: offloading output layer to GPU
0.00.348.150 I load_tensors: offloaded 25/25 layers to GPU
0.00.348.170 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.348.171 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.349.071 I llama_init_from_model: n_seq_max     = 1
0.00.349.076 I llama_init_from_model: n_ctx         = 128
0.00.349.076 I llama_init_from_model: n_ctx_per_seq = 128
0.00.349.076 I llama_init_from_model: n_batch       = 128
0.00.349.077 I llama_init_from_model: n_ubatch      = 128
0.00.349.077 I llama_init_from_model: flash_attn    = 0
0.00.349.079 I llama_init_from_model: freq_base     = 10000.0
0.00.349.079 I llama_init_from_model: freq_scale    = 1
0.00.349.080 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.349.081 I ggml_metal_init: allocating
0.00.349.131 I ggml_metal_init: found device: Apple M4
0.00.349.143 I ggml_metal_init: picking default device: Apple M4
0.00.350.292 I ggml_metal_init: using embedded metal library
0.00.354.595 I ggml_metal_init: GPU name:   Apple M4
0.00.354.603 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.354.604 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.354.605 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.354.605 I ggml_metal_init: simdgroup reduction   = true
0.00.354.605 I ggml_metal_init: simdgroup matrix mul. = true
0.00.354.606 I ggml_metal_init: has residency sets    = true
0.00.354.606 I ggml_metal_init: has bfloat            = true
0.00.354.606 I ggml_metal_init: use bfloat            = true
0.00.354.607 I ggml_metal_init: hasUnifiedMemory      = true
0.00.354.610 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.372.385 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.374.007 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.374.010 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.374.037 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.375.567 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.375.568 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.375.568 I llama_init_from_model: graph nodes  = 967
0.00.375.569 I llama_init_from_model: graph splits = 2
0.00.375.570 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.375.570 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.401.859 I 
0.00.401.896 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.401.899 I perplexity: tokenizing the input ..
0.00.406.157 I perplexity: tokenization took 4.256 ms
0.00.406.163 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.537.203 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.538.646 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.538.670 I llama_perf_context_print:        load time =     392.73 ms
0.00.538.671 I llama_perf_context_print: prompt eval time =     130.81 ms /   128 tokens (    1.02 ms per token,   978.50 tokens per second)
0.00.538.671 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.538.672 I llama_perf_context_print:       total time =     136.81 ms /   129 tokens
0.00.539.001 I ggml_metal_free: deallocating

real	0m0.555s
user	0m0.071s
sys	0m0.069s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.010.123 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.780 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.785 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.791 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.791 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.793 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.794 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.794 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.795 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.795 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.796 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.796 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.799 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.800 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.800 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.801 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.802 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.802 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.606 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.665 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.405 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.406 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.406 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.407 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.407 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.407 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.408 I llama_model_loader: - type  f32:  194 tensors
0.00.025.408 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.408 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.409 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.409 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.409 I print_info: file format = GGUF V3 (latest)
0.00.025.410 I print_info: file type   = Q3_K - Medium
0.00.025.411 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.504 I load: special tokens cache size = 25
0.00.039.479 I load: token to piece cache size = 0.2984 MB
0.00.039.483 I print_info: arch             = gptneox
0.00.039.483 I print_info: vocab_only       = 0
0.00.039.484 I print_info: n_ctx_train      = 2048
0.00.039.484 I print_info: n_embd           = 2048
0.00.039.484 I print_info: n_layer          = 24
0.00.039.487 I print_info: n_head           = 16
0.00.039.488 I print_info: n_head_kv        = 16
0.00.039.488 I print_info: n_rot            = 32
0.00.039.488 I print_info: n_swa            = 0
0.00.039.488 I print_info: n_embd_head_k    = 128
0.00.039.489 I print_info: n_embd_head_v    = 128
0.00.039.491 I print_info: n_gqa            = 1
0.00.039.492 I print_info: n_embd_k_gqa     = 2048
0.00.039.493 I print_info: n_embd_v_gqa     = 2048
0.00.039.494 I print_info: f_norm_eps       = 1.0e-05
0.00.039.495 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.497 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.497 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.497 I print_info: f_logit_scale    = 0.0e+00
0.00.039.498 I print_info: n_ff             = 8192
0.00.039.502 I print_info: n_expert         = 0
0.00.039.502 I print_info: n_expert_used    = 0
0.00.039.502 I print_info: causal attn      = 1
0.00.039.502 I print_info: pooling type     = 0
0.00.039.504 I print_info: rope type        = 2
0.00.039.504 I print_info: rope scaling     = linear
0.00.039.504 I print_info: freq_base_train  = 10000.0
0.00.039.504 I print_info: freq_scale_train = 1
0.00.039.504 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.505 I print_info: rope_finetuned   = unknown
0.00.039.505 I print_info: ssm_d_conv       = 0
0.00.039.505 I print_info: ssm_d_inner      = 0
0.00.039.505 I print_info: ssm_d_state      = 0
0.00.039.505 I print_info: ssm_dt_rank      = 0
0.00.039.505 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.505 I print_info: model type       = 1.4B
0.00.039.506 I print_info: model params     = 1.41 B
0.00.039.506 I print_info: general.name     = 1.4B
0.00.039.506 I print_info: vocab type       = BPE
0.00.039.506 I print_info: n_vocab          = 50304
0.00.039.507 I print_info: n_merges         = 50009
0.00.039.507 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.507 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.507 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.507 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.508 I print_info: LF token         = 187 ''
0.00.039.508 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.508 I print_info: max token length = 1024
0.00.039.508 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.461.106 I load_tensors: offloading 24 repeating layers to GPU
0.00.461.120 I load_tensors: offloading output layer to GPU
0.00.461.121 I load_tensors: offloaded 25/25 layers to GPU
0.00.461.160 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.461.166 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.462.568 I llama_init_from_model: n_seq_max     = 1
0.00.462.571 I llama_init_from_model: n_ctx         = 2048
0.00.462.572 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.462.572 I llama_init_from_model: n_batch       = 2048
0.00.462.572 I llama_init_from_model: n_ubatch      = 512
0.00.462.573 I llama_init_from_model: flash_attn    = 0
0.00.462.575 I llama_init_from_model: freq_base     = 10000.0
0.00.462.575 I llama_init_from_model: freq_scale    = 1
0.00.462.578 I ggml_metal_init: allocating
0.00.462.656 I ggml_metal_init: found device: Apple M4
0.00.462.669 I ggml_metal_init: picking default device: Apple M4
0.00.464.621 I ggml_metal_init: using embedded metal library
0.00.470.312 I ggml_metal_init: GPU name:   Apple M4
0.00.470.316 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.470.317 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.470.318 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.470.319 I ggml_metal_init: simdgroup reduction   = true
0.00.470.319 I ggml_metal_init: simdgroup matrix mul. = true
0.00.470.319 I ggml_metal_init: has residency sets    = true
0.00.470.320 I ggml_metal_init: has bfloat            = true
0.00.470.320 I ggml_metal_init: use bfloat            = true
0.00.470.321 I ggml_metal_init: hasUnifiedMemory      = true
0.00.470.322 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.490.616 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.543.981 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.543.989 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.544.024 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.548.125 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.548.127 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.548.128 I llama_init_from_model: graph nodes  = 967
0.00.548.128 I llama_init_from_model: graph splits = 2
0.00.548.133 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.548.262 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.548.263 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.606.042 I main: llama threadpool init, n_threads = 4
0.00.606.086 I 
0.00.606.108 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.606.109 I 
0.00.606.261 I sampler seed: 1234
0.00.606.265 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.606.276 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.606.277 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.606.277 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.350.358 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50714.29 tokens per second)
0.01.350.360 I llama_perf_context_print:        load time =     595.21 ms
0.01.350.360 I llama_perf_context_print: prompt eval time =      50.36 ms /     7 tokens (    7.19 ms per token,   139.00 tokens per second)
0.01.350.361 I llama_perf_context_print:        eval time =     690.78 ms /    63 runs   (   10.96 ms per token,    91.20 tokens per second)
0.01.350.361 I llama_perf_context_print:       total time =     745.03 ms /    70 tokens
0.01.350.597 I ggml_metal_free: deallocating

real	0m1.368s
user	0m0.110s
sys	0m0.198s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.661 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.565 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.571 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.573 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.573 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.574 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.574 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.574 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.575 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.575 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.576 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.578 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.579 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.579 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.579 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.581 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.581 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.582 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.335 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.495 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.370 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.372 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.372 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.372 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.373 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.373 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.374 I llama_model_loader: - type  f32:  194 tensors
0.00.024.374 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.374 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.374 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.375 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.375 I print_info: file format = GGUF V3 (latest)
0.00.024.376 I print_info: file type   = Q3_K - Medium
0.00.024.377 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.730 I load: special tokens cache size = 25
0.00.038.708 I load: token to piece cache size = 0.2984 MB
0.00.038.713 I print_info: arch             = gptneox
0.00.038.713 I print_info: vocab_only       = 0
0.00.038.713 I print_info: n_ctx_train      = 2048
0.00.038.713 I print_info: n_embd           = 2048
0.00.038.714 I print_info: n_layer          = 24
0.00.038.717 I print_info: n_head           = 16
0.00.038.718 I print_info: n_head_kv        = 16
0.00.038.718 I print_info: n_rot            = 32
0.00.038.718 I print_info: n_swa            = 0
0.00.038.718 I print_info: n_embd_head_k    = 128
0.00.038.719 I print_info: n_embd_head_v    = 128
0.00.038.719 I print_info: n_gqa            = 1
0.00.038.720 I print_info: n_embd_k_gqa     = 2048
0.00.038.720 I print_info: n_embd_v_gqa     = 2048
0.00.038.721 I print_info: f_norm_eps       = 1.0e-05
0.00.038.721 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.721 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.721 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.721 I print_info: f_logit_scale    = 0.0e+00
0.00.038.722 I print_info: n_ff             = 8192
0.00.038.722 I print_info: n_expert         = 0
0.00.038.722 I print_info: n_expert_used    = 0
0.00.038.722 I print_info: causal attn      = 1
0.00.038.723 I print_info: pooling type     = 0
0.00.038.723 I print_info: rope type        = 2
0.00.038.724 I print_info: rope scaling     = linear
0.00.038.725 I print_info: freq_base_train  = 10000.0
0.00.038.725 I print_info: freq_scale_train = 1
0.00.038.725 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.726 I print_info: rope_finetuned   = unknown
0.00.038.726 I print_info: ssm_d_conv       = 0
0.00.038.726 I print_info: ssm_d_inner      = 0
0.00.038.726 I print_info: ssm_d_state      = 0
0.00.038.726 I print_info: ssm_dt_rank      = 0
0.00.038.726 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.727 I print_info: model type       = 1.4B
0.00.038.727 I print_info: model params     = 1.41 B
0.00.038.727 I print_info: general.name     = 1.4B
0.00.038.728 I print_info: vocab type       = BPE
0.00.038.728 I print_info: n_vocab          = 50304
0.00.038.728 I print_info: n_merges         = 50009
0.00.038.728 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.728 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.728 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.730 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.731 I print_info: LF token         = 187 ''
0.00.038.731 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.731 I print_info: max token length = 1024
0.00.038.731 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.441.524 I load_tensors: offloading 24 repeating layers to GPU
0.00.441.537 I load_tensors: offloading output layer to GPU
0.00.441.538 I load_tensors: offloaded 25/25 layers to GPU
0.00.441.571 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.441.584 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.442.735 I llama_init_from_model: n_seq_max     = 1
0.00.442.745 I llama_init_from_model: n_ctx         = 128
0.00.442.745 I llama_init_from_model: n_ctx_per_seq = 128
0.00.442.746 I llama_init_from_model: n_batch       = 128
0.00.442.746 I llama_init_from_model: n_ubatch      = 128
0.00.442.747 I llama_init_from_model: flash_attn    = 0
0.00.442.749 I llama_init_from_model: freq_base     = 10000.0
0.00.442.749 I llama_init_from_model: freq_scale    = 1
0.00.442.750 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.442.752 I ggml_metal_init: allocating
0.00.442.848 I ggml_metal_init: found device: Apple M4
0.00.442.873 I ggml_metal_init: picking default device: Apple M4
0.00.444.623 I ggml_metal_init: using embedded metal library
0.00.450.309 I ggml_metal_init: GPU name:   Apple M4
0.00.450.325 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.450.326 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.450.326 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.450.327 I ggml_metal_init: simdgroup reduction   = true
0.00.450.327 I ggml_metal_init: simdgroup matrix mul. = true
0.00.450.327 I ggml_metal_init: has residency sets    = true
0.00.450.327 I ggml_metal_init: has bfloat            = true
0.00.450.328 I ggml_metal_init: use bfloat            = true
0.00.450.330 I ggml_metal_init: hasUnifiedMemory      = true
0.00.450.335 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.469.643 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.473.151 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.473.156 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.473.206 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.476.412 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.476.414 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.476.415 I llama_init_from_model: graph nodes  = 967
0.00.476.415 I llama_init_from_model: graph splits = 2
0.00.476.419 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.476.421 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.507.011 I 
0.00.507.088 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.507.094 I perplexity: tokenizing the input ..
0.00.513.309 I perplexity: tokenization took 6.213 ms
0.00.513.315 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.655.602 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.656.971 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.656.994 I llama_perf_context_print:        load time =     498.34 ms
0.00.656.995 I llama_perf_context_print: prompt eval time =     142.05 ms /   128 tokens (    1.11 ms per token,   901.10 tokens per second)
0.00.656.996 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.656.996 I llama_perf_context_print:       total time =     149.99 ms /   129 tokens
0.00.657.355 I ggml_metal_free: deallocating

real	0m0.671s
user	0m0.078s
sys	0m0.105s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.872 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.335 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.340 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.341 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.342 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.342 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.343 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.343 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.344 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.344 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.345 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.345 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.345 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.346 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.346 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.347 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.348 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.348 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.129 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.300 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.060 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.061 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.061 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.062 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.062 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.062 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.063 I llama_model_loader: - type  f32:  194 tensors
0.00.025.063 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.063 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.064 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.064 I print_info: file format = GGUF V3 (latest)
0.00.025.065 I print_info: file type   = Q4_K - Medium
0.00.025.066 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.201 I load: special tokens cache size = 25
0.00.039.237 I load: token to piece cache size = 0.2984 MB
0.00.039.239 I print_info: arch             = gptneox
0.00.039.240 I print_info: vocab_only       = 0
0.00.039.240 I print_info: n_ctx_train      = 2048
0.00.039.240 I print_info: n_embd           = 2048
0.00.039.240 I print_info: n_layer          = 24
0.00.039.243 I print_info: n_head           = 16
0.00.039.244 I print_info: n_head_kv        = 16
0.00.039.244 I print_info: n_rot            = 32
0.00.039.244 I print_info: n_swa            = 0
0.00.039.245 I print_info: n_embd_head_k    = 128
0.00.039.245 I print_info: n_embd_head_v    = 128
0.00.039.246 I print_info: n_gqa            = 1
0.00.039.246 I print_info: n_embd_k_gqa     = 2048
0.00.039.247 I print_info: n_embd_v_gqa     = 2048
0.00.039.247 I print_info: f_norm_eps       = 1.0e-05
0.00.039.248 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.248 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.250 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.250 I print_info: f_logit_scale    = 0.0e+00
0.00.039.251 I print_info: n_ff             = 8192
0.00.039.251 I print_info: n_expert         = 0
0.00.039.251 I print_info: n_expert_used    = 0
0.00.039.251 I print_info: causal attn      = 1
0.00.039.251 I print_info: pooling type     = 0
0.00.039.251 I print_info: rope type        = 2
0.00.039.252 I print_info: rope scaling     = linear
0.00.039.252 I print_info: freq_base_train  = 10000.0
0.00.039.252 I print_info: freq_scale_train = 1
0.00.039.253 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.253 I print_info: rope_finetuned   = unknown
0.00.039.253 I print_info: ssm_d_conv       = 0
0.00.039.255 I print_info: ssm_d_inner      = 0
0.00.039.255 I print_info: ssm_d_state      = 0
0.00.039.255 I print_info: ssm_dt_rank      = 0
0.00.039.256 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.256 I print_info: model type       = 1.4B
0.00.039.256 I print_info: model params     = 1.41 B
0.00.039.257 I print_info: general.name     = 1.4B
0.00.039.257 I print_info: vocab type       = BPE
0.00.039.257 I print_info: n_vocab          = 50304
0.00.039.257 I print_info: n_merges         = 50009
0.00.039.257 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.258 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.258 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.262 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.264 I print_info: LF token         = 187 ''
0.00.039.264 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.264 I print_info: max token length = 1024
0.00.039.264 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.532.716 I load_tensors: offloading 24 repeating layers to GPU
0.00.532.727 I load_tensors: offloading output layer to GPU
0.00.532.728 I load_tensors: offloaded 25/25 layers to GPU
0.00.532.767 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.532.770 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.534.356 I llama_init_from_model: n_seq_max     = 1
0.00.534.359 I llama_init_from_model: n_ctx         = 2048
0.00.534.360 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.534.360 I llama_init_from_model: n_batch       = 2048
0.00.534.360 I llama_init_from_model: n_ubatch      = 512
0.00.534.361 I llama_init_from_model: flash_attn    = 0
0.00.534.362 I llama_init_from_model: freq_base     = 10000.0
0.00.534.363 I llama_init_from_model: freq_scale    = 1
0.00.534.365 I ggml_metal_init: allocating
0.00.534.444 I ggml_metal_init: found device: Apple M4
0.00.534.458 I ggml_metal_init: picking default device: Apple M4
0.00.536.363 I ggml_metal_init: using embedded metal library
0.00.542.384 I ggml_metal_init: GPU name:   Apple M4
0.00.542.389 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.542.390 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.542.391 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.542.392 I ggml_metal_init: simdgroup reduction   = true
0.00.542.392 I ggml_metal_init: simdgroup matrix mul. = true
0.00.542.392 I ggml_metal_init: has residency sets    = true
0.00.542.392 I ggml_metal_init: has bfloat            = true
0.00.542.393 I ggml_metal_init: use bfloat            = true
0.00.542.394 I ggml_metal_init: hasUnifiedMemory      = true
0.00.542.395 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.562.492 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.618.422 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.618.429 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.618.461 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.622.557 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.622.559 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.622.559 I llama_init_from_model: graph nodes  = 967
0.00.622.560 I llama_init_from_model: graph splits = 2
0.00.622.566 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.622.695 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.622.695 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.700 I main: llama threadpool init, n_threads = 4
0.00.681.741 I 
0.00.681.764 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.764 I 
0.00.681.935 I sampler seed: 1234
0.00.681.939 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.681.950 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.681.951 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.681.951 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.442.931 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49339.82 tokens per second)
0.01.442.932 I llama_perf_context_print:        load time =     671.02 ms
0.01.442.933 I llama_perf_context_print: prompt eval time =      55.09 ms /     7 tokens (    7.87 ms per token,   127.06 tokens per second)
0.01.442.934 I llama_perf_context_print:        eval time =     702.99 ms /    63 runs   (   11.16 ms per token,    89.62 tokens per second)
0.01.442.934 I llama_perf_context_print:       total time =     762.04 ms /    70 tokens
0.01.443.166 I ggml_metal_free: deallocating

real	0m1.462s
user	0m0.111s
sys	0m0.203s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.728 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.537 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.544 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.546 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.546 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.547 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.547 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.547 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.548 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.549 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.549 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.549 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.550 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.550 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.552 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.554 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.554 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.554 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.086 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.204 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.969 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.971 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.972 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.972 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.973 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.973 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.974 I llama_model_loader: - type  f32:  194 tensors
0.00.024.974 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.974 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.974 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.975 I print_info: file format = GGUF V3 (latest)
0.00.024.976 I print_info: file type   = Q4_K - Medium
0.00.024.977 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.878 I load: special tokens cache size = 25
0.00.038.903 I load: token to piece cache size = 0.2984 MB
0.00.038.908 I print_info: arch             = gptneox
0.00.038.908 I print_info: vocab_only       = 0
0.00.038.908 I print_info: n_ctx_train      = 2048
0.00.038.908 I print_info: n_embd           = 2048
0.00.038.909 I print_info: n_layer          = 24
0.00.038.912 I print_info: n_head           = 16
0.00.038.913 I print_info: n_head_kv        = 16
0.00.038.913 I print_info: n_rot            = 32
0.00.038.913 I print_info: n_swa            = 0
0.00.038.913 I print_info: n_embd_head_k    = 128
0.00.038.913 I print_info: n_embd_head_v    = 128
0.00.038.914 I print_info: n_gqa            = 1
0.00.038.915 I print_info: n_embd_k_gqa     = 2048
0.00.038.917 I print_info: n_embd_v_gqa     = 2048
0.00.038.917 I print_info: f_norm_eps       = 1.0e-05
0.00.038.918 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.920 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.920 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.920 I print_info: f_logit_scale    = 0.0e+00
0.00.038.921 I print_info: n_ff             = 8192
0.00.038.921 I print_info: n_expert         = 0
0.00.038.922 I print_info: n_expert_used    = 0
0.00.038.922 I print_info: causal attn      = 1
0.00.038.922 I print_info: pooling type     = 0
0.00.038.922 I print_info: rope type        = 2
0.00.038.923 I print_info: rope scaling     = linear
0.00.038.923 I print_info: freq_base_train  = 10000.0
0.00.038.923 I print_info: freq_scale_train = 1
0.00.038.924 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.924 I print_info: rope_finetuned   = unknown
0.00.038.924 I print_info: ssm_d_conv       = 0
0.00.038.924 I print_info: ssm_d_inner      = 0
0.00.038.924 I print_info: ssm_d_state      = 0
0.00.038.924 I print_info: ssm_dt_rank      = 0
0.00.038.924 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.924 I print_info: model type       = 1.4B
0.00.038.925 I print_info: model params     = 1.41 B
0.00.038.925 I print_info: general.name     = 1.4B
0.00.038.925 I print_info: vocab type       = BPE
0.00.038.926 I print_info: n_vocab          = 50304
0.00.038.926 I print_info: n_merges         = 50009
0.00.038.926 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.926 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.926 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.926 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.927 I print_info: LF token         = 187 ''
0.00.038.927 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.927 I print_info: max token length = 1024
0.00.038.927 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.544.165 I load_tensors: offloading 24 repeating layers to GPU
0.00.544.179 I load_tensors: offloading output layer to GPU
0.00.544.180 I load_tensors: offloaded 25/25 layers to GPU
0.00.544.225 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.544.227 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.546.011 I llama_init_from_model: n_seq_max     = 1
0.00.546.016 I llama_init_from_model: n_ctx         = 128
0.00.546.016 I llama_init_from_model: n_ctx_per_seq = 128
0.00.546.017 I llama_init_from_model: n_batch       = 128
0.00.546.017 I llama_init_from_model: n_ubatch      = 128
0.00.546.018 I llama_init_from_model: flash_attn    = 0
0.00.546.020 I llama_init_from_model: freq_base     = 10000.0
0.00.546.021 I llama_init_from_model: freq_scale    = 1
0.00.546.021 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.546.023 I ggml_metal_init: allocating
0.00.546.147 I ggml_metal_init: found device: Apple M4
0.00.546.161 I ggml_metal_init: picking default device: Apple M4
0.00.548.130 I ggml_metal_init: using embedded metal library
0.00.554.847 I ggml_metal_init: GPU name:   Apple M4
0.00.554.853 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.554.853 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.554.854 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.554.855 I ggml_metal_init: simdgroup reduction   = true
0.00.554.855 I ggml_metal_init: simdgroup matrix mul. = true
0.00.554.855 I ggml_metal_init: has residency sets    = true
0.00.554.856 I ggml_metal_init: has bfloat            = true
0.00.554.856 I ggml_metal_init: use bfloat            = true
0.00.554.857 I ggml_metal_init: hasUnifiedMemory      = true
0.00.554.869 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.572.766 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.576.376 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.576.380 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.576.421 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.579.577 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.579.579 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.579.580 I llama_init_from_model: graph nodes  = 967
0.00.579.580 I llama_init_from_model: graph splits = 2
0.00.579.583 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.579.583 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.607.271 I 
0.00.607.361 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.607.370 I perplexity: tokenizing the input ..
0.00.611.369 I perplexity: tokenization took 3.997 ms
0.00.611.372 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.742.861 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.744.193 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.744.211 I llama_perf_context_print:        load time =     597.53 ms
0.00.744.212 I llama_perf_context_print: prompt eval time =     131.26 ms /   128 tokens (    1.03 ms per token,   975.18 tokens per second)
0.00.744.213 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.744.213 I llama_perf_context_print:       total time =     136.94 ms /   129 tokens
0.00.744.584 I ggml_metal_free: deallocating

real	0m0.760s
user	0m0.076s
sys	0m0.141s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.792 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.280 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.285 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.287 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.288 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.288 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.288 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.289 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.291 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.291 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.291 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.293 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.294 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.294 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.295 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.296 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.297 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.297 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.987 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.073 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.751 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.752 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.752 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.753 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.753 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.753 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.754 I llama_model_loader: - type  f32:  194 tensors
0.00.023.754 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.754 I llama_model_loader: - type q6_K:   37 tensors
0.00.023.755 I print_info: file format = GGUF V3 (latest)
0.00.023.755 I print_info: file type   = Q5_K - Medium
0.00.023.756 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.031.610 I load: special tokens cache size = 25
0.00.037.575 I load: token to piece cache size = 0.2984 MB
0.00.037.578 I print_info: arch             = gptneox
0.00.037.579 I print_info: vocab_only       = 0
0.00.037.579 I print_info: n_ctx_train      = 2048
0.00.037.579 I print_info: n_embd           = 2048
0.00.037.579 I print_info: n_layer          = 24
0.00.037.582 I print_info: n_head           = 16
0.00.037.583 I print_info: n_head_kv        = 16
0.00.037.583 I print_info: n_rot            = 32
0.00.037.583 I print_info: n_swa            = 0
0.00.037.583 I print_info: n_embd_head_k    = 128
0.00.037.584 I print_info: n_embd_head_v    = 128
0.00.037.584 I print_info: n_gqa            = 1
0.00.037.586 I print_info: n_embd_k_gqa     = 2048
0.00.037.587 I print_info: n_embd_v_gqa     = 2048
0.00.037.587 I print_info: f_norm_eps       = 1.0e-05
0.00.037.588 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.588 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.588 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.588 I print_info: f_logit_scale    = 0.0e+00
0.00.037.589 I print_info: n_ff             = 8192
0.00.037.589 I print_info: n_expert         = 0
0.00.037.589 I print_info: n_expert_used    = 0
0.00.037.589 I print_info: causal attn      = 1
0.00.037.590 I print_info: pooling type     = 0
0.00.037.590 I print_info: rope type        = 2
0.00.037.591 I print_info: rope scaling     = linear
0.00.037.592 I print_info: freq_base_train  = 10000.0
0.00.037.592 I print_info: freq_scale_train = 1
0.00.037.592 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.593 I print_info: rope_finetuned   = unknown
0.00.037.593 I print_info: ssm_d_conv       = 0
0.00.037.593 I print_info: ssm_d_inner      = 0
0.00.037.593 I print_info: ssm_d_state      = 0
0.00.037.593 I print_info: ssm_dt_rank      = 0
0.00.037.593 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.593 I print_info: model type       = 1.4B
0.00.037.594 I print_info: model params     = 1.41 B
0.00.037.594 I print_info: general.name     = 1.4B
0.00.037.594 I print_info: vocab type       = BPE
0.00.037.594 I print_info: n_vocab          = 50304
0.00.037.595 I print_info: n_merges         = 50009
0.00.037.595 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.595 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.595 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.595 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.596 I print_info: LF token         = 187 ''
0.00.037.596 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.596 I print_info: max token length = 1024
0.00.037.596 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.590.777 I load_tensors: offloading 24 repeating layers to GPU
0.00.590.790 I load_tensors: offloading output layer to GPU
0.00.590.791 I load_tensors: offloaded 25/25 layers to GPU
0.00.590.826 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.590.832 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.592.481 I llama_init_from_model: n_seq_max     = 1
0.00.592.483 I llama_init_from_model: n_ctx         = 2048
0.00.592.484 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.592.484 I llama_init_from_model: n_batch       = 2048
0.00.592.485 I llama_init_from_model: n_ubatch      = 512
0.00.592.485 I llama_init_from_model: flash_attn    = 0
0.00.592.487 I llama_init_from_model: freq_base     = 10000.0
0.00.592.488 I llama_init_from_model: freq_scale    = 1
0.00.592.496 I ggml_metal_init: allocating
0.00.592.582 I ggml_metal_init: found device: Apple M4
0.00.592.595 I ggml_metal_init: picking default device: Apple M4
0.00.594.273 I ggml_metal_init: using embedded metal library
0.00.600.937 I ggml_metal_init: GPU name:   Apple M4
0.00.600.940 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.600.941 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.600.942 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.600.942 I ggml_metal_init: simdgroup reduction   = true
0.00.600.943 I ggml_metal_init: simdgroup matrix mul. = true
0.00.600.943 I ggml_metal_init: has residency sets    = true
0.00.600.943 I ggml_metal_init: has bfloat            = true
0.00.600.943 I ggml_metal_init: use bfloat            = true
0.00.600.944 I ggml_metal_init: hasUnifiedMemory      = true
0.00.600.946 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.618.892 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.675.155 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.675.162 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.675.199 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.679.775 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.679.778 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.679.778 I llama_init_from_model: graph nodes  = 967
0.00.679.778 I llama_init_from_model: graph splits = 2
0.00.679.790 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.679.920 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.679.921 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.716 I main: llama threadpool init, n_threads = 4
0.00.743.764 I 
0.00.743.784 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.784 I 
0.00.743.950 I sampler seed: 1234
0.00.743.954 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.743.965 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.743.965 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.743.965 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.594.893 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52631.58 tokens per second)
0.01.594.894 I llama_perf_context_print:        load time =     734.13 ms
0.01.594.895 I llama_perf_context_print: prompt eval time =      52.61 ms /     7 tokens (    7.52 ms per token,   133.05 tokens per second)
0.01.594.895 I llama_perf_context_print:        eval time =     795.45 ms /    63 runs   (   12.63 ms per token,    79.20 tokens per second)
0.01.594.896 I llama_perf_context_print:       total time =     851.97 ms /    70 tokens
0.01.595.121 I ggml_metal_free: deallocating

real	0m1.612s
user	0m0.109s
sys	0m0.213s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.940 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.689 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.702 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.704 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.704 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.704 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.705 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.705 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.706 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.706 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.707 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.707 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.708 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.708 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.708 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.710 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.711 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.711 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.407 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.537 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.310 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.312 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.312 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.313 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.313 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.313 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.314 I llama_model_loader: - type  f32:  194 tensors
0.00.024.314 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.315 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.315 I print_info: file format = GGUF V3 (latest)
0.00.024.320 I print_info: file type   = Q5_K - Medium
0.00.024.321 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.188 I load: special tokens cache size = 25
0.00.038.153 I load: token to piece cache size = 0.2984 MB
0.00.038.158 I print_info: arch             = gptneox
0.00.038.158 I print_info: vocab_only       = 0
0.00.038.158 I print_info: n_ctx_train      = 2048
0.00.038.158 I print_info: n_embd           = 2048
0.00.038.158 I print_info: n_layer          = 24
0.00.038.163 I print_info: n_head           = 16
0.00.038.164 I print_info: n_head_kv        = 16
0.00.038.164 I print_info: n_rot            = 32
0.00.038.165 I print_info: n_swa            = 0
0.00.038.165 I print_info: n_embd_head_k    = 128
0.00.038.165 I print_info: n_embd_head_v    = 128
0.00.038.166 I print_info: n_gqa            = 1
0.00.038.166 I print_info: n_embd_k_gqa     = 2048
0.00.038.167 I print_info: n_embd_v_gqa     = 2048
0.00.038.168 I print_info: f_norm_eps       = 1.0e-05
0.00.038.168 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.168 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.168 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.168 I print_info: f_logit_scale    = 0.0e+00
0.00.038.169 I print_info: n_ff             = 8192
0.00.038.169 I print_info: n_expert         = 0
0.00.038.169 I print_info: n_expert_used    = 0
0.00.038.169 I print_info: causal attn      = 1
0.00.038.170 I print_info: pooling type     = 0
0.00.038.170 I print_info: rope type        = 2
0.00.038.170 I print_info: rope scaling     = linear
0.00.038.170 I print_info: freq_base_train  = 10000.0
0.00.038.170 I print_info: freq_scale_train = 1
0.00.038.170 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.173 I print_info: rope_finetuned   = unknown
0.00.038.173 I print_info: ssm_d_conv       = 0
0.00.038.173 I print_info: ssm_d_inner      = 0
0.00.038.173 I print_info: ssm_d_state      = 0
0.00.038.173 I print_info: ssm_dt_rank      = 0
0.00.038.173 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.174 I print_info: model type       = 1.4B
0.00.038.174 I print_info: model params     = 1.41 B
0.00.038.174 I print_info: general.name     = 1.4B
0.00.038.175 I print_info: vocab type       = BPE
0.00.038.175 I print_info: n_vocab          = 50304
0.00.038.175 I print_info: n_merges         = 50009
0.00.038.175 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.175 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.175 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.176 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.177 I print_info: LF token         = 187 ''
0.00.038.177 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.177 I print_info: max token length = 1024
0.00.038.177 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.583.839 I load_tensors: offloading 24 repeating layers to GPU
0.00.583.856 I load_tensors: offloading output layer to GPU
0.00.583.857 I load_tensors: offloaded 25/25 layers to GPU
0.00.583.894 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.583.895 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.585.392 I llama_init_from_model: n_seq_max     = 1
0.00.585.395 I llama_init_from_model: n_ctx         = 128
0.00.585.395 I llama_init_from_model: n_ctx_per_seq = 128
0.00.585.396 I llama_init_from_model: n_batch       = 128
0.00.585.396 I llama_init_from_model: n_ubatch      = 128
0.00.585.397 I llama_init_from_model: flash_attn    = 0
0.00.585.399 I llama_init_from_model: freq_base     = 10000.0
0.00.585.399 I llama_init_from_model: freq_scale    = 1
0.00.585.400 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.585.402 I ggml_metal_init: allocating
0.00.585.544 I ggml_metal_init: found device: Apple M4
0.00.585.558 I ggml_metal_init: picking default device: Apple M4
0.00.587.308 I ggml_metal_init: using embedded metal library
0.00.593.896 I ggml_metal_init: GPU name:   Apple M4
0.00.593.902 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.593.903 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.593.904 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.593.904 I ggml_metal_init: simdgroup reduction   = true
0.00.593.904 I ggml_metal_init: simdgroup matrix mul. = true
0.00.593.905 I ggml_metal_init: has residency sets    = true
0.00.593.905 I ggml_metal_init: has bfloat            = true
0.00.593.905 I ggml_metal_init: use bfloat            = true
0.00.593.906 I ggml_metal_init: hasUnifiedMemory      = true
0.00.593.910 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.611.101 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.614.591 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.614.595 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.614.636 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.617.907 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.617.909 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.617.909 I llama_init_from_model: graph nodes  = 967
0.00.617.910 I llama_init_from_model: graph splits = 2
0.00.617.913 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.617.913 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.555 I 
0.00.650.643 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.651 I perplexity: tokenizing the input ..
0.00.657.267 I perplexity: tokenization took 6.614 ms
0.00.657.272 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.794.249 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.795.587 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.795.613 I llama_perf_context_print:        load time =     641.61 ms
0.00.795.613 I llama_perf_context_print: prompt eval time =     136.43 ms /   128 tokens (    1.07 ms per token,   938.22 tokens per second)
0.00.795.614 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.795.615 I llama_perf_context_print:       total time =     145.06 ms /   129 tokens
0.00.796.001 I ggml_metal_free: deallocating

real	0m0.810s
user	0m0.078s
sys	0m0.135s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.731 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.309 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.314 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.320 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.320 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.321 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.321 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.321 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.322 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.323 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.323 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.323 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.324 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.324 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.325 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.326 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.326 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.327 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.011 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.079 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.737 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.738 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.739 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.739 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.739 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.740 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.740 I llama_model_loader: - type  f32:  194 tensors
0.00.023.741 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.741 I print_info: file format = GGUF V3 (latest)
0.00.023.742 I print_info: file type   = Q6_K
0.00.023.742 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.502 I load: special tokens cache size = 25
0.00.037.510 I load: token to piece cache size = 0.2984 MB
0.00.037.512 I print_info: arch             = gptneox
0.00.037.513 I print_info: vocab_only       = 0
0.00.037.513 I print_info: n_ctx_train      = 2048
0.00.037.513 I print_info: n_embd           = 2048
0.00.037.513 I print_info: n_layer          = 24
0.00.037.516 I print_info: n_head           = 16
0.00.037.517 I print_info: n_head_kv        = 16
0.00.037.517 I print_info: n_rot            = 32
0.00.037.519 I print_info: n_swa            = 0
0.00.037.519 I print_info: n_embd_head_k    = 128
0.00.037.519 I print_info: n_embd_head_v    = 128
0.00.037.520 I print_info: n_gqa            = 1
0.00.037.521 I print_info: n_embd_k_gqa     = 2048
0.00.037.522 I print_info: n_embd_v_gqa     = 2048
0.00.037.522 I print_info: f_norm_eps       = 1.0e-05
0.00.037.523 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.523 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.523 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.523 I print_info: f_logit_scale    = 0.0e+00
0.00.037.524 I print_info: n_ff             = 8192
0.00.037.524 I print_info: n_expert         = 0
0.00.037.524 I print_info: n_expert_used    = 0
0.00.037.524 I print_info: causal attn      = 1
0.00.037.524 I print_info: pooling type     = 0
0.00.037.524 I print_info: rope type        = 2
0.00.037.525 I print_info: rope scaling     = linear
0.00.037.525 I print_info: freq_base_train  = 10000.0
0.00.037.525 I print_info: freq_scale_train = 1
0.00.037.525 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.526 I print_info: rope_finetuned   = unknown
0.00.037.526 I print_info: ssm_d_conv       = 0
0.00.037.526 I print_info: ssm_d_inner      = 0
0.00.037.526 I print_info: ssm_d_state      = 0
0.00.037.526 I print_info: ssm_dt_rank      = 0
0.00.037.526 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.527 I print_info: model type       = 1.4B
0.00.037.527 I print_info: model params     = 1.41 B
0.00.037.528 I print_info: general.name     = 1.4B
0.00.037.528 I print_info: vocab type       = BPE
0.00.037.528 I print_info: n_vocab          = 50304
0.00.037.529 I print_info: n_merges         = 50009
0.00.037.529 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.529 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.529 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.530 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.530 I print_info: LF token         = 187 ''
0.00.037.530 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.530 I print_info: max token length = 1024
0.00.037.531 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.644.005 I load_tensors: offloading 24 repeating layers to GPU
0.00.644.008 I load_tensors: offloading output layer to GPU
0.00.644.009 I load_tensors: offloaded 25/25 layers to GPU
0.00.644.034 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.644.037 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.645.585 I llama_init_from_model: n_seq_max     = 1
0.00.645.587 I llama_init_from_model: n_ctx         = 2048
0.00.645.587 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.645.588 I llama_init_from_model: n_batch       = 2048
0.00.645.588 I llama_init_from_model: n_ubatch      = 512
0.00.645.589 I llama_init_from_model: flash_attn    = 0
0.00.645.590 I llama_init_from_model: freq_base     = 10000.0
0.00.645.591 I llama_init_from_model: freq_scale    = 1
0.00.645.592 I ggml_metal_init: allocating
0.00.645.623 I ggml_metal_init: found device: Apple M4
0.00.645.635 I ggml_metal_init: picking default device: Apple M4
0.00.647.059 I ggml_metal_init: using embedded metal library
0.00.653.000 I ggml_metal_init: GPU name:   Apple M4
0.00.653.004 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.653.005 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.653.005 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.653.006 I ggml_metal_init: simdgroup reduction   = true
0.00.653.006 I ggml_metal_init: simdgroup matrix mul. = true
0.00.653.006 I ggml_metal_init: has residency sets    = true
0.00.653.006 I ggml_metal_init: has bfloat            = true
0.00.653.007 I ggml_metal_init: use bfloat            = true
0.00.653.007 I ggml_metal_init: hasUnifiedMemory      = true
0.00.653.009 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.670.315 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.725.929 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.725.936 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.725.971 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.730.363 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.730.365 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.730.365 I llama_init_from_model: graph nodes  = 967
0.00.730.366 I llama_init_from_model: graph splits = 2
0.00.730.371 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.730.503 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.730.504 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.798.663 I main: llama threadpool init, n_threads = 4
0.00.798.707 I 
0.00.798.731 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.798.732 I 
0.00.798.900 I sampler seed: 1234
0.00.798.905 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.798.916 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.798.916 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.798.916 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.684.692 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54531.49 tokens per second)
0.01.684.693 I llama_perf_context_print:        load time =     789.20 ms
0.01.684.694 I llama_perf_context_print: prompt eval time =      57.77 ms /     7 tokens (    8.25 ms per token,   121.17 tokens per second)
0.01.684.694 I llama_perf_context_print:        eval time =     825.15 ms /    63 runs   (   13.10 ms per token,    76.35 tokens per second)
0.01.684.695 I llama_perf_context_print:       total time =     886.76 ms /    70 tokens
0.01.684.920 I ggml_metal_free: deallocating

real	0m1.703s
user	0m0.109s
sys	0m0.220s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4778 (a82c9e7c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.005 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.706 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.711 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.718 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.718 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.719 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.719 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.719 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.722 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.722 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.722 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.723 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.723 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.723 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.726 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.728 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.729 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.729 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.469 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.616 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.436 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.438 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.438 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.439 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.439 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.439 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.440 I llama_model_loader: - type  f32:  194 tensors
0.00.024.441 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.441 I print_info: file format = GGUF V3 (latest)
0.00.024.442 I print_info: file type   = Q6_K
0.00.024.443 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.856 I load: special tokens cache size = 25
0.00.038.926 I load: token to piece cache size = 0.2984 MB
0.00.038.930 I print_info: arch             = gptneox
0.00.038.931 I print_info: vocab_only       = 0
0.00.038.931 I print_info: n_ctx_train      = 2048
0.00.038.931 I print_info: n_embd           = 2048
0.00.038.931 I print_info: n_layer          = 24
0.00.038.935 I print_info: n_head           = 16
0.00.038.936 I print_info: n_head_kv        = 16
0.00.038.938 I print_info: n_rot            = 32
0.00.038.938 I print_info: n_swa            = 0
0.00.038.938 I print_info: n_embd_head_k    = 128
0.00.038.938 I print_info: n_embd_head_v    = 128
0.00.038.939 I print_info: n_gqa            = 1
0.00.038.941 I print_info: n_embd_k_gqa     = 2048
0.00.038.941 I print_info: n_embd_v_gqa     = 2048
0.00.038.942 I print_info: f_norm_eps       = 1.0e-05
0.00.038.943 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.943 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.943 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.943 I print_info: f_logit_scale    = 0.0e+00
0.00.038.944 I print_info: n_ff             = 8192
0.00.038.944 I print_info: n_expert         = 0
0.00.038.944 I print_info: n_expert_used    = 0
0.00.038.945 I print_info: causal attn      = 1
0.00.038.946 I print_info: pooling type     = 0
0.00.038.946 I print_info: rope type        = 2
0.00.038.947 I print_info: rope scaling     = linear
0.00.038.947 I print_info: freq_base_train  = 10000.0
0.00.038.947 I print_info: freq_scale_train = 1
0.00.038.948 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.948 I print_info: rope_finetuned   = unknown
0.00.038.948 I print_info: ssm_d_conv       = 0
0.00.038.948 I print_info: ssm_d_inner      = 0
0.00.038.948 I print_info: ssm_d_state      = 0
0.00.038.948 I print_info: ssm_dt_rank      = 0
0.00.038.948 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.949 I print_info: model type       = 1.4B
0.00.038.949 I print_info: model params     = 1.41 B
0.00.038.949 I print_info: general.name     = 1.4B
0.00.038.950 I print_info: vocab type       = BPE
0.00.038.950 I print_info: n_vocab          = 50304
0.00.038.950 I print_info: n_merges         = 50009
0.00.038.951 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.951 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.952 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.952 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.952 I print_info: LF token         = 187 ''
0.00.038.952 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.952 I print_info: max token length = 1024
0.00.038.953 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.596.405 I load_tensors: offloading 24 repeating layers to GPU
0.00.596.411 I load_tensors: offloading output layer to GPU
0.00.596.412 I load_tensors: offloaded 25/25 layers to GPU
0.00.596.440 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.596.441 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.597.891 I llama_init_from_model: n_seq_max     = 1
0.00.597.894 I llama_init_from_model: n_ctx         = 128
0.00.597.894 I llama_init_from_model: n_ctx_per_seq = 128
0.00.597.895 I llama_init_from_model: n_batch       = 128
0.00.597.895 I llama_init_from_model: n_ubatch      = 128
0.00.597.896 I llama_init_from_model: flash_attn    = 0
0.00.597.896 I llama_init_from_model: freq_base     = 10000.0
0.00.597.897 I llama_init_from_model: freq_scale    = 1
0.00.597.898 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.597.899 I ggml_metal_init: allocating
0.00.597.938 I ggml_metal_init: found device: Apple M4
0.00.597.949 I ggml_metal_init: picking default device: Apple M4
0.00.599.321 I ggml_metal_init: using embedded metal library
0.00.605.329 I ggml_metal_init: GPU name:   Apple M4
0.00.605.332 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.605.333 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.605.334 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.605.334 I ggml_metal_init: simdgroup reduction   = true
0.00.605.334 I ggml_metal_init: simdgroup matrix mul. = true
0.00.605.335 I ggml_metal_init: has residency sets    = true
0.00.605.335 I ggml_metal_init: has bfloat            = true
0.00.605.335 I ggml_metal_init: use bfloat            = true
0.00.605.336 I ggml_metal_init: hasUnifiedMemory      = true
0.00.605.338 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.621.851 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.625.346 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.625.349 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.625.391 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.628.678 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.628.679 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.628.680 I llama_init_from_model: graph nodes  = 967
0.00.628.680 I llama_init_from_model: graph splits = 2
0.00.628.683 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.628.683 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.659.859 I 
0.00.659.941 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.659.948 I perplexity: tokenizing the input ..
0.00.666.793 I perplexity: tokenization took 6.842 ms
0.00.666.800 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.465 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.800.907 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.800.945 I llama_perf_context_print:        load time =     650.84 ms
0.00.800.947 I llama_perf_context_print: prompt eval time =     131.69 ms /   128 tokens (    1.03 ms per token,   971.96 tokens per second)
0.00.800.950 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.950 I llama_perf_context_print:       total time =     141.09 ms /   129 tokens
0.00.801.330 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.078s
sys	0m0.139s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4778 (a82c9e7c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126106200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126106870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126106ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1261099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126109e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12610a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12610a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12610ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12610b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12610b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12610bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12610c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12610cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12610d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12610dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12610e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12610ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12610f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12610fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1261101e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126110900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x126111020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x126111740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126111fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126112700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1261129c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126112fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126113c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x126114180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126114440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1261148e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126114ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126115430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126115970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126115c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1261160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126116570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126116a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126116eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126117350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1261177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126117c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126118130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1261185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126118890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126118ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1261194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126119dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12611a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12611a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12611b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12611b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12611bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12611c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12611ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12611cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12611d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12611d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12611dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12611e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12611e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12611eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12611f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12611f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12611f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12611fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1261202a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126120740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126120be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x126121080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126121520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1261219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126121e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1261223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126122900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126122e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1261233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1261238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126123e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x126124390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1261248e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126124e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x126125380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1261258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126125e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x126126370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1261268c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126126e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126127360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1261278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126127e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126128350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1261288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126128df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126129340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126129890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126129de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126119ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12612a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12612aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12612af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12612b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12612b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12612bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12612c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12612c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12612cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12612d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12612d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12612df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12612e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12612e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12612ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12612f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12612f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12612fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126130190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126130630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126130ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126130f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126131410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1261318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126131d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1261321f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126132690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126132b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126132fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x126133470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126133910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126133db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126134250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1261346f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x126134b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126135030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1261354d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126135970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126135e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1261362b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126136750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126136bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126137090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126137530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1261379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126137e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x126138310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1261387b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x126138c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1261390f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126139590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126139a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126139ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12613a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12613a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12613acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12613b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12613b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12613ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12613bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12613c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12613c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12613cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12613d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12613d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12613daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12613df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12613e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12613e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12613ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12613f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12613f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12613fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12613fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126140490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126140930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126140dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126141270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126141710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126141bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126142050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1261424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126142990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126142e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1261432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126143770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126143c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1261440b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126144550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1261449f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126144e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x126145330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1261457d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126145c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126146110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126146660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126146bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126147100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126147650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126147910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126147f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x126148530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x126148b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x126149330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1261497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x126149a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12614a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12614a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12614aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12614b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12614b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12614bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12614c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12614c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12614ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12614d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12614d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12614dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12614e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12614e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12614eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12614f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12614f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12614fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1261503f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126150940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126150e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1261513e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126151930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126151e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1261523d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126152920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126152e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1261533c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126153910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x126153e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1261543b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126154900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126154e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1261553a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1261558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126155e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x126156390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1261568e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126156e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x126157380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1261578d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126157e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x126158370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1261588c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126158e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126159360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1261598b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126159e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12615a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12615a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12615adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12615b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12615b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12615bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12615c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12615c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12615cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12615d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12615d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12615ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12615e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12615e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12615edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12615f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12615f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12615fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126160030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1261604d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126160970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126160e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1261612b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126161750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126161bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126162090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126162530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1261629d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126162e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x126163310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x1261637b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x126163c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1261640f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x126164590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x126164a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x126164ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x126165370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x126165810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x126165cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x126166150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1261666a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126166dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1261674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126167c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126168320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1261685e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126168dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126169090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1261696a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.723.970 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.723.974 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x116804b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x116804f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x116805400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x116805870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x116805ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x116806150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1168065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x116806a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x116806ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x116807310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x116807780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x116807e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x116808990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x116809140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x116809950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11680a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11680a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11680aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11680b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11680bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11680c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11680cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11680d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11680d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11680e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11680e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11680e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11680ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11680ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11680f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11680f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11680fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x116810180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x116810440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1168108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x116810d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x116811190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x116811600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x116811a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x116811ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x116812350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1168127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x116812c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1168130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x116813510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x116813980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x116813df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x116814260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1168146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x116814b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x116814fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x116815420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x116815890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x116815d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x116816170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1168165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x116816b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x116817050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1168174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x116817930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x116817da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x116818210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x116818680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x116818af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x116818f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1168193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x116819840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x116819cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11681a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11681a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11681aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11681ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11681b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11681b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11681bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11681c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11681c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11681c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11681cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11681d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11681d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11681dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11681df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11681e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11681e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11681ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11681f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11681f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11681f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11681fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1168202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x116820730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x116820ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x116821010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x116821480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1168218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x116821d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1168221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x116822640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x116822ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x116822f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x116823390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x116823800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x116823c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1168240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x116824550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1168249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x116824e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1168252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x116825710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x116825b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x116825ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x116826460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1168268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x116826d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1168271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x116827620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x116827a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x116827f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x116828370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1168287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x116828c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1168290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x116829530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1168299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x116829e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11682a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11682a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11682ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11682afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11682b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11682b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11682bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11682c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11682c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11682ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11682cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11682d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11682d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11682dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11682e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11682e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11682e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11682edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11682f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11682f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11682fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11682ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x116830420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x116830890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x116830d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x116831170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1168315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x116831a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x116831ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x116832330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1168327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x116832c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x116833080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1168334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x116833960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x116833dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x116834240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1168346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x116834b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x116834f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x116835bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x116835e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x116836140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1168365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x116836a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x116836e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x116837300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x116837770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x116837be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x116838050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1168384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x116838930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x116838da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x116839210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x116839680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x116839af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x116839f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11683a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11683a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11683acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11683b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11683b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11683ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11683be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11683c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11683c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11683cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11683d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11683d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11683d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11683dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11683e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11683e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11683ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11683ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11683f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11683f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11683fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x116840290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x116840700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x116840b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x116840fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x116841500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x116841a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x116842580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x116842840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x116842e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1168433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x116843980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x116843f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x116844500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x116844ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x116845080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x116845640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x116845c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1168461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x116846780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x116846d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x116847300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1168478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x116847e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x116848440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x116848a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x116848fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x116849580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x116849b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11684a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11684a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11684ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11684b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11684b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11684bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11684c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11684c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11684cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11684d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11684da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11684e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11684e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11684ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11684f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11684f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11684fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1168502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x116850880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x116850e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x116851400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1168519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x116851f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x116852540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x116852b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1168530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x116853680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x116853c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x116854200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1168547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x116854d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x116855340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x116855900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x116855ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x116856480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x116856a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x116856f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x116857440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x116857940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x116857e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x116858340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x116858840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x116858d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x116859240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x116859740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x116859c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11685a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11685a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11685ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11685b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11685b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11685ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11685bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11685c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11685c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11685ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11685d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11685d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11685dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11685e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11685e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11685f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11685f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11685ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1168606b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x116860970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x116861160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x116861420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x116861a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126169350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126149d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126147bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1261487f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12611b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12611b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12611d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126112c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126119770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12611a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12611a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126119160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12611bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x126111c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12611c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12611e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12612a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1261688a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x126114e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x126115120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126148e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x126113290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x126113550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126113810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126169b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x126169dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12616a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12616a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12616a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12616a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12616ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12616ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12616b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12616b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12616b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12616b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12616bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12616bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12616c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12616c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12616c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12616c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12616cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12616cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12616d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12616d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12616d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12616da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12616dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12616dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12616e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12616e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12616e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12616eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12616ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12616f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12616f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12616f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12616f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12616fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12616fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1261700c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126170380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126170640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126170900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126170bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126170e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126171140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126171400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1261716c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126171980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126171c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126171f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1261721c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126172480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126172740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x126172a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126172cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126172f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x126173240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126173500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1261737c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x126173a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126173d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126174000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1261742c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x126174580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126174840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126174b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126174dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126175080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126175340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126175600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1261758c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126175b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126175e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126176100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1261763c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126176680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126176940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126176c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126176ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126177180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126177440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126177700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1261779c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126177c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126177f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126178200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1261784c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126178780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126178a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126178d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126178fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126179280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126179540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126179800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126179ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126179d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12617a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12617a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12617a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12617a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12617ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12617ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12617b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12617b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12617b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12617b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12617bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12617be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12617c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12617c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12617c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12617c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12617cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12617cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12617d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12617d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12617d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12617da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12617dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12617df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12617e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12617e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12617e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12617ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12617ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12617f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12617f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12617f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12617f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12617fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12617fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126180080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126180340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126180600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1261808c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126180b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126180e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126181100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1261813c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126181680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126181940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126181c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126181ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126182180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126182440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126182700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1261829c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126182c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126182f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126183200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1261834c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126183780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126183a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126183d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126183fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126184280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126184540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126184800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126184ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126184d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126185040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126185300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1261855c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126185880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126185b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126185e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1261860c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x126186380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126186640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126186900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126186bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126186e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126187140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126187400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1261876c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126187980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x126187c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x126187f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1261881c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x126188480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x126188740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126188a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126188cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126188f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126189240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126189500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1261897c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126189d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12618a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12618a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12618a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12618a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12618ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12618ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12618b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12618b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12618be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12618c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12618c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12618cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12618d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12618d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12618dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12618e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12618e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12618edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12618f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12618f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12618fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126190310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x126190860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x126190db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126191300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126191850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126191da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1261922f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126192840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x126192d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1261932e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126193830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x126193d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1261942d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126194820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x126194d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1261952c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126195810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126195d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1261962b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126196800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126196d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1261972a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1261977f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126197d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126198290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1261987e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x126198d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126199280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1261997d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x126199d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12619a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12619a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12619ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12619b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12619b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12619ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12619bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12619bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12619c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12619c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12619cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12619d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12619d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12619da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12619df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12619e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12619e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12619ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12619f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12619f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12619f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12619fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1261a0280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x1261a06f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x1261a0b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1261a0fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1261a1440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1261a18b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1261a1d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1261a2190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1261a2600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1261a3060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1261a3780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1261a3ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1261a45c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1261a4880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1261a4cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1261a52f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1261a5900 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.797s
user	0m0.277s
sys	0m0.316s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4778 (a82c9e7c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12170aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12170b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12170b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12170bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12170c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12170c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12170ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12170d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12170d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12170de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12170e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12170e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12170f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12170fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121710370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121710a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1217111b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1217118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121711ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1217127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121712ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121713600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121713d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1217145c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121714ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121714fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1217155b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121716220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121716760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121716a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121716ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121717180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121717a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121717f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121718210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1217186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121718b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121718ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121719490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121719930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121719dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12171a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12171a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12171abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12171ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12171b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12171ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12171c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12171c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12171cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12171d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12171dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12171e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12171e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12171f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12171f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12171f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12171fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121720210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121720a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121720cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121721160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121721600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121721aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121721f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1217223e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121722880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121722d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1217231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121723660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121723b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121723fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121724440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121724990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121724ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121725430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121725980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121725ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121726420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121726970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121726ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121727410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121727960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121727eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121728400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121728950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121728ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1217293f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121729940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121729e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12172a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12172a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12172ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12172b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12172b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12172be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12172c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12171c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12172c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12172cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12172d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12172da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12172dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12172e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12172ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12172efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12172f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12172fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12172ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121730500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121730a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121730fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1217314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121731990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121731e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1217322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121732770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121732c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1217330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121733550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1217339f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121733e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121734330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1217347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121734c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121735110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1217355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121735a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121735ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121736390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121736830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121736cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121737170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121737610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121737ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121737f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1217383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121738890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121738d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1217391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121739670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121739b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121739fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12173a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12173a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12173ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12173b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12173b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12173bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12173c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12173c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12173c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12173cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12173d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12173d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12173dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12173e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12173e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12173e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12173ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12173f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12173f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12173fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1217400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121740570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121740a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121740eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121741350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1217417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121741c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121742130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1217425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121742a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121742f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1217433b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121743850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121743cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121744190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121744630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121744ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121744f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121745410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1217458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121745d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1217461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121746690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121746b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121746fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121747470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121747910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121747db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121748250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1217486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121748c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121749190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1217496e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121749c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121749ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12174a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12174ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12174b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12174b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12174bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12174c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12174c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12174cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12174d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12174d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12174ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12174e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12174ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12174ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12174f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12174fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12174ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1217504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1217509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121750f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121751490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1217519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121751f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121752480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1217529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121752f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121753470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1217539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121753f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121754460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1217549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121754f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121755450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1217559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121755ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121756440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121756990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121756ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121757430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121757980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121757ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121758420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121758970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121758ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121759410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121759960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121759eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12175a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12175a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12175aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12175b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12175b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12175be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12175c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12175c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12175ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12175d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12175d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12175de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12175e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12175e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12175ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12175f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12175f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12175fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1217603a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1217608f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121760e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121761390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121761830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121761cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121762170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121762610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121762ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121762f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1217633f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121763890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121763d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1217641d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121764670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121764b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121764fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121765450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1217658f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x121765d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x121766230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1217666d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x121766b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x121767010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1217674b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x121767950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x121767df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x121768290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x121768730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121768c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1217693a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121769ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12176a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12176a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12176abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12176b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12176b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12176bc80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.100.435 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.440 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12174a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12174c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12176b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12174a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12174add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12171deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12171d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12171fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12174c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121715260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12171bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12171c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12171b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12171e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12171d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121714260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12172caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12176ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121717440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121717700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12174cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12174b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121715870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121715b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121715df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12176c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12176c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12176c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12176c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12176cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12176cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12176d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12176d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12176d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12176d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12176dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12176df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12176e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12176e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12176e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12176ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12176ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12176efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12176f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12176f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12176f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12176faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12176fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121770020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1217702e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1217705a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121770860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121770b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121770de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1217710a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121771360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121771620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1217718e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121771ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121771e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121772120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1217723e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1217726a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121772960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121772c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121772ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1217731a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121773460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121773720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1217739e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121773ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121773f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121774220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1217744e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1217747a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121774a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121774d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121774fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1217752a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121775560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121775820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121775ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121775da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121776060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121776320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1217765e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1217768a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121776b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121776e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1217770e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1217773a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121777660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121777920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121777be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121777ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121778160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121778420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1217786e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1217789a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121778c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121778f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1217791e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1217794a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121779760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121779a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121779ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121779fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12177a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12177a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12177a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12177aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12177ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12177b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12177b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12177b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12177b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12177bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12177bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12177c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12177c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12177c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12177c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12177cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12177ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12177d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12177d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12177d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12177d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12177dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12177dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12177e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12177e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12177e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12177e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12177eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12177ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12177f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12177f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12177f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12177fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12177fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12177ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1217802a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121780560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121780820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121780ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121780da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121781060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121781320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1217815e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1217818a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121781b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121781e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1217820e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1217823a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121782660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121782920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121782be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121782ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121783160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121783420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1217836e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1217839a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121783c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121783f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1217841e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1217844a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121784760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121784a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121784ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121784fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121785260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121785520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1217857e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121785aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121785d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121786020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1217862e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1217865a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121786860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121786b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121786de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1217870a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121787360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121787620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1217878e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121787ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121787e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121788120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1217883e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1217886a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121788960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121788c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121788ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1217891a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121789460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121789720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1217899e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121789ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121789f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12178a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12178a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12178a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12178aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12178ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12178afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12178b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12178b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12178b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12178bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12178c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12178c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12178c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12178c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12178cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12178ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12178d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12178d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12178de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12178e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12178e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12178ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12178f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12178f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12178fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121790390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1217908e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121790e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121791380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1217918d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121791e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121792370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1217928c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121792e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121793360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1217938b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121793e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121794350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1217948a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121794df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121795340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121795890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121795de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121796330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121796880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121796dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121797320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121797870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121797dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121798310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121798860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121798db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121799300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121799850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121799da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12179a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12179a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12179ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12179b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12179b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12179bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12179c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12179c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12179cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12179d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12179d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12179dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12179e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12179e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12179e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12179ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12179ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12179f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12179f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12179fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1217a0040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1217a04b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1217a0920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1217a0d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1217a1200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1217a1670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1217a1ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x1217a1f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x1217a23c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1217a2830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x1217a2ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x1217a3110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1217a3580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1217a39f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1217a3e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1217a42d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1217a4740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1217a4bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1217a5610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1217a5d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1217a6450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1217a6b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1217a6e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1217a7620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1217a78e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1217a7ef0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1228044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x122804950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x122804dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x122805230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1228056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x122805b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x122805f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1228063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x122806860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x122806cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x122807140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x122807810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x122808330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x122808ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1228092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x122809a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12280a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12280a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12280af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12280b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12280be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12280c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12280cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12280d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12280dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12280dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12280e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12280e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12280e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12280edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12280f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12280f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12280fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12280fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1228102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x122810760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x122810bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x122811040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1228114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x122811920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x122811d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x122812200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x122812670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x122812ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x122812f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1228133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x122813830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x122813ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x122814110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x122814580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1228149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x122814e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1228152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x122815740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x122815bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x122816020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x122816590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x122816a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x122816f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x122817370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1228177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x122817c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1228180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x122818530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1228189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x122818e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x122819280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1228196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x122819b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x122819fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12281a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12281a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12281ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12281b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12281b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12281ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12281bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12281c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12281c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12281cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12281d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12281d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12281d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12281ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12281e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12281e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12281eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12281efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12281f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12281f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12281fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x122820170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1228205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x122820a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x122820ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x122821330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1228217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x122821c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x122822470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x122822990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x122822f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1228234f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x122823aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x122824050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x122824600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x122824bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x122825160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x122825710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x122825cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x122826270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x122826820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x122826dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x122827380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x122827930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x122827e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x122828330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x122828830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x122828d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x122829230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x122829730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x122829c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12282a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12282a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12282ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12282b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12282b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12282ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12282bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12282c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12282c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12282ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12282d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12282d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12282dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12282e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12282e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12282ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12282f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12282f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12282fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x122830030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x122830530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x122830a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x122830f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x122831430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x122831930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x122831e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x122832330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x122832830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x122832d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x122833230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x122833730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x122833c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x122834130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x122834630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x122834b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x122835030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x122835530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x122835a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x122835f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x122836430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x122836930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x122836e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x122837330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x122837830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x122837d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x122838230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x122838730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x122838c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x122839130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x122839630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x122839b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12283a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12283a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12283aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12283af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12283b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12283b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12283be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12283c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12283c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12283cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12283d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12283d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12283dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12283e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12283e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12283eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12283f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12283f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12283fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12283ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x122840430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x122840930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x122840ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x122841490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x122841a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x122841ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x122842600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x122842c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x122843220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x122843a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x122843eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x122844170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x122844780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x122844d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x122845580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x122845a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x122845ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x122846360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x122846b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x122847060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1228475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x122847b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x122848050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1228485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x122848af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x122849040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x122849590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x122849ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12284a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12284a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12284aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12284b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12284b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12284bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12284c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12284c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12284cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12284d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12284d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12284daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12284dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12284e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12284ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12284efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12284f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12284fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12284ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x122850520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x122850a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x122850fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x122851510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x122851a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x122851fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x122852500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x122852a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x122852fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1228534f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x122853a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x122853f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1228544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x122854a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x122854f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1228554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x122855a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x122855f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1228564c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x122856a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x122856f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1228574b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x122857a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x122857f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1228584a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1228589f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x122858f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x122859490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x122859930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x122859dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12285a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12285a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12285abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12285b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12285b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12285b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12285be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12285c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12285c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12285cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12285d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12285d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12285d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12285de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12285e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12285e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12285ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12285f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12285f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12285fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12285fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x122860390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x122860830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x122860d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1228614a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x122861bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1228622e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x122862a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x122862cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1228634b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x122863770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x122863d80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.974s
user	0m0.231s
sys	0m0.191s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.44 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.17 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.60 sec*proc (2 tests)

Total Test time (real) =   1.62 sec
        1.64 real         0.52 user         0.21 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.24 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.56 real         0.12 user         0.08 sys
```
