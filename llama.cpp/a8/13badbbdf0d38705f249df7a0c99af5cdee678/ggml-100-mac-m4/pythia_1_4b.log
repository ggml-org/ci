Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:299 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.518s
user	0m0.846s
sys	0m1.201s
++ nproc
+ make -j10
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Built target build_info
[  7%] Built target sha256
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target sha1
[  7%] Built target xxhash
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 12%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 21%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama
[ 23%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 24%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 24%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 24%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple
[ 29%] Linking C executable ../bin/test-c
[ 29%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Built target llava
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 31%] Linking CXX shared library libllava_shared.dylib
[ 31%] Linking CXX static library libcommon.a
[ 32%] Linking CXX static library libllava_static.a
[ 32%] Built target llama-simple-chat
[ 32%] Built target test-c
[ 32%] Built target llama-simple
[ 32%] Built target llama-quantize-stats
[ 32%] Built target llava_static
[ 32%] Built target common
[ 32%] Built target llava_shared
[ 33%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 37%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-0
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Linking CXX executable ../bin/test-sampling
[ 43%] Linking CXX executable ../bin/test-llama-grammar
[ 44%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 45%] Linking CXX executable ../bin/test-log
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-arg-parser
[ 46%] Built target test-tokenizer-0
[ 46%] Built target test-tokenizer-1-spm
[ 46%] Built target test-tokenizer-1-bpe
[ 46%] Built target test-json-schema-to-grammar
[ 46%] Built target test-llama-grammar
[ 46%] Built target test-sampling
[ 46%] Built target test-grammar-parser
[ 46%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 46%] Built target test-log
[ 46%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 48%] Built target test-grammar-integration
[ 48%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 48%] Built target test-arg-parser
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Linking CXX executable ../bin/test-chat-template
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Linking CXX executable ../bin/test-gguf
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Linking CXX executable ../bin/test-backend-ops
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Linking CXX executable ../bin/test-model-load-cancel
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 57%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 57%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Built target test-gguf
[ 58%] Built target test-chat-template
[ 58%] Linking CXX executable ../bin/test-barrier
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../../bin/llama-batched-bench
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 60%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 60%] Built target test-backend-ops
[ 60%] Built target test-model-load-cancel
[ 61%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 61%] Linking CXX executable ../bin/test-rope
[ 61%] Built target test-autorelease
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Linking CXX executable ../../bin/llama-batched
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Built target test-barrier
[ 64%] Linking CXX executable ../../bin/llama-embedding
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Built target test-quantize-fns
[ 66%] Built target llama-batched-bench
[ 66%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Built target test-rope
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 67%] Built target test-quantize-perf
[ 67%] Linking CXX executable ../../bin/llama-gguf-split
[ 67%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Built target llama-batched
[ 68%] Linking CXX executable ../../bin/llama-gritlm
[ 68%] Built target llama-embedding
[ 69%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 69%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-bench
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-gguf-split
[ 71%] Built target llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 72%] Built target llama-gritlm
[ 73%] Linking CXX executable ../../bin/llama-lookup
[ 74%] Linking CXX executable ../../bin/llama-lookup-create
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 75%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Built target llama-infill
[ 75%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 75%] Built target llama-bench
[ 75%] Built target llama-lookahead
[ 76%] Linking CXX executable ../../bin/llama-lookup-merge
[ 76%] Built target llama-imatrix
[ 76%] Linking CXX executable ../../bin/llama-cli
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Built target llama-lookup
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-lookup-create
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Built target llama-lookup-merge
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Generating index.html.gz.hpp
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-lookup-stats
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Built target llama-cli
[ 84%] Built target llama-parallel
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Built target llama-passkey
[ 86%] Built target llama-quantize
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Built target llama-retrieval
[ 86%] Linking CXX executable ../../bin/llama-run
[ 86%] Built target llama-perplexity
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-run
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Built target llama-save-load-state
[ 93%] Linking CXX executable ../../bin/llama-tts
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-speculative
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-speculative-simple
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Built target llama-tokenize
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Built target llama-tts
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Built target llama-gen-docs
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.932s
user	0m5.942s
sys	0m8.838s

main: quantize time =  5463.67 ms
main:    total time =  5463.67 ms

main: quantize time =  1958.71 ms
main:    total time =  1958.71 ms

main: quantize time =  2152.74 ms
main:    total time =  2152.74 ms

main: quantize time =  2523.50 ms
main:    total time =  2523.50 ms

main: quantize time =  2039.36 ms
main:    total time =  2039.36 ms

main: quantize time =  4987.03 ms
main:    total time =  4987.03 ms

main: quantize time =  5634.67 ms
main:    total time =  5634.67 ms

main: quantize time =  6658.02 ms
main:    total time =  6658.02 ms

main: quantize time =  5968.40 ms
main:    total time =  5968.40 ms

main: quantize time =  4509.07 ms
main:    total time =  4509.07 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.074 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.171 I main: llama backend init
0.00.000.176 I main: load the model and apply lora adapter, if any
0.00.027.312 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.833 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.844 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.846 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.847 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.848 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.848 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.849 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.852 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.853 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.853 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.854 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.854 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.855 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.855 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.858 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.859 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.860 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.854 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.288 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.977 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.981 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.981 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.981 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.982 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.982 I llama_model_loader: - type  f32:  194 tensors
0.00.052.983 I llama_model_loader: - type  f16:   98 tensors
0.00.073.561 I llm_load_vocab: special tokens cache size = 25
0.00.079.552 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.079.555 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.079.555 I llm_load_print_meta: arch             = gptneox
0.00.079.555 I llm_load_print_meta: vocab type       = BPE
0.00.079.556 I llm_load_print_meta: n_vocab          = 50304
0.00.079.556 I llm_load_print_meta: n_merges         = 50009
0.00.079.556 I llm_load_print_meta: vocab_only       = 0
0.00.079.556 I llm_load_print_meta: n_ctx_train      = 2048
0.00.079.556 I llm_load_print_meta: n_embd           = 2048
0.00.079.557 I llm_load_print_meta: n_layer          = 24
0.00.079.560 I llm_load_print_meta: n_head           = 16
0.00.079.561 I llm_load_print_meta: n_head_kv        = 16
0.00.079.561 I llm_load_print_meta: n_rot            = 32
0.00.079.561 I llm_load_print_meta: n_swa            = 0
0.00.079.562 I llm_load_print_meta: n_embd_head_k    = 128
0.00.079.562 I llm_load_print_meta: n_embd_head_v    = 128
0.00.079.562 I llm_load_print_meta: n_gqa            = 1
0.00.079.563 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.079.564 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.079.565 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.079.565 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.079.565 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.079.565 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.079.565 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.079.566 I llm_load_print_meta: n_ff             = 8192
0.00.079.566 I llm_load_print_meta: n_expert         = 0
0.00.079.567 I llm_load_print_meta: n_expert_used    = 0
0.00.079.568 I llm_load_print_meta: causal attn      = 1
0.00.079.568 I llm_load_print_meta: pooling type     = 0
0.00.079.568 I llm_load_print_meta: rope type        = 2
0.00.079.568 I llm_load_print_meta: rope scaling     = linear
0.00.079.569 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.079.569 I llm_load_print_meta: freq_scale_train = 1
0.00.079.569 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.079.569 I llm_load_print_meta: rope_finetuned   = unknown
0.00.079.569 I llm_load_print_meta: ssm_d_conv       = 0
0.00.079.569 I llm_load_print_meta: ssm_d_inner      = 0
0.00.079.570 I llm_load_print_meta: ssm_d_state      = 0
0.00.079.570 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.079.570 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.079.570 I llm_load_print_meta: model type       = 1.4B
0.00.079.572 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.079.572 I llm_load_print_meta: model params     = 1.41 B
0.00.079.573 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.079.573 I llm_load_print_meta: general.name     = 1.4B
0.00.079.573 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.079.573 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.079.573 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.079.573 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.079.574 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.079.574 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.079.574 I llm_load_print_meta: max token length = 1024
0.00.081.456 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.081.456 I llm_load_tensors: offloading output layer to GPU
0.00.081.456 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.081.476 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.081.477 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.082.380 I llama_new_context_with_model: n_seq_max     = 1
0.00.082.381 I llama_new_context_with_model: n_ctx         = 2048
0.00.082.381 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.082.381 I llama_new_context_with_model: n_batch       = 2048
0.00.082.382 I llama_new_context_with_model: n_ubatch      = 512
0.00.082.382 I llama_new_context_with_model: flash_attn    = 0
0.00.082.382 I llama_new_context_with_model: freq_base     = 10000.0
0.00.082.383 I llama_new_context_with_model: freq_scale    = 1
0.00.082.383 I ggml_metal_init: allocating
0.00.082.394 I ggml_metal_init: found device: Apple M4
0.00.082.397 I ggml_metal_init: picking default device: Apple M4
0.00.083.116 I ggml_metal_init: using embedded metal library
0.00.105.114 I ggml_metal_init: GPU name:   Apple M4
0.00.105.118 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.105.119 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.105.119 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.105.119 I ggml_metal_init: simdgroup reduction   = true
0.00.105.119 I ggml_metal_init: simdgroup matrix mul. = true
0.00.105.120 I ggml_metal_init: has bfloat            = true
0.00.105.120 I ggml_metal_init: use bfloat            = true
0.00.105.120 I ggml_metal_init: hasUnifiedMemory      = true
0.00.105.127 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.148.092 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.166.946 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.166.951 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.166.971 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.167.856 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.167.857 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.167.858 I llama_new_context_with_model: graph nodes  = 967
0.00.167.858 I llama_new_context_with_model: graph splits = 2
0.00.167.875 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.167.994 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.167.994 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.249.085 I main: llama threadpool init, n_threads = 4
0.00.249.117 I 
0.00.249.138 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.249.138 I 
0.00.249.213 I sampler seed: 1234
0.00.249.218 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.249.243 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.249.246 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.249.247 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.096.144 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57165.86 tokens per second)
0.02.096.144 I llama_perf_context_print:        load time =     221.77 ms
0.02.096.145 I llama_perf_context_print: prompt eval time =      54.43 ms /     7 tokens (    7.78 ms per token,   128.60 tokens per second)
0.02.096.146 I llama_perf_context_print:        eval time =    1789.56 ms /    63 runs   (   28.41 ms per token,    35.20 tokens per second)
0.02.096.146 I llama_perf_context_print:       total time =    1847.06 ms /    70 tokens
0.02.096.339 I ggml_metal_free: deallocating

real	0m2.410s
user	0m0.127s
sys	0m0.092s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.813 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.541 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.546 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.548 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.548 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.549 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.549 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.549 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.550 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.551 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.551 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.551 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.552 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.552 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.552 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.554 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.554 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.554 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.452 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.547 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.528 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.530 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.530 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.530 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.531 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.531 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.532 I llama_model_loader: - type  f32:  194 tensors
0.00.027.532 I llama_model_loader: - type q8_0:   98 tensors
0.00.049.495 I llm_load_vocab: special tokens cache size = 25
0.00.055.227 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.231 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.231 I llm_load_print_meta: arch             = gptneox
0.00.055.232 I llm_load_print_meta: vocab type       = BPE
0.00.055.234 I llm_load_print_meta: n_vocab          = 50304
0.00.055.234 I llm_load_print_meta: n_merges         = 50009
0.00.055.234 I llm_load_print_meta: vocab_only       = 0
0.00.055.234 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.235 I llm_load_print_meta: n_embd           = 2048
0.00.055.235 I llm_load_print_meta: n_layer          = 24
0.00.055.239 I llm_load_print_meta: n_head           = 16
0.00.055.240 I llm_load_print_meta: n_head_kv        = 16
0.00.055.240 I llm_load_print_meta: n_rot            = 32
0.00.055.240 I llm_load_print_meta: n_swa            = 0
0.00.055.241 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.241 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.242 I llm_load_print_meta: n_gqa            = 1
0.00.055.242 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.243 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.244 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.244 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.244 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.244 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.245 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.245 I llm_load_print_meta: n_ff             = 8192
0.00.055.246 I llm_load_print_meta: n_expert         = 0
0.00.055.246 I llm_load_print_meta: n_expert_used    = 0
0.00.055.246 I llm_load_print_meta: causal attn      = 1
0.00.055.246 I llm_load_print_meta: pooling type     = 0
0.00.055.247 I llm_load_print_meta: rope type        = 2
0.00.055.247 I llm_load_print_meta: rope scaling     = linear
0.00.055.248 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.248 I llm_load_print_meta: freq_scale_train = 1
0.00.055.248 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.248 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.249 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.249 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.249 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.249 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.249 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.249 I llm_load_print_meta: model type       = 1.4B
0.00.055.250 I llm_load_print_meta: model ftype      = Q8_0
0.00.055.252 I llm_load_print_meta: model params     = 1.41 B
0.00.055.253 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.055.253 I llm_load_print_meta: general.name     = 1.4B
0.00.055.253 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.253 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.253 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.253 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.254 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.055.254 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.254 I llm_load_print_meta: max token length = 1024
0.00.057.700 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.700 I llm_load_tensors: offloading output layer to GPU
0.00.057.700 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.712 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.057.713 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.058.738 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.739 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.740 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.740 I llama_new_context_with_model: n_batch       = 2048
0.00.058.740 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.740 I llama_new_context_with_model: flash_attn    = 0
0.00.058.741 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.741 I llama_new_context_with_model: freq_scale    = 1
0.00.058.741 I ggml_metal_init: allocating
0.00.058.744 I ggml_metal_init: found device: Apple M4
0.00.058.747 I ggml_metal_init: picking default device: Apple M4
0.00.059.487 I ggml_metal_init: using embedded metal library
0.00.062.013 I ggml_metal_init: GPU name:   Apple M4
0.00.062.015 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.015 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.016 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.016 I ggml_metal_init: simdgroup reduction   = true
0.00.062.016 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.016 I ggml_metal_init: has bfloat            = true
0.00.062.016 I ggml_metal_init: use bfloat            = true
0.00.062.017 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.018 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.440 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.096.992 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.004 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.029 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.188 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.098.190 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.098.190 I llama_new_context_with_model: graph nodes  = 967
0.00.098.190 I llama_new_context_with_model: graph splits = 2
0.00.098.207 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.098.348 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.349 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.220.357 I main: llama threadpool init, n_threads = 4
0.01.220.389 I 
0.01.220.413 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.220.413 I 
0.01.220.628 I sampler seed: 1234
0.01.220.633 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.220.644 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.220.644 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.220.644 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.305.083 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61206.90 tokens per second)
0.02.305.084 I llama_perf_context_print:        load time =    1210.54 ms
0.02.305.085 I llama_perf_context_print: prompt eval time =      39.84 ms /     7 tokens (    5.69 ms per token,   175.70 tokens per second)
0.02.305.086 I llama_perf_context_print:        eval time =    1041.71 ms /    63 runs   (   16.54 ms per token,    60.48 tokens per second)
0.02.305.086 I llama_perf_context_print:       total time =    1084.73 ms /    70 tokens
0.02.305.273 I ggml_metal_free: deallocating

real	0m2.324s
user	0m0.113s
sys	0m0.222s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.016.235 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.076 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.033.083 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.085 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.091 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.091 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.091 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.092 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.093 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.093 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.093 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.094 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.094 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.094 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.095 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.097 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.097 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.098 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.417 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.551 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.795 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.796 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.797 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.797 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.797 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.798 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.042.798 I llama_model_loader: - type  f32:  194 tensors
0.00.042.799 I llama_model_loader: - type q4_0:   97 tensors
0.00.042.799 I llama_model_loader: - type q6_K:    1 tensors
0.00.070.002 I llm_load_vocab: special tokens cache size = 25
0.00.078.751 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.078.755 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.078.756 I llm_load_print_meta: arch             = gptneox
0.00.078.756 I llm_load_print_meta: vocab type       = BPE
0.00.078.756 I llm_load_print_meta: n_vocab          = 50304
0.00.078.757 I llm_load_print_meta: n_merges         = 50009
0.00.078.757 I llm_load_print_meta: vocab_only       = 0
0.00.078.757 I llm_load_print_meta: n_ctx_train      = 2048
0.00.078.757 I llm_load_print_meta: n_embd           = 2048
0.00.078.757 I llm_load_print_meta: n_layer          = 24
0.00.078.763 I llm_load_print_meta: n_head           = 16
0.00.078.764 I llm_load_print_meta: n_head_kv        = 16
0.00.078.764 I llm_load_print_meta: n_rot            = 32
0.00.078.764 I llm_load_print_meta: n_swa            = 0
0.00.078.765 I llm_load_print_meta: n_embd_head_k    = 128
0.00.078.765 I llm_load_print_meta: n_embd_head_v    = 128
0.00.078.766 I llm_load_print_meta: n_gqa            = 1
0.00.078.767 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.078.768 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.078.768 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.078.769 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.078.769 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.078.770 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.078.772 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.078.772 I llm_load_print_meta: n_ff             = 8192
0.00.078.773 I llm_load_print_meta: n_expert         = 0
0.00.078.773 I llm_load_print_meta: n_expert_used    = 0
0.00.078.773 I llm_load_print_meta: causal attn      = 1
0.00.078.773 I llm_load_print_meta: pooling type     = 0
0.00.078.774 I llm_load_print_meta: rope type        = 2
0.00.078.774 I llm_load_print_meta: rope scaling     = linear
0.00.078.774 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.078.775 I llm_load_print_meta: freq_scale_train = 1
0.00.078.775 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.078.775 I llm_load_print_meta: rope_finetuned   = unknown
0.00.078.776 I llm_load_print_meta: ssm_d_conv       = 0
0.00.078.776 I llm_load_print_meta: ssm_d_inner      = 0
0.00.078.776 I llm_load_print_meta: ssm_d_state      = 0
0.00.078.776 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.078.776 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.078.777 I llm_load_print_meta: model type       = 1.4B
0.00.078.777 I llm_load_print_meta: model ftype      = Q4_0
0.00.078.778 I llm_load_print_meta: model params     = 1.41 B
0.00.078.778 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.078.778 I llm_load_print_meta: general.name     = 1.4B
0.00.078.779 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.078.779 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.078.779 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.078.779 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.078.780 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.078.780 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.078.780 I llm_load_print_meta: max token length = 1024
0.00.081.542 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.081.543 I llm_load_tensors: offloading output layer to GPU
0.00.081.543 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.081.555 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.081.556 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.083.021 I llama_new_context_with_model: n_seq_max     = 1
0.00.083.023 I llama_new_context_with_model: n_ctx         = 2048
0.00.083.023 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.083.023 I llama_new_context_with_model: n_batch       = 2048
0.00.083.024 I llama_new_context_with_model: n_ubatch      = 512
0.00.083.024 I llama_new_context_with_model: flash_attn    = 0
0.00.083.025 I llama_new_context_with_model: freq_base     = 10000.0
0.00.083.025 I llama_new_context_with_model: freq_scale    = 1
0.00.083.026 I ggml_metal_init: allocating
0.00.083.035 I ggml_metal_init: found device: Apple M4
0.00.083.039 I ggml_metal_init: picking default device: Apple M4
0.00.084.023 I ggml_metal_init: using embedded metal library
0.00.087.863 I ggml_metal_init: GPU name:   Apple M4
0.00.087.866 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.087.866 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.087.867 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.087.867 I ggml_metal_init: simdgroup reduction   = true
0.00.087.867 I ggml_metal_init: simdgroup matrix mul. = true
0.00.087.868 I ggml_metal_init: has bfloat            = true
0.00.087.868 I ggml_metal_init: use bfloat            = true
0.00.087.868 I ggml_metal_init: hasUnifiedMemory      = true
0.00.087.871 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.946 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.125.034 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.125.042 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.125.067 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.126.218 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.126.221 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.126.222 I llama_new_context_with_model: graph nodes  = 967
0.00.126.222 I llama_new_context_with_model: graph splits = 2
0.00.126.241 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.126.363 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.126.364 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.085 I main: llama threadpool init, n_threads = 4
0.00.691.125 I 
0.00.691.148 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.148 I 
0.00.691.376 I sampler seed: 1234
0.00.691.380 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.691.423 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.691.423 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.691.423 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.362.848 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60016.91 tokens per second)
0.01.362.849 I llama_perf_context_print:        load time =     674.84 ms
0.01.362.850 I llama_perf_context_print: prompt eval time =      39.75 ms /     7 tokens (    5.68 ms per token,   176.10 tokens per second)
0.01.362.850 I llama_perf_context_print:        eval time =     628.78 ms /    63 runs   (    9.98 ms per token,   100.19 tokens per second)
0.01.362.851 I llama_perf_context_print:       total time =     671.77 ms /    70 tokens
0.01.363.052 I ggml_metal_free: deallocating

real	0m1.384s
user	0m0.127s
sys	0m0.163s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.607 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.873 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.877 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.878 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.879 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.879 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.879 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.879 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.880 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.881 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.881 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.881 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.881 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.882 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.882 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.885 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.885 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.885 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.647 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.627 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.318 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.319 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.320 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.320 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.320 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.321 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.321 I llama_model_loader: - type  f32:  194 tensors
0.00.025.322 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.322 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.492 I llm_load_vocab: special tokens cache size = 25
0.00.052.347 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.350 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.350 I llm_load_print_meta: arch             = gptneox
0.00.052.350 I llm_load_print_meta: vocab type       = BPE
0.00.052.350 I llm_load_print_meta: n_vocab          = 50304
0.00.052.351 I llm_load_print_meta: n_merges         = 50009
0.00.052.351 I llm_load_print_meta: vocab_only       = 0
0.00.052.351 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.351 I llm_load_print_meta: n_embd           = 2048
0.00.052.351 I llm_load_print_meta: n_layer          = 24
0.00.052.354 I llm_load_print_meta: n_head           = 16
0.00.052.355 I llm_load_print_meta: n_head_kv        = 16
0.00.052.357 I llm_load_print_meta: n_rot            = 32
0.00.052.357 I llm_load_print_meta: n_swa            = 0
0.00.052.358 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.358 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.358 I llm_load_print_meta: n_gqa            = 1
0.00.052.359 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.360 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.360 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.361 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.361 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.361 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.361 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.362 I llm_load_print_meta: n_ff             = 8192
0.00.052.362 I llm_load_print_meta: n_expert         = 0
0.00.052.362 I llm_load_print_meta: n_expert_used    = 0
0.00.052.364 I llm_load_print_meta: causal attn      = 1
0.00.052.365 I llm_load_print_meta: pooling type     = 0
0.00.052.365 I llm_load_print_meta: rope type        = 2
0.00.052.365 I llm_load_print_meta: rope scaling     = linear
0.00.052.366 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.366 I llm_load_print_meta: freq_scale_train = 1
0.00.052.366 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.373 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.373 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.374 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.374 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.374 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.374 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.374 I llm_load_print_meta: model type       = 1.4B
0.00.052.375 I llm_load_print_meta: model ftype      = Q4_1
0.00.052.375 I llm_load_print_meta: model params     = 1.41 B
0.00.052.376 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.052.376 I llm_load_print_meta: general.name     = 1.4B
0.00.052.376 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.376 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.376 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.377 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.378 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.380 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.380 I llm_load_print_meta: max token length = 1024
0.00.054.342 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.342 I llm_load_tensors: offloading output layer to GPU
0.00.054.342 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.353 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.054.354 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.055.318 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.318 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.319 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.319 I llama_new_context_with_model: n_batch       = 2048
0.00.055.319 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.319 I llama_new_context_with_model: flash_attn    = 0
0.00.055.320 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.320 I llama_new_context_with_model: freq_scale    = 1
0.00.055.320 I ggml_metal_init: allocating
0.00.055.324 I ggml_metal_init: found device: Apple M4
0.00.055.326 I ggml_metal_init: picking default device: Apple M4
0.00.055.919 I ggml_metal_init: using embedded metal library
0.00.058.282 I ggml_metal_init: GPU name:   Apple M4
0.00.058.283 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.284 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.284 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.284 I ggml_metal_init: simdgroup reduction   = true
0.00.058.284 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.284 I ggml_metal_init: has bfloat            = true
0.00.058.286 I ggml_metal_init: use bfloat            = true
0.00.058.286 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.287 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.936 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.804 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.814 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.852 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.827 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.829 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.829 I llama_new_context_with_model: graph nodes  = 967
0.00.088.829 I llama_new_context_with_model: graph splits = 2
0.00.088.844 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.972 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.972 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.484 I main: llama threadpool init, n_threads = 4
0.00.693.534 I 
0.00.693.563 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.564 I 
0.00.693.787 I sampler seed: 1234
0.00.693.792 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.693.834 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.693.836 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.693.836 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.422.942 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62555.07 tokens per second)
0.01.422.943 I llama_perf_context_print:        load time =     683.87 ms
0.01.422.944 I llama_perf_context_print: prompt eval time =      42.08 ms /     7 tokens (    6.01 ms per token,   166.35 tokens per second)
0.01.422.944 I llama_perf_context_print:        eval time =     684.07 ms /    63 runs   (   10.86 ms per token,    92.10 tokens per second)
0.01.422.945 I llama_perf_context_print:       total time =     729.46 ms /    70 tokens
0.01.423.123 I ggml_metal_free: deallocating

real	0m1.441s
user	0m0.110s
sys	0m0.150s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.973 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.303 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.308 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.315 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.315 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.315 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.316 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.316 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.319 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.319 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.319 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.320 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.320 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.320 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.321 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.323 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.323 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.323 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.161 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.228 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.101 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.103 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.103 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.104 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.104 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.104 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.105 I llama_model_loader: - type  f32:  194 tensors
0.00.026.105 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.105 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.453 I llm_load_vocab: special tokens cache size = 25
0.00.053.317 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.322 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.322 I llm_load_print_meta: arch             = gptneox
0.00.053.323 I llm_load_print_meta: vocab type       = BPE
0.00.053.323 I llm_load_print_meta: n_vocab          = 50304
0.00.053.323 I llm_load_print_meta: n_merges         = 50009
0.00.053.329 I llm_load_print_meta: vocab_only       = 0
0.00.053.329 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.329 I llm_load_print_meta: n_embd           = 2048
0.00.053.329 I llm_load_print_meta: n_layer          = 24
0.00.053.334 I llm_load_print_meta: n_head           = 16
0.00.053.335 I llm_load_print_meta: n_head_kv        = 16
0.00.053.335 I llm_load_print_meta: n_rot            = 32
0.00.053.335 I llm_load_print_meta: n_swa            = 0
0.00.053.335 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.335 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.336 I llm_load_print_meta: n_gqa            = 1
0.00.053.337 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.337 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.338 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.338 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.338 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.340 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.340 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.340 I llm_load_print_meta: n_ff             = 8192
0.00.053.341 I llm_load_print_meta: n_expert         = 0
0.00.053.341 I llm_load_print_meta: n_expert_used    = 0
0.00.053.341 I llm_load_print_meta: causal attn      = 1
0.00.053.341 I llm_load_print_meta: pooling type     = 0
0.00.053.341 I llm_load_print_meta: rope type        = 2
0.00.053.343 I llm_load_print_meta: rope scaling     = linear
0.00.053.343 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.343 I llm_load_print_meta: freq_scale_train = 1
0.00.053.343 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.344 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.344 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.344 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.344 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.344 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.344 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.344 I llm_load_print_meta: model type       = 1.4B
0.00.053.345 I llm_load_print_meta: model ftype      = Q5_0
0.00.053.345 I llm_load_print_meta: model params     = 1.41 B
0.00.053.346 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.053.346 I llm_load_print_meta: general.name     = 1.4B
0.00.053.346 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.346 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.346 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.346 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.347 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.347 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.347 I llm_load_print_meta: max token length = 1024
0.00.055.340 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.340 I llm_load_tensors: offloading output layer to GPU
0.00.055.340 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.351 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.055.352 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.056.234 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.235 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.235 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.235 I llama_new_context_with_model: n_batch       = 2048
0.00.056.235 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.236 I llama_new_context_with_model: flash_attn    = 0
0.00.056.236 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.236 I llama_new_context_with_model: freq_scale    = 1
0.00.056.237 I ggml_metal_init: allocating
0.00.056.240 I ggml_metal_init: found device: Apple M4
0.00.056.242 I ggml_metal_init: picking default device: Apple M4
0.00.056.865 I ggml_metal_init: using embedded metal library
0.00.059.205 I ggml_metal_init: GPU name:   Apple M4
0.00.059.207 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.207 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.207 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.208 I ggml_metal_init: simdgroup reduction   = true
0.00.059.208 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.208 I ggml_metal_init: has bfloat            = true
0.00.059.208 I ggml_metal_init: use bfloat            = true
0.00.059.209 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.209 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.400 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.349 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.353 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.372 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.335 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.337 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.337 I llama_new_context_with_model: graph nodes  = 967
0.00.090.337 I llama_new_context_with_model: graph splits = 2
0.00.090.353 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.494 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.495 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.707 I main: llama threadpool init, n_threads = 4
0.00.750.747 I 
0.00.750.774 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.774 I 
0.00.751.036 I sampler seed: 1234
0.00.751.041 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.751.077 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.751.081 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.751.081 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.534.077 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60220.53 tokens per second)
0.01.534.078 I llama_perf_context_print:        load time =     740.73 ms
0.01.534.079 I llama_perf_context_print: prompt eval time =      43.26 ms /     7 tokens (    6.18 ms per token,   161.82 tokens per second)
0.01.534.079 I llama_perf_context_print:        eval time =     736.78 ms /    63 runs   (   11.69 ms per token,    85.51 tokens per second)
0.01.534.080 I llama_perf_context_print:       total time =     783.37 ms /    70 tokens
0.01.534.223 I ggml_metal_free: deallocating

real	0m1.553s
user	0m0.111s
sys	0m0.155s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.708 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.097 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.101 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.107 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.108 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.108 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.109 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.109 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.110 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.110 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.111 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.111 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.111 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.112 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.112 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.114 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.114 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.114 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.121 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.161 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.059 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.060 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.061 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.061 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.061 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.061 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.062 I llama_model_loader: - type  f32:  194 tensors
0.00.024.062 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.063 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.988 I llm_load_vocab: special tokens cache size = 25
0.00.050.923 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.926 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.926 I llm_load_print_meta: arch             = gptneox
0.00.050.926 I llm_load_print_meta: vocab type       = BPE
0.00.050.927 I llm_load_print_meta: n_vocab          = 50304
0.00.050.927 I llm_load_print_meta: n_merges         = 50009
0.00.050.927 I llm_load_print_meta: vocab_only       = 0
0.00.050.927 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.927 I llm_load_print_meta: n_embd           = 2048
0.00.050.928 I llm_load_print_meta: n_layer          = 24
0.00.050.930 I llm_load_print_meta: n_head           = 16
0.00.050.931 I llm_load_print_meta: n_head_kv        = 16
0.00.050.931 I llm_load_print_meta: n_rot            = 32
0.00.050.932 I llm_load_print_meta: n_swa            = 0
0.00.050.932 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.932 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.933 I llm_load_print_meta: n_gqa            = 1
0.00.050.934 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.934 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.935 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.935 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.935 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.936 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.936 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.936 I llm_load_print_meta: n_ff             = 8192
0.00.050.937 I llm_load_print_meta: n_expert         = 0
0.00.050.937 I llm_load_print_meta: n_expert_used    = 0
0.00.050.937 I llm_load_print_meta: causal attn      = 1
0.00.050.937 I llm_load_print_meta: pooling type     = 0
0.00.050.937 I llm_load_print_meta: rope type        = 2
0.00.050.938 I llm_load_print_meta: rope scaling     = linear
0.00.050.938 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.938 I llm_load_print_meta: freq_scale_train = 1
0.00.050.939 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.939 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.939 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.939 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.939 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.939 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.940 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.940 I llm_load_print_meta: model type       = 1.4B
0.00.050.940 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.941 I llm_load_print_meta: model params     = 1.41 B
0.00.050.941 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.942 I llm_load_print_meta: general.name     = 1.4B
0.00.050.942 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.942 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.942 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.942 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.943 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.943 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.943 I llm_load_print_meta: max token length = 1024
0.00.052.629 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.630 I llm_load_tensors: offloading output layer to GPU
0.00.052.630 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.640 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.641 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.474 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.475 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.475 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.475 I llama_new_context_with_model: n_batch       = 2048
0.00.053.476 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.476 I llama_new_context_with_model: flash_attn    = 0
0.00.053.476 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.477 I llama_new_context_with_model: freq_scale    = 1
0.00.053.477 I ggml_metal_init: allocating
0.00.053.484 I ggml_metal_init: found device: Apple M4
0.00.053.486 I ggml_metal_init: picking default device: Apple M4
0.00.054.109 I ggml_metal_init: using embedded metal library
0.00.056.471 I ggml_metal_init: GPU name:   Apple M4
0.00.056.473 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.473 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.473 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.474 I ggml_metal_init: simdgroup reduction   = true
0.00.056.474 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.474 I ggml_metal_init: has bfloat            = true
0.00.056.474 I ggml_metal_init: use bfloat            = true
0.00.056.475 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.475 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.127 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.782 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.789 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.807 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.755 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.756 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.756 I llama_new_context_with_model: graph nodes  = 967
0.00.087.757 I llama_new_context_with_model: graph splits = 2
0.00.087.773 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.915 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.915 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.268 I main: llama threadpool init, n_threads = 4
0.00.762.307 I 
0.00.762.354 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.762.355 I 
0.00.762.582 I sampler seed: 1234
0.00.762.586 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.762.627 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.762.627 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.762.627 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.604.558 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57350.57 tokens per second)
0.01.604.558 I llama_perf_context_print:        load time =     753.56 ms
0.01.604.559 I llama_perf_context_print: prompt eval time =      46.14 ms /     7 tokens (    6.59 ms per token,   151.71 tokens per second)
0.01.604.560 I llama_perf_context_print:        eval time =     792.77 ms /    63 runs   (   12.58 ms per token,    79.47 tokens per second)
0.01.604.560 I llama_perf_context_print:       total time =     842.29 ms /    70 tokens
0.01.604.714 I ggml_metal_free: deallocating

real	0m1.622s
user	0m0.110s
sys	0m0.160s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.972 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.442 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.447 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.448 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.449 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.449 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.450 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.450 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.451 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.451 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.452 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.452 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.453 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.455 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.456 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.457 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.457 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.458 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.268 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.305 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.151 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.152 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.152 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.153 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.153 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.153 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.154 I llama_model_loader: - type  f32:  194 tensors
0.00.024.154 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.154 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.155 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.325 I llm_load_vocab: special tokens cache size = 25
0.00.050.049 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.052 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.052 I llm_load_print_meta: arch             = gptneox
0.00.050.053 I llm_load_print_meta: vocab type       = BPE
0.00.050.053 I llm_load_print_meta: n_vocab          = 50304
0.00.050.053 I llm_load_print_meta: n_merges         = 50009
0.00.050.053 I llm_load_print_meta: vocab_only       = 0
0.00.050.053 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.054 I llm_load_print_meta: n_embd           = 2048
0.00.050.054 I llm_load_print_meta: n_layer          = 24
0.00.050.056 I llm_load_print_meta: n_head           = 16
0.00.050.057 I llm_load_print_meta: n_head_kv        = 16
0.00.050.057 I llm_load_print_meta: n_rot            = 32
0.00.050.058 I llm_load_print_meta: n_swa            = 0
0.00.050.058 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.058 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.059 I llm_load_print_meta: n_gqa            = 1
0.00.050.060 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.060 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.061 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.061 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.061 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.062 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.062 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.062 I llm_load_print_meta: n_ff             = 8192
0.00.050.063 I llm_load_print_meta: n_expert         = 0
0.00.050.063 I llm_load_print_meta: n_expert_used    = 0
0.00.050.063 I llm_load_print_meta: causal attn      = 1
0.00.050.063 I llm_load_print_meta: pooling type     = 0
0.00.050.063 I llm_load_print_meta: rope type        = 2
0.00.050.063 I llm_load_print_meta: rope scaling     = linear
0.00.050.064 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.064 I llm_load_print_meta: freq_scale_train = 1
0.00.050.064 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.064 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.065 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.066 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.066 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.067 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.068 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.068 I llm_load_print_meta: model type       = 1.4B
0.00.050.068 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.068 I llm_load_print_meta: model params     = 1.41 B
0.00.050.069 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.069 I llm_load_print_meta: general.name     = 1.4B
0.00.050.069 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.070 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.070 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.070 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.070 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.070 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.071 I llm_load_print_meta: max token length = 1024
0.00.051.911 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.911 I llm_load_tensors: offloading output layer to GPU
0.00.051.911 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.922 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.923 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.860 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.860 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.861 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.861 I llama_new_context_with_model: n_batch       = 2048
0.00.052.861 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.861 I llama_new_context_with_model: flash_attn    = 0
0.00.052.862 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.862 I llama_new_context_with_model: freq_scale    = 1
0.00.052.862 I ggml_metal_init: allocating
0.00.052.866 I ggml_metal_init: found device: Apple M4
0.00.052.868 I ggml_metal_init: picking default device: Apple M4
0.00.053.441 I ggml_metal_init: using embedded metal library
0.00.055.751 I ggml_metal_init: GPU name:   Apple M4
0.00.055.752 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.752 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.753 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.753 I ggml_metal_init: simdgroup reduction   = true
0.00.055.753 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.753 I ggml_metal_init: has bfloat            = true
0.00.055.753 I ggml_metal_init: use bfloat            = true
0.00.055.754 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.755 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.368 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.084.640 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.645 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.662 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.764 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.765 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.765 I llama_new_context_with_model: graph nodes  = 967
0.00.085.766 I llama_new_context_with_model: graph splits = 2
0.00.085.782 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.930 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.931 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.452.775 I main: llama threadpool init, n_threads = 4
0.00.452.821 I 
0.00.452.855 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.452.857 I 
0.00.453.105 I sampler seed: 1234
0.00.453.110 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.453.121 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.453.121 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.453.123 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.136.705 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62280.70 tokens per second)
0.01.136.706 I llama_perf_context_print:        load time =     442.80 ms
0.01.136.707 I llama_perf_context_print: prompt eval time =      39.74 ms /     7 tokens (    5.68 ms per token,   176.15 tokens per second)
0.01.136.707 I llama_perf_context_print:        eval time =     640.91 ms /    63 runs   (   10.17 ms per token,    98.30 tokens per second)
0.01.136.708 I llama_perf_context_print:       total time =     683.93 ms /    70 tokens
0.01.136.908 I ggml_metal_free: deallocating

real	0m1.155s
user	0m0.109s
sys	0m0.113s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.788 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.459 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.464 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.470 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.470 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.471 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.471 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.471 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.472 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.473 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.473 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.473 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.474 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.474 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.475 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.476 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.476 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.477 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.419 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.518 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.387 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.388 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.389 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.389 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.389 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.389 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.390 I llama_model_loader: - type  f32:  194 tensors
0.00.024.390 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.391 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.391 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.391 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.356 I llm_load_vocab: special tokens cache size = 25
0.00.051.259 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.261 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.262 I llm_load_print_meta: arch             = gptneox
0.00.051.262 I llm_load_print_meta: vocab type       = BPE
0.00.051.262 I llm_load_print_meta: n_vocab          = 50304
0.00.051.263 I llm_load_print_meta: n_merges         = 50009
0.00.051.263 I llm_load_print_meta: vocab_only       = 0
0.00.051.263 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.263 I llm_load_print_meta: n_embd           = 2048
0.00.051.263 I llm_load_print_meta: n_layer          = 24
0.00.051.266 I llm_load_print_meta: n_head           = 16
0.00.051.267 I llm_load_print_meta: n_head_kv        = 16
0.00.051.267 I llm_load_print_meta: n_rot            = 32
0.00.051.267 I llm_load_print_meta: n_swa            = 0
0.00.051.267 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.268 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.268 I llm_load_print_meta: n_gqa            = 1
0.00.051.269 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.270 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.270 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.271 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.271 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.271 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.271 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.272 I llm_load_print_meta: n_ff             = 8192
0.00.051.272 I llm_load_print_meta: n_expert         = 0
0.00.051.272 I llm_load_print_meta: n_expert_used    = 0
0.00.051.273 I llm_load_print_meta: causal attn      = 1
0.00.051.274 I llm_load_print_meta: pooling type     = 0
0.00.051.274 I llm_load_print_meta: rope type        = 2
0.00.051.274 I llm_load_print_meta: rope scaling     = linear
0.00.051.275 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.275 I llm_load_print_meta: freq_scale_train = 1
0.00.051.275 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.275 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.276 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.276 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.276 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.276 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.276 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.276 I llm_load_print_meta: model type       = 1.4B
0.00.051.277 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.277 I llm_load_print_meta: model params     = 1.41 B
0.00.051.278 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.278 I llm_load_print_meta: general.name     = 1.4B
0.00.051.278 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.279 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.279 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.279 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.279 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.281 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.281 I llm_load_print_meta: max token length = 1024
0.00.053.249 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.249 I llm_load_tensors: offloading output layer to GPU
0.00.053.249 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.260 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.261 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.161 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.162 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.162 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.162 I llama_new_context_with_model: n_batch       = 2048
0.00.054.162 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.162 I llama_new_context_with_model: flash_attn    = 0
0.00.054.163 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.163 I llama_new_context_with_model: freq_scale    = 1
0.00.054.164 I ggml_metal_init: allocating
0.00.054.167 I ggml_metal_init: found device: Apple M4
0.00.054.169 I ggml_metal_init: picking default device: Apple M4
0.00.054.788 I ggml_metal_init: using embedded metal library
0.00.057.111 I ggml_metal_init: GPU name:   Apple M4
0.00.057.113 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.113 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.113 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.114 I ggml_metal_init: simdgroup reduction   = true
0.00.057.114 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.114 I ggml_metal_init: has bfloat            = true
0.00.057.114 I ggml_metal_init: use bfloat            = true
0.00.057.114 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.115 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.939 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.536 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.541 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.563 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.591 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.592 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.592 I llama_new_context_with_model: graph nodes  = 967
0.00.088.592 I llama_new_context_with_model: graph splits = 2
0.00.088.608 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.750 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.751 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.526.654 I main: llama threadpool init, n_threads = 4
0.00.526.699 I 
0.00.526.720 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.526.720 I 
0.00.526.865 I sampler seed: 1234
0.00.526.870 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.526.905 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.526.908 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.526.908 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.275.414 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56082.15 tokens per second)
0.01.275.414 I llama_perf_context_print:        load time =     517.86 ms
0.01.275.415 I llama_perf_context_print: prompt eval time =      43.38 ms /     7 tokens (    6.20 ms per token,   161.35 tokens per second)
0.01.275.416 I llama_perf_context_print:        eval time =     702.00 ms /    63 runs   (   11.14 ms per token,    89.74 tokens per second)
0.01.275.420 I llama_perf_context_print:       total time =     748.76 ms /    70 tokens
0.01.275.611 I ggml_metal_free: deallocating

real	0m1.292s
user	0m0.111s
sys	0m0.116s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.795 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.516 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.521 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.522 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.523 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.523 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.523 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.524 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.524 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.525 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.525 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.525 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.526 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.526 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.527 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.528 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.529 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.529 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.331 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.407 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.224 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.225 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.225 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.226 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.226 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.226 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.227 I llama_model_loader: - type  f32:  194 tensors
0.00.023.227 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.228 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.228 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.428 I llm_load_vocab: special tokens cache size = 25
0.00.049.413 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.415 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.415 I llm_load_print_meta: arch             = gptneox
0.00.049.416 I llm_load_print_meta: vocab type       = BPE
0.00.049.416 I llm_load_print_meta: n_vocab          = 50304
0.00.049.416 I llm_load_print_meta: n_merges         = 50009
0.00.049.416 I llm_load_print_meta: vocab_only       = 0
0.00.049.417 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.417 I llm_load_print_meta: n_embd           = 2048
0.00.049.417 I llm_load_print_meta: n_layer          = 24
0.00.049.420 I llm_load_print_meta: n_head           = 16
0.00.049.421 I llm_load_print_meta: n_head_kv        = 16
0.00.049.421 I llm_load_print_meta: n_rot            = 32
0.00.049.421 I llm_load_print_meta: n_swa            = 0
0.00.049.421 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.422 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.422 I llm_load_print_meta: n_gqa            = 1
0.00.049.423 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.424 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.424 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.425 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.425 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.425 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.425 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.426 I llm_load_print_meta: n_ff             = 8192
0.00.049.426 I llm_load_print_meta: n_expert         = 0
0.00.049.426 I llm_load_print_meta: n_expert_used    = 0
0.00.049.427 I llm_load_print_meta: causal attn      = 1
0.00.049.427 I llm_load_print_meta: pooling type     = 0
0.00.049.427 I llm_load_print_meta: rope type        = 2
0.00.049.427 I llm_load_print_meta: rope scaling     = linear
0.00.049.428 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.428 I llm_load_print_meta: freq_scale_train = 1
0.00.049.428 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.429 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.429 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.430 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.430 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.430 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.432 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.432 I llm_load_print_meta: model type       = 1.4B
0.00.049.432 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.433 I llm_load_print_meta: model params     = 1.41 B
0.00.049.433 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.434 I llm_load_print_meta: general.name     = 1.4B
0.00.049.434 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.434 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.434 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.434 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.438 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.439 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.439 I llm_load_print_meta: max token length = 1024
0.00.051.403 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.403 I llm_load_tensors: offloading output layer to GPU
0.00.051.403 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.413 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.414 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.317 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.318 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.318 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.318 I llama_new_context_with_model: n_batch       = 2048
0.00.052.318 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.319 I llama_new_context_with_model: flash_attn    = 0
0.00.052.319 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.319 I llama_new_context_with_model: freq_scale    = 1
0.00.052.320 I ggml_metal_init: allocating
0.00.052.322 I ggml_metal_init: found device: Apple M4
0.00.052.324 I ggml_metal_init: picking default device: Apple M4
0.00.052.911 I ggml_metal_init: using embedded metal library
0.00.055.208 I ggml_metal_init: GPU name:   Apple M4
0.00.055.209 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.210 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.210 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.210 I ggml_metal_init: simdgroup reduction   = true
0.00.055.210 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.210 I ggml_metal_init: has bfloat            = true
0.00.055.211 I ggml_metal_init: use bfloat            = true
0.00.055.211 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.212 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.797 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.083.870 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.876 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.894 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.893 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.894 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.895 I llama_new_context_with_model: graph nodes  = 967
0.00.084.895 I llama_new_context_with_model: graph splits = 2
0.00.084.911 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.053 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.054 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.627.694 I main: llama threadpool init, n_threads = 4
0.00.627.731 I 
0.00.627.754 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.627.754 I 
0.00.627.982 I sampler seed: 1234
0.00.627.992 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.628.005 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.628.005 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.628.005 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.375.958 I llama_perf_sampler_print:    sampling time =       1.53 ms /    71 runs   (    0.02 ms per token, 46344.65 tokens per second)
0.01.375.959 I llama_perf_context_print:        load time =     618.89 ms
0.01.375.960 I llama_perf_context_print: prompt eval time =      47.06 ms /     7 tokens (    6.72 ms per token,   148.75 tokens per second)
0.01.375.961 I llama_perf_context_print:        eval time =     698.35 ms /    63 runs   (   11.08 ms per token,    90.21 tokens per second)
0.01.375.961 I llama_perf_context_print:       total time =     748.27 ms /    70 tokens
0.01.376.199 I ggml_metal_free: deallocating

real	0m1.392s
user	0m0.109s
sys	0m0.145s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.999 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.580 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.587 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.589 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.589 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.589 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.590 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.590 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.591 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.591 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.592 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.592 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.592 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.593 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.593 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.595 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.595 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.596 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.480 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.565 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.653 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.655 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.655 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.655 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.656 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.656 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.657 I llama_model_loader: - type  f32:  194 tensors
0.00.024.657 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.657 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.146 I llm_load_vocab: special tokens cache size = 25
0.00.052.025 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.028 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.029 I llm_load_print_meta: arch             = gptneox
0.00.052.029 I llm_load_print_meta: vocab type       = BPE
0.00.052.029 I llm_load_print_meta: n_vocab          = 50304
0.00.052.029 I llm_load_print_meta: n_merges         = 50009
0.00.052.030 I llm_load_print_meta: vocab_only       = 0
0.00.052.030 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.030 I llm_load_print_meta: n_embd           = 2048
0.00.052.030 I llm_load_print_meta: n_layer          = 24
0.00.052.037 I llm_load_print_meta: n_head           = 16
0.00.052.039 I llm_load_print_meta: n_head_kv        = 16
0.00.052.039 I llm_load_print_meta: n_rot            = 32
0.00.052.039 I llm_load_print_meta: n_swa            = 0
0.00.052.039 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.039 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.040 I llm_load_print_meta: n_gqa            = 1
0.00.052.045 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.046 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.047 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.047 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.048 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.050 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.050 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.050 I llm_load_print_meta: n_ff             = 8192
0.00.052.050 I llm_load_print_meta: n_expert         = 0
0.00.052.051 I llm_load_print_meta: n_expert_used    = 0
0.00.052.051 I llm_load_print_meta: causal attn      = 1
0.00.052.051 I llm_load_print_meta: pooling type     = 0
0.00.052.051 I llm_load_print_meta: rope type        = 2
0.00.052.051 I llm_load_print_meta: rope scaling     = linear
0.00.052.051 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.052 I llm_load_print_meta: freq_scale_train = 1
0.00.052.052 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.052 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.052 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.052 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.052 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.053 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.053 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.053 I llm_load_print_meta: model type       = 1.4B
0.00.052.053 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.054 I llm_load_print_meta: model params     = 1.41 B
0.00.052.054 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.054 I llm_load_print_meta: general.name     = 1.4B
0.00.052.055 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.055 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.055 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.055 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.056 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.056 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.056 I llm_load_print_meta: max token length = 1024
0.00.054.207 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.207 I llm_load_tensors: offloading output layer to GPU
0.00.054.207 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.218 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.219 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.168 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.169 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.169 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.169 I llama_new_context_with_model: n_batch       = 2048
0.00.055.170 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.170 I llama_new_context_with_model: flash_attn    = 0
0.00.055.170 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.171 I llama_new_context_with_model: freq_scale    = 1
0.00.055.171 I ggml_metal_init: allocating
0.00.055.180 I ggml_metal_init: found device: Apple M4
0.00.055.182 I ggml_metal_init: picking default device: Apple M4
0.00.055.824 I ggml_metal_init: using embedded metal library
0.00.058.198 I ggml_metal_init: GPU name:   Apple M4
0.00.058.200 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.200 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.201 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.201 I ggml_metal_init: simdgroup reduction   = true
0.00.058.201 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.201 I ggml_metal_init: has bfloat            = true
0.00.058.202 I ggml_metal_init: use bfloat            = true
0.00.058.202 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.203 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.404 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.088.829 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.839 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.859 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.822 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.824 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.824 I llama_new_context_with_model: graph nodes  = 967
0.00.089.824 I llama_new_context_with_model: graph splits = 2
0.00.089.839 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.981 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.982 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.696.097 I main: llama threadpool init, n_threads = 4
0.00.696.138 I 
0.00.696.170 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.696.172 I 
0.00.696.414 I sampler seed: 1234
0.00.696.419 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.696.456 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.696.458 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.696.458 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.549.646 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61685.49 tokens per second)
0.01.549.646 I llama_perf_context_print:        load time =     686.09 ms
0.01.549.649 I llama_perf_context_print: prompt eval time =      55.45 ms /     7 tokens (    7.92 ms per token,   126.25 tokens per second)
0.01.549.650 I llama_perf_context_print:        eval time =     794.86 ms /    63 runs   (   12.62 ms per token,    79.26 tokens per second)
0.01.549.650 I llama_perf_context_print:       total time =     853.55 ms /    70 tokens
0.01.549.850 I ggml_metal_free: deallocating

real	0m1.570s
user	0m0.112s
sys	0m0.161s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.668 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.603 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.607 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.613 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.614 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.614 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.615 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.615 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.616 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.616 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.616 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.617 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.617 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.618 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.618 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.620 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.620 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.620 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.572 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.616 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.521 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.523 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.523 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.523 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.524 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.524 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.524 I llama_model_loader: - type  f32:  194 tensors
0.00.024.525 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.814 I llm_load_vocab: special tokens cache size = 25
0.00.050.497 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.500 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.500 I llm_load_print_meta: arch             = gptneox
0.00.050.501 I llm_load_print_meta: vocab type       = BPE
0.00.050.501 I llm_load_print_meta: n_vocab          = 50304
0.00.050.501 I llm_load_print_meta: n_merges         = 50009
0.00.050.501 I llm_load_print_meta: vocab_only       = 0
0.00.050.501 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.501 I llm_load_print_meta: n_embd           = 2048
0.00.050.502 I llm_load_print_meta: n_layer          = 24
0.00.050.504 I llm_load_print_meta: n_head           = 16
0.00.050.505 I llm_load_print_meta: n_head_kv        = 16
0.00.050.505 I llm_load_print_meta: n_rot            = 32
0.00.050.505 I llm_load_print_meta: n_swa            = 0
0.00.050.505 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.506 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.506 I llm_load_print_meta: n_gqa            = 1
0.00.050.507 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.510 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.510 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.511 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.511 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.511 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.511 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.512 I llm_load_print_meta: n_ff             = 8192
0.00.050.512 I llm_load_print_meta: n_expert         = 0
0.00.050.512 I llm_load_print_meta: n_expert_used    = 0
0.00.050.513 I llm_load_print_meta: causal attn      = 1
0.00.050.514 I llm_load_print_meta: pooling type     = 0
0.00.050.514 I llm_load_print_meta: rope type        = 2
0.00.050.514 I llm_load_print_meta: rope scaling     = linear
0.00.050.514 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.515 I llm_load_print_meta: freq_scale_train = 1
0.00.050.515 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.515 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.515 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.515 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.515 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.516 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.516 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.516 I llm_load_print_meta: model type       = 1.4B
0.00.050.516 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.517 I llm_load_print_meta: model params     = 1.41 B
0.00.050.517 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.517 I llm_load_print_meta: general.name     = 1.4B
0.00.050.517 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.518 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.518 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.518 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.518 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.519 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.519 I llm_load_print_meta: max token length = 1024
0.00.052.539 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.539 I llm_load_tensors: offloading output layer to GPU
0.00.052.539 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.549 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.551 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.478 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.479 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.479 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.480 I llama_new_context_with_model: n_batch       = 2048
0.00.053.480 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.480 I llama_new_context_with_model: flash_attn    = 0
0.00.053.480 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.481 I llama_new_context_with_model: freq_scale    = 1
0.00.053.481 I ggml_metal_init: allocating
0.00.053.484 I ggml_metal_init: found device: Apple M4
0.00.053.486 I ggml_metal_init: picking default device: Apple M4
0.00.054.137 I ggml_metal_init: using embedded metal library
0.00.056.459 I ggml_metal_init: GPU name:   Apple M4
0.00.056.460 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.461 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.461 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.461 I ggml_metal_init: simdgroup reduction   = true
0.00.056.461 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.461 I ggml_metal_init: has bfloat            = true
0.00.056.462 I ggml_metal_init: use bfloat            = true
0.00.056.462 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.462 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.082 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.008 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.015 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.040 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.037 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.038 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.038 I llama_new_context_with_model: graph nodes  = 967
0.00.087.039 I llama_new_context_with_model: graph splits = 2
0.00.087.054 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.182 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.182 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.271 I main: llama threadpool init, n_threads = 4
0.00.750.306 I 
0.00.750.327 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.327 I 
0.00.750.560 I sampler seed: 1234
0.00.750.567 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.750.578 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.750.579 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.750.579 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.626.316 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57959.18 tokens per second)
0.01.626.317 I llama_perf_context_print:        load time =     741.60 ms
0.01.626.318 I llama_perf_context_print: prompt eval time =      54.46 ms /     7 tokens (    7.78 ms per token,   128.54 tokens per second)
0.01.626.318 I llama_perf_context_print:        eval time =     818.31 ms /    63 runs   (   12.99 ms per token,    76.99 tokens per second)
0.01.626.320 I llama_perf_context_print:       total time =     876.05 ms /    70 tokens
0.01.626.536 I ggml_metal_free: deallocating

real	0m1.642s
user	0m0.108s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.653 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.990 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.953 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.958 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.962 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.963 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.963 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.964 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.964 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.965 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.966 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.966 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.966 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.967 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.967 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.967 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.971 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.971 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.972 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.041 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.261 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.368 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.370 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.371 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.371 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.371 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.372 I llama_model_loader: - type  f32:  194 tensors
0.00.055.373 I llama_model_loader: - type  f16:   98 tensors
0.00.085.407 I llm_load_vocab: special tokens cache size = 25
0.00.092.027 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.092.029 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.092.029 I llm_load_print_meta: arch             = gptneox
0.00.092.030 I llm_load_print_meta: vocab type       = BPE
0.00.092.030 I llm_load_print_meta: n_vocab          = 50304
0.00.092.030 I llm_load_print_meta: n_merges         = 50009
0.00.092.030 I llm_load_print_meta: vocab_only       = 0
0.00.092.030 I llm_load_print_meta: n_ctx_train      = 2048
0.00.092.031 I llm_load_print_meta: n_embd           = 2048
0.00.092.031 I llm_load_print_meta: n_layer          = 24
0.00.092.034 I llm_load_print_meta: n_head           = 16
0.00.092.034 I llm_load_print_meta: n_head_kv        = 16
0.00.092.035 I llm_load_print_meta: n_rot            = 32
0.00.092.035 I llm_load_print_meta: n_swa            = 0
0.00.092.035 I llm_load_print_meta: n_embd_head_k    = 128
0.00.092.035 I llm_load_print_meta: n_embd_head_v    = 128
0.00.092.036 I llm_load_print_meta: n_gqa            = 1
0.00.092.037 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.092.037 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.092.040 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.092.040 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.092.040 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.092.040 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.092.040 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.092.041 I llm_load_print_meta: n_ff             = 8192
0.00.092.041 I llm_load_print_meta: n_expert         = 0
0.00.092.041 I llm_load_print_meta: n_expert_used    = 0
0.00.092.041 I llm_load_print_meta: causal attn      = 1
0.00.092.041 I llm_load_print_meta: pooling type     = 0
0.00.092.042 I llm_load_print_meta: rope type        = 2
0.00.092.042 I llm_load_print_meta: rope scaling     = linear
0.00.092.042 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.092.042 I llm_load_print_meta: freq_scale_train = 1
0.00.092.044 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.092.045 I llm_load_print_meta: rope_finetuned   = unknown
0.00.092.045 I llm_load_print_meta: ssm_d_conv       = 0
0.00.092.046 I llm_load_print_meta: ssm_d_inner      = 0
0.00.092.046 I llm_load_print_meta: ssm_d_state      = 0
0.00.092.046 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.092.046 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.092.046 I llm_load_print_meta: model type       = 1.4B
0.00.092.047 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.092.048 I llm_load_print_meta: model params     = 1.41 B
0.00.092.049 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.092.049 I llm_load_print_meta: general.name     = 1.4B
0.00.092.049 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.092.049 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.092.050 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.092.050 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.092.051 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.092.051 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.092.051 I llm_load_print_meta: max token length = 1024
0.00.094.625 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.094.625 I llm_load_tensors: offloading output layer to GPU
0.00.094.625 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.094.636 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.637 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.095.569 I llama_new_context_with_model: n_seq_max     = 1
0.00.095.570 I llama_new_context_with_model: n_ctx         = 128
0.00.095.570 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.095.570 I llama_new_context_with_model: n_batch       = 128
0.00.095.571 I llama_new_context_with_model: n_ubatch      = 128
0.00.095.571 I llama_new_context_with_model: flash_attn    = 0
0.00.095.571 I llama_new_context_with_model: freq_base     = 10000.0
0.00.095.572 I llama_new_context_with_model: freq_scale    = 1
0.00.095.572 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.095.572 I ggml_metal_init: allocating
0.00.095.580 I ggml_metal_init: found device: Apple M4
0.00.095.582 I ggml_metal_init: picking default device: Apple M4
0.00.096.191 I ggml_metal_init: using embedded metal library
0.00.098.755 I ggml_metal_init: GPU name:   Apple M4
0.00.098.757 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.757 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.758 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.758 I ggml_metal_init: simdgroup reduction   = true
0.00.098.758 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.758 I ggml_metal_init: has bfloat            = true
0.00.098.758 I ggml_metal_init: use bfloat            = true
0.00.098.759 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.760 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.782 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.109.186 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.190 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.205 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.013 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.110.014 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.110.014 I llama_new_context_with_model: graph nodes  = 967
0.00.110.014 I llama_new_context_with_model: graph splits = 2
0.00.110.021 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.110.022 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.252.599 I 
0.01.252.640 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.252.681 I perplexity: tokenizing the input ..
0.01.264.766 I perplexity: tokenization took 12.083 ms
0.01.264.771 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.385.639 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.387.584 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.387.601 I llama_perf_context_print:        load time =    1226.59 ms
0.01.387.603 I llama_perf_context_print: prompt eval time =     120.49 ms /   128 tokens (    0.94 ms per token,  1062.32 tokens per second)
0.01.387.604 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.387.605 I llama_perf_context_print:       total time =     135.01 ms /   129 tokens
0.01.388.341 I ggml_metal_free: deallocating

real	0m1.579s
user	0m0.123s
sys	0m0.219s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.134 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.738 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.880 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.886 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.888 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.889 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.889 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.889 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.890 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.891 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.891 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.892 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.892 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.894 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.894 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.895 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.897 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.898 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.898 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.763 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.602 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.332 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.335 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.335 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.336 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.336 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.336 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.337 I llama_model_loader: - type  f32:  194 tensors
0.00.034.337 I llama_model_loader: - type q8_0:   98 tensors
0.00.062.951 I llm_load_vocab: special tokens cache size = 25
0.00.069.616 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.069.619 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.069.619 I llm_load_print_meta: arch             = gptneox
0.00.069.620 I llm_load_print_meta: vocab type       = BPE
0.00.069.620 I llm_load_print_meta: n_vocab          = 50304
0.00.069.620 I llm_load_print_meta: n_merges         = 50009
0.00.069.620 I llm_load_print_meta: vocab_only       = 0
0.00.069.620 I llm_load_print_meta: n_ctx_train      = 2048
0.00.069.620 I llm_load_print_meta: n_embd           = 2048
0.00.069.621 I llm_load_print_meta: n_layer          = 24
0.00.069.624 I llm_load_print_meta: n_head           = 16
0.00.069.624 I llm_load_print_meta: n_head_kv        = 16
0.00.069.624 I llm_load_print_meta: n_rot            = 32
0.00.069.625 I llm_load_print_meta: n_swa            = 0
0.00.069.627 I llm_load_print_meta: n_embd_head_k    = 128
0.00.069.627 I llm_load_print_meta: n_embd_head_v    = 128
0.00.069.628 I llm_load_print_meta: n_gqa            = 1
0.00.069.629 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.069.629 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.069.630 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.069.630 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.069.637 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.069.639 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.069.640 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.069.648 I llm_load_print_meta: n_ff             = 8192
0.00.069.648 I llm_load_print_meta: n_expert         = 0
0.00.069.650 I llm_load_print_meta: n_expert_used    = 0
0.00.069.650 I llm_load_print_meta: causal attn      = 1
0.00.069.651 I llm_load_print_meta: pooling type     = 0
0.00.069.651 I llm_load_print_meta: rope type        = 2
0.00.069.651 I llm_load_print_meta: rope scaling     = linear
0.00.069.651 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.069.652 I llm_load_print_meta: freq_scale_train = 1
0.00.069.652 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.069.652 I llm_load_print_meta: rope_finetuned   = unknown
0.00.069.652 I llm_load_print_meta: ssm_d_conv       = 0
0.00.069.653 I llm_load_print_meta: ssm_d_inner      = 0
0.00.069.653 I llm_load_print_meta: ssm_d_state      = 0
0.00.069.653 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.069.654 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.069.655 I llm_load_print_meta: model type       = 1.4B
0.00.069.655 I llm_load_print_meta: model ftype      = Q8_0
0.00.069.655 I llm_load_print_meta: model params     = 1.41 B
0.00.069.656 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.069.657 I llm_load_print_meta: general.name     = 1.4B
0.00.069.657 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.069.657 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.069.657 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.069.658 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.069.658 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.069.658 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.069.658 I llm_load_print_meta: max token length = 1024
0.00.072.124 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.072.124 I llm_load_tensors: offloading output layer to GPU
0.00.072.125 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.072.135 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.072.137 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.073.138 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.139 I llama_new_context_with_model: n_ctx         = 128
0.00.073.139 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.073.140 I llama_new_context_with_model: n_batch       = 128
0.00.073.140 I llama_new_context_with_model: n_ubatch      = 128
0.00.073.140 I llama_new_context_with_model: flash_attn    = 0
0.00.073.140 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.141 I llama_new_context_with_model: freq_scale    = 1
0.00.073.141 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.073.141 I ggml_metal_init: allocating
0.00.073.144 I ggml_metal_init: found device: Apple M4
0.00.073.146 I ggml_metal_init: picking default device: Apple M4
0.00.073.795 I ggml_metal_init: using embedded metal library
0.00.076.428 I ggml_metal_init: GPU name:   Apple M4
0.00.076.430 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.430 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.431 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.431 I ggml_metal_init: simdgroup reduction   = true
0.00.076.431 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.431 I ggml_metal_init: has bfloat            = true
0.00.076.431 I ggml_metal_init: use bfloat            = true
0.00.076.432 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.432 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.985 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.250 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.087.253 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.087.267 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.098 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.088.099 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.088.099 I llama_new_context_with_model: graph nodes  = 967
0.00.088.100 I llama_new_context_with_model: graph splits = 2
0.00.088.112 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.088.113 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.841.780 I 
0.00.841.802 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.841.815 I perplexity: tokenizing the input ..
0.00.849.847 I perplexity: tokenization took 8.029 ms
0.00.849.854 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.974.357 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.975.530 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.975.553 I llama_perf_context_print:        load time =     830.04 ms
0.00.975.554 I llama_perf_context_print: prompt eval time =     124.24 ms /   128 tokens (    0.97 ms per token,  1030.25 tokens per second)
0.00.975.555 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.975.556 I llama_perf_context_print:       total time =     133.77 ms /   129 tokens
0.00.975.974 I ggml_metal_free: deallocating

real	0m0.994s
user	0m0.098s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.982 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.769 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.773 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.775 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.775 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.775 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.776 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.776 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.777 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.777 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.778 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.778 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.778 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.779 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.779 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.780 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.781 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.781 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.567 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.549 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.191 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.192 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.193 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.193 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.193 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.193 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.194 I llama_model_loader: - type  f32:  194 tensors
0.00.024.194 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.194 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.286 I llm_load_vocab: special tokens cache size = 25
0.00.050.378 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.381 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.381 I llm_load_print_meta: arch             = gptneox
0.00.050.382 I llm_load_print_meta: vocab type       = BPE
0.00.050.382 I llm_load_print_meta: n_vocab          = 50304
0.00.050.382 I llm_load_print_meta: n_merges         = 50009
0.00.050.382 I llm_load_print_meta: vocab_only       = 0
0.00.050.382 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.383 I llm_load_print_meta: n_embd           = 2048
0.00.050.383 I llm_load_print_meta: n_layer          = 24
0.00.050.386 I llm_load_print_meta: n_head           = 16
0.00.050.387 I llm_load_print_meta: n_head_kv        = 16
0.00.050.387 I llm_load_print_meta: n_rot            = 32
0.00.050.387 I llm_load_print_meta: n_swa            = 0
0.00.050.387 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.387 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.391 I llm_load_print_meta: n_gqa            = 1
0.00.050.391 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.393 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.394 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.395 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.395 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.396 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.396 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.396 I llm_load_print_meta: n_ff             = 8192
0.00.050.397 I llm_load_print_meta: n_expert         = 0
0.00.050.397 I llm_load_print_meta: n_expert_used    = 0
0.00.050.397 I llm_load_print_meta: causal attn      = 1
0.00.050.397 I llm_load_print_meta: pooling type     = 0
0.00.050.397 I llm_load_print_meta: rope type        = 2
0.00.050.397 I llm_load_print_meta: rope scaling     = linear
0.00.050.402 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.402 I llm_load_print_meta: freq_scale_train = 1
0.00.050.402 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.402 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.403 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.403 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.403 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.403 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.403 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.403 I llm_load_print_meta: model type       = 1.4B
0.00.050.404 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.404 I llm_load_print_meta: model params     = 1.41 B
0.00.050.404 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.405 I llm_load_print_meta: general.name     = 1.4B
0.00.050.405 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.405 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.405 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.405 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.405 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.406 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.406 I llm_load_print_meta: max token length = 1024
0.00.052.307 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.307 I llm_load_tensors: offloading output layer to GPU
0.00.052.308 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.318 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.319 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.209 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.210 I llama_new_context_with_model: n_ctx         = 128
0.00.053.210 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.211 I llama_new_context_with_model: n_batch       = 128
0.00.053.211 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.211 I llama_new_context_with_model: flash_attn    = 0
0.00.053.211 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.212 I llama_new_context_with_model: freq_scale    = 1
0.00.053.212 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.212 I ggml_metal_init: allocating
0.00.053.216 I ggml_metal_init: found device: Apple M4
0.00.053.218 I ggml_metal_init: picking default device: Apple M4
0.00.053.791 I ggml_metal_init: using embedded metal library
0.00.056.063 I ggml_metal_init: GPU name:   Apple M4
0.00.056.064 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.065 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.065 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.065 I ggml_metal_init: simdgroup reduction   = true
0.00.056.065 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.065 I ggml_metal_init: has bfloat            = true
0.00.056.066 I ggml_metal_init: use bfloat            = true
0.00.056.066 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.067 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.766 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.010 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.014 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.029 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.867 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.868 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.868 I llama_new_context_with_model: graph nodes  = 967
0.00.067.869 I llama_new_context_with_model: graph splits = 2
0.00.067.881 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.881 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.589.321 I 
0.00.589.352 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.589.366 I perplexity: tokenizing the input ..
0.00.597.351 I perplexity: tokenization took 7.983 ms
0.00.597.354 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.720.117 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.721.385 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.721.401 I llama_perf_context_print:        load time =     579.33 ms
0.00.721.402 I llama_perf_context_print: prompt eval time =     122.52 ms /   128 tokens (    0.96 ms per token,  1044.73 tokens per second)
0.00.721.403 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.721.405 I llama_perf_context_print:       total time =     132.08 ms /   129 tokens
0.00.721.834 I ggml_metal_free: deallocating

real	0m0.738s
user	0m0.077s
sys	0m0.098s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.905 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.926 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.931 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.933 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.934 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.934 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.934 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.935 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.935 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.936 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.936 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.936 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.937 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.937 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.938 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.940 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.940 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.940 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.831 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.845 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.857 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.859 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.859 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.859 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.860 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.860 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.861 I llama_model_loader: - type  f32:  194 tensors
0.00.023.861 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.861 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.231 I llm_load_vocab: special tokens cache size = 25
0.00.051.270 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.276 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.276 I llm_load_print_meta: arch             = gptneox
0.00.051.277 I llm_load_print_meta: vocab type       = BPE
0.00.051.277 I llm_load_print_meta: n_vocab          = 50304
0.00.051.277 I llm_load_print_meta: n_merges         = 50009
0.00.051.277 I llm_load_print_meta: vocab_only       = 0
0.00.051.277 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.277 I llm_load_print_meta: n_embd           = 2048
0.00.051.278 I llm_load_print_meta: n_layer          = 24
0.00.051.282 I llm_load_print_meta: n_head           = 16
0.00.051.282 I llm_load_print_meta: n_head_kv        = 16
0.00.051.283 I llm_load_print_meta: n_rot            = 32
0.00.051.283 I llm_load_print_meta: n_swa            = 0
0.00.051.283 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.283 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.284 I llm_load_print_meta: n_gqa            = 1
0.00.051.284 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.285 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.286 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.286 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.286 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.286 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.286 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.287 I llm_load_print_meta: n_ff             = 8192
0.00.051.287 I llm_load_print_meta: n_expert         = 0
0.00.051.287 I llm_load_print_meta: n_expert_used    = 0
0.00.051.287 I llm_load_print_meta: causal attn      = 1
0.00.051.287 I llm_load_print_meta: pooling type     = 0
0.00.051.287 I llm_load_print_meta: rope type        = 2
0.00.051.288 I llm_load_print_meta: rope scaling     = linear
0.00.051.288 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.288 I llm_load_print_meta: freq_scale_train = 1
0.00.051.288 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.289 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.289 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.289 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.290 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.291 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.291 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.291 I llm_load_print_meta: model type       = 1.4B
0.00.051.291 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.292 I llm_load_print_meta: model params     = 1.41 B
0.00.051.292 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.292 I llm_load_print_meta: general.name     = 1.4B
0.00.051.293 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.293 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.293 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.293 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.294 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.294 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.294 I llm_load_print_meta: max token length = 1024
0.00.053.301 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.302 I llm_load_tensors: offloading output layer to GPU
0.00.053.302 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.313 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.315 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.218 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.219 I llama_new_context_with_model: n_ctx         = 128
0.00.054.219 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.219 I llama_new_context_with_model: n_batch       = 128
0.00.054.219 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.219 I llama_new_context_with_model: flash_attn    = 0
0.00.054.220 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.220 I llama_new_context_with_model: freq_scale    = 1
0.00.054.221 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.221 I ggml_metal_init: allocating
0.00.054.229 I ggml_metal_init: found device: Apple M4
0.00.054.232 I ggml_metal_init: picking default device: Apple M4
0.00.054.848 I ggml_metal_init: using embedded metal library
0.00.057.258 I ggml_metal_init: GPU name:   Apple M4
0.00.057.259 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.259 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.260 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.260 I ggml_metal_init: simdgroup reduction   = true
0.00.057.260 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.260 I ggml_metal_init: has bfloat            = true
0.00.057.261 I ggml_metal_init: use bfloat            = true
0.00.057.261 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.262 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.313 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.642 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.645 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.661 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.599 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.600 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.602 I llama_new_context_with_model: graph nodes  = 967
0.00.068.602 I llama_new_context_with_model: graph splits = 2
0.00.068.615 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.616 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.627.990 I 
0.00.628.027 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.628.044 I perplexity: tokenizing the input ..
0.00.635.762 I perplexity: tokenization took 7.715 ms
0.00.635.765 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.757.606 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.758.880 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.758.897 I llama_perf_context_print:        load time =     619.08 ms
0.00.758.902 I llama_perf_context_print: prompt eval time =     121.62 ms /   128 tokens (    0.95 ms per token,  1052.50 tokens per second)
0.00.758.903 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.758.903 I llama_perf_context_print:       total time =     130.91 ms /   129 tokens
0.00.759.318 I ggml_metal_free: deallocating

real	0m0.775s
user	0m0.079s
sys	0m0.098s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.626 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.609 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.614 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.620 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.621 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.621 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.621 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.622 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.623 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.623 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.623 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.624 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.624 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.624 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.625 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.627 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.627 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.627 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.497 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.606 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.473 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.474 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.475 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.475 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.475 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.476 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.476 I llama_model_loader: - type  f32:  194 tensors
0.00.024.477 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.477 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.610 I llm_load_vocab: special tokens cache size = 25
0.00.051.494 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.501 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.501 I llm_load_print_meta: arch             = gptneox
0.00.051.502 I llm_load_print_meta: vocab type       = BPE
0.00.051.502 I llm_load_print_meta: n_vocab          = 50304
0.00.051.502 I llm_load_print_meta: n_merges         = 50009
0.00.051.502 I llm_load_print_meta: vocab_only       = 0
0.00.051.502 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.502 I llm_load_print_meta: n_embd           = 2048
0.00.051.503 I llm_load_print_meta: n_layer          = 24
0.00.051.506 I llm_load_print_meta: n_head           = 16
0.00.051.507 I llm_load_print_meta: n_head_kv        = 16
0.00.051.509 I llm_load_print_meta: n_rot            = 32
0.00.051.509 I llm_load_print_meta: n_swa            = 0
0.00.051.509 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.509 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.510 I llm_load_print_meta: n_gqa            = 1
0.00.051.511 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.513 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.514 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.514 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.514 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.514 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.515 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.517 I llm_load_print_meta: n_ff             = 8192
0.00.051.517 I llm_load_print_meta: n_expert         = 0
0.00.051.518 I llm_load_print_meta: n_expert_used    = 0
0.00.051.518 I llm_load_print_meta: causal attn      = 1
0.00.051.518 I llm_load_print_meta: pooling type     = 0
0.00.051.518 I llm_load_print_meta: rope type        = 2
0.00.051.518 I llm_load_print_meta: rope scaling     = linear
0.00.051.519 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.519 I llm_load_print_meta: freq_scale_train = 1
0.00.051.519 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.523 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.523 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.523 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.523 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.523 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.523 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.523 I llm_load_print_meta: model type       = 1.4B
0.00.051.524 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.524 I llm_load_print_meta: model params     = 1.41 B
0.00.051.525 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.525 I llm_load_print_meta: general.name     = 1.4B
0.00.051.525 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.525 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.525 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.525 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.526 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.526 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.526 I llm_load_print_meta: max token length = 1024
0.00.053.392 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.392 I llm_load_tensors: offloading output layer to GPU
0.00.053.392 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.398 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.398 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.301 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.302 I llama_new_context_with_model: n_ctx         = 128
0.00.054.302 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.302 I llama_new_context_with_model: n_batch       = 128
0.00.054.302 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.302 I llama_new_context_with_model: flash_attn    = 0
0.00.054.303 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.303 I llama_new_context_with_model: freq_scale    = 1
0.00.054.304 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.304 I ggml_metal_init: allocating
0.00.054.309 I ggml_metal_init: found device: Apple M4
0.00.054.311 I ggml_metal_init: picking default device: Apple M4
0.00.054.916 I ggml_metal_init: using embedded metal library
0.00.057.494 I ggml_metal_init: GPU name:   Apple M4
0.00.057.496 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.497 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.497 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.497 I ggml_metal_init: simdgroup reduction   = true
0.00.057.497 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.498 I ggml_metal_init: has bfloat            = true
0.00.057.498 I ggml_metal_init: use bfloat            = true
0.00.057.498 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.499 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.736 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.069.014 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.018 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.043 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.918 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.919 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.919 I llama_new_context_with_model: graph nodes  = 967
0.00.069.920 I llama_new_context_with_model: graph splits = 2
0.00.069.928 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.928 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.688.958 I 
0.00.688.986 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.000 I perplexity: tokenizing the input ..
0.00.697.107 I perplexity: tokenization took 8.105 ms
0.00.697.111 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.832.483 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.833.645 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.833.656 I llama_perf_context_print:        load time =     679.33 ms
0.00.833.657 I llama_perf_context_print: prompt eval time =     135.15 ms /   128 tokens (    1.06 ms per token,   947.12 tokens per second)
0.00.833.658 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.833.658 I llama_perf_context_print:       total time =     144.70 ms /   129 tokens
0.00.833.973 I ggml_metal_free: deallocating

real	0m0.849s
user	0m0.080s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.927 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.345 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.350 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.351 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.352 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.352 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.352 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.353 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.354 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.354 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.354 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.355 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.355 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.355 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.356 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.357 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.357 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.358 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.222 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.251 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.128 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.129 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.129 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.130 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.130 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.130 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.131 I llama_model_loader: - type  f32:  194 tensors
0.00.023.131 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.131 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.088 I llm_load_vocab: special tokens cache size = 25
0.00.050.053 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.055 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.056 I llm_load_print_meta: arch             = gptneox
0.00.050.056 I llm_load_print_meta: vocab type       = BPE
0.00.050.056 I llm_load_print_meta: n_vocab          = 50304
0.00.050.056 I llm_load_print_meta: n_merges         = 50009
0.00.050.057 I llm_load_print_meta: vocab_only       = 0
0.00.050.057 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.057 I llm_load_print_meta: n_embd           = 2048
0.00.050.057 I llm_load_print_meta: n_layer          = 24
0.00.050.060 I llm_load_print_meta: n_head           = 16
0.00.050.060 I llm_load_print_meta: n_head_kv        = 16
0.00.050.061 I llm_load_print_meta: n_rot            = 32
0.00.050.061 I llm_load_print_meta: n_swa            = 0
0.00.050.061 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.061 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.062 I llm_load_print_meta: n_gqa            = 1
0.00.050.063 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.063 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.064 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.064 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.065 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.065 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.065 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.066 I llm_load_print_meta: n_ff             = 8192
0.00.050.068 I llm_load_print_meta: n_expert         = 0
0.00.050.068 I llm_load_print_meta: n_expert_used    = 0
0.00.050.068 I llm_load_print_meta: causal attn      = 1
0.00.050.069 I llm_load_print_meta: pooling type     = 0
0.00.050.069 I llm_load_print_meta: rope type        = 2
0.00.050.069 I llm_load_print_meta: rope scaling     = linear
0.00.050.069 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.070 I llm_load_print_meta: freq_scale_train = 1
0.00.050.070 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.070 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.070 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.070 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.071 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.071 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.071 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.071 I llm_load_print_meta: model type       = 1.4B
0.00.050.071 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.072 I llm_load_print_meta: model params     = 1.41 B
0.00.050.072 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.072 I llm_load_print_meta: general.name     = 1.4B
0.00.050.073 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.073 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.074 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.074 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.074 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.074 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.075 I llm_load_print_meta: max token length = 1024
0.00.052.107 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.107 I llm_load_tensors: offloading output layer to GPU
0.00.052.108 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.118 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.119 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.055 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.056 I llama_new_context_with_model: n_ctx         = 128
0.00.053.056 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.056 I llama_new_context_with_model: n_batch       = 128
0.00.053.056 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.057 I llama_new_context_with_model: flash_attn    = 0
0.00.053.057 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.057 I llama_new_context_with_model: freq_scale    = 1
0.00.053.057 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.058 I ggml_metal_init: allocating
0.00.053.061 I ggml_metal_init: found device: Apple M4
0.00.053.063 I ggml_metal_init: picking default device: Apple M4
0.00.053.627 I ggml_metal_init: using embedded metal library
0.00.055.968 I ggml_metal_init: GPU name:   Apple M4
0.00.055.970 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.970 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.970 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.970 I ggml_metal_init: simdgroup reduction   = true
0.00.055.971 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.971 I ggml_metal_init: has bfloat            = true
0.00.055.971 I ggml_metal_init: use bfloat            = true
0.00.055.971 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.972 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.795 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.072 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.074 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.089 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.010 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.011 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.012 I llama_new_context_with_model: graph nodes  = 967
0.00.068.012 I llama_new_context_with_model: graph splits = 2
0.00.068.025 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.025 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.070 I 
0.00.693.101 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.114 I perplexity: tokenizing the input ..
0.00.701.495 I perplexity: tokenization took 8.38 ms
0.00.701.501 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.835.693 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.836.921 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.836.934 I llama_perf_context_print:        load time =     684.14 ms
0.00.836.936 I llama_perf_context_print: prompt eval time =     133.97 ms /   128 tokens (    1.05 ms per token,   955.47 tokens per second)
0.00.836.937 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.836.937 I llama_perf_context_print:       total time =     143.86 ms /   129 tokens
0.00.837.310 I ggml_metal_free: deallocating

real	0m0.851s
user	0m0.079s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.336 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.815 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.819 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.821 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.822 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.822 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.822 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.823 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.824 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.824 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.824 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.825 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.825 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.825 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.826 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.828 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.829 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.829 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.687 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.686 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.470 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.471 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.471 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.472 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.472 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.472 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.473 I llama_model_loader: - type  f32:  194 tensors
0.00.023.473 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.473 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.473 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.566 I llm_load_vocab: special tokens cache size = 25
0.00.049.411 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.415 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.416 I llm_load_print_meta: arch             = gptneox
0.00.049.416 I llm_load_print_meta: vocab type       = BPE
0.00.049.416 I llm_load_print_meta: n_vocab          = 50304
0.00.049.416 I llm_load_print_meta: n_merges         = 50009
0.00.049.417 I llm_load_print_meta: vocab_only       = 0
0.00.049.417 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.418 I llm_load_print_meta: n_embd           = 2048
0.00.049.419 I llm_load_print_meta: n_layer          = 24
0.00.049.423 I llm_load_print_meta: n_head           = 16
0.00.049.423 I llm_load_print_meta: n_head_kv        = 16
0.00.049.423 I llm_load_print_meta: n_rot            = 32
0.00.049.425 I llm_load_print_meta: n_swa            = 0
0.00.049.425 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.425 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.426 I llm_load_print_meta: n_gqa            = 1
0.00.049.427 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.427 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.428 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.429 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.429 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.429 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.429 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.430 I llm_load_print_meta: n_ff             = 8192
0.00.049.430 I llm_load_print_meta: n_expert         = 0
0.00.049.430 I llm_load_print_meta: n_expert_used    = 0
0.00.049.430 I llm_load_print_meta: causal attn      = 1
0.00.049.430 I llm_load_print_meta: pooling type     = 0
0.00.049.431 I llm_load_print_meta: rope type        = 2
0.00.049.431 I llm_load_print_meta: rope scaling     = linear
0.00.049.431 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.432 I llm_load_print_meta: freq_scale_train = 1
0.00.049.432 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.432 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.432 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.432 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.432 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.432 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.433 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.433 I llm_load_print_meta: model type       = 1.4B
0.00.049.433 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.434 I llm_load_print_meta: model params     = 1.41 B
0.00.049.435 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.435 I llm_load_print_meta: general.name     = 1.4B
0.00.049.435 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.435 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.436 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.436 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.436 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.436 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.436 I llm_load_print_meta: max token length = 1024
0.00.051.250 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.250 I llm_load_tensors: offloading output layer to GPU
0.00.051.250 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.260 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.261 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.158 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.158 I llama_new_context_with_model: n_ctx         = 128
0.00.052.158 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.159 I llama_new_context_with_model: n_batch       = 128
0.00.052.159 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.159 I llama_new_context_with_model: flash_attn    = 0
0.00.052.159 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.159 I llama_new_context_with_model: freq_scale    = 1
0.00.052.160 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.160 I ggml_metal_init: allocating
0.00.052.163 I ggml_metal_init: found device: Apple M4
0.00.052.165 I ggml_metal_init: picking default device: Apple M4
0.00.052.721 I ggml_metal_init: using embedded metal library
0.00.054.987 I ggml_metal_init: GPU name:   Apple M4
0.00.054.988 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.989 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.989 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.989 I ggml_metal_init: simdgroup reduction   = true
0.00.054.989 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.989 I ggml_metal_init: has bfloat            = true
0.00.054.990 I ggml_metal_init: use bfloat            = true
0.00.054.990 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.990 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.539 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.847 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.852 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.868 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.746 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.747 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.747 I llama_new_context_with_model: graph nodes  = 967
0.00.066.747 I llama_new_context_with_model: graph splits = 2
0.00.066.760 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.761 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.389.638 I 
0.00.389.671 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.389.685 I perplexity: tokenizing the input ..
0.00.397.568 I perplexity: tokenization took 7.881 ms
0.00.397.571 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.529.628 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.530.774 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.530.788 I llama_perf_context_print:        load time =     380.30 ms
0.00.530.789 I llama_perf_context_print: prompt eval time =     131.83 ms /   128 tokens (    1.03 ms per token,   970.93 tokens per second)
0.00.530.790 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.530.790 I llama_perf_context_print:       total time =     141.15 ms /   129 tokens
0.00.531.256 I ggml_metal_free: deallocating

real	0m0.546s
user	0m0.078s
sys	0m0.069s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.700 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.612 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.617 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.619 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.619 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.619 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.620 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.620 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.623 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.623 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.623 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.624 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.624 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.624 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.628 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.630 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.630 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.632 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.452 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.498 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.316 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.317 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.317 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.317 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.318 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.318 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.319 I llama_model_loader: - type  f32:  194 tensors
0.00.023.319 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.319 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.319 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.320 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.173 I llm_load_vocab: special tokens cache size = 25
0.00.049.904 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.907 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.907 I llm_load_print_meta: arch             = gptneox
0.00.049.908 I llm_load_print_meta: vocab type       = BPE
0.00.049.908 I llm_load_print_meta: n_vocab          = 50304
0.00.049.908 I llm_load_print_meta: n_merges         = 50009
0.00.049.908 I llm_load_print_meta: vocab_only       = 0
0.00.049.908 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.909 I llm_load_print_meta: n_embd           = 2048
0.00.049.909 I llm_load_print_meta: n_layer          = 24
0.00.049.912 I llm_load_print_meta: n_head           = 16
0.00.049.912 I llm_load_print_meta: n_head_kv        = 16
0.00.049.913 I llm_load_print_meta: n_rot            = 32
0.00.049.913 I llm_load_print_meta: n_swa            = 0
0.00.049.913 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.914 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.915 I llm_load_print_meta: n_gqa            = 1
0.00.049.916 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.916 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.917 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.918 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.918 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.919 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.919 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.919 I llm_load_print_meta: n_ff             = 8192
0.00.049.920 I llm_load_print_meta: n_expert         = 0
0.00.049.920 I llm_load_print_meta: n_expert_used    = 0
0.00.049.920 I llm_load_print_meta: causal attn      = 1
0.00.049.920 I llm_load_print_meta: pooling type     = 0
0.00.049.920 I llm_load_print_meta: rope type        = 2
0.00.049.920 I llm_load_print_meta: rope scaling     = linear
0.00.049.921 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.922 I llm_load_print_meta: freq_scale_train = 1
0.00.049.922 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.922 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.922 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.922 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.923 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.923 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.923 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.923 I llm_load_print_meta: model type       = 1.4B
0.00.049.923 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.924 I llm_load_print_meta: model params     = 1.41 B
0.00.049.924 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.925 I llm_load_print_meta: general.name     = 1.4B
0.00.049.925 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.925 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.925 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.925 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.926 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.926 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.926 I llm_load_print_meta: max token length = 1024
0.00.051.891 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.892 I llm_load_tensors: offloading output layer to GPU
0.00.051.892 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.902 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.903 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.788 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.789 I llama_new_context_with_model: n_ctx         = 128
0.00.052.789 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.789 I llama_new_context_with_model: n_batch       = 128
0.00.052.790 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.790 I llama_new_context_with_model: flash_attn    = 0
0.00.052.790 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.790 I llama_new_context_with_model: freq_scale    = 1
0.00.052.791 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.791 I ggml_metal_init: allocating
0.00.052.798 I ggml_metal_init: found device: Apple M4
0.00.052.800 I ggml_metal_init: picking default device: Apple M4
0.00.053.353 I ggml_metal_init: using embedded metal library
0.00.055.691 I ggml_metal_init: GPU name:   Apple M4
0.00.055.692 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.693 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.693 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.693 I ggml_metal_init: simdgroup reduction   = true
0.00.055.694 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.694 I ggml_metal_init: has bfloat            = true
0.00.055.694 I ggml_metal_init: use bfloat            = true
0.00.055.694 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.695 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.355 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.622 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.624 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.637 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.537 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.538 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.539 I llama_new_context_with_model: graph nodes  = 967
0.00.067.539 I llama_new_context_with_model: graph splits = 2
0.00.067.551 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.552 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.478.777 I 
0.00.478.817 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.478.828 I perplexity: tokenizing the input ..
0.00.486.923 I perplexity: tokenization took 8.094 ms
0.00.486.927 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.618.199 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.619.395 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.619.412 I llama_perf_context_print:        load time =     470.07 ms
0.00.619.413 I llama_perf_context_print: prompt eval time =     131.04 ms /   128 tokens (    1.02 ms per token,   976.79 tokens per second)
0.00.619.413 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.619.414 I llama_perf_context_print:       total time =     140.64 ms /   129 tokens
0.00.619.846 I ggml_metal_free: deallocating

real	0m0.632s
user	0m0.078s
sys	0m0.085s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.123 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.615 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.620 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.622 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.622 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.623 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.623 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.623 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.624 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.625 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.625 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.626 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.626 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.627 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.627 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.629 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.629 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.629 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.506 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.556 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.449 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.450 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.450 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.451 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.451 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.451 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.452 I llama_model_loader: - type  f32:  194 tensors
0.00.023.452 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.453 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.453 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.539 I llm_load_vocab: special tokens cache size = 25
0.00.049.456 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.458 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.458 I llm_load_print_meta: arch             = gptneox
0.00.049.459 I llm_load_print_meta: vocab type       = BPE
0.00.049.459 I llm_load_print_meta: n_vocab          = 50304
0.00.049.459 I llm_load_print_meta: n_merges         = 50009
0.00.049.460 I llm_load_print_meta: vocab_only       = 0
0.00.049.460 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.460 I llm_load_print_meta: n_embd           = 2048
0.00.049.460 I llm_load_print_meta: n_layer          = 24
0.00.049.463 I llm_load_print_meta: n_head           = 16
0.00.049.465 I llm_load_print_meta: n_head_kv        = 16
0.00.049.465 I llm_load_print_meta: n_rot            = 32
0.00.049.465 I llm_load_print_meta: n_swa            = 0
0.00.049.466 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.466 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.467 I llm_load_print_meta: n_gqa            = 1
0.00.049.468 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.468 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.469 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.469 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.469 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.469 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.470 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.470 I llm_load_print_meta: n_ff             = 8192
0.00.049.471 I llm_load_print_meta: n_expert         = 0
0.00.049.471 I llm_load_print_meta: n_expert_used    = 0
0.00.049.471 I llm_load_print_meta: causal attn      = 1
0.00.049.471 I llm_load_print_meta: pooling type     = 0
0.00.049.471 I llm_load_print_meta: rope type        = 2
0.00.049.471 I llm_load_print_meta: rope scaling     = linear
0.00.049.474 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.474 I llm_load_print_meta: freq_scale_train = 1
0.00.049.474 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.474 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.474 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.475 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.475 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.475 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.475 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.475 I llm_load_print_meta: model type       = 1.4B
0.00.049.476 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.476 I llm_load_print_meta: model params     = 1.41 B
0.00.049.477 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.477 I llm_load_print_meta: general.name     = 1.4B
0.00.049.477 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.477 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.478 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.478 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.478 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.478 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.478 I llm_load_print_meta: max token length = 1024
0.00.051.471 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.471 I llm_load_tensors: offloading output layer to GPU
0.00.051.471 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.481 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.482 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.422 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.423 I llama_new_context_with_model: n_ctx         = 128
0.00.052.423 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.423 I llama_new_context_with_model: n_batch       = 128
0.00.052.424 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.424 I llama_new_context_with_model: flash_attn    = 0
0.00.052.424 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.424 I llama_new_context_with_model: freq_scale    = 1
0.00.052.425 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.425 I ggml_metal_init: allocating
0.00.052.429 I ggml_metal_init: found device: Apple M4
0.00.052.431 I ggml_metal_init: picking default device: Apple M4
0.00.052.974 I ggml_metal_init: using embedded metal library
0.00.055.267 I ggml_metal_init: GPU name:   Apple M4
0.00.055.268 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.269 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.269 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.269 I ggml_metal_init: simdgroup reduction   = true
0.00.055.270 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.270 I ggml_metal_init: has bfloat            = true
0.00.055.270 I ggml_metal_init: use bfloat            = true
0.00.055.270 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.271 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.874 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.176 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.181 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.197 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.101 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.102 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.102 I llama_new_context_with_model: graph nodes  = 967
0.00.067.102 I llama_new_context_with_model: graph splits = 2
0.00.067.115 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.115 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.547.026 I 
0.00.547.074 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.547.087 I perplexity: tokenizing the input ..
0.00.554.964 I perplexity: tokenization took 7.875 ms
0.00.554.968 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.688.687 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.689.842 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.689.852 I llama_perf_context_print:        load time =     537.90 ms
0.00.689.853 I llama_perf_context_print: prompt eval time =     133.49 ms /   128 tokens (    1.04 ms per token,   958.84 tokens per second)
0.00.689.853 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.689.854 I llama_perf_context_print:       total time =     142.83 ms /   129 tokens
0.00.690.173 I ggml_metal_free: deallocating

real	0m0.702s
user	0m0.078s
sys	0m0.092s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.645 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.028 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.033 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.035 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.035 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.035 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.036 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.036 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.037 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.037 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.037 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.038 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.038 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.038 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.039 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.041 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.041 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.041 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.773 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.834 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.626 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.628 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.628 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.628 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.628 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.629 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.629 I llama_model_loader: - type  f32:  194 tensors
0.00.023.629 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.629 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.630 I llm_load_vocab: special tokens cache size = 25
0.00.049.487 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.489 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.489 I llm_load_print_meta: arch             = gptneox
0.00.049.490 I llm_load_print_meta: vocab type       = BPE
0.00.049.490 I llm_load_print_meta: n_vocab          = 50304
0.00.049.490 I llm_load_print_meta: n_merges         = 50009
0.00.049.490 I llm_load_print_meta: vocab_only       = 0
0.00.049.491 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.491 I llm_load_print_meta: n_embd           = 2048
0.00.049.491 I llm_load_print_meta: n_layer          = 24
0.00.049.493 I llm_load_print_meta: n_head           = 16
0.00.049.494 I llm_load_print_meta: n_head_kv        = 16
0.00.049.496 I llm_load_print_meta: n_rot            = 32
0.00.049.496 I llm_load_print_meta: n_swa            = 0
0.00.049.497 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.497 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.498 I llm_load_print_meta: n_gqa            = 1
0.00.049.499 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.499 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.500 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.500 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.500 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.501 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.501 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.501 I llm_load_print_meta: n_ff             = 8192
0.00.049.501 I llm_load_print_meta: n_expert         = 0
0.00.049.502 I llm_load_print_meta: n_expert_used    = 0
0.00.049.502 I llm_load_print_meta: causal attn      = 1
0.00.049.502 I llm_load_print_meta: pooling type     = 0
0.00.049.502 I llm_load_print_meta: rope type        = 2
0.00.049.506 I llm_load_print_meta: rope scaling     = linear
0.00.049.507 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.507 I llm_load_print_meta: freq_scale_train = 1
0.00.049.507 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.507 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.508 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.508 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.508 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.508 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.508 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.508 I llm_load_print_meta: model type       = 1.4B
0.00.049.509 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.515 I llm_load_print_meta: model params     = 1.41 B
0.00.049.516 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.517 I llm_load_print_meta: general.name     = 1.4B
0.00.049.517 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.517 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.518 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.518 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.518 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.518 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.518 I llm_load_print_meta: max token length = 1024
0.00.051.268 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.268 I llm_load_tensors: offloading output layer to GPU
0.00.051.268 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.274 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.274 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.204 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.204 I llama_new_context_with_model: n_ctx         = 128
0.00.052.204 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.205 I llama_new_context_with_model: n_batch       = 128
0.00.052.205 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.205 I llama_new_context_with_model: flash_attn    = 0
0.00.052.205 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.206 I llama_new_context_with_model: freq_scale    = 1
0.00.052.206 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.207 I ggml_metal_init: allocating
0.00.052.210 I ggml_metal_init: found device: Apple M4
0.00.052.212 I ggml_metal_init: picking default device: Apple M4
0.00.052.782 I ggml_metal_init: using embedded metal library
0.00.055.049 I ggml_metal_init: GPU name:   Apple M4
0.00.055.051 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.051 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.052 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.052 I ggml_metal_init: simdgroup reduction   = true
0.00.055.052 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.052 I ggml_metal_init: has bfloat            = true
0.00.055.052 I ggml_metal_init: use bfloat            = true
0.00.055.053 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.053 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.583 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.913 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.916 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.939 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.763 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.764 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.764 I llama_new_context_with_model: graph nodes  = 967
0.00.066.764 I llama_new_context_with_model: graph splits = 2
0.00.066.772 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.772 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.730 I 
0.00.652.765 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.652.779 I perplexity: tokenizing the input ..
0.00.660.835 I perplexity: tokenization took 8.054 ms
0.00.660.838 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.818 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.803.013 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.803.029 I llama_perf_context_print:        load time =     643.08 ms
0.00.803.032 I llama_perf_context_print: prompt eval time =     140.75 ms /   128 tokens (    1.10 ms per token,   909.38 tokens per second)
0.00.803.033 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.803.034 I llama_perf_context_print:       total time =     150.30 ms /   129 tokens
0.00.803.497 I ggml_metal_free: deallocating

real	0m0.819s
user	0m0.077s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.565 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.275 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.279 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.281 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.282 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.282 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.282 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.283 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.283 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.284 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.284 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.285 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.285 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.285 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.286 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.287 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.287 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.288 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.078 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.107 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.975 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.976 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.977 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.977 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.977 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.978 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.022.978 I llama_model_loader: - type  f32:  194 tensors
0.00.022.978 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.129 I llm_load_vocab: special tokens cache size = 25
0.00.049.069 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.072 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.072 I llm_load_print_meta: arch             = gptneox
0.00.049.072 I llm_load_print_meta: vocab type       = BPE
0.00.049.073 I llm_load_print_meta: n_vocab          = 50304
0.00.049.073 I llm_load_print_meta: n_merges         = 50009
0.00.049.073 I llm_load_print_meta: vocab_only       = 0
0.00.049.073 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.073 I llm_load_print_meta: n_embd           = 2048
0.00.049.074 I llm_load_print_meta: n_layer          = 24
0.00.049.076 I llm_load_print_meta: n_head           = 16
0.00.049.077 I llm_load_print_meta: n_head_kv        = 16
0.00.049.077 I llm_load_print_meta: n_rot            = 32
0.00.049.078 I llm_load_print_meta: n_swa            = 0
0.00.049.078 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.078 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.079 I llm_load_print_meta: n_gqa            = 1
0.00.049.080 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.080 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.081 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.081 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.081 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.082 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.082 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.083 I llm_load_print_meta: n_ff             = 8192
0.00.049.083 I llm_load_print_meta: n_expert         = 0
0.00.049.083 I llm_load_print_meta: n_expert_used    = 0
0.00.049.083 I llm_load_print_meta: causal attn      = 1
0.00.049.083 I llm_load_print_meta: pooling type     = 0
0.00.049.083 I llm_load_print_meta: rope type        = 2
0.00.049.084 I llm_load_print_meta: rope scaling     = linear
0.00.049.084 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.084 I llm_load_print_meta: freq_scale_train = 1
0.00.049.085 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.085 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.085 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.085 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.086 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.086 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.086 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.087 I llm_load_print_meta: model type       = 1.4B
0.00.049.087 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.087 I llm_load_print_meta: model params     = 1.41 B
0.00.049.088 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.088 I llm_load_print_meta: general.name     = 1.4B
0.00.049.088 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.089 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.089 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.089 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.089 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.089 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.089 I llm_load_print_meta: max token length = 1024
0.00.051.039 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.039 I llm_load_tensors: offloading output layer to GPU
0.00.051.039 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.049 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.051 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.051.919 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.920 I llama_new_context_with_model: n_ctx         = 128
0.00.051.920 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.920 I llama_new_context_with_model: n_batch       = 128
0.00.051.920 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.920 I llama_new_context_with_model: flash_attn    = 0
0.00.051.921 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.921 I llama_new_context_with_model: freq_scale    = 1
0.00.051.921 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.922 I ggml_metal_init: allocating
0.00.051.928 I ggml_metal_init: found device: Apple M4
0.00.051.930 I ggml_metal_init: picking default device: Apple M4
0.00.052.484 I ggml_metal_init: using embedded metal library
0.00.054.880 I ggml_metal_init: GPU name:   Apple M4
0.00.054.882 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.882 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.882 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.883 I ggml_metal_init: simdgroup reduction   = true
0.00.054.883 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.883 I ggml_metal_init: has bfloat            = true
0.00.054.883 I ggml_metal_init: use bfloat            = true
0.00.054.883 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.884 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.336 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.609 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.611 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.637 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.509 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.510 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.510 I llama_new_context_with_model: graph nodes  = 967
0.00.066.510 I llama_new_context_with_model: graph splits = 2
0.00.066.522 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.523 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.494.704 I 
0.00.494.738 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.494.752 I perplexity: tokenizing the input ..
0.00.502.576 I perplexity: tokenization took 7.823 ms
0.00.502.580 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.643.288 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.644.497 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.644.510 I llama_perf_context_print:        load time =     486.13 ms
0.00.644.511 I llama_perf_context_print: prompt eval time =     140.48 ms /   128 tokens (    1.10 ms per token,   911.16 tokens per second)
0.00.644.512 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.644.512 I llama_perf_context_print:       total time =     149.81 ms /   129 tokens
0.00.644.950 I ggml_metal_free: deallocating

real	0m0.659s
user	0m0.077s
sys	0m0.099s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.236 I build: 4397 (a813badb) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.158 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.619 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.624 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.626 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.627 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.627 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.627 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.629 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.630 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.631 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.631 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.631 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.632 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.632 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.633 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.635 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.635 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.636 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.626 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.935 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.462 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.464 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.465 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.466 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.466 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.467 I llama_model_loader: - type  f32:  194 tensors
0.00.052.467 I llama_model_loader: - type  f16:   98 tensors
0.00.082.634 I llm_load_vocab: special tokens cache size = 25
0.00.089.037 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.089.040 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.089.040 I llm_load_print_meta: arch             = gptneox
0.00.089.040 I llm_load_print_meta: vocab type       = BPE
0.00.089.041 I llm_load_print_meta: n_vocab          = 50304
0.00.089.041 I llm_load_print_meta: n_merges         = 50009
0.00.089.041 I llm_load_print_meta: vocab_only       = 0
0.00.089.041 I llm_load_print_meta: n_ctx_train      = 2048
0.00.089.041 I llm_load_print_meta: n_embd           = 2048
0.00.089.041 I llm_load_print_meta: n_layer          = 24
0.00.089.044 I llm_load_print_meta: n_head           = 16
0.00.089.045 I llm_load_print_meta: n_head_kv        = 16
0.00.089.045 I llm_load_print_meta: n_rot            = 32
0.00.089.046 I llm_load_print_meta: n_swa            = 0
0.00.089.046 I llm_load_print_meta: n_embd_head_k    = 128
0.00.089.046 I llm_load_print_meta: n_embd_head_v    = 128
0.00.089.047 I llm_load_print_meta: n_gqa            = 1
0.00.089.047 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.089.049 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.089.050 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.089.050 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.089.050 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.089.052 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.089.052 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.089.053 I llm_load_print_meta: n_ff             = 8192
0.00.089.053 I llm_load_print_meta: n_expert         = 0
0.00.089.053 I llm_load_print_meta: n_expert_used    = 0
0.00.089.053 I llm_load_print_meta: causal attn      = 1
0.00.089.053 I llm_load_print_meta: pooling type     = 0
0.00.089.053 I llm_load_print_meta: rope type        = 2
0.00.089.053 I llm_load_print_meta: rope scaling     = linear
0.00.089.054 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.089.054 I llm_load_print_meta: freq_scale_train = 1
0.00.089.054 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.089.054 I llm_load_print_meta: rope_finetuned   = unknown
0.00.089.054 I llm_load_print_meta: ssm_d_conv       = 0
0.00.089.055 I llm_load_print_meta: ssm_d_inner      = 0
0.00.089.055 I llm_load_print_meta: ssm_d_state      = 0
0.00.089.055 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.089.055 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.089.055 I llm_load_print_meta: model type       = 1.4B
0.00.089.056 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.089.056 I llm_load_print_meta: model params     = 1.41 B
0.00.089.061 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.089.061 I llm_load_print_meta: general.name     = 1.4B
0.00.089.061 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.089.061 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.089.062 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.089.062 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.089.062 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.089.062 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.089.062 I llm_load_print_meta: max token length = 1024
0.00.091.576 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.091.576 I llm_load_tensors: offloading output layer to GPU
0.00.091.576 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.091.587 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.091.588 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.092.530 I llama_new_context_with_model: n_seq_max     = 1
0.00.092.531 I llama_new_context_with_model: n_ctx         = 128
0.00.092.531 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.092.531 I llama_new_context_with_model: n_batch       = 128
0.00.092.531 I llama_new_context_with_model: n_ubatch      = 128
0.00.092.532 I llama_new_context_with_model: flash_attn    = 0
0.00.092.532 I llama_new_context_with_model: freq_base     = 10000.0
0.00.092.532 I llama_new_context_with_model: freq_scale    = 1
0.00.092.533 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.533 I ggml_metal_init: allocating
0.00.092.541 I ggml_metal_init: found device: Apple M4
0.00.092.543 I ggml_metal_init: picking default device: Apple M4
0.00.093.179 I ggml_metal_init: using embedded metal library
0.00.095.768 I ggml_metal_init: GPU name:   Apple M4
0.00.095.770 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.770 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.770 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.771 I ggml_metal_init: simdgroup reduction   = true
0.00.095.771 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.771 I ggml_metal_init: has bfloat            = true
0.00.095.771 I ggml_metal_init: use bfloat            = true
0.00.095.772 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.772 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.803 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.106.062 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.064 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.077 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.106.913 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.106.914 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.106.914 I llama_new_context_with_model: graph nodes  = 967
0.00.106.914 I llama_new_context_with_model: graph splits = 2
0.00.106.926 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.106.927 I 
0.00.106.949 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.106.950 I compute_imatrix: tokenizing the input ..
0.00.113.712 I compute_imatrix: tokenization took 6.761 ms
0.00.113.713 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.604.249 I compute_imatrix: 1.49 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.609.024 I llama_perf_context_print:        load time =    1582.09 ms
0.01.609.025 I llama_perf_context_print: prompt eval time =    1489.90 ms /   128 tokens (   11.64 ms per token,    85.91 tokens per second)
0.01.609.026 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.609.026 I llama_perf_context_print:       total time =    1586.85 ms /   129 tokens
0.01.609.563 I ggml_metal_free: deallocating

real	0m1.795s
user	0m0.172s
sys	0m0.231s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4397 (a813badb)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133609bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13360a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13360a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13360ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13360b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13360b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13360bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13360c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13360ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13360cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13360d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13360d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13360e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13360ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13360f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13360fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1336102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1336109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1336110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1336118c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133611fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x133612700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133612e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1336136c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133613de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1336140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1336146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x133615320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x133615860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x133615b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133615fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133616280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x133616b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133617050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133617310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1336177b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x133617c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1336180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133618590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133618a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x133618ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133619370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133619810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133619cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133619f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13361a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13361ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13361b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13361bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13361c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13361c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13361ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13361d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13361d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13361e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13361e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13361ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13361ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13361f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13361fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13361fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133620260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133620700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133620ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133621040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1336214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x133621980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133621e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1336222c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x133622760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133622c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1336230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x133623540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x133623a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x133623fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x133624530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x133624a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x133624fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x133625520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x133625a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x133625fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x133626510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x133626a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x133626fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x133627500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x133627a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x133627fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1336284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x133628a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x133628f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1336294e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133629a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133629f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13362a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13362aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13362af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13362b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13361b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13362b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13362c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13362c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13362cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13362d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13362d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13362db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13362e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13362e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13362eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13362f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13362f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13362fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1336300a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1336305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133630a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133630f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1336313d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x133631870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133631d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1336321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x133632650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133632af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133632f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133633430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1336338d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133633d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133634210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1336346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x133634b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x133634ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x133635490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x133635930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x133635dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x133636270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x133636710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x133636bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x133637050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1336374f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x133637990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x133637e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1336382d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x133638770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x133638c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1336390b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x133639550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1336399f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133639e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13363a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13363a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13363ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13363b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13363b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13363ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13363bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13363c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13363c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13363ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13363d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13363d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13363dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13363df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13363e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13363e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13363ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13363f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13363f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13363fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13363ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133640450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1336408f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133640d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133641230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1336416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133641b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x133642010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1336424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x133642950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133642df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x133643290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133643730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133643bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133644070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133644510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1336449b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x133644e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1336452f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x133645790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133645c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1336460d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x133646570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x133646a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x133646eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133647350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1336477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x133647d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x133648290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1336487e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x133648d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x133648ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x133649600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x133649c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13364a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13364aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13364aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13364b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13364b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13364bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13364c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13364ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13364cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13364d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13364db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13364e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13364e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13364eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13364f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13364f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13364faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133650040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x133650590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133650ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133651030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x133651580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133651ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133652020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x133652570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133652ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133653010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x133653560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133653ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133654000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x133654550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133654aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x133654ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x133655540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x133655a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x133655fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x133656530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x133656a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x133656fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x133657520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x133657a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x133657fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x133658510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x133658a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x133658fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x133659500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x133659a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x133659fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13365a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13365aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13365af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13365b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13365ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13365bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13365c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13365ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13365cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13365d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13365da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13365df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13365e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13365ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13365ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13365f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13365f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13365ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x133660490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133660930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133660dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133661270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133661710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133661bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133662050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1336624f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x133662990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133662e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1336632d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133663770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x133663c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1336640b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x133664550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1336649f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x133664f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x133665660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x133665d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1336664a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x133666bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x133666e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x133667670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x133667930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x133667f40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.124.162 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.124.165 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133504bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x133505040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1335054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133505920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133505d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133506200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133506670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133506ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x133506f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1335073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x133507830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x133507f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x133508a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1335091f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x133509a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13350a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13350a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13350af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13350b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13350bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13350c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13350cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13350d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13350da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13350e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13350e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13350e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13350eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13350efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13350f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13350f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13350fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x133510230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1335104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133510960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133510dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x133511240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1335116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133511b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133511f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x133512400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133512870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133512ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133513150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1335135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x133513a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133513ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133514310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133514780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133514bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133515060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1335154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133515940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133515db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133516220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133516690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133516c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x133517100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133517570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1335179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133517e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1335182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133518730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133518ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133519010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133519480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1335198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133519d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13351a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13351a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13351aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13351af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13351b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13351b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13351bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13351c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13351c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13351c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13351ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13351d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13351d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13351db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13351dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13351e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13351e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13351ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13351f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13351f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13351fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13351ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133520370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1335207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133520c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1335210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133521530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1335219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133521e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133522280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1335226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133522b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133522fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x133523440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1335238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133523d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x133524190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x133524600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133524a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x133524ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x133525350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1335257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x133525c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1335260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133526510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133526980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133526df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133527260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1335276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133527b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133527fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x133528420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133528890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133528d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133529170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1335295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133529a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133529ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13352a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13352a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13352ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13352b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13352b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13352b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13352bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13352c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13352c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13352cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13352cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13352d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13352d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13352dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13352e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13352e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13352ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13352eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13352f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13352f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13352fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133530060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1335304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133530940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133530db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133531220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133531690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133531b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133531f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1335323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133532850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133532cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133533130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1335335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133533a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x133533e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1335342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133534760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133534bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133535040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1335354b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133535920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133535d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133536200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133536670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133536ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133536f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1335373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133537830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x133537ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133538110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x133538580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1335389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133538e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1335392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133539740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x133539bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13353a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13353a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13353a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13353ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13353b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13353b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13353bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13353bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13353c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13353c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13353cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13353d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13353d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13353d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13353de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13353e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13353e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13353eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13353f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13353f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13353f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13353fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1335401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133540630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133540bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133541030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1335414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133541ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1335422b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133542570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1335429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x133542e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1335432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x133543730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133543ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x133544010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133544480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1335448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x133544d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1335451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133545640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x133545ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133545f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133546390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x133546800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133546c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1335470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x133547550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1335479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x133547e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1335482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x133548710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x133548b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x133548ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x133549460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1335498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x133549d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13354a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13354a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13354aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13354af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13354b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13354b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13354bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13354c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13354c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13354c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13354ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13354d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13354d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13354db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13354dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13354e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13354e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13354ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13354f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13354f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13354fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13354fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133550350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1335507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133550c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1335510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x133551510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133551980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133551df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133552260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1335526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133552b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133552fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133553420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x133553890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133553d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133554170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1335545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x133554a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x133554ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x133555330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1335557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x133555c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x133556680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x133556da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1335574c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x133557be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x133557ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x133558310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x133558910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x133558f20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133504ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x133504f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1335053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133505830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133505ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133506110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133506580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1335069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x133506e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1335072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x133507740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x133507d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x133508610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x133508d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x133509570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x133509c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13350a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13350aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13350b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13350bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13350c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13350c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13350cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13350d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13350dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13350e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13350e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13350eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13350ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13350f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13350f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13350fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1335100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1335103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133510810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133510c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1335110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133511560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1335119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133511e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1335122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133512720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133512b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133513000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133513470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1335138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133513d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1335141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133514630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133514aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133514f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133515380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1335157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133515c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1335160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133516540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1335169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x133516e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133517290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133517700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133517b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133517fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133518450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1335188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133518d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1335191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x133519610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133519a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x133519ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13351a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13351a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13351ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13351b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13351b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13351b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13351be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13351c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13351c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13351cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13351cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13351d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13351d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13351dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13351e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13351e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13351ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13351eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13351f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13351f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13351fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133520090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133520500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133520970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133520de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133521250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1335216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133521b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133521fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x133522410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133522880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133522cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x133523160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1335235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133523a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x133523eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x133524320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133524790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x133524c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x133525070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1335254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x133525950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133525dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133526230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1335266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133526b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133526f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1335273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133527860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133527cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x133528140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1335285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133528a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133528e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x133529300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133529770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133529be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13352a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13352a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13352a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13352ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13352b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13352b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13352baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13352bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13352c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13352c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13352ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13352d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13352d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13352da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13352de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13352e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13352e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13352ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13352f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13352f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13352f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13352fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1335301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133530660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133530ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133530f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1335313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133531820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133531c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133532100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133532570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1335329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133532e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1335332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133533730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x133533ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133534010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133534480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1335348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133534d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1335351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133535640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133535ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133535f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133536390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133536800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133536c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1335370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133537550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1335379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133537e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1335382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133538710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133538b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133538ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133539460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1335398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x133539d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13353a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13353a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13353aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13353af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13353b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13353b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13353bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13353c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13353c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13353c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13353ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13353d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13353d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13353db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13353dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13353e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13353e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13353ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13353f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13353f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13353fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13353fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133540350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1335407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133540c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1335410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133541820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x133541c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133542100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133542570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1335429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x133542e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1335432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133543730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x133543ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133544010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133544480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1335448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133544d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1335451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x133545640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133545ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133545f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x133546390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133546800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133546c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1335470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133547550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1335479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x133547e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1335482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x133548710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x133548b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x133548ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x133549460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1335498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x133549d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13354a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13354a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13354aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13354af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13354b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13354b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13354bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13354c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13354c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13354c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13354ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13354d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13354d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13354db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13354dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13354e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13354e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13354ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13354f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13354f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13354fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13354fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x133550350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1335507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133550c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1335510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133551510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133551980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133551df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133552260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1335526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133552b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133552fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x133553420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133553890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133553d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133554170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1335545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x133554a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x133554ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x133555330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1335557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x133556000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1335566f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x133556de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1335574d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x133557940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x133557db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x133558220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x133558690 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.731s
user	0m0.265s
sys	0m0.274s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4397 (a813badb)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12a60b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12a60bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12a60c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12a60c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12a60ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12a60d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12a60d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12a60dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12a60e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12a60e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12a60ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12a60f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12a60fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12a610560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12a610d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12a611490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12a611bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12a6122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12a6129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12a6131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12a6138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12a614000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12a614720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12a614fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12a6156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12a6159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12a615fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12a616c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12a617160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12a617420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12a6178c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12a617b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12a618410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12a618950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12a618c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12a6190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12a619550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12a6199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12a619e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12a61a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12a61a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12a61ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12a61b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12a61b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12a61b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12a61be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12a61c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12a61cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12a61d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12a61d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12a61dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12a61e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12a61ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12a61f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12a61fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12a61fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12a620340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12a620600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12a620c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12a621400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12a6216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12a621b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12a622000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12a6224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12a622940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12a622de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12a623280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12a623720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12a623bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12a624060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12a624500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12a6249a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12a624e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12a625390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12a6258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12a625e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12a626380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12a6268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12a626e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12a627370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12a6278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12a627e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12a628360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12a6288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12a628e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12a629350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12a6298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12a629df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12a62a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12a62a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12a62ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12a62b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12a62b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12a62bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12a62c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12a62c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12a62cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12a61caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12a62d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12a62d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12a62df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12a62e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12a62e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12a62ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12a62f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12a62f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12a62ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12a630460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12a6309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12a630f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12a631450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12a6319a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12a631ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12a632390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12a632830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12a632cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12a633170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12a633610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12a633ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12a633f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12a6343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12a634890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12a634d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12a6351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12a635670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12a635b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12a635fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12a636450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12a6368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12a636d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12a637230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12a6376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12a637b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12a638010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12a6384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12a638950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12a638df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12a639290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12a639730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12a639bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12a63a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12a63a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12a63a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12a63ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12a63b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12a63b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12a63bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12a63c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12a63c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12a63ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12a63ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12a63d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12a63d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12a63dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12a63e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12a63e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12a63ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12a63ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12a63f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12a63f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12a63fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12a640190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12a640630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12a640ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12a640f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12a641410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12a6418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12a641d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12a6421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12a642690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12a642b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12a642fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12a643470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12a643910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12a643db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12a644250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12a6446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12a644b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12a645030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12a6454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12a645970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12a645e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12a6462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12a646750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12a646bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12a647090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12a647530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12a6479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12a647e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12a648310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12a6487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12a648c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12a6490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12a649640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12a649b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12a64a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12a64a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12a64a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12a64af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12a64b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12a64bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12a64c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12a64c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12a64ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12a64d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12a64d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12a64de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12a64e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12a64e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12a64ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12a64f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12a64f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12a64feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12a650400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12a650950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12a650ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12a6513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12a651940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12a651e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12a6523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12a652930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12a652e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12a6533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12a653920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12a653e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12a6543c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12a654910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12a654e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12a6553b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12a655900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12a655e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12a6563a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12a6568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12a656e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12a657390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12a6578e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12a657e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12a658380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12a6588d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12a658e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12a659370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12a6598c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12a659e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12a65a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12a65a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12a65ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12a65b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12a65b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12a65bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12a65c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12a65c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12a65cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12a65d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12a65d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12a65ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12a65e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12a65e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12a65edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12a65f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12a65f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12a65fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12a660300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12a660850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12a660da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12a6612f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12a661840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12a661d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12a662230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12a6626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12a662b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12a663010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12a6634b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12a663950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12a663df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12a664290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12a664730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12a664bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12a665070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12a665510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12a6659b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12a665e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12a6662f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12a666840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12a666f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12a667680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12a667da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12a6684c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12a668780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12a668f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12a669230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12a669840 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.941 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.945 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ee04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ee04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ee053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ee05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ee05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ee06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ee06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ee069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ee06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ee073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ee07850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ee07ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ee089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ee091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ee099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ee0a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ee0a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ee0af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ee0b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ee0be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ee0c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ee0cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ee0d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ee0da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ee0e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ee0e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ee0e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ee0eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ee0f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ee0f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ee0f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ee0fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ee10280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ee10540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ee109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ee10e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ee11290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ee11700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ee11b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ee11fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ee12450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ee128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ee12d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ee131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ee13610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ee13a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ee13ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ee14360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ee147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ee14c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ee150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ee15520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ee15990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ee15e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ee16270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ee166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ee16c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ee17150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ee175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ee17a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ee17ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ee18310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ee18780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ee18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ee19060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ee194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ee19940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ee19db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ee1a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ee1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ee1ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ee1af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ee1b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12ee1b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12ee1bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12ee1c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ee1c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12ee1ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ee1ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12ee1d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12ee1d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12ee1dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12ee1e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12ee1e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12ee1e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12ee1ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12ee1f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12ee1f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12ee1fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12ee1ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12ee203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12ee20830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12ee20ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12ee21110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12ee21580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12ee219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12ee21e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12ee222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12ee22740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12ee22bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12ee23020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12ee23490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12ee23900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12ee23d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12ee241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ee24650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12ee24ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12ee24f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12ee253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12ee25810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12ee25c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12ee260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12ee26560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ee269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ee26e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12ee272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12ee27720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12ee27b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12ee28000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ee28470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12ee288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12ee28d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ee291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ee29630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ee29aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12ee29f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12ee2a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ee2a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ee2ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12ee2b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12ee2b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12ee2b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12ee2be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12ee2c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12ee2c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ee2cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ee2cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12ee2d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ee2d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ee2dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12ee2e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ee2e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ee2ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ee2eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ee2f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ee2f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ee2fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ee300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ee30520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ee30990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ee30e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ee31270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ee316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ee31b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ee31fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ee32430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ee328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ee32d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ee33180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ee335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ee33a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ee33ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ee34340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ee347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ee34c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ee35090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ee35500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ee35970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ee35de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ee36250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ee366c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ee36b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ee36fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ee37410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ee37880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ee37cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ee38160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ee385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ee38a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ee38eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ee39320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ee39790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ee39c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ee3a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ee3a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ee3a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ee3adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ee3b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ee3b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ee3bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ee3bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ee3c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ee3c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ee3ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ee3d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ee3d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ee3da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ee3de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ee3e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ee3e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ee3ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ee3f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ee3f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ee3f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ee3fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ee40210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ee40680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ee40c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ee41080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ee414f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ee42040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ee42300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ee425c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ee42a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ee42ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ee43310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ee43780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ee43bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ee44060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ee444d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ee44940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ee44db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ee45220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ee45690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ee45b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ee45f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ee463e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ee46850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ee46cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ee47130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ee475a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ee47a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ee47e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ee482f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ee48760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ee48bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ee49040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ee494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ee49920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ee49d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ee4a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ee4a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ee4aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ee4af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ee4b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ee4b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ee4bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ee4c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ee4c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ee4c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ee4ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ee4d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ee4d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ee4dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ee4e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ee4e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ee4e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ee4ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ee4f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ee4f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ee4fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ee4ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ee503a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ee50810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ee50c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ee510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ee51560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ee519d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ee51e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ee522b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ee52720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ee52b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ee53000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ee53470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ee538e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ee53d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ee541c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ee54630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ee54aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ee54f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ee55380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ee557f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ee55c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ee566d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ee56df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ee57510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ee57c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ee57ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12ee58360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ee58960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ee58f70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12b8046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12b804b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12b804fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12b805430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12b8058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12b805d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12b806180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12b8065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12b806a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12b806ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12b807340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12b807a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12b808530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12b808ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12b8094f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12b809c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12b80a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12b80aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12b80b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12b80b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12b80c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12b80c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12b80cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12b80d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12b80dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12b80dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12b80e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12b80e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12b80eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12b80efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12b80f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12b80f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12b80fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12b810080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12b8104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12b810960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12b810dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12b811240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12b8116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12b811b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12b811f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12b812400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12b812870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12b812ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12b813150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12b8135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12b813a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12b813ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12b814310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12b814780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12b814bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12b815060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12b8154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12b815940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12b815db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12b816220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12b816790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12b816c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12b817100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12b817570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12b8179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12b817e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12b8182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12b818730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12b818ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12b819010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12b819480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12b8198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12b819d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12b81a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12b81a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12b81aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12b81af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12b81b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12b81b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12b81bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12b81c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12b81c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12b81c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12b81ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12b81d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12b81d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12b81db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12b81dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12b81e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12b81e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12b81ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12b81f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12b81f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12b81fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12b81ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12b820370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12b8207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12b820c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12b8210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12b821530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12b8219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12b821e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12b822280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12b8226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12b822b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12b822fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12b823440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12b8238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12b823d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12b824190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12b824600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12b824a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12b824ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12b825350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12b8257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12b825c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12b8260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12b826510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12b826980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12b826df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12b827260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12b8276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12b827b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12b827fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12b828420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12b828890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12b828d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12b829170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12b8295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12b829a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12b829ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12b82a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12b82a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12b82ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12b82b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12b82b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12b82b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12b82bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12b82c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12b82c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12b82cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12b82cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12b82d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12b82d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12b82dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12b82e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12b82e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12b82ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12b82eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12b82f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12b82f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12b82fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12b830060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12b8304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12b830940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12b830db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12b831220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12b831690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12b831b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12b831f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12b8323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12b832850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12b832cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12b833130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12b8335a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12b833a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12b833e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12b8342f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12b834760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12b834bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12b835040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12b8354b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12b835920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12b835d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12b836200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12b836670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12b836ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12b836f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12b8373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12b837830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12b837ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12b838110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12b838580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12b8389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12b838e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12b8392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12b839740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12b839bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12b83a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12b83a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12b83a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12b83ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12b83b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12b83b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12b83bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12b83bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12b83c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12b83c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12b83cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12b83d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12b83d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12b83d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12b83de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12b83e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12b83e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12b83eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12b83f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12b83f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12b83f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12b83fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12b8401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12b840750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12b840bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12b841030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12b841b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12b841e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12b842100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12b842570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12b8429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12b842e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12b8432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12b843730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12b843ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12b844010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12b844480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12b8448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12b844d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12b8451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12b845640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12b845ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12b845f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12b846390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12b846800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12b846c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12b8470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12b847550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12b8479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12b847e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12b8482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12b848710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12b848b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12b848ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12b849460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12b8498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12b849d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12b84a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12b84a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12b84aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12b84b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12b84b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12b84bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12b84bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12b84c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12b84c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12b84ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12b84d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12b84d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12b84da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12b84deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12b84e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12b84e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12b84ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12b84f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12b84f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12b84f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12b84fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12b850230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12b8506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12b850b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12b850f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12b8513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12b851860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12b851cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12b852140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12b8525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12b852a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12b852e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12b853300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12b853770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12b853be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12b854050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12b8544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12b854930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12b854da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12b855210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12b855680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12b855af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12b856560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12b856c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12b8573a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12b857ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12b857d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12b8581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12b8587f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12b858e00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.944s
user	0m0.242s
sys	0m0.147s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
