### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.65 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.15 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.19 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.47 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.30 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.23 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.70 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.23 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.63 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.23 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.23 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.29 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.20 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.29 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.21 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   18.82 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.33 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.15 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.23 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.38 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    3.16 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    1.09 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  106.19 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.88 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.72 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.35 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.23 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 170.69 sec*proc (29 tests)

Total Test time (real) = 170.70 sec

real	2m50.714s
user	4m43.975s
sys	0m5.846s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.35 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.93 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.24 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.19 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.89 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.24 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.34 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.19 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.25 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.47 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.42 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.65 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.28 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.21 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  49.16 sec*proc (29 tests)

Total Test time (real) =  49.17 sec

real	0m49.184s
user	0m55.055s
sys	0m5.248s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.133 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.027 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.548 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.023.556 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.559 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.023.559 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.560 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.023.561 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.023.562 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.023.563 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.023.564 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.023.564 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.023.565 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.023.566 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.023.568 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.023.569 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.023.570 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.023.571 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.023.571 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.023.572 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.023.572 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.028.647 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.030.003 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.006 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.030.006 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.030.007 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.030.007 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.030.008 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.030.009 I llama_model_loader: - type  f32:  124 tensors
0.00.030.009 I llama_model_loader: - type  f16:   73 tensors
0.00.030.010 I print_info: file format = GGUF V3 (latest)
0.00.030.011 I print_info: file type   = F16
0.00.030.012 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.034.738 I load: special tokens cache size = 5
0.00.037.069 I load: token to piece cache size = 0.2032 MB
0.00.037.095 I print_info: arch             = bert
0.00.037.097 I print_info: vocab_only       = 0
0.00.037.097 I print_info: n_ctx_train      = 512
0.00.037.097 I print_info: n_embd           = 384
0.00.037.097 I print_info: n_layer          = 12
0.00.037.100 I print_info: n_head           = 12
0.00.037.101 I print_info: n_head_kv        = 12
0.00.037.101 I print_info: n_rot            = 32
0.00.037.102 I print_info: n_swa            = 0
0.00.037.102 I print_info: n_embd_head_k    = 32
0.00.037.102 I print_info: n_embd_head_v    = 32
0.00.037.103 I print_info: n_gqa            = 1
0.00.037.104 I print_info: n_embd_k_gqa     = 384
0.00.037.105 I print_info: n_embd_v_gqa     = 384
0.00.037.105 I print_info: f_norm_eps       = 1.0e-12
0.00.037.106 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.106 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.106 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.107 I print_info: f_logit_scale    = 0.0e+00
0.00.037.107 I print_info: n_ff             = 1536
0.00.037.108 I print_info: n_expert         = 0
0.00.037.108 I print_info: n_expert_used    = 0
0.00.037.108 I print_info: causal attn      = 0
0.00.037.108 I print_info: pooling type     = 2
0.00.037.108 I print_info: rope type        = 2
0.00.037.109 I print_info: rope scaling     = linear
0.00.037.109 I print_info: freq_base_train  = 10000.0
0.00.037.110 I print_info: freq_scale_train = 1
0.00.037.110 I print_info: n_ctx_orig_yarn  = 512
0.00.037.110 I print_info: rope_finetuned   = unknown
0.00.037.110 I print_info: ssm_d_conv       = 0
0.00.037.111 I print_info: ssm_d_inner      = 0
0.00.037.111 I print_info: ssm_d_state      = 0
0.00.037.111 I print_info: ssm_dt_rank      = 0
0.00.037.111 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.112 I print_info: model type       = 33M
0.00.037.112 I print_info: model params     = 33.21 M
0.00.037.113 I print_info: general.name     = Bge Small
0.00.037.114 I print_info: vocab type       = WPM
0.00.037.114 I print_info: n_vocab          = 30522
0.00.037.114 I print_info: n_merges         = 0
0.00.037.115 I print_info: BOS token        = 101 '[CLS]'
0.00.037.115 I print_info: UNK token        = 100 '[UNK]'
0.00.037.115 I print_info: SEP token        = 102 '[SEP]'
0.00.037.115 I print_info: PAD token        = 0 '[PAD]'
0.00.037.116 I print_info: MASK token       = 103 '[MASK]'
0.00.037.116 I print_info: LF token         = 0 '[PAD]'
0.00.037.116 I print_info: max token length = 21
0.00.037.117 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.040.055 I load_tensors: offloading 12 repeating layers to GPU
0.00.040.057 I load_tensors: offloading output layer to GPU
0.00.040.058 I load_tensors: offloaded 13/13 layers to GPU
0.00.040.082 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.040.083 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.040.359 I llama_init_from_model: n_seq_max     = 1
0.00.040.360 I llama_init_from_model: n_ctx         = 512
0.00.040.360 I llama_init_from_model: n_ctx_per_seq = 512
0.00.040.361 I llama_init_from_model: n_batch       = 2048
0.00.040.361 I llama_init_from_model: n_ubatch      = 2048
0.00.040.361 I llama_init_from_model: flash_attn    = 0
0.00.040.362 I llama_init_from_model: freq_base     = 10000.0
0.00.040.362 I llama_init_from_model: freq_scale    = 1
0.00.040.362 I ggml_metal_init: allocating
0.00.040.367 I ggml_metal_init: found device: Apple M4
0.00.040.371 I ggml_metal_init: picking default device: Apple M4
0.00.041.173 I ggml_metal_init: using embedded metal library
0.00.045.168 I ggml_metal_init: GPU name:   Apple M4
0.00.045.171 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.045.171 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.045.172 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.045.172 I ggml_metal_init: simdgroup reduction   = true
0.00.045.172 I ggml_metal_init: simdgroup matrix mul. = true
0.00.045.173 I ggml_metal_init: has residency sets    = true
0.00.045.173 I ggml_metal_init: has bfloat            = true
0.00.045.173 I ggml_metal_init: use bfloat            = true
0.00.045.173 I ggml_metal_init: hasUnifiedMemory      = true
0.00.045.174 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.057.962 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.058.660 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.058.663 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.058.664 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.059.784 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.059.786 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.059.786 I llama_init_from_model: graph nodes  = 429
0.00.059.786 I llama_init_from_model: graph splits = 2
0.00.059.788 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.059.788 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.065.465 I 
0.00.065.493 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.066.143 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.071.359 I llama_perf_context_print:        load time =      47.43 ms
0.00.071.361 I llama_perf_context_print: prompt eval time =       5.05 ms /     9 tokens (    0.56 ms per token,  1780.42 tokens per second)
0.00.071.362 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.071.363 I llama_perf_context_print:       total time =       5.90 ms /    10 tokens
0.00.071.520 I ggml_metal_free: deallocating

real	0m0.277s
user	0m0.052s
sys	0m0.032s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.046 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.718 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.014.722 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.014.727 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.728 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.014.729 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.729 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.014.730 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.014.730 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.014.731 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.014.731 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.014.732 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.014.732 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.014.732 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.014.734 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.014.735 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.014.735 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.014.736 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.014.736 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.014.736 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.017.337 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.018.031 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.018.032 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.018.032 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.018.033 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.018.033 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.018.033 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.018.034 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.018.034 I llama_model_loader: - type  f32:  124 tensors
0.00.018.034 I llama_model_loader: - type q8_0:   73 tensors
0.00.018.035 I print_info: file format = GGUF V3 (latest)
0.00.018.035 I print_info: file type   = Q8_0
0.00.018.037 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.020.682 I load: special tokens cache size = 5
0.00.022.111 I load: token to piece cache size = 0.2032 MB
0.00.022.120 I print_info: arch             = bert
0.00.022.121 I print_info: vocab_only       = 0
0.00.022.122 I print_info: n_ctx_train      = 512
0.00.022.122 I print_info: n_embd           = 384
0.00.022.122 I print_info: n_layer          = 12
0.00.022.125 I print_info: n_head           = 12
0.00.022.127 I print_info: n_head_kv        = 12
0.00.022.127 I print_info: n_rot            = 32
0.00.022.128 I print_info: n_swa            = 0
0.00.022.128 I print_info: n_embd_head_k    = 32
0.00.022.128 I print_info: n_embd_head_v    = 32
0.00.022.128 I print_info: n_gqa            = 1
0.00.022.129 I print_info: n_embd_k_gqa     = 384
0.00.022.130 I print_info: n_embd_v_gqa     = 384
0.00.022.130 I print_info: f_norm_eps       = 1.0e-12
0.00.022.131 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.022.131 I print_info: f_clamp_kqv      = 0.0e+00
0.00.022.131 I print_info: f_max_alibi_bias = 0.0e+00
0.00.022.132 I print_info: f_logit_scale    = 0.0e+00
0.00.022.132 I print_info: n_ff             = 1536
0.00.022.132 I print_info: n_expert         = 0
0.00.022.132 I print_info: n_expert_used    = 0
0.00.022.133 I print_info: causal attn      = 0
0.00.022.133 I print_info: pooling type     = 2
0.00.022.133 I print_info: rope type        = 2
0.00.022.133 I print_info: rope scaling     = linear
0.00.022.136 I print_info: freq_base_train  = 10000.0
0.00.022.136 I print_info: freq_scale_train = 1
0.00.022.136 I print_info: n_ctx_orig_yarn  = 512
0.00.022.136 I print_info: rope_finetuned   = unknown
0.00.022.136 I print_info: ssm_d_conv       = 0
0.00.022.137 I print_info: ssm_d_inner      = 0
0.00.022.137 I print_info: ssm_d_state      = 0
0.00.022.137 I print_info: ssm_dt_rank      = 0
0.00.022.137 I print_info: ssm_dt_b_c_rms   = 0
0.00.022.137 I print_info: model type       = 33M
0.00.022.138 I print_info: model params     = 33.21 M
0.00.022.138 I print_info: general.name     = Bge Small
0.00.022.139 I print_info: vocab type       = WPM
0.00.022.139 I print_info: n_vocab          = 30522
0.00.022.141 I print_info: n_merges         = 0
0.00.022.141 I print_info: BOS token        = 101 '[CLS]'
0.00.022.141 I print_info: UNK token        = 100 '[UNK]'
0.00.022.142 I print_info: SEP token        = 102 '[SEP]'
0.00.022.142 I print_info: PAD token        = 0 '[PAD]'
0.00.022.142 I print_info: MASK token       = 103 '[MASK]'
0.00.022.142 I print_info: LF token         = 0 '[PAD]'
0.00.022.142 I print_info: max token length = 21
0.00.022.143 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.024.021 I load_tensors: offloading 12 repeating layers to GPU
0.00.024.022 I load_tensors: offloading output layer to GPU
0.00.024.023 I load_tensors: offloaded 13/13 layers to GPU
0.00.024.028 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.024.030 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.024.228 I llama_init_from_model: n_seq_max     = 1
0.00.024.229 I llama_init_from_model: n_ctx         = 512
0.00.024.229 I llama_init_from_model: n_ctx_per_seq = 512
0.00.024.229 I llama_init_from_model: n_batch       = 2048
0.00.024.230 I llama_init_from_model: n_ubatch      = 2048
0.00.024.230 I llama_init_from_model: flash_attn    = 0
0.00.024.230 I llama_init_from_model: freq_base     = 10000.0
0.00.024.230 I llama_init_from_model: freq_scale    = 1
0.00.024.231 I ggml_metal_init: allocating
0.00.024.234 I ggml_metal_init: found device: Apple M4
0.00.024.237 I ggml_metal_init: picking default device: Apple M4
0.00.024.800 I ggml_metal_init: using embedded metal library
0.00.027.540 I ggml_metal_init: GPU name:   Apple M4
0.00.027.542 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.027.543 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.027.543 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.027.543 I ggml_metal_init: simdgroup reduction   = true
0.00.027.544 I ggml_metal_init: simdgroup matrix mul. = true
0.00.027.544 I ggml_metal_init: has residency sets    = true
0.00.027.544 I ggml_metal_init: has bfloat            = true
0.00.027.544 I ggml_metal_init: use bfloat            = true
0.00.027.544 I ggml_metal_init: hasUnifiedMemory      = true
0.00.027.545 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.037.931 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.038.573 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.038.576 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.038.577 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.039.529 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.039.530 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.039.531 I llama_init_from_model: graph nodes  = 429
0.00.039.531 I llama_init_from_model: graph splits = 2
0.00.039.533 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.039.533 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.043.682 I 
0.00.043.708 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.044.282 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.048.739 I llama_perf_context_print:        load time =      31.96 ms
0.00.048.740 I llama_perf_context_print: prompt eval time =       4.34 ms /     9 tokens (    0.48 ms per token,  2074.69 tokens per second)
0.00.048.741 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.048.741 I llama_perf_context_print:       total time =       5.06 ms /    10 tokens
0.00.048.959 I ggml_metal_free: deallocating

real	0m0.061s
user	0m0.032s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.227 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.499 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.269 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.031.274 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.276 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.031.277 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.278 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.031.278 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.031.279 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.031.284 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.031.284 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.031.291 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.031.292 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.031.292 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.031.296 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.031.296 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.031.297 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.031.298 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.298 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.038.707 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.040.773 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.171 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.045.173 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.173 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.045.174 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.045.174 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.045.174 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.045.175 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.045.175 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.045.176 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.045.176 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.045.176 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.045.177 I llama_model_loader: - type  f32:   40 tensors
0.00.045.177 I llama_model_loader: - type  f16:   30 tensors
0.00.045.178 I print_info: file format = GGUF V3 (latest)
0.00.045.178 I print_info: file type   = F16
0.00.045.179 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.049.305 W load: empty token at index 5
0.00.054.341 W load: model vocab missing newline token, using special_pad_id instead
0.00.055.883 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.055.920 I load: special tokens cache size = 5
0.00.324.676 I load: token to piece cache size = 1.5060 MB
0.00.324.704 I print_info: arch             = jina-bert-v2
0.00.324.705 I print_info: vocab_only       = 0
0.00.324.705 I print_info: n_ctx_train      = 8192
0.00.324.705 I print_info: n_embd           = 384
0.00.324.706 I print_info: n_layer          = 4
0.00.324.712 I print_info: n_head           = 12
0.00.324.713 I print_info: n_head_kv        = 12
0.00.324.713 I print_info: n_rot            = 32
0.00.324.713 I print_info: n_swa            = 0
0.00.324.713 I print_info: n_embd_head_k    = 32
0.00.324.714 I print_info: n_embd_head_v    = 32
0.00.324.719 I print_info: n_gqa            = 1
0.00.324.720 I print_info: n_embd_k_gqa     = 384
0.00.324.727 I print_info: n_embd_v_gqa     = 384
0.00.324.729 I print_info: f_norm_eps       = 1.0e-12
0.00.324.730 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.324.731 I print_info: f_clamp_kqv      = 0.0e+00
0.00.324.733 I print_info: f_max_alibi_bias = 8.0e+00
0.00.324.733 I print_info: f_logit_scale    = 0.0e+00
0.00.324.735 I print_info: n_ff             = 1536
0.00.324.735 I print_info: n_expert         = 0
0.00.324.735 I print_info: n_expert_used    = 0
0.00.324.735 I print_info: causal attn      = 0
0.00.324.736 I print_info: pooling type     = -1
0.00.324.736 I print_info: rope type        = -1
0.00.324.736 I print_info: rope scaling     = linear
0.00.324.736 I print_info: freq_base_train  = 10000.0
0.00.324.737 I print_info: freq_scale_train = 1
0.00.324.737 I print_info: n_ctx_orig_yarn  = 8192
0.00.324.737 I print_info: rope_finetuned   = unknown
0.00.324.740 I print_info: ssm_d_conv       = 0
0.00.324.740 I print_info: ssm_d_inner      = 0
0.00.324.741 I print_info: ssm_d_state      = 0
0.00.324.741 I print_info: ssm_dt_rank      = 0
0.00.324.741 I print_info: ssm_dt_b_c_rms   = 0
0.00.324.741 I print_info: model type       = 33M
0.00.324.741 I print_info: model params     = 32.90 M
0.00.324.743 I print_info: general.name     = Jina Bert Implementation
0.00.324.744 I print_info: vocab type       = BPE
0.00.324.745 I print_info: n_vocab          = 61056
0.00.324.745 I print_info: n_merges         = 39382
0.00.324.746 I print_info: BOS token        = 0 '<s>'
0.00.324.746 I print_info: EOS token        = 2 '</s>'
0.00.324.747 I print_info: UNK token        = 3 '<unk>'
0.00.324.747 I print_info: SEP token        = 2 '</s>'
0.00.324.747 I print_info: PAD token        = 1 '<pad>'
0.00.324.747 I print_info: MASK token       = 4 '<mask>'
0.00.324.748 I print_info: EOG token        = 2 '</s>'
0.00.324.748 I print_info: max token length = 45
0.00.324.748 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.327.173 I load_tensors: offloading 4 repeating layers to GPU
0.00.327.174 I load_tensors: offloading output layer to GPU
0.00.327.175 I load_tensors: offloaded 5/5 layers to GPU
0.00.327.199 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.327.200 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.327.658 I llama_init_from_model: n_seq_max     = 1
0.00.327.659 I llama_init_from_model: n_ctx         = 8192
0.00.327.660 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.327.660 I llama_init_from_model: n_batch       = 2048
0.00.327.660 I llama_init_from_model: n_ubatch      = 2048
0.00.327.661 I llama_init_from_model: flash_attn    = 0
0.00.327.661 I llama_init_from_model: freq_base     = 10000.0
0.00.327.662 I llama_init_from_model: freq_scale    = 1
0.00.327.662 I ggml_metal_init: allocating
0.00.327.670 I ggml_metal_init: found device: Apple M4
0.00.327.675 I ggml_metal_init: picking default device: Apple M4
0.00.328.520 I ggml_metal_init: using embedded metal library
0.00.331.060 I ggml_metal_init: GPU name:   Apple M4
0.00.331.062 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.331.062 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.331.062 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.331.063 I ggml_metal_init: simdgroup reduction   = true
0.00.331.063 I ggml_metal_init: simdgroup matrix mul. = true
0.00.331.063 I ggml_metal_init: has residency sets    = true
0.00.331.063 I ggml_metal_init: has bfloat            = true
0.00.331.063 I ggml_metal_init: use bfloat            = true
0.00.331.064 I ggml_metal_init: hasUnifiedMemory      = true
0.00.331.065 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.340.894 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.343.859 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.343.862 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.343.863 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.349.836 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.349.838 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.349.838 I llama_init_from_model: graph nodes  = 154
0.00.349.839 I llama_init_from_model: graph splits = 2
0.00.349.840 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.349.840 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.356.564 I 
0.00.356.594 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.356.889 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.356.890 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.356.895 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.356.895 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.356.901 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.356.902 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.357.413 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.360.856 I llama_perf_context_print:        load time =     338.06 ms
0.00.360.857 I llama_perf_context_print: prompt eval time =       3.44 ms /    62 tokens (    0.06 ms per token, 18044.24 tokens per second)
0.00.360.859 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.360.860 I llama_perf_context_print:       total time =       4.29 ms /    63 tokens
0.00.361.113 I ggml_metal_free: deallocating

real	0m1.159s
user	0m0.338s
sys	0m0.050s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.194 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.391 I main: llama backend init
0.00.000.398 I main: load the model and apply lora adapter, if any
0.00.051.902 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.069.040 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.069.055 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.069.059 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.069.060 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.069.060 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.069.061 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.069.062 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.069.065 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.069.065 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.069.066 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.069.067 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.069.067 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.069.068 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.069.069 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.069.073 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.069.074 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.069.074 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.078.743 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.080.988 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.088.343 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.088.347 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.088.347 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.088.348 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.088.348 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.088.349 I llama_model_loader: - type  f32:  194 tensors
0.00.088.350 I llama_model_loader: - type  f16:   98 tensors
0.00.088.351 I print_info: file format = GGUF V3 (latest)
0.00.088.352 I print_info: file type   = all F32 (guessed)
0.00.088.353 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.102.428 I load: special tokens cache size = 25
0.00.111.225 I load: token to piece cache size = 0.2984 MB
0.00.111.249 I print_info: arch             = gptneox
0.00.111.250 I print_info: vocab_only       = 0
0.00.111.251 I print_info: n_ctx_train      = 2048
0.00.111.251 I print_info: n_embd           = 2048
0.00.111.251 I print_info: n_layer          = 24
0.00.111.254 I print_info: n_head           = 16
0.00.111.255 I print_info: n_head_kv        = 16
0.00.111.256 I print_info: n_rot            = 32
0.00.111.256 I print_info: n_swa            = 0
0.00.111.256 I print_info: n_embd_head_k    = 128
0.00.111.256 I print_info: n_embd_head_v    = 128
0.00.111.259 I print_info: n_gqa            = 1
0.00.111.259 I print_info: n_embd_k_gqa     = 2048
0.00.111.260 I print_info: n_embd_v_gqa     = 2048
0.00.111.261 I print_info: f_norm_eps       = 1.0e-05
0.00.111.263 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.111.263 I print_info: f_clamp_kqv      = 0.0e+00
0.00.111.264 I print_info: f_max_alibi_bias = 0.0e+00
0.00.111.264 I print_info: f_logit_scale    = 0.0e+00
0.00.111.265 I print_info: n_ff             = 8192
0.00.111.265 I print_info: n_expert         = 0
0.00.111.265 I print_info: n_expert_used    = 0
0.00.111.266 I print_info: causal attn      = 1
0.00.111.267 I print_info: pooling type     = 0
0.00.111.267 I print_info: rope type        = 2
0.00.111.267 I print_info: rope scaling     = linear
0.00.111.267 I print_info: freq_base_train  = 10000.0
0.00.111.268 I print_info: freq_scale_train = 1
0.00.111.268 I print_info: n_ctx_orig_yarn  = 2048
0.00.111.268 I print_info: rope_finetuned   = unknown
0.00.111.268 I print_info: ssm_d_conv       = 0
0.00.111.268 I print_info: ssm_d_inner      = 0
0.00.111.269 I print_info: ssm_d_state      = 0
0.00.111.269 I print_info: ssm_dt_rank      = 0
0.00.111.269 I print_info: ssm_dt_b_c_rms   = 0
0.00.111.269 I print_info: model type       = 1.4B
0.00.111.274 I print_info: model params     = 1.41 B
0.00.111.274 I print_info: general.name     = 1.4B
0.00.111.275 I print_info: vocab type       = BPE
0.00.111.275 I print_info: n_vocab          = 50304
0.00.111.275 I print_info: n_merges         = 50009
0.00.111.275 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.111.276 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.111.276 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.111.277 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.111.277 I print_info: LF token         = 187 ''
0.00.111.278 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.111.278 I print_info: max token length = 1024
0.00.111.278 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.165.767 I load_tensors: offloading 24 repeating layers to GPU
0.00.165.770 I load_tensors: offloading output layer to GPU
0.00.165.770 I load_tensors: offloaded 25/25 layers to GPU
0.00.165.797 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.165.799 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.166.265 I llama_init_from_model: n_seq_max     = 1
0.00.166.267 I llama_init_from_model: n_ctx         = 2048
0.00.166.267 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.166.267 I llama_init_from_model: n_batch       = 2048
0.00.166.267 I llama_init_from_model: n_ubatch      = 512
0.00.166.267 I llama_init_from_model: flash_attn    = 0
0.00.166.268 I llama_init_from_model: freq_base     = 10000.0
0.00.166.268 I llama_init_from_model: freq_scale    = 1
0.00.166.270 I ggml_metal_init: allocating
0.00.166.306 I ggml_metal_init: found device: Apple M4
0.00.166.311 I ggml_metal_init: picking default device: Apple M4
0.00.167.102 I ggml_metal_init: using embedded metal library
0.00.176.902 I ggml_metal_init: GPU name:   Apple M4
0.00.176.904 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.176.904 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.176.905 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.176.905 I ggml_metal_init: simdgroup reduction   = true
0.00.176.905 I ggml_metal_init: simdgroup matrix mul. = true
0.00.176.905 I ggml_metal_init: has residency sets    = true
0.00.176.905 I ggml_metal_init: has bfloat            = true
0.00.176.905 I ggml_metal_init: use bfloat            = true
0.00.176.906 I ggml_metal_init: hasUnifiedMemory      = true
0.00.176.907 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.206.624 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.235.504 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.235.509 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.235.541 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.239.658 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.239.661 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.239.662 I llama_init_from_model: graph nodes  = 967
0.00.239.662 I llama_init_from_model: graph splits = 2
0.00.239.667 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.239.799 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.239.799 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.293.020 I main: llama threadpool init, n_threads = 4
0.00.293.093 I 
0.00.293.122 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.293.122 I 
0.00.293.258 I sampler seed: 1234
0.00.293.264 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.293.295 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.293.297 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.293.297 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.097.184 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.02.097.185 I llama_perf_context_print:        load time =     240.21 ms
0.02.097.185 I llama_perf_context_print: prompt eval time =      43.61 ms /     7 tokens (    6.23 ms per token,   160.51 tokens per second)
0.02.097.186 I llama_perf_context_print:        eval time =    1757.40 ms /    63 runs   (   27.90 ms per token,    35.85 tokens per second)
0.02.097.187 I llama_perf_context_print:       total time =    1805.06 ms /    70 tokens
0.02.097.401 I ggml_metal_free: deallocating

real	0m2.470s
user	0m0.141s
sys	0m0.137s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.514 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.626 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.646 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.653 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.655 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.656 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.657 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.658 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.658 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.660 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.661 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.662 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.662 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.663 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.664 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.664 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.667 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.668 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.668 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.032 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.966 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.349 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.351 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.351 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.352 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.352 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.353 I llama_model_loader: - type  f32:  194 tensors
0.00.055.353 I llama_model_loader: - type  f16:   98 tensors
0.00.055.354 I print_info: file format = GGUF V3 (latest)
0.00.055.355 I print_info: file type   = all F32 (guessed)
0.00.055.356 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.232 I load: special tokens cache size = 25
0.00.075.527 I load: token to piece cache size = 0.2984 MB
0.00.075.541 I print_info: arch             = gptneox
0.00.075.542 I print_info: vocab_only       = 0
0.00.075.542 I print_info: n_ctx_train      = 2048
0.00.075.542 I print_info: n_embd           = 2048
0.00.075.543 I print_info: n_layer          = 24
0.00.075.545 I print_info: n_head           = 16
0.00.075.546 I print_info: n_head_kv        = 16
0.00.075.546 I print_info: n_rot            = 32
0.00.075.546 I print_info: n_swa            = 0
0.00.075.547 I print_info: n_embd_head_k    = 128
0.00.075.547 I print_info: n_embd_head_v    = 128
0.00.075.548 I print_info: n_gqa            = 1
0.00.075.548 I print_info: n_embd_k_gqa     = 2048
0.00.075.549 I print_info: n_embd_v_gqa     = 2048
0.00.075.549 I print_info: f_norm_eps       = 1.0e-05
0.00.075.550 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.550 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.550 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.551 I print_info: f_logit_scale    = 0.0e+00
0.00.075.551 I print_info: n_ff             = 8192
0.00.075.554 I print_info: n_expert         = 0
0.00.075.554 I print_info: n_expert_used    = 0
0.00.075.554 I print_info: causal attn      = 1
0.00.075.554 I print_info: pooling type     = 0
0.00.075.554 I print_info: rope type        = 2
0.00.075.555 I print_info: rope scaling     = linear
0.00.075.556 I print_info: freq_base_train  = 10000.0
0.00.075.556 I print_info: freq_scale_train = 1
0.00.075.557 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.557 I print_info: rope_finetuned   = unknown
0.00.075.557 I print_info: ssm_d_conv       = 0
0.00.075.557 I print_info: ssm_d_inner      = 0
0.00.075.557 I print_info: ssm_d_state      = 0
0.00.075.557 I print_info: ssm_dt_rank      = 0
0.00.075.557 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.558 I print_info: model type       = 1.4B
0.00.075.558 I print_info: model params     = 1.41 B
0.00.075.558 I print_info: general.name     = 1.4B
0.00.075.559 I print_info: vocab type       = BPE
0.00.075.559 I print_info: n_vocab          = 50304
0.00.075.559 I print_info: n_merges         = 50009
0.00.075.561 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.561 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.561 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.561 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.561 I print_info: LF token         = 187 ''
0.00.075.562 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.562 I print_info: max token length = 1024
0.00.075.562 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.575.731 I load_tensors: offloading 24 repeating layers to GPU
0.01.575.734 I load_tensors: offloading output layer to GPU
0.01.575.735 I load_tensors: offloaded 25/25 layers to GPU
0.01.575.758 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.575.759 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.576.480 I llama_init_from_model: n_seq_max     = 1
0.01.576.481 I llama_init_from_model: n_ctx         = 128
0.01.576.481 I llama_init_from_model: n_ctx_per_seq = 128
0.01.576.481 I llama_init_from_model: n_batch       = 128
0.01.576.482 I llama_init_from_model: n_ubatch      = 128
0.01.576.482 I llama_init_from_model: flash_attn    = 0
0.01.576.482 I llama_init_from_model: freq_base     = 10000.0
0.01.576.482 I llama_init_from_model: freq_scale    = 1
0.01.576.483 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.576.487 I ggml_metal_init: allocating
0.01.576.520 I ggml_metal_init: found device: Apple M4
0.01.576.525 I ggml_metal_init: picking default device: Apple M4
0.01.577.642 I ggml_metal_init: using embedded metal library
0.01.581.136 I ggml_metal_init: GPU name:   Apple M4
0.01.581.138 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.581.139 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.581.139 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.581.140 I ggml_metal_init: simdgroup reduction   = true
0.01.581.140 I ggml_metal_init: simdgroup matrix mul. = true
0.01.581.140 I ggml_metal_init: has residency sets    = true
0.01.581.140 I ggml_metal_init: has bfloat            = true
0.01.581.140 I ggml_metal_init: use bfloat            = true
0.01.581.140 I ggml_metal_init: hasUnifiedMemory      = true
0.01.581.142 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.591.295 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.592.884 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.592.885 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.592.913 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.594.269 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.594.270 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.594.270 I llama_init_from_model: graph nodes  = 967
0.01.594.270 I llama_init_from_model: graph splits = 2
0.01.594.272 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.594.272 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.627.798 I 
0.01.627.835 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.627.839 I perplexity: tokenizing the input ..
0.01.632.028 I perplexity: tokenization took 4.187 ms
0.01.632.031 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.750.203 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.751.473 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.751.506 I llama_perf_context_print:        load time =    1605.16 ms
0.01.751.508 I llama_perf_context_print: prompt eval time =     117.91 ms /   128 tokens (    0.92 ms per token,  1085.56 tokens per second)
0.01.751.509 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.751.509 I llama_perf_context_print:       total time =     123.71 ms /   129 tokens
0.01.751.904 I ggml_metal_free: deallocating

real	0m1.975s
user	0m0.099s
sys	0m0.364s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.513 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.574 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.579 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.582 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.582 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.583 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.583 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.583 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.584 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.584 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.585 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.585 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.586 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.586 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.588 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.590 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.590 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.590 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.469 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.525 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.353 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.354 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.355 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.355 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.356 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.356 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.357 I llama_model_loader: - type  f32:  194 tensors
0.00.031.357 I llama_model_loader: - type q8_0:   98 tensors
0.00.031.358 I print_info: file format = GGUF V3 (latest)
0.00.031.358 I print_info: file type   = Q8_0
0.00.031.359 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.039.940 I load: special tokens cache size = 25
0.00.046.237 I load: token to piece cache size = 0.2984 MB
0.00.046.255 I print_info: arch             = gptneox
0.00.046.256 I print_info: vocab_only       = 0
0.00.046.257 I print_info: n_ctx_train      = 2048
0.00.046.257 I print_info: n_embd           = 2048
0.00.046.257 I print_info: n_layer          = 24
0.00.046.263 I print_info: n_head           = 16
0.00.046.264 I print_info: n_head_kv        = 16
0.00.046.264 I print_info: n_rot            = 32
0.00.046.264 I print_info: n_swa            = 0
0.00.046.264 I print_info: n_embd_head_k    = 128
0.00.046.264 I print_info: n_embd_head_v    = 128
0.00.046.265 I print_info: n_gqa            = 1
0.00.046.265 I print_info: n_embd_k_gqa     = 2048
0.00.046.266 I print_info: n_embd_v_gqa     = 2048
0.00.046.267 I print_info: f_norm_eps       = 1.0e-05
0.00.046.267 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.046.267 I print_info: f_clamp_kqv      = 0.0e+00
0.00.046.268 I print_info: f_max_alibi_bias = 0.0e+00
0.00.046.268 I print_info: f_logit_scale    = 0.0e+00
0.00.046.269 I print_info: n_ff             = 8192
0.00.046.269 I print_info: n_expert         = 0
0.00.046.269 I print_info: n_expert_used    = 0
0.00.046.271 I print_info: causal attn      = 1
0.00.046.271 I print_info: pooling type     = 0
0.00.046.273 I print_info: rope type        = 2
0.00.046.274 I print_info: rope scaling     = linear
0.00.046.274 I print_info: freq_base_train  = 10000.0
0.00.046.274 I print_info: freq_scale_train = 1
0.00.046.275 I print_info: n_ctx_orig_yarn  = 2048
0.00.046.275 I print_info: rope_finetuned   = unknown
0.00.046.275 I print_info: ssm_d_conv       = 0
0.00.046.275 I print_info: ssm_d_inner      = 0
0.00.046.275 I print_info: ssm_d_state      = 0
0.00.046.275 I print_info: ssm_dt_rank      = 0
0.00.046.275 I print_info: ssm_dt_b_c_rms   = 0
0.00.046.276 I print_info: model type       = 1.4B
0.00.046.280 I print_info: model params     = 1.41 B
0.00.046.281 I print_info: general.name     = 1.4B
0.00.046.282 I print_info: vocab type       = BPE
0.00.046.282 I print_info: n_vocab          = 50304
0.00.046.282 I print_info: n_merges         = 50009
0.00.046.282 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.046.284 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.046.284 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.046.284 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.046.284 I print_info: LF token         = 187 ''
0.00.046.284 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.046.285 I print_info: max token length = 1024
0.00.046.285 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.358.857 I load_tensors: offloading 24 repeating layers to GPU
0.01.358.863 I load_tensors: offloading output layer to GPU
0.01.358.865 I load_tensors: offloaded 25/25 layers to GPU
0.01.358.891 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.358.892 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.359.574 I llama_init_from_model: n_seq_max     = 1
0.01.359.576 I llama_init_from_model: n_ctx         = 2048
0.01.359.576 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.359.576 I llama_init_from_model: n_batch       = 2048
0.01.359.577 I llama_init_from_model: n_ubatch      = 512
0.01.359.577 I llama_init_from_model: flash_attn    = 0
0.01.359.578 I llama_init_from_model: freq_base     = 10000.0
0.01.359.578 I llama_init_from_model: freq_scale    = 1
0.01.359.579 I ggml_metal_init: allocating
0.01.359.596 I ggml_metal_init: found device: Apple M4
0.01.359.602 I ggml_metal_init: picking default device: Apple M4
0.01.360.932 I ggml_metal_init: using embedded metal library
0.01.366.322 I ggml_metal_init: GPU name:   Apple M4
0.01.366.325 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.366.326 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.366.326 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.366.327 I ggml_metal_init: simdgroup reduction   = true
0.01.366.327 I ggml_metal_init: simdgroup matrix mul. = true
0.01.366.327 I ggml_metal_init: has residency sets    = true
0.01.366.328 I ggml_metal_init: has bfloat            = true
0.01.366.328 I ggml_metal_init: use bfloat            = true
0.01.366.329 I ggml_metal_init: hasUnifiedMemory      = true
0.01.366.331 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.383.024 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.436.131 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.436.138 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.436.170 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.442.013 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.442.016 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.442.017 I llama_init_from_model: graph nodes  = 967
0.01.442.017 I llama_init_from_model: graph splits = 2
0.01.442.020 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.442.154 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.442.155 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.488.481 I main: llama threadpool init, n_threads = 4
0.01.488.531 I 
0.01.488.552 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.488.554 I 
0.01.488.664 I sampler seed: 1234
0.01.488.669 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.488.705 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.488.708 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.488.708 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.581.028 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55124.22 tokens per second)
0.02.581.029 I llama_perf_context_print:        load time =    1478.21 ms
0.02.581.029 I llama_perf_context_print: prompt eval time =      49.90 ms /     7 tokens (    7.13 ms per token,   140.28 tokens per second)
0.02.581.030 I llama_perf_context_print:        eval time =    1039.52 ms /    63 runs   (   16.50 ms per token,    60.60 tokens per second)
0.02.581.031 I llama_perf_context_print:       total time =    1093.31 ms /    70 tokens
0.02.581.317 I ggml_metal_free: deallocating

real	0m2.600s
user	0m0.107s
sys	0m0.344s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.260 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.962 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.048 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.053 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.055 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.056 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.056 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.056 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.057 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.064 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.065 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.065 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.065 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.066 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.066 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.066 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.068 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.068 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.069 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.812 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.847 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.609 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.611 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.612 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.612 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.612 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.613 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.613 I llama_model_loader: - type  f32:  194 tensors
0.00.026.614 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.614 I print_info: file format = GGUF V3 (latest)
0.00.026.615 I print_info: file type   = Q8_0
0.00.026.616 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.061 I load: special tokens cache size = 25
0.00.041.545 I load: token to piece cache size = 0.2984 MB
0.00.041.561 I print_info: arch             = gptneox
0.00.041.562 I print_info: vocab_only       = 0
0.00.041.563 I print_info: n_ctx_train      = 2048
0.00.041.563 I print_info: n_embd           = 2048
0.00.041.563 I print_info: n_layer          = 24
0.00.041.567 I print_info: n_head           = 16
0.00.041.567 I print_info: n_head_kv        = 16
0.00.041.568 I print_info: n_rot            = 32
0.00.041.568 I print_info: n_swa            = 0
0.00.041.568 I print_info: n_embd_head_k    = 128
0.00.041.568 I print_info: n_embd_head_v    = 128
0.00.041.568 I print_info: n_gqa            = 1
0.00.041.569 I print_info: n_embd_k_gqa     = 2048
0.00.041.570 I print_info: n_embd_v_gqa     = 2048
0.00.041.570 I print_info: f_norm_eps       = 1.0e-05
0.00.041.571 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.571 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.571 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.571 I print_info: f_logit_scale    = 0.0e+00
0.00.041.572 I print_info: n_ff             = 8192
0.00.041.572 I print_info: n_expert         = 0
0.00.041.572 I print_info: n_expert_used    = 0
0.00.041.572 I print_info: causal attn      = 1
0.00.041.572 I print_info: pooling type     = 0
0.00.041.572 I print_info: rope type        = 2
0.00.041.573 I print_info: rope scaling     = linear
0.00.041.573 I print_info: freq_base_train  = 10000.0
0.00.041.573 I print_info: freq_scale_train = 1
0.00.041.573 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.576 I print_info: rope_finetuned   = unknown
0.00.041.576 I print_info: ssm_d_conv       = 0
0.00.041.576 I print_info: ssm_d_inner      = 0
0.00.041.576 I print_info: ssm_d_state      = 0
0.00.041.576 I print_info: ssm_dt_rank      = 0
0.00.041.577 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.577 I print_info: model type       = 1.4B
0.00.041.577 I print_info: model params     = 1.41 B
0.00.041.577 I print_info: general.name     = 1.4B
0.00.041.578 I print_info: vocab type       = BPE
0.00.041.578 I print_info: n_vocab          = 50304
0.00.041.578 I print_info: n_merges         = 50009
0.00.041.578 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.578 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.584 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.584 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.584 I print_info: LF token         = 187 ''
0.00.041.584 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.584 I print_info: max token length = 1024
0.00.041.585 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.038.308 I load_tensors: offloading 24 repeating layers to GPU
0.01.038.314 I load_tensors: offloading output layer to GPU
0.01.038.315 I load_tensors: offloaded 25/25 layers to GPU
0.01.038.342 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.038.344 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.039.302 I llama_init_from_model: n_seq_max     = 1
0.01.039.303 I llama_init_from_model: n_ctx         = 128
0.01.039.304 I llama_init_from_model: n_ctx_per_seq = 128
0.01.039.304 I llama_init_from_model: n_batch       = 128
0.01.039.304 I llama_init_from_model: n_ubatch      = 128
0.01.039.304 I llama_init_from_model: flash_attn    = 0
0.01.039.305 I llama_init_from_model: freq_base     = 10000.0
0.01.039.306 I llama_init_from_model: freq_scale    = 1
0.01.039.306 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.039.307 I ggml_metal_init: allocating
0.01.039.365 I ggml_metal_init: found device: Apple M4
0.01.039.373 I ggml_metal_init: picking default device: Apple M4
0.01.040.727 I ggml_metal_init: using embedded metal library
0.01.045.311 I ggml_metal_init: GPU name:   Apple M4
0.01.045.314 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.045.315 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.045.316 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.045.316 I ggml_metal_init: simdgroup reduction   = true
0.01.045.316 I ggml_metal_init: simdgroup matrix mul. = true
0.01.045.316 I ggml_metal_init: has residency sets    = true
0.01.045.316 I ggml_metal_init: has bfloat            = true
0.01.045.317 I ggml_metal_init: use bfloat            = true
0.01.045.317 I ggml_metal_init: hasUnifiedMemory      = true
0.01.045.323 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.058.520 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.060.323 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.060.326 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.060.340 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.062.037 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.062.038 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.062.038 I llama_init_from_model: graph nodes  = 967
0.01.062.039 I llama_init_from_model: graph splits = 2
0.01.062.040 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.062.040 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.086.252 I 
0.01.086.283 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.086.286 I perplexity: tokenizing the input ..
0.01.091.151 I perplexity: tokenization took 4.864 ms
0.01.091.155 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.228.856 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.230.120 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.230.141 I llama_perf_context_print:        load time =    1075.28 ms
0.01.230.142 I llama_perf_context_print: prompt eval time =     137.48 ms /   128 tokens (    1.07 ms per token,   931.07 tokens per second)
0.01.230.143 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.230.143 I llama_perf_context_print:       total time =     143.89 ms /   129 tokens
0.01.230.493 I ggml_metal_free: deallocating

real	0m1.246s
user	0m0.071s
sys	0m0.236s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.018.830 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.230 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.029.235 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.237 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.238 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.238 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.239 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.239 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.240 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.240 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.241 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.243 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.244 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.244 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.244 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.252 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.253 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.253 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.688 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.863 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.145 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.146 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.147 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.147 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.147 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.147 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.039.148 I llama_model_loader: - type  f32:  194 tensors
0.00.039.148 I llama_model_loader: - type q4_0:   97 tensors
0.00.039.149 I llama_model_loader: - type q6_K:    1 tensors
0.00.039.149 I print_info: file format = GGUF V3 (latest)
0.00.039.150 I print_info: file type   = Q4_0
0.00.039.150 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.050.206 I load: special tokens cache size = 25
0.00.059.275 I load: token to piece cache size = 0.2984 MB
0.00.059.292 I print_info: arch             = gptneox
0.00.059.293 I print_info: vocab_only       = 0
0.00.059.293 I print_info: n_ctx_train      = 2048
0.00.059.294 I print_info: n_embd           = 2048
0.00.059.294 I print_info: n_layer          = 24
0.00.059.299 I print_info: n_head           = 16
0.00.059.300 I print_info: n_head_kv        = 16
0.00.059.300 I print_info: n_rot            = 32
0.00.059.301 I print_info: n_swa            = 0
0.00.059.301 I print_info: n_embd_head_k    = 128
0.00.059.301 I print_info: n_embd_head_v    = 128
0.00.059.302 I print_info: n_gqa            = 1
0.00.059.303 I print_info: n_embd_k_gqa     = 2048
0.00.059.304 I print_info: n_embd_v_gqa     = 2048
0.00.059.305 I print_info: f_norm_eps       = 1.0e-05
0.00.059.305 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.305 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.306 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.306 I print_info: f_logit_scale    = 0.0e+00
0.00.059.307 I print_info: n_ff             = 8192
0.00.059.307 I print_info: n_expert         = 0
0.00.059.307 I print_info: n_expert_used    = 0
0.00.059.309 I print_info: causal attn      = 1
0.00.059.309 I print_info: pooling type     = 0
0.00.059.309 I print_info: rope type        = 2
0.00.059.310 I print_info: rope scaling     = linear
0.00.059.310 I print_info: freq_base_train  = 10000.0
0.00.059.310 I print_info: freq_scale_train = 1
0.00.059.311 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.311 I print_info: rope_finetuned   = unknown
0.00.059.311 I print_info: ssm_d_conv       = 0
0.00.059.312 I print_info: ssm_d_inner      = 0
0.00.059.312 I print_info: ssm_d_state      = 0
0.00.059.312 I print_info: ssm_dt_rank      = 0
0.00.059.312 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.314 I print_info: model type       = 1.4B
0.00.059.315 I print_info: model params     = 1.41 B
0.00.059.315 I print_info: general.name     = 1.4B
0.00.059.316 I print_info: vocab type       = BPE
0.00.059.316 I print_info: n_vocab          = 50304
0.00.059.316 I print_info: n_merges         = 50009
0.00.059.316 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.317 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.317 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.317 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.318 I print_info: LF token         = 187 ''
0.00.059.318 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.318 I print_info: max token length = 1024
0.00.059.319 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.679.092 I load_tensors: offloading 24 repeating layers to GPU
0.00.679.103 I load_tensors: offloading output layer to GPU
0.00.679.104 I load_tensors: offloaded 25/25 layers to GPU
0.00.679.138 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.679.139 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.680.427 I llama_init_from_model: n_seq_max     = 1
0.00.680.429 I llama_init_from_model: n_ctx         = 2048
0.00.680.430 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.680.431 I llama_init_from_model: n_batch       = 2048
0.00.680.432 I llama_init_from_model: n_ubatch      = 512
0.00.680.432 I llama_init_from_model: flash_attn    = 0
0.00.680.434 I llama_init_from_model: freq_base     = 10000.0
0.00.680.434 I llama_init_from_model: freq_scale    = 1
0.00.680.436 I ggml_metal_init: allocating
0.00.680.503 I ggml_metal_init: found device: Apple M4
0.00.680.516 I ggml_metal_init: picking default device: Apple M4
0.00.682.509 I ggml_metal_init: using embedded metal library
0.00.689.062 I ggml_metal_init: GPU name:   Apple M4
0.00.689.067 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.689.068 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.689.068 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.689.071 I ggml_metal_init: simdgroup reduction   = true
0.00.689.071 I ggml_metal_init: simdgroup matrix mul. = true
0.00.689.071 I ggml_metal_init: has residency sets    = true
0.00.689.071 I ggml_metal_init: has bfloat            = true
0.00.689.072 I ggml_metal_init: use bfloat            = true
0.00.689.073 I ggml_metal_init: hasUnifiedMemory      = true
0.00.689.074 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.707.694 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.762.634 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.762.641 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.762.678 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.767.575 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.767.577 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.767.578 I llama_init_from_model: graph nodes  = 967
0.00.767.578 I llama_init_from_model: graph splits = 2
0.00.767.583 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.767.709 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.767.710 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.816.286 I main: llama threadpool init, n_threads = 4
0.00.816.337 I 
0.00.816.361 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.816.361 I 
0.00.816.516 I sampler seed: 1234
0.00.816.550 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.816.602 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.816.603 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.816.603 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.499.769 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48999.31 tokens per second)
0.01.499.769 I llama_perf_context_print:        load time =     796.72 ms
0.01.499.770 I llama_perf_context_print: prompt eval time =      45.82 ms /     7 tokens (    6.55 ms per token,   152.77 tokens per second)
0.01.499.771 I llama_perf_context_print:        eval time =     634.34 ms /    63 runs   (   10.07 ms per token,    99.32 tokens per second)
0.01.499.771 I llama_perf_context_print:       total time =     684.22 ms /    70 tokens
0.01.500.053 I ggml_metal_free: deallocating

real	0m1.532s
user	0m0.118s
sys	0m0.242s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.264 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.864 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.296 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.302 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.304 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.305 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.305 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.305 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.306 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.308 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.309 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.309 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.309 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.310 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.310 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.311 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.313 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.315 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.315 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.111 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.132 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.070 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.072 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.072 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.072 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.073 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.073 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.074 I llama_model_loader: - type  f32:  194 tensors
0.00.027.074 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.074 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.075 I print_info: file format = GGUF V3 (latest)
0.00.027.076 I print_info: file type   = Q4_0
0.00.027.077 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.710 I load: special tokens cache size = 25
0.00.041.973 I load: token to piece cache size = 0.2984 MB
0.00.041.992 I print_info: arch             = gptneox
0.00.041.993 I print_info: vocab_only       = 0
0.00.041.993 I print_info: n_ctx_train      = 2048
0.00.041.993 I print_info: n_embd           = 2048
0.00.041.993 I print_info: n_layer          = 24
0.00.041.997 I print_info: n_head           = 16
0.00.041.998 I print_info: n_head_kv        = 16
0.00.041.998 I print_info: n_rot            = 32
0.00.041.998 I print_info: n_swa            = 0
0.00.041.998 I print_info: n_embd_head_k    = 128
0.00.041.999 I print_info: n_embd_head_v    = 128
0.00.041.999 I print_info: n_gqa            = 1
0.00.042.000 I print_info: n_embd_k_gqa     = 2048
0.00.042.000 I print_info: n_embd_v_gqa     = 2048
0.00.042.001 I print_info: f_norm_eps       = 1.0e-05
0.00.042.001 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.001 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.002 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.002 I print_info: f_logit_scale    = 0.0e+00
0.00.042.003 I print_info: n_ff             = 8192
0.00.042.007 I print_info: n_expert         = 0
0.00.042.007 I print_info: n_expert_used    = 0
0.00.042.008 I print_info: causal attn      = 1
0.00.042.008 I print_info: pooling type     = 0
0.00.042.008 I print_info: rope type        = 2
0.00.042.008 I print_info: rope scaling     = linear
0.00.042.009 I print_info: freq_base_train  = 10000.0
0.00.042.009 I print_info: freq_scale_train = 1
0.00.042.009 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.009 I print_info: rope_finetuned   = unknown
0.00.042.009 I print_info: ssm_d_conv       = 0
0.00.042.009 I print_info: ssm_d_inner      = 0
0.00.042.010 I print_info: ssm_d_state      = 0
0.00.042.010 I print_info: ssm_dt_rank      = 0
0.00.042.010 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.010 I print_info: model type       = 1.4B
0.00.042.010 I print_info: model params     = 1.41 B
0.00.042.011 I print_info: general.name     = 1.4B
0.00.042.011 I print_info: vocab type       = BPE
0.00.042.011 I print_info: n_vocab          = 50304
0.00.042.011 I print_info: n_merges         = 50009
0.00.042.012 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.012 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.014 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.014 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.014 I print_info: LF token         = 187 ''
0.00.042.015 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.015 I print_info: max token length = 1024
0.00.042.015 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.643.150 I load_tensors: offloading 24 repeating layers to GPU
0.00.643.158 I load_tensors: offloading output layer to GPU
0.00.643.159 I load_tensors: offloaded 25/25 layers to GPU
0.00.643.188 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.643.191 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.644.383 I llama_init_from_model: n_seq_max     = 1
0.00.644.385 I llama_init_from_model: n_ctx         = 128
0.00.644.386 I llama_init_from_model: n_ctx_per_seq = 128
0.00.644.386 I llama_init_from_model: n_batch       = 128
0.00.644.387 I llama_init_from_model: n_ubatch      = 128
0.00.644.387 I llama_init_from_model: flash_attn    = 0
0.00.644.389 I llama_init_from_model: freq_base     = 10000.0
0.00.644.389 I llama_init_from_model: freq_scale    = 1
0.00.644.390 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.644.393 I ggml_metal_init: allocating
0.00.644.470 I ggml_metal_init: found device: Apple M4
0.00.644.484 I ggml_metal_init: picking default device: Apple M4
0.00.646.071 I ggml_metal_init: using embedded metal library
0.00.652.233 I ggml_metal_init: GPU name:   Apple M4
0.00.652.238 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.652.239 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.652.239 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.652.240 I ggml_metal_init: simdgroup reduction   = true
0.00.652.240 I ggml_metal_init: simdgroup matrix mul. = true
0.00.652.241 I ggml_metal_init: has residency sets    = true
0.00.652.241 I ggml_metal_init: has bfloat            = true
0.00.652.241 I ggml_metal_init: use bfloat            = true
0.00.652.242 I ggml_metal_init: hasUnifiedMemory      = true
0.00.652.244 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.669.888 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.673.182 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.673.185 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.673.217 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.676.301 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.676.303 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.676.304 I llama_init_from_model: graph nodes  = 967
0.00.676.304 I llama_init_from_model: graph splits = 2
0.00.676.307 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.676.307 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.880 I 
0.00.703.964 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.972 I perplexity: tokenizing the input ..
0.00.710.906 I perplexity: tokenization took 6.931 ms
0.00.710.920 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.847.496 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.848.756 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.848.776 I llama_perf_context_print:        load time =     693.01 ms
0.00.848.777 I llama_perf_context_print: prompt eval time =     135.68 ms /   128 tokens (    1.06 ms per token,   943.42 tokens per second)
0.00.848.778 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.848.778 I llama_perf_context_print:       total time =     144.90 ms /   129 tokens
0.00.849.128 I ggml_metal_free: deallocating

real	0m0.864s
user	0m0.080s
sys	0m0.165s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.735 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.936 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.024.940 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.942 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.947 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.947 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.948 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.948 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.949 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.949 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.950 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.950 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.952 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.952 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.953 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.954 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.955 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.955 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.784 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.803 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.582 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.583 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.583 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.584 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.584 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.584 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.033.585 I llama_model_loader: - type  f32:  194 tensors
0.00.033.585 I llama_model_loader: - type q4_1:   97 tensors
0.00.033.585 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.586 I print_info: file format = GGUF V3 (latest)
0.00.033.586 I print_info: file type   = Q4_1
0.00.033.587 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.042.383 I load: special tokens cache size = 25
0.00.048.764 I load: token to piece cache size = 0.2984 MB
0.00.048.780 I print_info: arch             = gptneox
0.00.048.782 I print_info: vocab_only       = 0
0.00.048.782 I print_info: n_ctx_train      = 2048
0.00.048.782 I print_info: n_embd           = 2048
0.00.048.782 I print_info: n_layer          = 24
0.00.048.785 I print_info: n_head           = 16
0.00.048.786 I print_info: n_head_kv        = 16
0.00.048.786 I print_info: n_rot            = 32
0.00.048.786 I print_info: n_swa            = 0
0.00.048.786 I print_info: n_embd_head_k    = 128
0.00.048.786 I print_info: n_embd_head_v    = 128
0.00.048.787 I print_info: n_gqa            = 1
0.00.048.788 I print_info: n_embd_k_gqa     = 2048
0.00.048.788 I print_info: n_embd_v_gqa     = 2048
0.00.048.789 I print_info: f_norm_eps       = 1.0e-05
0.00.048.789 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.791 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.792 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.792 I print_info: f_logit_scale    = 0.0e+00
0.00.048.793 I print_info: n_ff             = 8192
0.00.048.793 I print_info: n_expert         = 0
0.00.048.793 I print_info: n_expert_used    = 0
0.00.048.793 I print_info: causal attn      = 1
0.00.048.793 I print_info: pooling type     = 0
0.00.048.794 I print_info: rope type        = 2
0.00.048.794 I print_info: rope scaling     = linear
0.00.048.794 I print_info: freq_base_train  = 10000.0
0.00.048.794 I print_info: freq_scale_train = 1
0.00.048.794 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.795 I print_info: rope_finetuned   = unknown
0.00.048.795 I print_info: ssm_d_conv       = 0
0.00.048.795 I print_info: ssm_d_inner      = 0
0.00.048.795 I print_info: ssm_d_state      = 0
0.00.048.795 I print_info: ssm_dt_rank      = 0
0.00.048.795 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.795 I print_info: model type       = 1.4B
0.00.048.796 I print_info: model params     = 1.41 B
0.00.048.796 I print_info: general.name     = 1.4B
0.00.048.796 I print_info: vocab type       = BPE
0.00.048.796 I print_info: n_vocab          = 50304
0.00.048.797 I print_info: n_merges         = 50009
0.00.048.797 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.797 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.797 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.797 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.798 I print_info: LF token         = 187 ''
0.00.048.798 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.798 I print_info: max token length = 1024
0.00.048.798 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.808.344 I load_tensors: offloading 24 repeating layers to GPU
0.00.808.356 I load_tensors: offloading output layer to GPU
0.00.808.357 I load_tensors: offloaded 25/25 layers to GPU
0.00.808.387 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.808.392 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.809.719 I llama_init_from_model: n_seq_max     = 1
0.00.809.721 I llama_init_from_model: n_ctx         = 2048
0.00.809.721 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.809.721 I llama_init_from_model: n_batch       = 2048
0.00.809.722 I llama_init_from_model: n_ubatch      = 512
0.00.809.722 I llama_init_from_model: flash_attn    = 0
0.00.809.723 I llama_init_from_model: freq_base     = 10000.0
0.00.809.724 I llama_init_from_model: freq_scale    = 1
0.00.809.725 I ggml_metal_init: allocating
0.00.809.754 I ggml_metal_init: found device: Apple M4
0.00.809.765 I ggml_metal_init: picking default device: Apple M4
0.00.811.358 I ggml_metal_init: using embedded metal library
0.00.817.892 I ggml_metal_init: GPU name:   Apple M4
0.00.817.896 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.817.896 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.817.897 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.817.898 I ggml_metal_init: simdgroup reduction   = true
0.00.817.898 I ggml_metal_init: simdgroup matrix mul. = true
0.00.817.898 I ggml_metal_init: has residency sets    = true
0.00.817.899 I ggml_metal_init: has bfloat            = true
0.00.817.899 I ggml_metal_init: use bfloat            = true
0.00.817.900 I ggml_metal_init: hasUnifiedMemory      = true
0.00.817.901 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.835.831 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.889.765 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.889.770 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.889.792 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.894.837 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.894.839 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.894.839 I llama_init_from_model: graph nodes  = 967
0.00.894.840 I llama_init_from_model: graph splits = 2
0.00.894.845 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.894.977 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.894.978 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.940.285 I main: llama threadpool init, n_threads = 4
0.00.940.336 I 
0.00.940.356 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.940.357 I 
0.00.940.499 I sampler seed: 1234
0.00.940.504 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.940.527 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.940.527 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.940.527 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.675.566 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57304.28 tokens per second)
0.01.675.567 I llama_perf_context_print:        load time =     930.80 ms
0.01.675.568 I llama_perf_context_print: prompt eval time =      49.50 ms /     7 tokens (    7.07 ms per token,   141.42 tokens per second)
0.01.675.569 I llama_perf_context_print:        eval time =     682.75 ms /    63 runs   (   10.84 ms per token,    92.27 tokens per second)
0.01.675.571 I llama_perf_context_print:       total time =     736.03 ms /    70 tokens
0.01.675.832 I ggml_metal_free: deallocating

real	0m1.692s
user	0m0.110s
sys	0m0.240s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.403 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.726 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.731 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.735 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.735 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.736 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.736 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.736 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.737 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.738 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.738 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.738 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.739 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.739 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.739 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.742 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.742 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.742 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.448 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.476 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.337 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.339 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.339 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.340 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.340 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.340 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.341 I llama_model_loader: - type  f32:  194 tensors
0.00.026.341 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.342 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.343 I print_info: file format = GGUF V3 (latest)
0.00.026.343 I print_info: file type   = Q4_1
0.00.026.346 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.703 I load: special tokens cache size = 25
0.00.041.115 I load: token to piece cache size = 0.2984 MB
0.00.041.132 I print_info: arch             = gptneox
0.00.041.133 I print_info: vocab_only       = 0
0.00.041.134 I print_info: n_ctx_train      = 2048
0.00.041.134 I print_info: n_embd           = 2048
0.00.041.134 I print_info: n_layer          = 24
0.00.041.140 I print_info: n_head           = 16
0.00.041.140 I print_info: n_head_kv        = 16
0.00.041.140 I print_info: n_rot            = 32
0.00.041.140 I print_info: n_swa            = 0
0.00.041.141 I print_info: n_embd_head_k    = 128
0.00.041.141 I print_info: n_embd_head_v    = 128
0.00.041.141 I print_info: n_gqa            = 1
0.00.041.142 I print_info: n_embd_k_gqa     = 2048
0.00.041.143 I print_info: n_embd_v_gqa     = 2048
0.00.041.143 I print_info: f_norm_eps       = 1.0e-05
0.00.041.144 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.148 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.149 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.149 I print_info: f_logit_scale    = 0.0e+00
0.00.041.149 I print_info: n_ff             = 8192
0.00.041.149 I print_info: n_expert         = 0
0.00.041.150 I print_info: n_expert_used    = 0
0.00.041.150 I print_info: causal attn      = 1
0.00.041.150 I print_info: pooling type     = 0
0.00.041.150 I print_info: rope type        = 2
0.00.041.150 I print_info: rope scaling     = linear
0.00.041.150 I print_info: freq_base_train  = 10000.0
0.00.041.155 I print_info: freq_scale_train = 1
0.00.041.156 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.156 I print_info: rope_finetuned   = unknown
0.00.041.156 I print_info: ssm_d_conv       = 0
0.00.041.158 I print_info: ssm_d_inner      = 0
0.00.041.158 I print_info: ssm_d_state      = 0
0.00.041.158 I print_info: ssm_dt_rank      = 0
0.00.041.158 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.159 I print_info: model type       = 1.4B
0.00.041.159 I print_info: model params     = 1.41 B
0.00.041.159 I print_info: general.name     = 1.4B
0.00.041.160 I print_info: vocab type       = BPE
0.00.041.160 I print_info: n_vocab          = 50304
0.00.041.160 I print_info: n_merges         = 50009
0.00.041.160 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.160 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.161 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.161 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.161 I print_info: LF token         = 187 ''
0.00.041.161 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.161 I print_info: max token length = 1024
0.00.041.162 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.687.544 I load_tensors: offloading 24 repeating layers to GPU
0.00.687.552 I load_tensors: offloading output layer to GPU
0.00.687.553 I load_tensors: offloaded 25/25 layers to GPU
0.00.687.581 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.687.585 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.688.516 I llama_init_from_model: n_seq_max     = 1
0.00.688.518 I llama_init_from_model: n_ctx         = 128
0.00.688.519 I llama_init_from_model: n_ctx_per_seq = 128
0.00.688.519 I llama_init_from_model: n_batch       = 128
0.00.688.519 I llama_init_from_model: n_ubatch      = 128
0.00.688.520 I llama_init_from_model: flash_attn    = 0
0.00.688.521 I llama_init_from_model: freq_base     = 10000.0
0.00.688.522 I llama_init_from_model: freq_scale    = 1
0.00.688.522 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.688.524 I ggml_metal_init: allocating
0.00.688.593 I ggml_metal_init: found device: Apple M4
0.00.688.607 I ggml_metal_init: picking default device: Apple M4
0.00.690.289 I ggml_metal_init: using embedded metal library
0.00.696.524 I ggml_metal_init: GPU name:   Apple M4
0.00.696.528 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.696.529 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.696.529 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.696.530 I ggml_metal_init: simdgroup reduction   = true
0.00.696.530 I ggml_metal_init: simdgroup matrix mul. = true
0.00.696.531 I ggml_metal_init: has residency sets    = true
0.00.696.531 I ggml_metal_init: has bfloat            = true
0.00.696.531 I ggml_metal_init: use bfloat            = true
0.00.696.533 I ggml_metal_init: hasUnifiedMemory      = true
0.00.696.535 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.714.749 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.718.114 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.718.121 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.718.152 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.721.215 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.721.217 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.721.218 I llama_init_from_model: graph nodes  = 967
0.00.721.218 I llama_init_from_model: graph splits = 2
0.00.721.221 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.721.221 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.746.227 I 
0.00.746.307 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.746.313 I perplexity: tokenizing the input ..
0.00.753.660 I perplexity: tokenization took 7.343 ms
0.00.753.673 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.889.546 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.890.804 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.890.828 I llama_perf_context_print:        load time =     735.82 ms
0.00.890.829 I llama_perf_context_print: prompt eval time =     134.92 ms /   128 tokens (    1.05 ms per token,   948.68 tokens per second)
0.00.890.829 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.890.830 I llama_perf_context_print:       total time =     144.60 ms /   129 tokens
0.00.891.192 I ggml_metal_free: deallocating

real	0m0.904s
user	0m0.081s
sys	0m0.164s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.425 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.780 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.029.785 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.791 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.791 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.793 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.793 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.793 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.794 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.794 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.795 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.795 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.797 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.798 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.798 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.799 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.800 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.800 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.643 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.738 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.090 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.092 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.092 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.093 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.093 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.093 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.039.094 I llama_model_loader: - type  f32:  194 tensors
0.00.039.094 I llama_model_loader: - type q5_0:   97 tensors
0.00.039.094 I llama_model_loader: - type q6_K:    1 tensors
0.00.039.095 I print_info: file format = GGUF V3 (latest)
0.00.039.095 I print_info: file type   = Q5_0
0.00.039.096 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.048.623 I load: special tokens cache size = 25
0.00.056.767 I load: token to piece cache size = 0.2984 MB
0.00.056.784 I print_info: arch             = gptneox
0.00.056.785 I print_info: vocab_only       = 0
0.00.056.786 I print_info: n_ctx_train      = 2048
0.00.056.786 I print_info: n_embd           = 2048
0.00.056.786 I print_info: n_layer          = 24
0.00.056.789 I print_info: n_head           = 16
0.00.056.790 I print_info: n_head_kv        = 16
0.00.056.790 I print_info: n_rot            = 32
0.00.056.790 I print_info: n_swa            = 0
0.00.056.790 I print_info: n_embd_head_k    = 128
0.00.056.790 I print_info: n_embd_head_v    = 128
0.00.056.791 I print_info: n_gqa            = 1
0.00.056.792 I print_info: n_embd_k_gqa     = 2048
0.00.056.792 I print_info: n_embd_v_gqa     = 2048
0.00.056.793 I print_info: f_norm_eps       = 1.0e-05
0.00.056.793 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.793 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.794 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.794 I print_info: f_logit_scale    = 0.0e+00
0.00.056.794 I print_info: n_ff             = 8192
0.00.056.795 I print_info: n_expert         = 0
0.00.056.795 I print_info: n_expert_used    = 0
0.00.056.795 I print_info: causal attn      = 1
0.00.056.795 I print_info: pooling type     = 0
0.00.056.795 I print_info: rope type        = 2
0.00.056.795 I print_info: rope scaling     = linear
0.00.056.798 I print_info: freq_base_train  = 10000.0
0.00.056.798 I print_info: freq_scale_train = 1
0.00.056.798 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.799 I print_info: rope_finetuned   = unknown
0.00.056.799 I print_info: ssm_d_conv       = 0
0.00.056.799 I print_info: ssm_d_inner      = 0
0.00.056.799 I print_info: ssm_d_state      = 0
0.00.056.799 I print_info: ssm_dt_rank      = 0
0.00.056.799 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.799 I print_info: model type       = 1.4B
0.00.056.800 I print_info: model params     = 1.41 B
0.00.056.800 I print_info: general.name     = 1.4B
0.00.056.801 I print_info: vocab type       = BPE
0.00.056.801 I print_info: n_vocab          = 50304
0.00.056.801 I print_info: n_merges         = 50009
0.00.056.801 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.801 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.802 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.802 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.802 I print_info: LF token         = 187 ''
0.00.056.802 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.802 I print_info: max token length = 1024
0.00.056.803 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.783.095 I load_tensors: offloading 24 repeating layers to GPU
0.00.783.099 I load_tensors: offloading output layer to GPU
0.00.783.101 I load_tensors: offloaded 25/25 layers to GPU
0.00.783.122 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.783.123 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.784.099 I llama_init_from_model: n_seq_max     = 1
0.00.784.101 I llama_init_from_model: n_ctx         = 2048
0.00.784.101 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.784.102 I llama_init_from_model: n_batch       = 2048
0.00.784.103 I llama_init_from_model: n_ubatch      = 512
0.00.784.103 I llama_init_from_model: flash_attn    = 0
0.00.784.104 I llama_init_from_model: freq_base     = 10000.0
0.00.784.104 I llama_init_from_model: freq_scale    = 1
0.00.784.106 I ggml_metal_init: allocating
0.00.784.114 I ggml_metal_init: found device: Apple M4
0.00.784.121 I ggml_metal_init: picking default device: Apple M4
0.00.785.525 I ggml_metal_init: using embedded metal library
0.00.791.383 I ggml_metal_init: GPU name:   Apple M4
0.00.791.387 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.791.387 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.791.388 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.791.389 I ggml_metal_init: simdgroup reduction   = true
0.00.791.389 I ggml_metal_init: simdgroup matrix mul. = true
0.00.791.389 I ggml_metal_init: has residency sets    = true
0.00.791.390 I ggml_metal_init: has bfloat            = true
0.00.791.390 I ggml_metal_init: use bfloat            = true
0.00.791.390 I ggml_metal_init: hasUnifiedMemory      = true
0.00.791.391 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.807.527 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.862.024 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.862.030 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.862.053 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.867.366 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.867.368 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.867.369 I llama_init_from_model: graph nodes  = 967
0.00.867.369 I llama_init_from_model: graph splits = 2
0.00.867.375 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.867.508 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.867.509 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.914.229 I main: llama threadpool init, n_threads = 4
0.00.914.282 I 
0.00.914.303 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.914.303 I 
0.00.914.419 I sampler seed: 1234
0.00.914.424 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.914.437 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.914.439 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.914.439 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.692.546 I llama_perf_sampler_print:    sampling time =       1.53 ms /    71 runs   (    0.02 ms per token, 46284.22 tokens per second)
0.01.692.547 I llama_perf_context_print:        load time =     903.06 ms
0.01.692.547 I llama_perf_context_print: prompt eval time =      42.71 ms /     7 tokens (    6.10 ms per token,   163.91 tokens per second)
0.01.692.548 I llama_perf_context_print:        eval time =     732.63 ms /    63 runs   (   11.63 ms per token,    85.99 tokens per second)
0.01.692.554 I llama_perf_context_print:       total time =     779.06 ms /    70 tokens
0.01.692.848 I ggml_metal_free: deallocating

real	0m1.715s
user	0m0.112s
sys	0m0.252s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.696 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.931 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.937 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.939 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.940 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.940 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.941 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.941 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.942 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.942 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.945 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.945 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.946 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.946 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.946 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.949 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.949 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.949 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.764 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.824 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.706 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.707 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.708 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.708 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.708 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.708 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.709 I llama_model_loader: - type  f32:  194 tensors
0.00.026.709 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.710 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.710 I print_info: file format = GGUF V3 (latest)
0.00.026.711 I print_info: file type   = Q5_0
0.00.026.712 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.208 I load: special tokens cache size = 25
0.00.041.467 I load: token to piece cache size = 0.2984 MB
0.00.041.484 I print_info: arch             = gptneox
0.00.041.485 I print_info: vocab_only       = 0
0.00.041.485 I print_info: n_ctx_train      = 2048
0.00.041.485 I print_info: n_embd           = 2048
0.00.041.486 I print_info: n_layer          = 24
0.00.041.490 I print_info: n_head           = 16
0.00.041.491 I print_info: n_head_kv        = 16
0.00.041.491 I print_info: n_rot            = 32
0.00.041.491 I print_info: n_swa            = 0
0.00.041.491 I print_info: n_embd_head_k    = 128
0.00.041.491 I print_info: n_embd_head_v    = 128
0.00.041.492 I print_info: n_gqa            = 1
0.00.041.492 I print_info: n_embd_k_gqa     = 2048
0.00.041.493 I print_info: n_embd_v_gqa     = 2048
0.00.041.493 I print_info: f_norm_eps       = 1.0e-05
0.00.041.494 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.494 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.494 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.496 I print_info: f_logit_scale    = 0.0e+00
0.00.041.497 I print_info: n_ff             = 8192
0.00.041.497 I print_info: n_expert         = 0
0.00.041.497 I print_info: n_expert_used    = 0
0.00.041.497 I print_info: causal attn      = 1
0.00.041.497 I print_info: pooling type     = 0
0.00.041.497 I print_info: rope type        = 2
0.00.041.501 I print_info: rope scaling     = linear
0.00.041.501 I print_info: freq_base_train  = 10000.0
0.00.041.501 I print_info: freq_scale_train = 1
0.00.041.501 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.502 I print_info: rope_finetuned   = unknown
0.00.041.502 I print_info: ssm_d_conv       = 0
0.00.041.502 I print_info: ssm_d_inner      = 0
0.00.041.502 I print_info: ssm_d_state      = 0
0.00.041.502 I print_info: ssm_dt_rank      = 0
0.00.041.502 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.502 I print_info: model type       = 1.4B
0.00.041.503 I print_info: model params     = 1.41 B
0.00.041.503 I print_info: general.name     = 1.4B
0.00.041.503 I print_info: vocab type       = BPE
0.00.041.504 I print_info: n_vocab          = 50304
0.00.041.504 I print_info: n_merges         = 50009
0.00.041.504 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.504 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.504 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.505 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.505 I print_info: LF token         = 187 ''
0.00.041.505 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.505 I print_info: max token length = 1024
0.00.041.506 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.734.007 I load_tensors: offloading 24 repeating layers to GPU
0.00.734.014 I load_tensors: offloading output layer to GPU
0.00.734.015 I load_tensors: offloaded 25/25 layers to GPU
0.00.734.041 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.734.042 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.735.232 I llama_init_from_model: n_seq_max     = 1
0.00.735.238 I llama_init_from_model: n_ctx         = 128
0.00.735.238 I llama_init_from_model: n_ctx_per_seq = 128
0.00.735.238 I llama_init_from_model: n_batch       = 128
0.00.735.238 I llama_init_from_model: n_ubatch      = 128
0.00.735.239 I llama_init_from_model: flash_attn    = 0
0.00.735.240 I llama_init_from_model: freq_base     = 10000.0
0.00.735.241 I llama_init_from_model: freq_scale    = 1
0.00.735.241 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.735.243 I ggml_metal_init: allocating
0.00.735.296 I ggml_metal_init: found device: Apple M4
0.00.735.309 I ggml_metal_init: picking default device: Apple M4
0.00.736.981 I ggml_metal_init: using embedded metal library
0.00.743.090 I ggml_metal_init: GPU name:   Apple M4
0.00.743.094 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.743.095 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.743.095 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.743.096 I ggml_metal_init: simdgroup reduction   = true
0.00.743.096 I ggml_metal_init: simdgroup matrix mul. = true
0.00.743.096 I ggml_metal_init: has residency sets    = true
0.00.743.096 I ggml_metal_init: has bfloat            = true
0.00.743.097 I ggml_metal_init: use bfloat            = true
0.00.743.098 I ggml_metal_init: hasUnifiedMemory      = true
0.00.743.101 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.759.892 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.763.421 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.763.425 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.763.450 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.766.350 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.766.352 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.766.353 I llama_init_from_model: graph nodes  = 967
0.00.766.353 I llama_init_from_model: graph splits = 2
0.00.766.356 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.766.356 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.792.792 I 
0.00.792.874 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.792.883 I perplexity: tokenizing the input ..
0.00.799.111 I perplexity: tokenization took 6.226 ms
0.00.799.119 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.933.080 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.934.374 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.934.408 I llama_perf_context_print:        load time =     782.09 ms
0.00.934.409 I llama_perf_context_print: prompt eval time =     133.74 ms /   128 tokens (    1.04 ms per token,   957.10 tokens per second)
0.00.934.410 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.934.410 I llama_perf_context_print:       total time =     141.62 ms /   129 tokens
0.00.934.810 I ggml_metal_free: deallocating

real	0m0.952s
user	0m0.078s
sys	0m0.181s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.102 I main: llama backend init
0.00.000.104 I main: load the model and apply lora adapter, if any
0.00.012.253 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.195 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.032.200 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.201 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.201 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.205 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.206 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.206 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.207 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.207 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.207 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.208 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.208 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.208 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.209 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.210 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.210 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.211 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.186 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.272 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.113 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.041.115 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.115 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.115 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.116 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.116 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.041.116 I llama_model_loader: - type  f32:  194 tensors
0.00.041.117 I llama_model_loader: - type q5_1:   97 tensors
0.00.041.117 I llama_model_loader: - type q6_K:    1 tensors
0.00.041.117 I print_info: file format = GGUF V3 (latest)
0.00.041.118 I print_info: file type   = Q5_1
0.00.041.120 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.049.565 I load: special tokens cache size = 25
0.00.057.002 I load: token to piece cache size = 0.2984 MB
0.00.057.016 I print_info: arch             = gptneox
0.00.057.017 I print_info: vocab_only       = 0
0.00.057.018 I print_info: n_ctx_train      = 2048
0.00.057.018 I print_info: n_embd           = 2048
0.00.057.018 I print_info: n_layer          = 24
0.00.057.021 I print_info: n_head           = 16
0.00.057.021 I print_info: n_head_kv        = 16
0.00.057.022 I print_info: n_rot            = 32
0.00.057.022 I print_info: n_swa            = 0
0.00.057.022 I print_info: n_embd_head_k    = 128
0.00.057.024 I print_info: n_embd_head_v    = 128
0.00.057.025 I print_info: n_gqa            = 1
0.00.057.025 I print_info: n_embd_k_gqa     = 2048
0.00.057.027 I print_info: n_embd_v_gqa     = 2048
0.00.057.028 I print_info: f_norm_eps       = 1.0e-05
0.00.057.028 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.028 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.028 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.029 I print_info: f_logit_scale    = 0.0e+00
0.00.057.029 I print_info: n_ff             = 8192
0.00.057.030 I print_info: n_expert         = 0
0.00.057.030 I print_info: n_expert_used    = 0
0.00.057.030 I print_info: causal attn      = 1
0.00.057.030 I print_info: pooling type     = 0
0.00.057.030 I print_info: rope type        = 2
0.00.057.030 I print_info: rope scaling     = linear
0.00.057.031 I print_info: freq_base_train  = 10000.0
0.00.057.031 I print_info: freq_scale_train = 1
0.00.057.031 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.032 I print_info: rope_finetuned   = unknown
0.00.057.032 I print_info: ssm_d_conv       = 0
0.00.057.032 I print_info: ssm_d_inner      = 0
0.00.057.033 I print_info: ssm_d_state      = 0
0.00.057.033 I print_info: ssm_dt_rank      = 0
0.00.057.033 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.033 I print_info: model type       = 1.4B
0.00.057.034 I print_info: model params     = 1.41 B
0.00.057.034 I print_info: general.name     = 1.4B
0.00.057.035 I print_info: vocab type       = BPE
0.00.057.035 I print_info: n_vocab          = 50304
0.00.057.035 I print_info: n_merges         = 50009
0.00.057.035 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.035 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.035 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.036 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.036 I print_info: LF token         = 187 ''
0.00.057.036 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.036 I print_info: max token length = 1024
0.00.057.037 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.878.156 I load_tensors: offloading 24 repeating layers to GPU
0.00.878.163 I load_tensors: offloading output layer to GPU
0.00.878.164 I load_tensors: offloaded 25/25 layers to GPU
0.00.878.203 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.878.206 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.878.736 I llama_init_from_model: n_seq_max     = 1
0.00.878.737 I llama_init_from_model: n_ctx         = 2048
0.00.878.737 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.878.737 I llama_init_from_model: n_batch       = 2048
0.00.878.738 I llama_init_from_model: n_ubatch      = 512
0.00.878.738 I llama_init_from_model: flash_attn    = 0
0.00.878.739 I llama_init_from_model: freq_base     = 10000.0
0.00.878.739 I llama_init_from_model: freq_scale    = 1
0.00.878.740 I ggml_metal_init: allocating
0.00.878.765 I ggml_metal_init: found device: Apple M4
0.00.878.771 I ggml_metal_init: picking default device: Apple M4
0.00.879.806 I ggml_metal_init: using embedded metal library
0.00.883.500 I ggml_metal_init: GPU name:   Apple M4
0.00.883.502 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.883.503 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.883.503 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.883.504 I ggml_metal_init: simdgroup reduction   = true
0.00.883.504 I ggml_metal_init: simdgroup matrix mul. = true
0.00.883.504 I ggml_metal_init: has residency sets    = true
0.00.883.504 I ggml_metal_init: has bfloat            = true
0.00.883.504 I ggml_metal_init: use bfloat            = true
0.00.883.505 I ggml_metal_init: hasUnifiedMemory      = true
0.00.883.506 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.894.281 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.927.636 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.927.643 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.927.666 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.932.902 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.932.904 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.932.905 I llama_init_from_model: graph nodes  = 967
0.00.932.905 I llama_init_from_model: graph splits = 2
0.00.932.911 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.933.040 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.933.041 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.983.591 I main: llama threadpool init, n_threads = 4
0.00.983.647 I 
0.00.983.666 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.983.668 I 
0.00.983.802 I sampler seed: 1234
0.00.983.806 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.983.839 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.983.840 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.983.841 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.829.887 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51598.84 tokens per second)
0.01.829.888 I llama_perf_context_print:        load time =     970.60 ms
0.01.829.889 I llama_perf_context_print: prompt eval time =      52.54 ms /     7 tokens (    7.51 ms per token,   133.23 tokens per second)
0.01.829.889 I llama_perf_context_print:        eval time =     790.55 ms /    63 runs   (   12.55 ms per token,    79.69 tokens per second)
0.01.829.890 I llama_perf_context_print:       total time =     847.03 ms /    70 tokens
0.01.830.112 I ggml_metal_free: deallocating

real	0m1.846s
user	0m0.103s
sys	0m0.247s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.930 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.046 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.052 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.054 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.054 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.055 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.055 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.055 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.056 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.056 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.057 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.060 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.060 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.060 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.061 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.065 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.066 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.066 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.857 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.883 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.663 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.665 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.665 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.666 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.666 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.666 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.667 I llama_model_loader: - type  f32:  194 tensors
0.00.025.667 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.667 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.668 I print_info: file format = GGUF V3 (latest)
0.00.025.669 I print_info: file type   = Q5_1
0.00.025.670 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.216 I load: special tokens cache size = 25
0.00.040.575 I load: token to piece cache size = 0.2984 MB
0.00.040.593 I print_info: arch             = gptneox
0.00.040.594 I print_info: vocab_only       = 0
0.00.040.594 I print_info: n_ctx_train      = 2048
0.00.040.595 I print_info: n_embd           = 2048
0.00.040.595 I print_info: n_layer          = 24
0.00.040.599 I print_info: n_head           = 16
0.00.040.600 I print_info: n_head_kv        = 16
0.00.040.600 I print_info: n_rot            = 32
0.00.040.600 I print_info: n_swa            = 0
0.00.040.600 I print_info: n_embd_head_k    = 128
0.00.040.600 I print_info: n_embd_head_v    = 128
0.00.040.601 I print_info: n_gqa            = 1
0.00.040.601 I print_info: n_embd_k_gqa     = 2048
0.00.040.602 I print_info: n_embd_v_gqa     = 2048
0.00.040.603 I print_info: f_norm_eps       = 1.0e-05
0.00.040.603 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.603 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.603 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.603 I print_info: f_logit_scale    = 0.0e+00
0.00.040.604 I print_info: n_ff             = 8192
0.00.040.604 I print_info: n_expert         = 0
0.00.040.604 I print_info: n_expert_used    = 0
0.00.040.604 I print_info: causal attn      = 1
0.00.040.606 I print_info: pooling type     = 0
0.00.040.606 I print_info: rope type        = 2
0.00.040.606 I print_info: rope scaling     = linear
0.00.040.612 I print_info: freq_base_train  = 10000.0
0.00.040.612 I print_info: freq_scale_train = 1
0.00.040.612 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.612 I print_info: rope_finetuned   = unknown
0.00.040.612 I print_info: ssm_d_conv       = 0
0.00.040.613 I print_info: ssm_d_inner      = 0
0.00.040.613 I print_info: ssm_d_state      = 0
0.00.040.613 I print_info: ssm_dt_rank      = 0
0.00.040.614 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.614 I print_info: model type       = 1.4B
0.00.040.614 I print_info: model params     = 1.41 B
0.00.040.614 I print_info: general.name     = 1.4B
0.00.040.615 I print_info: vocab type       = BPE
0.00.040.615 I print_info: n_vocab          = 50304
0.00.040.615 I print_info: n_merges         = 50009
0.00.040.616 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.616 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.616 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.617 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.617 I print_info: LF token         = 187 ''
0.00.040.617 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.617 I print_info: max token length = 1024
0.00.040.618 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.682.586 I load_tensors: offloading 24 repeating layers to GPU
0.00.682.595 I load_tensors: offloading output layer to GPU
0.00.682.596 I load_tensors: offloaded 25/25 layers to GPU
0.00.682.623 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.682.626 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.683.791 I llama_init_from_model: n_seq_max     = 1
0.00.683.793 I llama_init_from_model: n_ctx         = 128
0.00.683.794 I llama_init_from_model: n_ctx_per_seq = 128
0.00.683.794 I llama_init_from_model: n_batch       = 128
0.00.683.794 I llama_init_from_model: n_ubatch      = 128
0.00.683.795 I llama_init_from_model: flash_attn    = 0
0.00.683.796 I llama_init_from_model: freq_base     = 10000.0
0.00.683.797 I llama_init_from_model: freq_scale    = 1
0.00.683.797 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.683.799 I ggml_metal_init: allocating
0.00.683.855 I ggml_metal_init: found device: Apple M4
0.00.683.866 I ggml_metal_init: picking default device: Apple M4
0.00.685.420 I ggml_metal_init: using embedded metal library
0.00.691.283 I ggml_metal_init: GPU name:   Apple M4
0.00.691.286 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.691.287 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.691.288 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.691.288 I ggml_metal_init: simdgroup reduction   = true
0.00.691.288 I ggml_metal_init: simdgroup matrix mul. = true
0.00.691.289 I ggml_metal_init: has residency sets    = true
0.00.691.289 I ggml_metal_init: has bfloat            = true
0.00.691.289 I ggml_metal_init: use bfloat            = true
0.00.691.290 I ggml_metal_init: hasUnifiedMemory      = true
0.00.691.292 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.708.041 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.711.283 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.711.287 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.711.313 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.714.325 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.714.327 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.714.327 I llama_init_from_model: graph nodes  = 967
0.00.714.328 I llama_init_from_model: graph splits = 2
0.00.714.331 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.714.331 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.503 I 
0.00.741.591 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.597 I perplexity: tokenizing the input ..
0.00.747.228 I perplexity: tokenization took 5.629 ms
0.00.747.233 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.881.286 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.882.546 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.882.582 I llama_perf_context_print:        load time =     731.57 ms
0.00.882.584 I llama_perf_context_print: prompt eval time =     133.82 ms /   128 tokens (    1.05 ms per token,   956.49 tokens per second)
0.00.882.584 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.882.587 I llama_perf_context_print:       total time =     141.08 ms /   129 tokens
0.00.883.052 I ggml_metal_free: deallocating

real	0m0.898s
user	0m0.077s
sys	0m0.193s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.884 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.529 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.535 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.537 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.537 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.538 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.538 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.538 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.540 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.541 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.541 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.541 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.543 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.544 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.544 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.546 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.546 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.546 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.277 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.228 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.911 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.913 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.913 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.913 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.914 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.914 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.914 I llama_model_loader: - type  f32:  194 tensors
0.00.024.915 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.915 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.915 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.916 I print_info: file format = GGUF V3 (latest)
0.00.024.916 I print_info: file type   = Q2_K - Medium
0.00.024.917 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.886 I load: special tokens cache size = 25
0.00.038.917 I load: token to piece cache size = 0.2984 MB
0.00.038.932 I print_info: arch             = gptneox
0.00.038.933 I print_info: vocab_only       = 0
0.00.038.933 I print_info: n_ctx_train      = 2048
0.00.038.934 I print_info: n_embd           = 2048
0.00.038.934 I print_info: n_layer          = 24
0.00.038.936 I print_info: n_head           = 16
0.00.038.937 I print_info: n_head_kv        = 16
0.00.038.937 I print_info: n_rot            = 32
0.00.038.937 I print_info: n_swa            = 0
0.00.038.937 I print_info: n_embd_head_k    = 128
0.00.038.938 I print_info: n_embd_head_v    = 128
0.00.038.938 I print_info: n_gqa            = 1
0.00.038.939 I print_info: n_embd_k_gqa     = 2048
0.00.038.940 I print_info: n_embd_v_gqa     = 2048
0.00.038.940 I print_info: f_norm_eps       = 1.0e-05
0.00.038.941 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.941 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.941 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.941 I print_info: f_logit_scale    = 0.0e+00
0.00.038.942 I print_info: n_ff             = 8192
0.00.038.942 I print_info: n_expert         = 0
0.00.038.942 I print_info: n_expert_used    = 0
0.00.038.942 I print_info: causal attn      = 1
0.00.038.950 I print_info: pooling type     = 0
0.00.038.951 I print_info: rope type        = 2
0.00.038.952 I print_info: rope scaling     = linear
0.00.038.953 I print_info: freq_base_train  = 10000.0
0.00.038.953 I print_info: freq_scale_train = 1
0.00.038.953 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.953 I print_info: rope_finetuned   = unknown
0.00.038.953 I print_info: ssm_d_conv       = 0
0.00.038.955 I print_info: ssm_d_inner      = 0
0.00.038.955 I print_info: ssm_d_state      = 0
0.00.038.955 I print_info: ssm_dt_rank      = 0
0.00.038.955 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.955 I print_info: model type       = 1.4B
0.00.038.955 I print_info: model params     = 1.41 B
0.00.038.956 I print_info: general.name     = 1.4B
0.00.038.957 I print_info: vocab type       = BPE
0.00.038.957 I print_info: n_vocab          = 50304
0.00.038.957 I print_info: n_merges         = 50009
0.00.038.958 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.958 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.958 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.958 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.958 I print_info: LF token         = 187 ''
0.00.038.959 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.959 I print_info: max token length = 1024
0.00.038.959 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.375.299 I load_tensors: offloading 24 repeating layers to GPU
0.00.375.311 I load_tensors: offloading output layer to GPU
0.00.375.312 I load_tensors: offloaded 25/25 layers to GPU
0.00.375.343 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.375.344 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.376.758 I llama_init_from_model: n_seq_max     = 1
0.00.376.761 I llama_init_from_model: n_ctx         = 2048
0.00.376.761 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.376.762 I llama_init_from_model: n_batch       = 2048
0.00.376.762 I llama_init_from_model: n_ubatch      = 512
0.00.376.763 I llama_init_from_model: flash_attn    = 0
0.00.376.765 I llama_init_from_model: freq_base     = 10000.0
0.00.376.765 I llama_init_from_model: freq_scale    = 1
0.00.376.768 I ggml_metal_init: allocating
0.00.376.841 I ggml_metal_init: found device: Apple M4
0.00.376.856 I ggml_metal_init: picking default device: Apple M4
0.00.378.817 I ggml_metal_init: using embedded metal library
0.00.384.716 I ggml_metal_init: GPU name:   Apple M4
0.00.384.724 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.384.725 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.384.726 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.384.727 I ggml_metal_init: simdgroup reduction   = true
0.00.384.727 I ggml_metal_init: simdgroup matrix mul. = true
0.00.384.727 I ggml_metal_init: has residency sets    = true
0.00.384.728 I ggml_metal_init: has bfloat            = true
0.00.384.728 I ggml_metal_init: use bfloat            = true
0.00.384.732 I ggml_metal_init: hasUnifiedMemory      = true
0.00.384.736 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.405.144 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.462.752 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.462.761 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.462.788 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.467.888 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.467.890 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.467.891 I llama_init_from_model: graph nodes  = 967
0.00.467.891 I llama_init_from_model: graph splits = 2
0.00.467.895 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.468.033 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.468.036 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.518.338 I main: llama threadpool init, n_threads = 4
0.00.518.386 I 
0.00.518.446 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.518.454 I 
0.00.518.640 I sampler seed: 1234
0.00.518.646 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.518.671 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.518.672 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.518.674 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.212.054 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54489.64 tokens per second)
0.01.212.056 I llama_perf_context_print:        load time =     507.65 ms
0.01.212.057 I llama_perf_context_print: prompt eval time =      44.99 ms /     7 tokens (    6.43 ms per token,   155.59 tokens per second)
0.01.212.058 I llama_perf_context_print:        eval time =     645.63 ms /    63 runs   (   10.25 ms per token,    97.58 tokens per second)
0.01.212.058 I llama_perf_context_print:       total time =     694.52 ms /    70 tokens
0.01.212.303 I ggml_metal_free: deallocating

real	0m1.229s
user	0m0.111s
sys	0m0.180s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.387 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.289 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.296 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.298 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.298 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.299 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.299 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.300 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.300 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.301 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.301 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.301 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.302 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.302 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.303 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.305 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.305 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.305 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.172 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.162 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.901 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.903 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.903 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.904 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.904 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.904 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.905 I llama_model_loader: - type  f32:  194 tensors
0.00.025.905 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.905 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.906 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.906 I print_info: file format = GGUF V3 (latest)
0.00.025.907 I print_info: file type   = Q2_K - Medium
0.00.025.908 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.488 I load: special tokens cache size = 25
0.00.040.775 I load: token to piece cache size = 0.2984 MB
0.00.040.792 I print_info: arch             = gptneox
0.00.040.793 I print_info: vocab_only       = 0
0.00.040.793 I print_info: n_ctx_train      = 2048
0.00.040.793 I print_info: n_embd           = 2048
0.00.040.793 I print_info: n_layer          = 24
0.00.040.797 I print_info: n_head           = 16
0.00.040.797 I print_info: n_head_kv        = 16
0.00.040.798 I print_info: n_rot            = 32
0.00.040.798 I print_info: n_swa            = 0
0.00.040.798 I print_info: n_embd_head_k    = 128
0.00.040.798 I print_info: n_embd_head_v    = 128
0.00.040.799 I print_info: n_gqa            = 1
0.00.040.799 I print_info: n_embd_k_gqa     = 2048
0.00.040.800 I print_info: n_embd_v_gqa     = 2048
0.00.040.800 I print_info: f_norm_eps       = 1.0e-05
0.00.040.801 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.801 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.801 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.801 I print_info: f_logit_scale    = 0.0e+00
0.00.040.807 I print_info: n_ff             = 8192
0.00.040.807 I print_info: n_expert         = 0
0.00.040.807 I print_info: n_expert_used    = 0
0.00.040.808 I print_info: causal attn      = 1
0.00.040.808 I print_info: pooling type     = 0
0.00.040.808 I print_info: rope type        = 2
0.00.040.808 I print_info: rope scaling     = linear
0.00.040.808 I print_info: freq_base_train  = 10000.0
0.00.040.809 I print_info: freq_scale_train = 1
0.00.040.809 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.809 I print_info: rope_finetuned   = unknown
0.00.040.809 I print_info: ssm_d_conv       = 0
0.00.040.809 I print_info: ssm_d_inner      = 0
0.00.040.809 I print_info: ssm_d_state      = 0
0.00.040.810 I print_info: ssm_dt_rank      = 0
0.00.040.810 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.810 I print_info: model type       = 1.4B
0.00.040.810 I print_info: model params     = 1.41 B
0.00.040.810 I print_info: general.name     = 1.4B
0.00.040.811 I print_info: vocab type       = BPE
0.00.040.811 I print_info: n_vocab          = 50304
0.00.040.811 I print_info: n_merges         = 50009
0.00.040.811 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.811 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.812 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.812 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.812 I print_info: LF token         = 187 ''
0.00.040.813 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.814 I print_info: max token length = 1024
0.00.040.814 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.367.975 I load_tensors: offloading 24 repeating layers to GPU
0.00.367.989 I load_tensors: offloading output layer to GPU
0.00.367.990 I load_tensors: offloaded 25/25 layers to GPU
0.00.368.022 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.368.028 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.369.613 I llama_init_from_model: n_seq_max     = 1
0.00.369.618 I llama_init_from_model: n_ctx         = 128
0.00.369.619 I llama_init_from_model: n_ctx_per_seq = 128
0.00.369.619 I llama_init_from_model: n_batch       = 128
0.00.369.619 I llama_init_from_model: n_ubatch      = 128
0.00.369.620 I llama_init_from_model: flash_attn    = 0
0.00.369.621 I llama_init_from_model: freq_base     = 10000.0
0.00.369.622 I llama_init_from_model: freq_scale    = 1
0.00.369.622 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.369.633 I ggml_metal_init: allocating
0.00.369.712 I ggml_metal_init: found device: Apple M4
0.00.369.727 I ggml_metal_init: picking default device: Apple M4
0.00.371.515 I ggml_metal_init: using embedded metal library
0.00.376.973 I ggml_metal_init: GPU name:   Apple M4
0.00.376.983 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.376.984 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.376.985 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.376.985 I ggml_metal_init: simdgroup reduction   = true
0.00.376.985 I ggml_metal_init: simdgroup matrix mul. = true
0.00.376.986 I ggml_metal_init: has residency sets    = true
0.00.376.986 I ggml_metal_init: has bfloat            = true
0.00.376.986 I ggml_metal_init: use bfloat            = true
0.00.376.988 I ggml_metal_init: hasUnifiedMemory      = true
0.00.376.992 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.389.399 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.391.029 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.391.034 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.391.053 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.392.642 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.392.643 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.392.644 I llama_init_from_model: graph nodes  = 967
0.00.392.644 I llama_init_from_model: graph splits = 2
0.00.392.646 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.392.646 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.419.884 I 
0.00.419.923 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.419.926 I perplexity: tokenizing the input ..
0.00.423.699 I perplexity: tokenization took 3.771 ms
0.00.423.708 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.564.013 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.565.261 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.565.284 I llama_perf_context_print:        load time =     409.49 ms
0.00.565.284 I llama_perf_context_print: prompt eval time =     140.08 ms /   128 tokens (    1.09 ms per token,   913.78 tokens per second)
0.00.565.285 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.565.285 I llama_perf_context_print:       total time =     145.40 ms /   129 tokens
0.00.565.669 I ggml_metal_free: deallocating

real	0m0.582s
user	0m0.069s
sys	0m0.102s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.804 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.694 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.700 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.702 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.702 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.703 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.703 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.703 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.704 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.704 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.707 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.707 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.708 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.708 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.708 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.711 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.711 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.711 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.521 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.477 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.209 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.211 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.211 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.212 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.212 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.212 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.213 I llama_model_loader: - type  f32:  194 tensors
0.00.025.213 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.213 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.213 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.214 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.214 I print_info: file format = GGUF V3 (latest)
0.00.025.215 I print_info: file type   = Q3_K - Medium
0.00.025.216 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.341 I load: special tokens cache size = 25
0.00.039.578 I load: token to piece cache size = 0.2984 MB
0.00.039.592 I print_info: arch             = gptneox
0.00.039.593 I print_info: vocab_only       = 0
0.00.039.594 I print_info: n_ctx_train      = 2048
0.00.039.594 I print_info: n_embd           = 2048
0.00.039.594 I print_info: n_layer          = 24
0.00.039.599 I print_info: n_head           = 16
0.00.039.600 I print_info: n_head_kv        = 16
0.00.039.600 I print_info: n_rot            = 32
0.00.039.600 I print_info: n_swa            = 0
0.00.039.600 I print_info: n_embd_head_k    = 128
0.00.039.600 I print_info: n_embd_head_v    = 128
0.00.039.603 I print_info: n_gqa            = 1
0.00.039.603 I print_info: n_embd_k_gqa     = 2048
0.00.039.605 I print_info: n_embd_v_gqa     = 2048
0.00.039.606 I print_info: f_norm_eps       = 1.0e-05
0.00.039.606 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.606 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.606 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.606 I print_info: f_logit_scale    = 0.0e+00
0.00.039.607 I print_info: n_ff             = 8192
0.00.039.607 I print_info: n_expert         = 0
0.00.039.607 I print_info: n_expert_used    = 0
0.00.039.608 I print_info: causal attn      = 1
0.00.039.609 I print_info: pooling type     = 0
0.00.039.609 I print_info: rope type        = 2
0.00.039.610 I print_info: rope scaling     = linear
0.00.039.610 I print_info: freq_base_train  = 10000.0
0.00.039.610 I print_info: freq_scale_train = 1
0.00.039.610 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.611 I print_info: rope_finetuned   = unknown
0.00.039.611 I print_info: ssm_d_conv       = 0
0.00.039.612 I print_info: ssm_d_inner      = 0
0.00.039.612 I print_info: ssm_d_state      = 0
0.00.039.612 I print_info: ssm_dt_rank      = 0
0.00.039.613 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.613 I print_info: model type       = 1.4B
0.00.039.613 I print_info: model params     = 1.41 B
0.00.039.614 I print_info: general.name     = 1.4B
0.00.039.614 I print_info: vocab type       = BPE
0.00.039.615 I print_info: n_vocab          = 50304
0.00.039.615 I print_info: n_merges         = 50009
0.00.039.615 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.615 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.615 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.616 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.616 I print_info: LF token         = 187 ''
0.00.039.616 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.616 I print_info: max token length = 1024
0.00.039.617 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.497.327 I load_tensors: offloading 24 repeating layers to GPU
0.00.497.341 I load_tensors: offloading output layer to GPU
0.00.497.342 I load_tensors: offloaded 25/25 layers to GPU
0.00.497.371 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.497.372 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.498.783 I llama_init_from_model: n_seq_max     = 1
0.00.498.786 I llama_init_from_model: n_ctx         = 2048
0.00.498.787 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.498.787 I llama_init_from_model: n_batch       = 2048
0.00.498.788 I llama_init_from_model: n_ubatch      = 512
0.00.498.788 I llama_init_from_model: flash_attn    = 0
0.00.498.790 I llama_init_from_model: freq_base     = 10000.0
0.00.498.791 I llama_init_from_model: freq_scale    = 1
0.00.498.796 I ggml_metal_init: allocating
0.00.498.875 I ggml_metal_init: found device: Apple M4
0.00.498.900 I ggml_metal_init: picking default device: Apple M4
0.00.500.356 I ggml_metal_init: using embedded metal library
0.00.506.764 I ggml_metal_init: GPU name:   Apple M4
0.00.506.768 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.506.769 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.506.770 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.506.770 I ggml_metal_init: simdgroup reduction   = true
0.00.506.771 I ggml_metal_init: simdgroup matrix mul. = true
0.00.506.771 I ggml_metal_init: has residency sets    = true
0.00.506.771 I ggml_metal_init: has bfloat            = true
0.00.506.771 I ggml_metal_init: use bfloat            = true
0.00.506.772 I ggml_metal_init: hasUnifiedMemory      = true
0.00.506.773 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.525.097 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.582.247 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.582.256 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.582.280 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.587.129 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.587.131 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.587.131 I llama_init_from_model: graph nodes  = 967
0.00.587.131 I llama_init_from_model: graph splits = 2
0.00.587.136 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.587.268 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.587.269 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.637.021 I main: llama threadpool init, n_threads = 4
0.00.637.074 I 
0.00.637.095 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.637.095 I 
0.00.637.217 I sampler seed: 1234
0.00.637.222 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.637.236 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.637.238 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.637.238 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.388.929 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50605.84 tokens per second)
0.01.388.929 I llama_perf_context_print:        load time =     627.46 ms
0.01.388.930 I llama_perf_context_print: prompt eval time =      50.44 ms /     7 tokens (    7.21 ms per token,   138.77 tokens per second)
0.01.388.931 I llama_perf_context_print:        eval time =     698.32 ms /    63 runs   (   11.08 ms per token,    90.22 tokens per second)
0.01.388.931 I llama_perf_context_print:       total time =     752.67 ms /    70 tokens
0.01.389.190 I ggml_metal_free: deallocating

real	0m1.405s
user	0m0.110s
sys	0m0.218s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.584 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.286 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.293 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.295 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.295 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.296 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.296 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.296 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.297 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.297 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.298 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.300 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.300 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.300 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.301 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.303 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.303 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.303 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.080 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.098 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.868 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.870 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.870 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.870 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.871 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.871 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.872 I llama_model_loader: - type  f32:  194 tensors
0.00.026.872 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.872 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.872 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.873 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.873 I print_info: file format = GGUF V3 (latest)
0.00.026.879 I print_info: file type   = Q3_K - Medium
0.00.026.880 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.766 I load: special tokens cache size = 25
0.00.041.265 I load: token to piece cache size = 0.2984 MB
0.00.041.280 I print_info: arch             = gptneox
0.00.041.281 I print_info: vocab_only       = 0
0.00.041.282 I print_info: n_ctx_train      = 2048
0.00.041.282 I print_info: n_embd           = 2048
0.00.041.282 I print_info: n_layer          = 24
0.00.041.286 I print_info: n_head           = 16
0.00.041.286 I print_info: n_head_kv        = 16
0.00.041.287 I print_info: n_rot            = 32
0.00.041.287 I print_info: n_swa            = 0
0.00.041.287 I print_info: n_embd_head_k    = 128
0.00.041.287 I print_info: n_embd_head_v    = 128
0.00.041.288 I print_info: n_gqa            = 1
0.00.041.288 I print_info: n_embd_k_gqa     = 2048
0.00.041.289 I print_info: n_embd_v_gqa     = 2048
0.00.041.290 I print_info: f_norm_eps       = 1.0e-05
0.00.041.290 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.290 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.298 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.300 I print_info: f_logit_scale    = 0.0e+00
0.00.041.302 I print_info: n_ff             = 8192
0.00.041.302 I print_info: n_expert         = 0
0.00.041.302 I print_info: n_expert_used    = 0
0.00.041.303 I print_info: causal attn      = 1
0.00.041.303 I print_info: pooling type     = 0
0.00.041.303 I print_info: rope type        = 2
0.00.041.303 I print_info: rope scaling     = linear
0.00.041.303 I print_info: freq_base_train  = 10000.0
0.00.041.304 I print_info: freq_scale_train = 1
0.00.041.304 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.304 I print_info: rope_finetuned   = unknown
0.00.041.304 I print_info: ssm_d_conv       = 0
0.00.041.304 I print_info: ssm_d_inner      = 0
0.00.041.304 I print_info: ssm_d_state      = 0
0.00.041.304 I print_info: ssm_dt_rank      = 0
0.00.041.305 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.305 I print_info: model type       = 1.4B
0.00.041.305 I print_info: model params     = 1.41 B
0.00.041.305 I print_info: general.name     = 1.4B
0.00.041.306 I print_info: vocab type       = BPE
0.00.041.306 I print_info: n_vocab          = 50304
0.00.041.306 I print_info: n_merges         = 50009
0.00.041.306 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.307 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.307 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.307 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.307 I print_info: LF token         = 187 ''
0.00.041.309 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.309 I print_info: max token length = 1024
0.00.041.309 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.496.849 I load_tensors: offloading 24 repeating layers to GPU
0.00.496.855 I load_tensors: offloading output layer to GPU
0.00.496.857 I load_tensors: offloaded 25/25 layers to GPU
0.00.496.880 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.496.883 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.497.987 I llama_init_from_model: n_seq_max     = 1
0.00.497.989 I llama_init_from_model: n_ctx         = 128
0.00.497.989 I llama_init_from_model: n_ctx_per_seq = 128
0.00.497.990 I llama_init_from_model: n_batch       = 128
0.00.497.990 I llama_init_from_model: n_ubatch      = 128
0.00.497.991 I llama_init_from_model: flash_attn    = 0
0.00.497.992 I llama_init_from_model: freq_base     = 10000.0
0.00.497.992 I llama_init_from_model: freq_scale    = 1
0.00.497.993 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.497.994 I ggml_metal_init: allocating
0.00.498.015 I ggml_metal_init: found device: Apple M4
0.00.498.025 I ggml_metal_init: picking default device: Apple M4
0.00.499.598 I ggml_metal_init: using embedded metal library
0.00.506.387 I ggml_metal_init: GPU name:   Apple M4
0.00.506.390 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.506.391 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.506.391 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.506.392 I ggml_metal_init: simdgroup reduction   = true
0.00.506.392 I ggml_metal_init: simdgroup matrix mul. = true
0.00.506.393 I ggml_metal_init: has residency sets    = true
0.00.506.393 I ggml_metal_init: has bfloat            = true
0.00.506.393 I ggml_metal_init: use bfloat            = true
0.00.506.394 I ggml_metal_init: hasUnifiedMemory      = true
0.00.506.395 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.524.462 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.527.841 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.527.844 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.527.869 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.530.788 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.530.789 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.530.790 I llama_init_from_model: graph nodes  = 967
0.00.530.791 I llama_init_from_model: graph splits = 2
0.00.530.793 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.530.793 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.557.604 I 
0.00.557.690 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.557.698 I perplexity: tokenizing the input ..
0.00.564.914 I perplexity: tokenization took 7.213 ms
0.00.564.921 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.709.919 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.711.200 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.711.234 I llama_perf_context_print:        load time =     548.01 ms
0.00.711.235 I llama_perf_context_print: prompt eval time =     144.09 ms /   128 tokens (    1.13 ms per token,   888.34 tokens per second)
0.00.711.236 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.711.236 I llama_perf_context_print:       total time =     153.64 ms /   129 tokens
0.00.711.703 I ggml_metal_free: deallocating

real	0m0.726s
user	0m0.081s
sys	0m0.152s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.718 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.374 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.380 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.382 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.382 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.383 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.383 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.384 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.384 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.385 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.385 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.386 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.386 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.387 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.388 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.389 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.390 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.392 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.201 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.198 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.982 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.983 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.983 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.984 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.984 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.984 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.985 I llama_model_loader: - type  f32:  194 tensors
0.00.024.985 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.985 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.986 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.986 I print_info: file format = GGUF V3 (latest)
0.00.024.987 I print_info: file type   = Q4_K - Medium
0.00.024.988 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.880 I load: special tokens cache size = 25
0.00.039.260 I load: token to piece cache size = 0.2984 MB
0.00.039.273 I print_info: arch             = gptneox
0.00.039.274 I print_info: vocab_only       = 0
0.00.039.275 I print_info: n_ctx_train      = 2048
0.00.039.275 I print_info: n_embd           = 2048
0.00.039.275 I print_info: n_layer          = 24
0.00.039.278 I print_info: n_head           = 16
0.00.039.279 I print_info: n_head_kv        = 16
0.00.039.279 I print_info: n_rot            = 32
0.00.039.279 I print_info: n_swa            = 0
0.00.039.279 I print_info: n_embd_head_k    = 128
0.00.039.279 I print_info: n_embd_head_v    = 128
0.00.039.280 I print_info: n_gqa            = 1
0.00.039.281 I print_info: n_embd_k_gqa     = 2048
0.00.039.281 I print_info: n_embd_v_gqa     = 2048
0.00.039.282 I print_info: f_norm_eps       = 1.0e-05
0.00.039.282 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.283 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.283 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.283 I print_info: f_logit_scale    = 0.0e+00
0.00.039.284 I print_info: n_ff             = 8192
0.00.039.284 I print_info: n_expert         = 0
0.00.039.284 I print_info: n_expert_used    = 0
0.00.039.284 I print_info: causal attn      = 1
0.00.039.284 I print_info: pooling type     = 0
0.00.039.284 I print_info: rope type        = 2
0.00.039.285 I print_info: rope scaling     = linear
0.00.039.285 I print_info: freq_base_train  = 10000.0
0.00.039.285 I print_info: freq_scale_train = 1
0.00.039.285 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.287 I print_info: rope_finetuned   = unknown
0.00.039.287 I print_info: ssm_d_conv       = 0
0.00.039.287 I print_info: ssm_d_inner      = 0
0.00.039.287 I print_info: ssm_d_state      = 0
0.00.039.287 I print_info: ssm_dt_rank      = 0
0.00.039.287 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.288 I print_info: model type       = 1.4B
0.00.039.288 I print_info: model params     = 1.41 B
0.00.039.288 I print_info: general.name     = 1.4B
0.00.039.288 I print_info: vocab type       = BPE
0.00.039.289 I print_info: n_vocab          = 50304
0.00.039.289 I print_info: n_merges         = 50009
0.00.039.289 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.290 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.290 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.290 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.290 I print_info: LF token         = 187 ''
0.00.039.290 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.291 I print_info: max token length = 1024
0.00.039.291 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.584.413 I load_tensors: offloading 24 repeating layers to GPU
0.00.584.416 I load_tensors: offloading output layer to GPU
0.00.584.417 I load_tensors: offloaded 25/25 layers to GPU
0.00.584.438 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.584.439 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.585.520 I llama_init_from_model: n_seq_max     = 1
0.00.585.521 I llama_init_from_model: n_ctx         = 2048
0.00.585.522 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.585.522 I llama_init_from_model: n_batch       = 2048
0.00.585.523 I llama_init_from_model: n_ubatch      = 512
0.00.585.523 I llama_init_from_model: flash_attn    = 0
0.00.585.524 I llama_init_from_model: freq_base     = 10000.0
0.00.585.524 I llama_init_from_model: freq_scale    = 1
0.00.585.526 I ggml_metal_init: allocating
0.00.585.552 I ggml_metal_init: found device: Apple M4
0.00.585.561 I ggml_metal_init: picking default device: Apple M4
0.00.586.994 I ggml_metal_init: using embedded metal library
0.00.592.942 I ggml_metal_init: GPU name:   Apple M4
0.00.592.946 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.592.946 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.592.947 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.592.947 I ggml_metal_init: simdgroup reduction   = true
0.00.592.948 I ggml_metal_init: simdgroup matrix mul. = true
0.00.592.948 I ggml_metal_init: has residency sets    = true
0.00.592.948 I ggml_metal_init: has bfloat            = true
0.00.592.948 I ggml_metal_init: use bfloat            = true
0.00.592.949 I ggml_metal_init: hasUnifiedMemory      = true
0.00.592.950 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.609.281 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.662.802 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.662.808 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.662.831 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.668.240 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.668.242 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.668.242 I llama_init_from_model: graph nodes  = 967
0.00.668.243 I llama_init_from_model: graph splits = 2
0.00.668.248 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.668.373 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.668.373 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.988 I main: llama threadpool init, n_threads = 4
0.00.718.041 I 
0.00.718.060 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.718.060 I 
0.00.718.200 I sampler seed: 1234
0.00.718.206 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.718.219 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.718.221 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.718.221 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.482.532 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49789.62 tokens per second)
0.01.482.532 I llama_perf_context_print:        load time =     708.54 ms
0.01.482.533 I llama_perf_context_print: prompt eval time =      58.53 ms /     7 tokens (    8.36 ms per token,   119.60 tokens per second)
0.01.482.534 I llama_perf_context_print:        eval time =     702.74 ms /    63 runs   (   11.15 ms per token,    89.65 tokens per second)
0.01.482.534 I llama_perf_context_print:       total time =     765.27 ms /    70 tokens
0.01.482.820 I ggml_metal_free: deallocating

real	0m1.498s
user	0m0.107s
sys	0m0.233s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.553 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.854 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.861 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.862 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.863 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.863 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.864 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.864 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.865 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.865 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.866 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.868 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.869 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.869 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.869 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.871 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.872 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.872 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.634 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.642 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.398 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.400 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.400 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.400 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.401 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.401 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.402 I llama_model_loader: - type  f32:  194 tensors
0.00.025.402 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.402 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.402 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.403 I print_info: file format = GGUF V3 (latest)
0.00.025.404 I print_info: file type   = Q4_K - Medium
0.00.025.405 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.550 I load: special tokens cache size = 25
0.00.039.877 I load: token to piece cache size = 0.2984 MB
0.00.039.894 I print_info: arch             = gptneox
0.00.039.895 I print_info: vocab_only       = 0
0.00.039.895 I print_info: n_ctx_train      = 2048
0.00.039.896 I print_info: n_embd           = 2048
0.00.039.896 I print_info: n_layer          = 24
0.00.039.899 I print_info: n_head           = 16
0.00.039.900 I print_info: n_head_kv        = 16
0.00.039.900 I print_info: n_rot            = 32
0.00.039.900 I print_info: n_swa            = 0
0.00.039.900 I print_info: n_embd_head_k    = 128
0.00.039.900 I print_info: n_embd_head_v    = 128
0.00.039.901 I print_info: n_gqa            = 1
0.00.039.902 I print_info: n_embd_k_gqa     = 2048
0.00.039.902 I print_info: n_embd_v_gqa     = 2048
0.00.039.904 I print_info: f_norm_eps       = 1.0e-05
0.00.039.904 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.904 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.904 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.905 I print_info: f_logit_scale    = 0.0e+00
0.00.039.905 I print_info: n_ff             = 8192
0.00.039.905 I print_info: n_expert         = 0
0.00.039.905 I print_info: n_expert_used    = 0
0.00.039.906 I print_info: causal attn      = 1
0.00.039.906 I print_info: pooling type     = 0
0.00.039.906 I print_info: rope type        = 2
0.00.039.906 I print_info: rope scaling     = linear
0.00.039.907 I print_info: freq_base_train  = 10000.0
0.00.039.907 I print_info: freq_scale_train = 1
0.00.039.907 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.909 I print_info: rope_finetuned   = unknown
0.00.039.910 I print_info: ssm_d_conv       = 0
0.00.039.910 I print_info: ssm_d_inner      = 0
0.00.039.910 I print_info: ssm_d_state      = 0
0.00.039.910 I print_info: ssm_dt_rank      = 0
0.00.039.910 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.910 I print_info: model type       = 1.4B
0.00.039.911 I print_info: model params     = 1.41 B
0.00.039.911 I print_info: general.name     = 1.4B
0.00.039.911 I print_info: vocab type       = BPE
0.00.039.911 I print_info: n_vocab          = 50304
0.00.039.911 I print_info: n_merges         = 50009
0.00.039.912 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.912 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.912 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.912 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.912 I print_info: LF token         = 187 ''
0.00.039.913 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.913 I print_info: max token length = 1024
0.00.039.913 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.579.626 I load_tensors: offloading 24 repeating layers to GPU
0.00.579.639 I load_tensors: offloading output layer to GPU
0.00.579.639 I load_tensors: offloaded 25/25 layers to GPU
0.00.579.672 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.579.673 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.581.010 I llama_init_from_model: n_seq_max     = 1
0.00.581.012 I llama_init_from_model: n_ctx         = 128
0.00.581.012 I llama_init_from_model: n_ctx_per_seq = 128
0.00.581.013 I llama_init_from_model: n_batch       = 128
0.00.581.013 I llama_init_from_model: n_ubatch      = 128
0.00.581.014 I llama_init_from_model: flash_attn    = 0
0.00.581.015 I llama_init_from_model: freq_base     = 10000.0
0.00.581.016 I llama_init_from_model: freq_scale    = 1
0.00.581.016 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.581.019 I ggml_metal_init: allocating
0.00.581.112 I ggml_metal_init: found device: Apple M4
0.00.581.125 I ggml_metal_init: picking default device: Apple M4
0.00.583.063 I ggml_metal_init: using embedded metal library
0.00.590.057 I ggml_metal_init: GPU name:   Apple M4
0.00.590.062 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.590.063 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.590.063 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.590.064 I ggml_metal_init: simdgroup reduction   = true
0.00.590.064 I ggml_metal_init: simdgroup matrix mul. = true
0.00.590.064 I ggml_metal_init: has residency sets    = true
0.00.590.065 I ggml_metal_init: has bfloat            = true
0.00.590.065 I ggml_metal_init: use bfloat            = true
0.00.590.066 I ggml_metal_init: hasUnifiedMemory      = true
0.00.590.069 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.607.454 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.610.872 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.610.875 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.610.903 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.614.027 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.614.029 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.614.029 I llama_init_from_model: graph nodes  = 967
0.00.614.030 I llama_init_from_model: graph splits = 2
0.00.614.033 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.614.033 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.643.878 I 
0.00.643.960 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.643.968 I perplexity: tokenizing the input ..
0.00.650.972 I perplexity: tokenization took 7.001 ms
0.00.650.985 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.791.605 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.792.857 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.792.880 I llama_perf_context_print:        load time =     634.32 ms
0.00.792.881 I llama_perf_context_print: prompt eval time =     139.68 ms /   128 tokens (    1.09 ms per token,   916.39 tokens per second)
0.00.792.881 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.792.882 I llama_perf_context_print:       total time =     149.00 ms /   129 tokens
0.00.793.224 I ggml_metal_free: deallocating

real	0m0.806s
user	0m0.080s
sys	0m0.166s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.179 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.984 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.989 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.991 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.991 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.992 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.992 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.992 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.993 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.993 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.994 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.994 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.995 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.995 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.995 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.998 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.999 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.999 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.854 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.849 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.551 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.552 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.553 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.553 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.553 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.554 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.554 I llama_model_loader: - type  f32:  194 tensors
0.00.025.554 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.554 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.555 I print_info: file format = GGUF V3 (latest)
0.00.025.555 I print_info: file type   = Q5_K - Medium
0.00.025.556 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.473 I load: special tokens cache size = 25
0.00.039.645 I load: token to piece cache size = 0.2984 MB
0.00.039.660 I print_info: arch             = gptneox
0.00.039.661 I print_info: vocab_only       = 0
0.00.039.661 I print_info: n_ctx_train      = 2048
0.00.039.661 I print_info: n_embd           = 2048
0.00.039.661 I print_info: n_layer          = 24
0.00.039.664 I print_info: n_head           = 16
0.00.039.665 I print_info: n_head_kv        = 16
0.00.039.665 I print_info: n_rot            = 32
0.00.039.665 I print_info: n_swa            = 0
0.00.039.665 I print_info: n_embd_head_k    = 128
0.00.039.666 I print_info: n_embd_head_v    = 128
0.00.039.666 I print_info: n_gqa            = 1
0.00.039.667 I print_info: n_embd_k_gqa     = 2048
0.00.039.668 I print_info: n_embd_v_gqa     = 2048
0.00.039.669 I print_info: f_norm_eps       = 1.0e-05
0.00.039.670 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.671 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.671 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.671 I print_info: f_logit_scale    = 0.0e+00
0.00.039.672 I print_info: n_ff             = 8192
0.00.039.672 I print_info: n_expert         = 0
0.00.039.672 I print_info: n_expert_used    = 0
0.00.039.672 I print_info: causal attn      = 1
0.00.039.672 I print_info: pooling type     = 0
0.00.039.673 I print_info: rope type        = 2
0.00.039.675 I print_info: rope scaling     = linear
0.00.039.675 I print_info: freq_base_train  = 10000.0
0.00.039.675 I print_info: freq_scale_train = 1
0.00.039.676 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.676 I print_info: rope_finetuned   = unknown
0.00.039.677 I print_info: ssm_d_conv       = 0
0.00.039.677 I print_info: ssm_d_inner      = 0
0.00.039.677 I print_info: ssm_d_state      = 0
0.00.039.677 I print_info: ssm_dt_rank      = 0
0.00.039.678 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.678 I print_info: model type       = 1.4B
0.00.039.678 I print_info: model params     = 1.41 B
0.00.039.679 I print_info: general.name     = 1.4B
0.00.039.680 I print_info: vocab type       = BPE
0.00.039.680 I print_info: n_vocab          = 50304
0.00.039.680 I print_info: n_merges         = 50009
0.00.039.680 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.680 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.680 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.681 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.681 I print_info: LF token         = 187 ''
0.00.039.681 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.681 I print_info: max token length = 1024
0.00.039.682 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.672.519 I load_tensors: offloading 24 repeating layers to GPU
0.00.672.523 I load_tensors: offloading output layer to GPU
0.00.672.524 I load_tensors: offloaded 25/25 layers to GPU
0.00.672.549 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.672.550 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.673.563 I llama_init_from_model: n_seq_max     = 1
0.00.673.565 I llama_init_from_model: n_ctx         = 2048
0.00.673.565 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.673.566 I llama_init_from_model: n_batch       = 2048
0.00.673.566 I llama_init_from_model: n_ubatch      = 512
0.00.673.566 I llama_init_from_model: flash_attn    = 0
0.00.673.567 I llama_init_from_model: freq_base     = 10000.0
0.00.673.568 I llama_init_from_model: freq_scale    = 1
0.00.673.569 I ggml_metal_init: allocating
0.00.673.584 I ggml_metal_init: found device: Apple M4
0.00.673.592 I ggml_metal_init: picking default device: Apple M4
0.00.675.067 I ggml_metal_init: using embedded metal library
0.00.680.627 I ggml_metal_init: GPU name:   Apple M4
0.00.680.630 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.680.631 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.680.632 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.680.632 I ggml_metal_init: simdgroup reduction   = true
0.00.680.633 I ggml_metal_init: simdgroup matrix mul. = true
0.00.680.633 I ggml_metal_init: has residency sets    = true
0.00.680.633 I ggml_metal_init: has bfloat            = true
0.00.680.633 I ggml_metal_init: use bfloat            = true
0.00.680.634 I ggml_metal_init: hasUnifiedMemory      = true
0.00.680.635 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.696.667 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.764.647 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.764.653 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.764.677 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.769.503 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.769.505 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.769.505 I llama_init_from_model: graph nodes  = 967
0.00.769.505 I llama_init_from_model: graph splits = 2
0.00.769.510 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.769.643 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.769.644 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.822.370 I main: llama threadpool init, n_threads = 4
0.00.822.421 I 
0.00.822.441 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.822.441 I 
0.00.822.553 I sampler seed: 1234
0.00.822.557 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.822.571 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.822.573 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.822.573 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.665.930 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53423.63 tokens per second)
0.01.665.930 I llama_perf_context_print:        load time =     812.45 ms
0.01.665.931 I llama_perf_context_print: prompt eval time =      52.59 ms /     7 tokens (    7.51 ms per token,   133.10 tokens per second)
0.01.665.932 I llama_perf_context_print:        eval time =     787.83 ms /    63 runs   (   12.51 ms per token,    79.97 tokens per second)
0.01.665.934 I llama_perf_context_print:       total time =     844.30 ms /    70 tokens
0.01.666.137 I ggml_metal_free: deallocating

real	0m1.686s
user	0m0.106s
sys	0m0.261s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.814 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.701 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.707 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.709 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.710 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.710 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.710 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.710 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.711 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.718 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.718 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.718 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.719 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.719 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.719 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.721 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.721 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.723 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.474 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.514 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.240 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.241 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.242 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.242 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.242 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.243 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.243 I llama_model_loader: - type  f32:  194 tensors
0.00.026.243 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.244 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.244 I print_info: file format = GGUF V3 (latest)
0.00.026.245 I print_info: file type   = Q5_K - Medium
0.00.026.246 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.319 I load: special tokens cache size = 25
0.00.040.770 I load: token to piece cache size = 0.2984 MB
0.00.040.787 I print_info: arch             = gptneox
0.00.040.787 I print_info: vocab_only       = 0
0.00.040.788 I print_info: n_ctx_train      = 2048
0.00.040.788 I print_info: n_embd           = 2048
0.00.040.788 I print_info: n_layer          = 24
0.00.040.794 I print_info: n_head           = 16
0.00.040.795 I print_info: n_head_kv        = 16
0.00.040.795 I print_info: n_rot            = 32
0.00.040.795 I print_info: n_swa            = 0
0.00.040.795 I print_info: n_embd_head_k    = 128
0.00.040.795 I print_info: n_embd_head_v    = 128
0.00.040.796 I print_info: n_gqa            = 1
0.00.040.796 I print_info: n_embd_k_gqa     = 2048
0.00.040.797 I print_info: n_embd_v_gqa     = 2048
0.00.040.798 I print_info: f_norm_eps       = 1.0e-05
0.00.040.798 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.798 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.798 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.798 I print_info: f_logit_scale    = 0.0e+00
0.00.040.799 I print_info: n_ff             = 8192
0.00.040.799 I print_info: n_expert         = 0
0.00.040.799 I print_info: n_expert_used    = 0
0.00.040.799 I print_info: causal attn      = 1
0.00.040.800 I print_info: pooling type     = 0
0.00.040.800 I print_info: rope type        = 2
0.00.040.801 I print_info: rope scaling     = linear
0.00.040.801 I print_info: freq_base_train  = 10000.0
0.00.040.801 I print_info: freq_scale_train = 1
0.00.040.802 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.802 I print_info: rope_finetuned   = unknown
0.00.040.802 I print_info: ssm_d_conv       = 0
0.00.040.802 I print_info: ssm_d_inner      = 0
0.00.040.802 I print_info: ssm_d_state      = 0
0.00.040.802 I print_info: ssm_dt_rank      = 0
0.00.040.802 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.803 I print_info: model type       = 1.4B
0.00.040.805 I print_info: model params     = 1.41 B
0.00.040.805 I print_info: general.name     = 1.4B
0.00.040.805 I print_info: vocab type       = BPE
0.00.040.806 I print_info: n_vocab          = 50304
0.00.040.806 I print_info: n_merges         = 50009
0.00.040.806 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.806 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.806 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.806 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.807 I print_info: LF token         = 187 ''
0.00.040.807 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.807 I print_info: max token length = 1024
0.00.040.808 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.669.568 I load_tensors: offloading 24 repeating layers to GPU
0.00.669.578 I load_tensors: offloading output layer to GPU
0.00.669.578 I load_tensors: offloaded 25/25 layers to GPU
0.00.669.608 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.669.611 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.670.829 I llama_init_from_model: n_seq_max     = 1
0.00.670.831 I llama_init_from_model: n_ctx         = 128
0.00.670.832 I llama_init_from_model: n_ctx_per_seq = 128
0.00.670.832 I llama_init_from_model: n_batch       = 128
0.00.670.833 I llama_init_from_model: n_ubatch      = 128
0.00.670.833 I llama_init_from_model: flash_attn    = 0
0.00.670.834 I llama_init_from_model: freq_base     = 10000.0
0.00.670.835 I llama_init_from_model: freq_scale    = 1
0.00.670.835 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.670.836 I ggml_metal_init: allocating
0.00.670.886 I ggml_metal_init: found device: Apple M4
0.00.670.897 I ggml_metal_init: picking default device: Apple M4
0.00.672.404 I ggml_metal_init: using embedded metal library
0.00.678.496 I ggml_metal_init: GPU name:   Apple M4
0.00.678.500 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.678.501 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.678.502 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.678.502 I ggml_metal_init: simdgroup reduction   = true
0.00.678.502 I ggml_metal_init: simdgroup matrix mul. = true
0.00.678.503 I ggml_metal_init: has residency sets    = true
0.00.678.503 I ggml_metal_init: has bfloat            = true
0.00.678.503 I ggml_metal_init: use bfloat            = true
0.00.678.504 I ggml_metal_init: hasUnifiedMemory      = true
0.00.678.506 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.695.450 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.698.880 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.698.884 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.698.908 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.701.968 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.701.969 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.701.970 I llama_init_from_model: graph nodes  = 967
0.00.701.970 I llama_init_from_model: graph splits = 2
0.00.701.974 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.701.974 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.737.890 I 
0.00.737.981 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.737.991 I perplexity: tokenizing the input ..
0.00.744.826 I perplexity: tokenization took 6.832 ms
0.00.744.841 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.893.852 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.895.141 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.895.174 I llama_perf_context_print:        load time =     727.07 ms
0.00.895.175 I llama_perf_context_print: prompt eval time =     148.42 ms /   128 tokens (    1.16 ms per token,   862.43 tokens per second)
0.00.895.175 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.895.176 I llama_perf_context_print:       total time =     157.29 ms /   129 tokens
0.00.895.587 I ggml_metal_free: deallocating

real	0m0.913s
user	0m0.078s
sys	0m0.192s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.389 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.050 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.055 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.056 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.057 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.057 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.058 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.058 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.059 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.059 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.060 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.060 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.060 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.061 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.061 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.063 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.063 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.063 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.883 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.896 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.627 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.628 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.629 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.629 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.629 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.630 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.630 I llama_model_loader: - type  f32:  194 tensors
0.00.024.631 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.631 I print_info: file format = GGUF V3 (latest)
0.00.024.632 I print_info: file type   = Q6_K
0.00.024.632 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.515 I load: special tokens cache size = 25
0.00.038.703 I load: token to piece cache size = 0.2984 MB
0.00.038.718 I print_info: arch             = gptneox
0.00.038.719 I print_info: vocab_only       = 0
0.00.038.719 I print_info: n_ctx_train      = 2048
0.00.038.720 I print_info: n_embd           = 2048
0.00.038.720 I print_info: n_layer          = 24
0.00.038.722 I print_info: n_head           = 16
0.00.038.723 I print_info: n_head_kv        = 16
0.00.038.723 I print_info: n_rot            = 32
0.00.038.723 I print_info: n_swa            = 0
0.00.038.724 I print_info: n_embd_head_k    = 128
0.00.038.724 I print_info: n_embd_head_v    = 128
0.00.038.729 I print_info: n_gqa            = 1
0.00.038.730 I print_info: n_embd_k_gqa     = 2048
0.00.038.730 I print_info: n_embd_v_gqa     = 2048
0.00.038.731 I print_info: f_norm_eps       = 1.0e-05
0.00.038.732 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.732 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.732 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.732 I print_info: f_logit_scale    = 0.0e+00
0.00.038.733 I print_info: n_ff             = 8192
0.00.038.733 I print_info: n_expert         = 0
0.00.038.734 I print_info: n_expert_used    = 0
0.00.038.734 I print_info: causal attn      = 1
0.00.038.734 I print_info: pooling type     = 0
0.00.038.736 I print_info: rope type        = 2
0.00.038.736 I print_info: rope scaling     = linear
0.00.038.736 I print_info: freq_base_train  = 10000.0
0.00.038.736 I print_info: freq_scale_train = 1
0.00.038.737 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.737 I print_info: rope_finetuned   = unknown
0.00.038.737 I print_info: ssm_d_conv       = 0
0.00.038.737 I print_info: ssm_d_inner      = 0
0.00.038.737 I print_info: ssm_d_state      = 0
0.00.038.738 I print_info: ssm_dt_rank      = 0
0.00.038.738 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.738 I print_info: model type       = 1.4B
0.00.038.738 I print_info: model params     = 1.41 B
0.00.038.738 I print_info: general.name     = 1.4B
0.00.038.739 I print_info: vocab type       = BPE
0.00.038.739 I print_info: n_vocab          = 50304
0.00.038.739 I print_info: n_merges         = 50009
0.00.038.740 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.740 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.740 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.740 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.740 I print_info: LF token         = 187 ''
0.00.038.741 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.741 I print_info: max token length = 1024
0.00.038.741 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.723.942 I load_tensors: offloading 24 repeating layers to GPU
0.00.723.945 I load_tensors: offloading output layer to GPU
0.00.723.946 I load_tensors: offloaded 25/25 layers to GPU
0.00.723.966 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.723.968 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.725.041 I llama_init_from_model: n_seq_max     = 1
0.00.725.043 I llama_init_from_model: n_ctx         = 2048
0.00.725.044 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.725.044 I llama_init_from_model: n_batch       = 2048
0.00.725.044 I llama_init_from_model: n_ubatch      = 512
0.00.725.045 I llama_init_from_model: flash_attn    = 0
0.00.725.045 I llama_init_from_model: freq_base     = 10000.0
0.00.725.046 I llama_init_from_model: freq_scale    = 1
0.00.725.047 I ggml_metal_init: allocating
0.00.725.068 I ggml_metal_init: found device: Apple M4
0.00.725.075 I ggml_metal_init: picking default device: Apple M4
0.00.726.484 I ggml_metal_init: using embedded metal library
0.00.731.697 I ggml_metal_init: GPU name:   Apple M4
0.00.731.700 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.731.701 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.731.701 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.731.702 I ggml_metal_init: simdgroup reduction   = true
0.00.731.702 I ggml_metal_init: simdgroup matrix mul. = true
0.00.731.702 I ggml_metal_init: has residency sets    = true
0.00.731.703 I ggml_metal_init: has bfloat            = true
0.00.731.703 I ggml_metal_init: use bfloat            = true
0.00.731.703 I ggml_metal_init: hasUnifiedMemory      = true
0.00.731.704 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.747.090 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.788.150 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.788.158 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.788.185 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.793.301 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.793.303 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.793.303 I llama_init_from_model: graph nodes  = 967
0.00.793.304 I llama_init_from_model: graph splits = 2
0.00.793.308 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.793.437 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.793.437 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.854.510 I main: llama threadpool init, n_threads = 4
0.00.854.583 I 
0.00.854.618 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.854.618 I 
0.00.854.774 I sampler seed: 1234
0.00.854.779 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.854.825 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.854.827 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.854.827 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.724.361 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52985.07 tokens per second)
0.01.724.362 I llama_perf_context_print:        load time =     845.04 ms
0.01.724.363 I llama_perf_context_print: prompt eval time =      57.46 ms /     7 tokens (    8.21 ms per token,   121.82 tokens per second)
0.01.724.363 I llama_perf_context_print:        eval time =     809.10 ms /    63 runs   (   12.84 ms per token,    77.86 tokens per second)
0.01.724.364 I llama_perf_context_print:       total time =     870.93 ms /    70 tokens
0.01.724.614 I ggml_metal_free: deallocating

real	0m1.740s
user	0m0.106s
sys	0m0.252s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4870 (3384f361) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.674 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.854 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.860 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.864 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.864 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.865 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.865 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.865 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.866 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.867 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.867 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.868 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.868 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.870 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.870 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.873 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.873 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.873 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.727 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.751 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.575 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.577 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.577 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.577 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.578 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.578 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.578 I llama_model_loader: - type  f32:  194 tensors
0.00.024.579 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.580 I print_info: file format = GGUF V3 (latest)
0.00.024.580 I print_info: file type   = Q6_K
0.00.024.581 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.105 I load: special tokens cache size = 25
0.00.039.648 I load: token to piece cache size = 0.2984 MB
0.00.039.666 I print_info: arch             = gptneox
0.00.039.666 I print_info: vocab_only       = 0
0.00.039.667 I print_info: n_ctx_train      = 2048
0.00.039.667 I print_info: n_embd           = 2048
0.00.039.667 I print_info: n_layer          = 24
0.00.039.671 I print_info: n_head           = 16
0.00.039.674 I print_info: n_head_kv        = 16
0.00.039.674 I print_info: n_rot            = 32
0.00.039.674 I print_info: n_swa            = 0
0.00.039.674 I print_info: n_embd_head_k    = 128
0.00.039.674 I print_info: n_embd_head_v    = 128
0.00.039.675 I print_info: n_gqa            = 1
0.00.039.675 I print_info: n_embd_k_gqa     = 2048
0.00.039.676 I print_info: n_embd_v_gqa     = 2048
0.00.039.677 I print_info: f_norm_eps       = 1.0e-05
0.00.039.677 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.677 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.677 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.677 I print_info: f_logit_scale    = 0.0e+00
0.00.039.678 I print_info: n_ff             = 8192
0.00.039.678 I print_info: n_expert         = 0
0.00.039.678 I print_info: n_expert_used    = 0
0.00.039.678 I print_info: causal attn      = 1
0.00.039.679 I print_info: pooling type     = 0
0.00.039.679 I print_info: rope type        = 2
0.00.039.679 I print_info: rope scaling     = linear
0.00.039.679 I print_info: freq_base_train  = 10000.0
0.00.039.685 I print_info: freq_scale_train = 1
0.00.039.685 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.685 I print_info: rope_finetuned   = unknown
0.00.039.686 I print_info: ssm_d_conv       = 0
0.00.039.686 I print_info: ssm_d_inner      = 0
0.00.039.686 I print_info: ssm_d_state      = 0
0.00.039.688 I print_info: ssm_dt_rank      = 0
0.00.039.688 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.688 I print_info: model type       = 1.4B
0.00.039.689 I print_info: model params     = 1.41 B
0.00.039.689 I print_info: general.name     = 1.4B
0.00.039.689 I print_info: vocab type       = BPE
0.00.039.690 I print_info: n_vocab          = 50304
0.00.039.690 I print_info: n_merges         = 50009
0.00.039.690 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.690 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.690 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.690 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.691 I print_info: LF token         = 187 ''
0.00.039.691 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.691 I print_info: max token length = 1024
0.00.039.692 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.660.915 I load_tensors: offloading 24 repeating layers to GPU
0.00.660.921 I load_tensors: offloading output layer to GPU
0.00.660.922 I load_tensors: offloaded 25/25 layers to GPU
0.00.660.947 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.660.950 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.661.960 I llama_init_from_model: n_seq_max     = 1
0.00.661.961 I llama_init_from_model: n_ctx         = 128
0.00.661.961 I llama_init_from_model: n_ctx_per_seq = 128
0.00.661.962 I llama_init_from_model: n_batch       = 128
0.00.661.962 I llama_init_from_model: n_ubatch      = 128
0.00.661.962 I llama_init_from_model: flash_attn    = 0
0.00.661.963 I llama_init_from_model: freq_base     = 10000.0
0.00.661.964 I llama_init_from_model: freq_scale    = 1
0.00.661.965 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.661.965 I ggml_metal_init: allocating
0.00.661.996 I ggml_metal_init: found device: Apple M4
0.00.662.005 I ggml_metal_init: picking default device: Apple M4
0.00.663.398 I ggml_metal_init: using embedded metal library
0.00.668.550 I ggml_metal_init: GPU name:   Apple M4
0.00.668.553 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.668.554 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.668.555 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.668.555 I ggml_metal_init: simdgroup reduction   = true
0.00.668.556 I ggml_metal_init: simdgroup matrix mul. = true
0.00.668.556 I ggml_metal_init: has residency sets    = true
0.00.668.556 I ggml_metal_init: has bfloat            = true
0.00.668.556 I ggml_metal_init: use bfloat            = true
0.00.668.557 I ggml_metal_init: hasUnifiedMemory      = true
0.00.668.559 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.683.652 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.686.987 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.686.990 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.687.015 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.690.071 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.690.073 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.690.073 I llama_init_from_model: graph nodes  = 967
0.00.690.074 I llama_init_from_model: graph splits = 2
0.00.690.076 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.690.077 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.719.688 I 
0.00.719.772 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.719.780 I perplexity: tokenizing the input ..
0.00.726.680 I perplexity: tokenization took 6.897 ms
0.00.726.695 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.858.255 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.859.505 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.859.530 I llama_perf_context_print:        load time =     711.00 ms
0.00.859.531 I llama_perf_context_print: prompt eval time =     130.89 ms /   128 tokens (    1.02 ms per token,   977.92 tokens per second)
0.00.859.532 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.859.532 I llama_perf_context_print:       total time =     139.85 ms /   129 tokens
0.00.859.898 I ggml_metal_free: deallocating

real	0m0.874s
user	0m0.076s
sys	0m0.181s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4870 (3384f361)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15120b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15120bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15120c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15120c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15120ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15120d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15120d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15120ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15120e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15120e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15120ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15120f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15120fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x151210540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x151210d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x151211470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x151211b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1512122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1512129d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1512131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1512138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x151213fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x151214700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x151214fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1512156c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x151215b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x151216000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1512166a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x151216b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x151216fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1512172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x151217990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x151217c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1512180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x151218590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x151218a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x151218ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x151219370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x151219810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x151219cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15121a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15121a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15121aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15121af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15121b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15121b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15121bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15121c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15121cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15121cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15121d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15121d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15121dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15121e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15121e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15121eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15121efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15121f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15121f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15121fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x151220100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1512205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x151220a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x151220ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x151221380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x151221820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x151221cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x151222160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x151222600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x151222aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x151222f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1512233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x151223880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x151223dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x151224320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x151224870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x151224dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x151225310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x151225860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x151225db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x151226300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x151226850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x151226da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1512272f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x151227840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x151227d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1512282e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x151228830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x151228d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1512292d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x151229820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x151229d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15122a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15122a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15122ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15122b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15122b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15121c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15122bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15122c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15122c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15122cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15122d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15122d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15122deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15122e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15122e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15122eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15122f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15122f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15122fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1512303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x151230930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x151230dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x151231270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x151231710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x151231bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x151232050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1512324f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x151232990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x151232e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1512332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x151233770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x151233c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1512340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x151234550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1512349f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x151234e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x151235330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1512357d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x151235c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x151236110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1512365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x151236a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x151236ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x151237390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x151237830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x151237cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x151238170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x151238610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x151238ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x151238f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1512393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x151239890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x151239d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15123a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15123a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15123ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15123afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15123b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15123b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15123bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15123c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15123c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15123cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15123d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15123d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15123d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15123ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15123e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15123e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15123ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15123f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15123f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15123f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15123fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1512402f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x151240790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x151240c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1512410d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x151241570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x151241a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x151241eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x151242350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1512427f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x151242c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151243130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1512435d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x151243a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x151243f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1512443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x151244850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x151244cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x151245190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x151245630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x151245ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x151245f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x151246410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1512468b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x151246d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1512471f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x151247690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x151247b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x151248080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1512485d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x151248b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x151249070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x151249510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1512499b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x151249e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15124a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15124a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15124ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15124b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15124b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15124bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15124bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15124c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15124c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15124cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15124d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15124dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15124dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15124e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15124e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15124ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15124f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15124f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15124ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x151250540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x151250af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1512510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x151251650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x151251c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1512521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x151252760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x151252d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1512532c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x151253870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x151253e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1512543d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x151254980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x151254f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1512554e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x151255a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x151256040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1512565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x151256ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x151257150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x151257700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x151257cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x151258260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x151258810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x151258dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x151259370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x151259920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x151259ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15125a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15125aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15125afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15125b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15125bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15125c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15125c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15125cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15125d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15125d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15125dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15125e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15125e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15125ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15125f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15125f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15125ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x151260530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x151260ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x151261090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x151261640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x151261b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x151262040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x151262540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x151262a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x151262f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x151263440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x151263940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x151263e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x151264340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x151264840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x151264d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x151265240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x151265740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x151265c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x151266140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x151266640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x151266b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x151267040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x151267540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x151267a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x151267f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x151268440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x151268940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x151268e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x151269340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x151269d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15126a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15126ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15126b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15126b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15126bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15126c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15126c640 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.780.951 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.780.955 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15124e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x151257410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x151256300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x151252fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x151250800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15125fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15125d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15125b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x151259080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x151251360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15124eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x151253b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x151254c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15125a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x151256e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15125eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x151251910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x151252a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x151259be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15125be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x151254690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1512557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15125acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1512579c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x151257f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x151252470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x151253580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x151260240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15125da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15124f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x151258ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15124e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x151250250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1512607f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x151255d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x151269600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15125e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1512540e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1512568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15125a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x151251ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15125c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x151250db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15125f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15125c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x151258520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x151261350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15124fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x151260da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15124f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15125f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x151259630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15125b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15125e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15125cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1512551f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x151217560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15126c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15126cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15126ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15126d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15126d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15126d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15126d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15126dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15126df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15126e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15126e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15126e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15126ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15126ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15126ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15126f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15126f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15126f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15126fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15126fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x151270000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1512702c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x151270580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x151270840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x151270b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x151270dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x151271080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x151271340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x151271600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1512718c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x151271b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x151271e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x151272100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1512723c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x151272680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x151272940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x151272c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x151272ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x151273180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x151273440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x151273700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1512739c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x151273c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x151273f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x151274200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1512744c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x151274780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x151274a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x151274d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x151274fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x151275280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x151275540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x151275800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x151275ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x151275d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x151276040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x151276300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1512765c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x151276880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x151276b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x151276e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1512770c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x151277380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x151277640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x151277900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x151277bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x151277e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x151278140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x151278400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1512786c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x151278980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x151278c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x151278f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1512791c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x151279480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x151279740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x151279a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x151279cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x151279f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15127a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15127a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15127a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15127aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15127ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15127b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15127b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15127b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15127b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15127bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15127bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15127c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15127c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15127c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15127c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15127cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15127ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15127d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15127d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15127d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15127d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15127dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15127dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15127e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15127e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15127e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15127e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15127ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15127ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15127f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15127f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15127f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15127fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15127fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15127ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x151280280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x151280540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x151280800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x151280ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x151280d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151281040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x151281300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1512815c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x151281880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x151281b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x151281e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1512822a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x151282740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x151282be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x151283080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x151283520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1512839c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x151283e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x151284300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1512847a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x151284c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1512850e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x151285630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x151285b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1512860d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x151286620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1512868e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x151286ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1512870a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1512875a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x151287aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x151287fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1512884c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x151288a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x151288f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x151289480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x151289b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x151289e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15128a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15128ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15128b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15128b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15128bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15128c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15128c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15128cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15128d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15128d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15128dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15128e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15128ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15128efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15128f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15128fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x151290120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1512906e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x151290ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x151291260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x151291820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x151291de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1512923a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x151292960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x151292f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1512934e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x151293aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x151294060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x151294620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x151294be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1512951a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x151295760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x151295d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1512962e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1512968a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x151296e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x151297420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1512979e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x151297fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x151298560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x151298b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1512990e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1512996a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x151299c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15129a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15129a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15129ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15129b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15129b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15129bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15129c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15129ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15129d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15129d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15129dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15129e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15129e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15129ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15129f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15129f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15129fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1512a01a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1512a06a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1512a0ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1512a10a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1512a15a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1512a1aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1512a1fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1512a24a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1512a29a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1512a2ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1512a33a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1512a38a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x1512a3da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x1512a42a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1512a47a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x1512a4ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x1512a51a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1512a56a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1512a5ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1512a60a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1512a65a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1512a6aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1512a6fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1512a79b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1512a80d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1512a87f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1512a8f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1512a91d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1512a9960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1512a9c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1512aa130 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1513055a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x151305a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x151305e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1513062f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x151306760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x151306bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x151307040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1513074b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x151307920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x151307e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1513082d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x151308950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x151309470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x151309c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15130a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15130ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15130b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15130b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15130c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15130c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15130cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15130d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15130dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15130e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15130ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15130f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15130f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15130fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15130fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x151310340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1513107e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x151310c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x151310f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1513113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x151311880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x151311d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1513121c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x151312660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x151312b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x151312fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x151313440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1513138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x151313d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x151314220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1513146c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x151314b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x151315000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1513154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x151315940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x151315de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x151316280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x151316720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x151316bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x151317060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x151317500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1513179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x151317e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x151318100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1513183c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x151318830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x151318ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x151319110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x151319580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1513199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x151319e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15131a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15131a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15131abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15131b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15131b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15131b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15131bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15131c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15131c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15131cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15131cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15131d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15131d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15131dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15131e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15131e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15131e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15131ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15131f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15131f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15131fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x151320000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x151320470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1513208e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x151320d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1513211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x151321630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x151321aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x151321f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x151322380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1513227f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x151322c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1513230d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1513237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x151323cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x151324280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x151324830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x151324de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x151325390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x151325940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x151325ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1513264a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x151326a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x151327000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1513275b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x151327b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x151328110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1513286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x151328c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x151329170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x151329670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x151329b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15132a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15132a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15132aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15132af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15132b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15132b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15132be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15132c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15132c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15132cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15132d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15132d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15132dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15132e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15132e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15132eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15132f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15132f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15132fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15132ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x151330470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x151330970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x151330e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x151331370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x151331870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x151331d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x151332270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x151332770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x151332c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x151333170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x151333670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x151333b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x151334070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x151334570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x151334a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x151334f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x151335470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x151335970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x151335e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x151336370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x151336870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x151336d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x151337270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x151337770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x151337c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x151338170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x151338670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x151338b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x151339070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x151339570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x151339a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x151339f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15133a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15133a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15133ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15133b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15133b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15133bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15133c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15133c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15133cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15133d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15133d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15133db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15133e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15133e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15133ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15133ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15133f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15133f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15133fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x151340370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x151340870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x151340d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x151341270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x151341770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x151341c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x151342220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1513427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x151342d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x151343330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x151343830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x151343d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x151344230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x151344910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x151344db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x151345070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x151345620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x151345b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x151346200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1513466a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x151346b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x151346fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x151347830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x151347af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1513480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x151348650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x151348c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1513491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x151349760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x151349d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15134a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15134a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15134ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15134b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15134b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15134bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15134c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15134ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15134d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15134d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15134dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15134e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15134e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15134ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15134f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15134f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15134fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x151350370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x151350920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x151350ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x151351480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x151351a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x151351fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x151352590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x151352b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1513530f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1513536a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x151353c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x151354200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1513547b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x151354d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x151355310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1513558c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x151355e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x151356420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1513569d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x151356f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x151357530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x151357ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x151358090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x151358640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x151358bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1513591a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x151359750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x151359d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15135a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15135a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15135ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15135b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15135b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15135be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15135c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15135c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15135cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15135d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15135d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15135dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15135e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15135e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15135eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15135f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15135f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15135fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15135ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x151360470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x151360970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x151360e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x151361370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x151361870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x151361d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x151362270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x151362770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x151362c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x151363170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x151363670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x151364080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1513647a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x151364ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1513655e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1513658a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x151366030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1513664d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x151366970 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.853s
user	0m0.279s
sys	0m0.343s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4870 (3384f361)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1231077b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123107ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1231084a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123108a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123109000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1231095b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123109b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12310a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12310a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12310abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12310b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12310b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12310c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12310c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12310d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12310d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12310dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12310e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12310ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12310f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12310fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123110330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123110a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1231112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x123111a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123111eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x123112350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1231129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123112e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x123113330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1231135f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123113ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123113fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123114440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1231148e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123114d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123115220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1231156c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123115b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123116000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1231164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123116940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123116de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123117280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123117540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123117a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123117f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123118960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123118e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1231192a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123119740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123119be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12311a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12311a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12311a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12311ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12311b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12311b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12311bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12311c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12311c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12311c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12311cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12311d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12311d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12311db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12311e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12311e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12311e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12311edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12311f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12311f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12311fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x123120120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x123120670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x123120bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x123121110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x123121660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x123121bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x123122100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x123122650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x123122ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1231230f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x123123640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x123123b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1231240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x123124630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x123124b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1231250d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x123125620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123125b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1231260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x123126610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123126b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1231270b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123127600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123127b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123118470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123127fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123128770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123128cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x123129210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123129760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123129cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12312a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12312a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12312aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12312b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12312b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12312bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12312c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12312c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12312cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12312d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12312d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12312da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12312df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12312e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12312e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12312ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12312f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12312f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12312fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12312ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123130400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1231308a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123130d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1231311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x123131680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123131b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123131fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x123132460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123132900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123132da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x123133240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1231336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123133b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123134020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1231344c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123134960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123134e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1231352a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123135740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x123135be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x123136080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123136520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1231369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123136e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123137300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1231377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123137c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1231380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123138580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123138a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123138ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x123139360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123139800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123139ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12313a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12313a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12313aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12313af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12313b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12313b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12313bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12313c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12313c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12313cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12313cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12313d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12313d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12313dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12313e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12313e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12313eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12313efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12313f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12313f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12313fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123140260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123140700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123140ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123141040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1231414e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123141980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123141e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1231422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x123142760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123142c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1231430a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123143540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1231439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123143e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1231443d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123144920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123144e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1231453c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x123145860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123145d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1231461a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x123146640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x123146ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x123146f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1231474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123147970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x123147e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1231482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123148750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123148bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123149090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1231498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123149e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12314a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12314a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12314ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12314b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12314b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12314bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12314c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12314c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12314ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12314d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12314d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12314df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12314e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12314eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12314f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12314f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12314fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123150170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123150720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123150cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123151280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123151830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123151de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123152390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123152940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123152ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1231534a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123153a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123154000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1231545b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123154b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123155110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1231556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x123155c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x123156220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1231567d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x123156d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x123157330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1231578e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123157e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x123158440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1231589f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x123158fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x123159550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x123159b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12315a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12315a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12315ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12315b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12315b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12315bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12315c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12315c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12315ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12315d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12315d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12315de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12315e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12315e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12315ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12315f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12315f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12315fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123160190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123160690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123160b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123161090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123161590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123161a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123161f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x123162490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x123162990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x123162e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x123163390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x123163890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x123163d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x123164290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x123164790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x123164c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x123165190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123165690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1231660a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1231667c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123166ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123167600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1231678c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x123168050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1231684f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123168990 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.096.515 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.519 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12270b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12270bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12270e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12270ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12270eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12270f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12270fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x122710230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1227106d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x122710b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x122711010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1227112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x122711cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x122712470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x122712c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1227133a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x122713ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1227141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x122714900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1227152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1227159d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1227160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x122716810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x122716f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x122717650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x122717af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x122717f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x122718430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1227188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x122718d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x122719210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1227196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x122719970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x122719e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12271a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12271a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12271abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12271b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12271b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12271b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12271be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12271c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12271c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12271cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12271d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12271d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12271da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12271ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12271e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12271e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12271ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12271f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12271f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12271fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12271ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1227203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x122720870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x122720d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x122721260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x122721700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x122721ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x122722040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1227224e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x122722980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x122722e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1227232c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x122723760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x122723c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1227240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x122724540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1227249e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x122724e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x122725320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x122725870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x122725dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x122726310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x122726860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x122726db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x122727300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x122727850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x122727da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1227282f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x122728840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x122728d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1227292e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x122729830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x122729d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12272a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12272a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12272ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12272b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12272b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12272bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12272c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12272c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12272cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12272d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12272d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12272dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12272e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12272e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12272ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12272f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12272f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12272fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x122730270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1227307c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x122730d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x122731260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1227317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x122731d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x122732250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1227327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x122732c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1227330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x122733580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x122733a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x122733ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x122734360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x122734800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x122734ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x122735140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1227355e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x122735a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x122735f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1227363c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x122736860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x122736d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1227371a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x122737640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x122737ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x122737f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x122738420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1227388c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x122738d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x122739200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1227396a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x122739b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x122739fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12273a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12273a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12273adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12273b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12273b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12273bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12273c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12273c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12273c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12273ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12273d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12273d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12273dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12273e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12273e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12273e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12273ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12273f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12273f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12273fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x122740100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1227405a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x122740a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x122740ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x122741380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x122741820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x122741cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1227423d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x122742690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x122742b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x122743090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x122743590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x122743a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x122743f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x122744490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x122744990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x122744e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x122745390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x122745890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x122745d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x122746290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x122746790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x122746c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x122747190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x122747690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x122747b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x122748090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x122748590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x122748a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x122748f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x122749490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x122749990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x122749e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12274a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12274a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12274ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12274b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12274b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12274bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12274c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12274c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12274ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12274d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12274d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12274dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12274e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12274e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12274ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12274f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12274f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12274fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x122750450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x122750710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x122750cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x122751270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x122751820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x122751dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x122752380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x122752930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x122752ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x122753490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x122753a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x122753ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1227545a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x122754b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x122755100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1227556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x122755c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x122756210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1227567c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x122756d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x122757320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1227578d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x122757e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x122758430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1227589e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x122758f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x122759540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x122759af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12275a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12275a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12275ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12275b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12275b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12275bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12275c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12275c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12275ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12275d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12275d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12275df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12275e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12275ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12275f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12275f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12275fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x122760150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x122760700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x122760cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x122761260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x122761810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x122761dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x122762370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x122762920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x122762ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x122763480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x122763a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x122763fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x122764590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x122764a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x122764f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x122765490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x122765990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x122765e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x122766390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x122766890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x122766d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x122767290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x122767790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x122767c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x122768190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x122768690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x122768b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x122769090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x122769590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x122769a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x122769f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12276a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12276a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12276ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12276b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12276b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12276bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12276c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12276cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12276d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12276dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12276e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12276e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12276ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12276f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12276f590 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12314a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123153760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123152650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12314f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12314cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12315bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123159810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1231575f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1231553d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12314d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12314aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12314fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123150f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1231564e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1231531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12315aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12314dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12314ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x123155f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x123158150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1231509e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123151af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123157040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123153d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1231542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12314e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12314f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12315c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123159dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12314ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123154e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12314a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12314c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12315cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1231520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123165950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12315a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123150430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123152c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123156a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12314e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123158700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12314d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12315b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123158cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123154870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12315d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12314bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12315d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12314b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12315ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123155980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x123157ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12315a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x123159260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x123151540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1231138b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x123168c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123168f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1231691d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x123169490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123169750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x123169a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123169cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123169f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12316a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12316a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12316a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12316aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12316ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12316b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12316b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12316b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12316b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12316bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12316bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12316c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12316c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12316c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12316c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12316cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12316ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12316d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12316d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12316d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12316d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12316dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12316ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12316e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12316e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12316e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12316e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12316ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12316ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12316f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12316f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12316f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12316fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12316fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12316ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123170290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x123170550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123170810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123170ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x123170d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x123171050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x123171310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1231715d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x123171890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x123171b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x123171e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1231720d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x123172390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123172650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123172910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123172bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123172e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x123173150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123173410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1231736d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123173990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123173c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123173f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1231741d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123174490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123174750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123174a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x123174cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x123174f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123175250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123175510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1231757d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123175a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123175d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x123176010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1231762d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123176590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123176850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123176b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123176dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123177090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x123177350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123177610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1231778d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x123177b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123177e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123178110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1231783d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123178690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123178950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123178c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123178ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123179190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123179450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123179710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1231799d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123179c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123179f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12317a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12317a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12317a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12317aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12317ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12317afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12317b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12317b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12317b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12317bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12317bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12317c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12317c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12317c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12317c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12317cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12317ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12317d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12317d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12317d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12317d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12317dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12317de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12317e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12317e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12317e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12317e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12317ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12317f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12317f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12317fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12317fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123180370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123180810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123180cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x123181200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123181750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123181ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1231821f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1231824b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123182770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x123182c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x123183170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x123183670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x123183b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123184090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123184640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x123184b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123185050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123185740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123185a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123185f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1231869b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123186c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123187230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1231877f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123187db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123188370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123188930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123188ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1231894b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123189a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12318a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12318a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12318abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12318b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12318b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12318bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12318c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12318c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12318ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12318d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12318d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12318df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12318e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12318eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12318f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12318f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12318fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1231901f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1231907b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123190d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123191330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1231918f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123191eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123192470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x123192a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x123192ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1231935b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x123193b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x123194130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1231946f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x123194cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123195270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x123195830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x123195df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1231963b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x123196970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x123196f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1231974f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x123197ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x123198070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123198630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x123198bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1231991b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x123199770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x123199d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12319a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12319a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12319ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12319b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12319b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12319bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12319c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12319c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12319cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12319d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12319d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12319db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12319e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12319e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12319ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12319ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12319f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12319f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12319fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1231a0370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x1231a0870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x1231a0d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1231a1270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1231a1770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1231a1c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1231a2170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1231a2670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1231a2b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1231a3580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1231a3ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1231a43c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1231a4ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1231a4da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1231a5530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1231a57f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1231a5d00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.907s
user	0m0.230s
sys	0m0.144s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.46 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.49 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.95 sec*proc (2 tests)

Total Test time (real) =   1.96 sec
        1.99 real         0.51 user         0.30 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.25 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.31 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.55 sec*proc (2 tests)

Total Test time (real) =   0.56 sec
        0.57 real         0.13 user         0.08 sys
```
