### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.27 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.14 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.55 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.28 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.10 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.21 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.88 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.89 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  194.80 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.88 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.03 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.32 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.25 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 255.80 sec*proc (29 tests)

Total Test time (real) = 255.81 sec

real	4m15.957s
user	8m38.603s
sys	0m7.109s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.16 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.83 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.18 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.19 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.42 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   31.08 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.37 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.08 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.20 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  55.02 sec*proc (29 tests)

Total Test time (real) =  55.03 sec

real	0m55.040s
user	1m17.896s
sys	0m6.439s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.135 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.091 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.371 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.379 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.382 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.383 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.383 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.384 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.385 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.386 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.387 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.387 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.388 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.389 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.392 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.392 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.393 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.394 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.395 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.395 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.396 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.024.928 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.026.112 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.115 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.026.115 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.026.116 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.026.116 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.026.117 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.026.117 I llama_model_loader: - type  f32:  124 tensors
0.00.026.118 I llama_model_loader: - type  f16:   73 tensors
0.00.026.118 I print_info: file format = GGUF V3 (latest)
0.00.026.119 I print_info: file type   = F16
0.00.026.122 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.030.383 I load: special tokens cache size = 5
0.00.032.551 I load: token to piece cache size = 0.2032 MB
0.00.032.580 I print_info: arch             = bert
0.00.032.582 I print_info: vocab_only       = 0
0.00.032.582 I print_info: n_ctx_train      = 512
0.00.032.582 I print_info: n_embd           = 384
0.00.032.582 I print_info: n_layer          = 12
0.00.032.586 I print_info: n_head           = 12
0.00.032.587 I print_info: n_head_kv        = 12
0.00.032.587 I print_info: n_rot            = 32
0.00.032.587 I print_info: n_swa            = 0
0.00.032.587 I print_info: n_embd_head_k    = 32
0.00.032.588 I print_info: n_embd_head_v    = 32
0.00.032.589 I print_info: n_gqa            = 1
0.00.032.589 I print_info: n_embd_k_gqa     = 384
0.00.032.590 I print_info: n_embd_v_gqa     = 384
0.00.032.591 I print_info: f_norm_eps       = 1.0e-12
0.00.032.592 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.032.592 I print_info: f_clamp_kqv      = 0.0e+00
0.00.032.592 I print_info: f_max_alibi_bias = 0.0e+00
0.00.032.593 I print_info: f_logit_scale    = 0.0e+00
0.00.032.594 I print_info: n_ff             = 1536
0.00.032.594 I print_info: n_expert         = 0
0.00.032.594 I print_info: n_expert_used    = 0
0.00.032.594 I print_info: causal attn      = 0
0.00.032.595 I print_info: pooling type     = 2
0.00.032.595 I print_info: rope type        = 2
0.00.032.595 I print_info: rope scaling     = linear
0.00.032.596 I print_info: freq_base_train  = 10000.0
0.00.032.596 I print_info: freq_scale_train = 1
0.00.032.596 I print_info: n_ctx_orig_yarn  = 512
0.00.032.599 I print_info: rope_finetuned   = unknown
0.00.032.599 I print_info: ssm_d_conv       = 0
0.00.032.599 I print_info: ssm_d_inner      = 0
0.00.032.600 I print_info: ssm_d_state      = 0
0.00.032.600 I print_info: ssm_dt_rank      = 0
0.00.032.600 I print_info: ssm_dt_b_c_rms   = 0
0.00.032.600 I print_info: model type       = 33M
0.00.032.601 I print_info: model params     = 33.21 M
0.00.032.601 I print_info: general.name     = Bge Small
0.00.032.602 I print_info: vocab type       = WPM
0.00.032.602 I print_info: n_vocab          = 30522
0.00.032.602 I print_info: n_merges         = 0
0.00.032.603 I print_info: BOS token        = 101 '[CLS]'
0.00.032.603 I print_info: UNK token        = 100 '[UNK]'
0.00.032.609 I print_info: SEP token        = 102 '[SEP]'
0.00.032.609 I print_info: PAD token        = 0 '[PAD]'
0.00.032.610 I print_info: MASK token       = 103 '[MASK]'
0.00.032.610 I print_info: LF token         = 0 '[PAD]'
0.00.032.610 I print_info: max token length = 21
0.00.032.611 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.035.907 I load_tensors: offloading 12 repeating layers to GPU
0.00.035.908 I load_tensors: offloading output layer to GPU
0.00.035.909 I load_tensors: offloaded 13/13 layers to GPU
0.00.035.932 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.035.934 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.036.195 I llama_context: n_seq_max     = 1
0.00.036.197 I llama_context: n_ctx         = 512
0.00.036.197 I llama_context: n_ctx_per_seq = 512
0.00.036.198 I llama_context: n_batch       = 2048
0.00.036.198 I llama_context: n_ubatch      = 2048
0.00.036.198 I llama_context: flash_attn    = 0
0.00.036.199 I llama_context: freq_base     = 10000.0
0.00.036.199 I llama_context: freq_scale    = 1
0.00.036.199 I ggml_metal_init: allocating
0.00.036.204 I ggml_metal_init: found device: Apple M4
0.00.036.209 I ggml_metal_init: picking default device: Apple M4
0.00.036.963 I ggml_metal_init: using embedded metal library
0.00.040.969 I ggml_metal_init: GPU name:   Apple M4
0.00.040.971 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.040.972 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.040.972 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.040.973 I ggml_metal_init: simdgroup reduction   = true
0.00.040.973 I ggml_metal_init: simdgroup matrix mul. = true
0.00.040.973 I ggml_metal_init: has residency sets    = true
0.00.040.973 I ggml_metal_init: has bfloat            = true
0.00.040.973 I ggml_metal_init: use bfloat            = true
0.00.040.974 I ggml_metal_init: hasUnifiedMemory      = true
0.00.040.974 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.052.600 I llama_context:        CPU  output buffer size =     0.00 MiB
0.00.052.603 I init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.053.299 I init:      Metal KV buffer size =     9.00 MiB
0.00.053.301 I llama_context_kv_self: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.054.465 I init:      Metal compute buffer size =    16.00 MiB
0.00.054.466 I init:        CPU compute buffer size =     2.51 MiB
0.00.054.467 I init: graph nodes  = 429
0.00.054.467 I init: graph splits = 2
0.00.054.469 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.054.469 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.060.151 I 
0.00.060.167 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.060.812 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.065.944 I llama_perf_context_print:        load time =      45.05 ms
0.00.065.945 I llama_perf_context_print: prompt eval time =       5.00 ms /     9 tokens (    0.56 ms per token,  1800.72 tokens per second)
0.00.065.945 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.065.946 I llama_perf_context_print:       total time =       5.79 ms /    10 tokens
0.00.066.156 I ggml_metal_free: deallocating

real	0m0.251s
user	0m0.048s
sys	0m0.031s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.045 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.428 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.173 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.177 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.178 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.179 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.179 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.180 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.180 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.181 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.181 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.182 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.182 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.182 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.184 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.185 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.185 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.185 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.186 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.186 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.646 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.369 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.370 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.370 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.371 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.371 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.371 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.371 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.372 I llama_model_loader: - type  f32:  124 tensors
0.00.015.372 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.373 I print_info: file format = GGUF V3 (latest)
0.00.015.373 I print_info: file type   = Q8_0
0.00.015.375 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.880 I load: special tokens cache size = 5
0.00.019.171 I load: token to piece cache size = 0.2032 MB
0.00.019.180 I print_info: arch             = bert
0.00.019.181 I print_info: vocab_only       = 0
0.00.019.181 I print_info: n_ctx_train      = 512
0.00.019.181 I print_info: n_embd           = 384
0.00.019.182 I print_info: n_layer          = 12
0.00.019.185 I print_info: n_head           = 12
0.00.019.185 I print_info: n_head_kv        = 12
0.00.019.185 I print_info: n_rot            = 32
0.00.019.186 I print_info: n_swa            = 0
0.00.019.186 I print_info: n_embd_head_k    = 32
0.00.019.186 I print_info: n_embd_head_v    = 32
0.00.019.186 I print_info: n_gqa            = 1
0.00.019.190 I print_info: n_embd_k_gqa     = 384
0.00.019.190 I print_info: n_embd_v_gqa     = 384
0.00.019.191 I print_info: f_norm_eps       = 1.0e-12
0.00.019.192 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.192 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.192 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.192 I print_info: f_logit_scale    = 0.0e+00
0.00.019.192 I print_info: n_ff             = 1536
0.00.019.193 I print_info: n_expert         = 0
0.00.019.193 I print_info: n_expert_used    = 0
0.00.019.193 I print_info: causal attn      = 0
0.00.019.194 I print_info: pooling type     = 2
0.00.019.195 I print_info: rope type        = 2
0.00.019.195 I print_info: rope scaling     = linear
0.00.019.195 I print_info: freq_base_train  = 10000.0
0.00.019.195 I print_info: freq_scale_train = 1
0.00.019.195 I print_info: n_ctx_orig_yarn  = 512
0.00.019.196 I print_info: rope_finetuned   = unknown
0.00.019.197 I print_info: ssm_d_conv       = 0
0.00.019.197 I print_info: ssm_d_inner      = 0
0.00.019.197 I print_info: ssm_d_state      = 0
0.00.019.197 I print_info: ssm_dt_rank      = 0
0.00.019.197 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.197 I print_info: model type       = 33M
0.00.019.198 I print_info: model params     = 33.21 M
0.00.019.198 I print_info: general.name     = Bge Small
0.00.019.200 I print_info: vocab type       = WPM
0.00.019.200 I print_info: n_vocab          = 30522
0.00.019.200 I print_info: n_merges         = 0
0.00.019.200 I print_info: BOS token        = 101 '[CLS]'
0.00.019.200 I print_info: UNK token        = 100 '[UNK]'
0.00.019.200 I print_info: SEP token        = 102 '[SEP]'
0.00.019.202 I print_info: PAD token        = 0 '[PAD]'
0.00.019.202 I print_info: MASK token       = 103 '[MASK]'
0.00.019.202 I print_info: LF token         = 0 '[PAD]'
0.00.019.202 I print_info: max token length = 21
0.00.019.203 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.932 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.933 I load_tensors: offloading output layer to GPU
0.00.020.934 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.940 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.940 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.112 I llama_context: n_seq_max     = 1
0.00.021.113 I llama_context: n_ctx         = 512
0.00.021.113 I llama_context: n_ctx_per_seq = 512
0.00.021.113 I llama_context: n_batch       = 2048
0.00.021.113 I llama_context: n_ubatch      = 2048
0.00.021.114 I llama_context: flash_attn    = 0
0.00.021.114 I llama_context: freq_base     = 10000.0
0.00.021.114 I llama_context: freq_scale    = 1
0.00.021.115 I ggml_metal_init: allocating
0.00.021.118 I ggml_metal_init: found device: Apple M4
0.00.021.122 I ggml_metal_init: picking default device: Apple M4
0.00.021.637 I ggml_metal_init: using embedded metal library
0.00.024.264 I ggml_metal_init: GPU name:   Apple M4
0.00.024.266 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.266 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.267 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.267 I ggml_metal_init: simdgroup reduction   = true
0.00.024.267 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.267 I ggml_metal_init: has residency sets    = true
0.00.024.267 I ggml_metal_init: has bfloat            = true
0.00.024.268 I ggml_metal_init: use bfloat            = true
0.00.024.268 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.269 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.769 I llama_context:        CPU  output buffer size =     0.00 MiB
0.00.034.771 I init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.379 I init:      Metal KV buffer size =     9.00 MiB
0.00.035.381 I llama_context_kv_self: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.036.373 I init:      Metal compute buffer size =    16.00 MiB
0.00.036.374 I init:        CPU compute buffer size =     2.51 MiB
0.00.036.374 I init: graph nodes  = 429
0.00.036.374 I init: graph splits = 2
0.00.036.376 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.376 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.524 I 
0.00.040.542 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.069 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.538 I llama_perf_context_print:        load time =      31.09 ms
0.00.045.539 I llama_perf_context_print: prompt eval time =       4.35 ms /     9 tokens (    0.48 ms per token,  2069.92 tokens per second)
0.00.045.540 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.540 I llama_perf_context_print:       total time =       5.01 ms /    10 tokens
0.00.045.808 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.254 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.675 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.343 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.348 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.350 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.351 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.353 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.354 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.355 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.356 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.357 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.358 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.358 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.359 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.362 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.363 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.364 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.364 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.365 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.043.252 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.379 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.713 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.715 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.715 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.716 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.716 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.716 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.717 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.717 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.718 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.718 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.718 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.049.719 I llama_model_loader: - type  f32:   40 tensors
0.00.049.719 I llama_model_loader: - type  f16:   30 tensors
0.00.049.720 I print_info: file format = GGUF V3 (latest)
0.00.049.721 I print_info: file type   = F16
0.00.049.722 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.053.915 W load: empty token at index 5
0.00.059.129 W load: model vocab missing newline token, using special_pad_id instead
0.00.060.725 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.060.759 I load: special tokens cache size = 5
0.00.326.857 I load: token to piece cache size = 1.5060 MB
0.00.326.891 I print_info: arch             = jina-bert-v2
0.00.326.892 I print_info: vocab_only       = 0
0.00.326.892 I print_info: n_ctx_train      = 8192
0.00.326.893 I print_info: n_embd           = 384
0.00.326.893 I print_info: n_layer          = 4
0.00.326.900 I print_info: n_head           = 12
0.00.326.900 I print_info: n_head_kv        = 12
0.00.326.900 I print_info: n_rot            = 32
0.00.326.900 I print_info: n_swa            = 0
0.00.326.901 I print_info: n_embd_head_k    = 32
0.00.326.901 I print_info: n_embd_head_v    = 32
0.00.326.901 I print_info: n_gqa            = 1
0.00.326.902 I print_info: n_embd_k_gqa     = 384
0.00.326.911 I print_info: n_embd_v_gqa     = 384
0.00.326.912 I print_info: f_norm_eps       = 1.0e-12
0.00.326.913 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.326.914 I print_info: f_clamp_kqv      = 0.0e+00
0.00.326.914 I print_info: f_max_alibi_bias = 8.0e+00
0.00.326.914 I print_info: f_logit_scale    = 0.0e+00
0.00.326.914 I print_info: n_ff             = 1536
0.00.326.915 I print_info: n_expert         = 0
0.00.326.915 I print_info: n_expert_used    = 0
0.00.326.915 I print_info: causal attn      = 0
0.00.326.915 I print_info: pooling type     = -1
0.00.326.915 I print_info: rope type        = -1
0.00.326.915 I print_info: rope scaling     = linear
0.00.326.916 I print_info: freq_base_train  = 10000.0
0.00.326.916 I print_info: freq_scale_train = 1
0.00.326.916 I print_info: n_ctx_orig_yarn  = 8192
0.00.326.916 I print_info: rope_finetuned   = unknown
0.00.326.916 I print_info: ssm_d_conv       = 0
0.00.326.916 I print_info: ssm_d_inner      = 0
0.00.326.917 I print_info: ssm_d_state      = 0
0.00.326.917 I print_info: ssm_dt_rank      = 0
0.00.326.917 I print_info: ssm_dt_b_c_rms   = 0
0.00.326.917 I print_info: model type       = 33M
0.00.326.918 I print_info: model params     = 32.90 M
0.00.326.918 I print_info: general.name     = Jina Bert Implementation
0.00.326.919 I print_info: vocab type       = BPE
0.00.326.920 I print_info: n_vocab          = 61056
0.00.326.920 I print_info: n_merges         = 39382
0.00.326.920 I print_info: BOS token        = 0 '<s>'
0.00.326.920 I print_info: EOS token        = 2 '</s>'
0.00.326.921 I print_info: UNK token        = 3 '<unk>'
0.00.326.921 I print_info: SEP token        = 2 '</s>'
0.00.326.921 I print_info: PAD token        = 1 '<pad>'
0.00.326.922 I print_info: MASK token       = 4 '<mask>'
0.00.326.922 I print_info: EOG token        = 2 '</s>'
0.00.326.922 I print_info: max token length = 45
0.00.326.923 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.329.094 I load_tensors: offloading 4 repeating layers to GPU
0.00.329.095 I load_tensors: offloading output layer to GPU
0.00.329.095 I load_tensors: offloaded 5/5 layers to GPU
0.00.329.119 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.329.120 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.329.416 I llama_context: n_seq_max     = 1
0.00.329.417 I llama_context: n_ctx         = 8192
0.00.329.417 I llama_context: n_ctx_per_seq = 8192
0.00.329.417 I llama_context: n_batch       = 2048
0.00.329.417 I llama_context: n_ubatch      = 2048
0.00.329.418 I llama_context: flash_attn    = 0
0.00.329.418 I llama_context: freq_base     = 10000.0
0.00.329.419 I llama_context: freq_scale    = 1
0.00.329.419 I ggml_metal_init: allocating
0.00.329.427 I ggml_metal_init: found device: Apple M4
0.00.329.431 I ggml_metal_init: picking default device: Apple M4
0.00.330.146 I ggml_metal_init: using embedded metal library
0.00.332.645 I ggml_metal_init: GPU name:   Apple M4
0.00.332.647 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.332.647 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.332.647 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.332.648 I ggml_metal_init: simdgroup reduction   = true
0.00.332.648 I ggml_metal_init: simdgroup matrix mul. = true
0.00.332.648 I ggml_metal_init: has residency sets    = true
0.00.332.648 I ggml_metal_init: has bfloat            = true
0.00.332.648 I ggml_metal_init: use bfloat            = true
0.00.332.649 I ggml_metal_init: hasUnifiedMemory      = true
0.00.332.649 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.341.962 I llama_context:        CPU  output buffer size =     0.00 MiB
0.00.341.964 I init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.344.938 I init:      Metal KV buffer size =    48.00 MiB
0.00.344.940 I llama_context_kv_self: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.351.230 I init:      Metal compute buffer size =   220.01 MiB
0.00.351.231 I init:        CPU compute buffer size =    22.02 MiB
0.00.351.232 I init: graph nodes  = 154
0.00.351.232 I init: graph splits = 2
0.00.351.233 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.351.233 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.358.727 I 
0.00.358.743 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.358.841 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.358.842 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.358.845 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.358.845 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.358.847 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.358.848 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.359.367 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.363.067 I llama_perf_context_print:        load time =     335.05 ms
0.00.363.068 I llama_perf_context_print: prompt eval time =       3.69 ms /    62 tokens (    0.06 ms per token, 16797.62 tokens per second)
0.00.363.068 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.363.069 I llama_perf_context_print:       total time =       4.34 ms /    63 tokens
0.00.363.541 I ggml_metal_free: deallocating

real	0m1.069s
user	0m0.333s
sys	0m0.046s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.123 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.291 I main: llama backend init
0.00.000.297 I main: load the model and apply lora adapter, if any
0.00.224.205 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.236.319 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.236.330 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.236.332 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.236.333 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.236.334 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.236.335 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.236.335 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.236.337 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.236.338 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.236.338 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.236.339 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.236.339 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.236.340 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.236.341 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.236.344 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.236.345 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.236.345 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.243.366 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.245.563 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.252.451 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.252.457 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.252.457 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.252.458 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.252.458 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.252.460 I llama_model_loader: - type  f32:  194 tensors
0.00.252.460 I llama_model_loader: - type  f16:   98 tensors
0.00.252.461 I print_info: file format = GGUF V3 (latest)
0.00.252.462 I print_info: file type   = all F32 (guessed)
0.00.252.468 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.262.405 I load: special tokens cache size = 25
0.00.268.682 I load: token to piece cache size = 0.2984 MB
0.00.268.706 I print_info: arch             = gptneox
0.00.268.707 I print_info: vocab_only       = 0
0.00.268.707 I print_info: n_ctx_train      = 2048
0.00.268.707 I print_info: n_embd           = 2048
0.00.268.707 I print_info: n_layer          = 24
0.00.268.712 I print_info: n_head           = 16
0.00.268.712 I print_info: n_head_kv        = 16
0.00.268.712 I print_info: n_rot            = 32
0.00.268.713 I print_info: n_swa            = 0
0.00.268.713 I print_info: n_embd_head_k    = 128
0.00.268.713 I print_info: n_embd_head_v    = 128
0.00.268.713 I print_info: n_gqa            = 1
0.00.268.714 I print_info: n_embd_k_gqa     = 2048
0.00.268.715 I print_info: n_embd_v_gqa     = 2048
0.00.268.715 I print_info: f_norm_eps       = 1.0e-05
0.00.268.716 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.268.716 I print_info: f_clamp_kqv      = 0.0e+00
0.00.268.716 I print_info: f_max_alibi_bias = 0.0e+00
0.00.268.716 I print_info: f_logit_scale    = 0.0e+00
0.00.268.717 I print_info: n_ff             = 8192
0.00.268.717 I print_info: n_expert         = 0
0.00.268.717 I print_info: n_expert_used    = 0
0.00.268.717 I print_info: causal attn      = 1
0.00.268.717 I print_info: pooling type     = 0
0.00.268.717 I print_info: rope type        = 2
0.00.268.717 I print_info: rope scaling     = linear
0.00.268.718 I print_info: freq_base_train  = 10000.0
0.00.268.718 I print_info: freq_scale_train = 1
0.00.268.718 I print_info: n_ctx_orig_yarn  = 2048
0.00.268.718 I print_info: rope_finetuned   = unknown
0.00.268.718 I print_info: ssm_d_conv       = 0
0.00.268.719 I print_info: ssm_d_inner      = 0
0.00.268.719 I print_info: ssm_d_state      = 0
0.00.268.719 I print_info: ssm_dt_rank      = 0
0.00.268.719 I print_info: ssm_dt_b_c_rms   = 0
0.00.268.719 I print_info: model type       = 1.4B
0.00.268.720 I print_info: model params     = 1.41 B
0.00.268.720 I print_info: general.name     = 1.4B
0.00.268.720 I print_info: vocab type       = BPE
0.00.268.720 I print_info: n_vocab          = 50304
0.00.268.720 I print_info: n_merges         = 50009
0.00.268.721 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.268.721 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.268.721 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.268.721 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.268.721 I print_info: LF token         = 187 'Ċ'
0.00.268.722 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.268.722 I print_info: max token length = 1024
0.00.268.722 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.304.142 I load_tensors: offloading 24 repeating layers to GPU
0.00.304.145 I load_tensors: offloading output layer to GPU
0.00.304.146 I load_tensors: offloaded 25/25 layers to GPU
0.00.304.169 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.304.170 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.304.547 I llama_context: n_seq_max     = 1
0.00.304.548 I llama_context: n_ctx         = 2048
0.00.304.548 I llama_context: n_ctx_per_seq = 2048
0.00.304.548 I llama_context: n_batch       = 2048
0.00.304.548 I llama_context: n_ubatch      = 512
0.00.304.549 I llama_context: flash_attn    = 0
0.00.304.549 I llama_context: freq_base     = 10000.0
0.00.304.549 I llama_context: freq_scale    = 1
0.00.304.550 I ggml_metal_init: allocating
0.00.304.567 I ggml_metal_init: found device: Apple M4
0.00.304.571 I ggml_metal_init: picking default device: Apple M4
0.00.305.216 I ggml_metal_init: using embedded metal library
0.00.354.212 I ggml_metal_init: GPU name:   Apple M4
0.00.354.214 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.354.215 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.354.215 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.354.215 I ggml_metal_init: simdgroup reduction   = true
0.00.354.216 I ggml_metal_init: simdgroup matrix mul. = true
0.00.354.216 I ggml_metal_init: has residency sets    = true
0.00.354.216 I ggml_metal_init: has bfloat            = true
0.00.354.216 I ggml_metal_init: use bfloat            = true
0.00.354.217 I ggml_metal_init: hasUnifiedMemory      = true
0.00.354.217 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.384.988 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.384.991 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.415.030 I init:      Metal KV buffer size =   384.00 MiB
0.00.415.036 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.419.047 I init:      Metal compute buffer size =   102.25 MiB
0.00.419.049 I init:        CPU compute buffer size =     8.01 MiB
0.00.419.049 I init: graph nodes  = 967
0.00.419.050 I init: graph splits = 2
0.00.419.053 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.419.182 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.419.182 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.486.373 I main: llama threadpool init, n_threads = 4
0.00.486.411 I 
0.00.486.425 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.486.425 I 
0.00.486.469 I sampler seed: 1234
0.00.486.474 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.486.498 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.486.501 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.486.501 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.332.090 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51862.67 tokens per second)
0.02.332.091 I llama_perf_context_print:        load time =     261.19 ms
0.02.332.092 I llama_perf_context_print: prompt eval time =      43.64 ms /     7 tokens (    6.23 ms per token,   160.39 tokens per second)
0.02.332.093 I llama_perf_context_print:        eval time =    1799.17 ms /    63 runs   (   28.56 ms per token,    35.02 tokens per second)
0.02.332.093 I llama_perf_context_print:       total time =    1846.69 ms /    70 tokens
0.02.335.163 I ggml_metal_free: deallocating

real	0m2.644s
user	0m0.122s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.682 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.692 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.120 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.126 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.134 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.134 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.135 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.135 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.136 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.137 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.138 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.138 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.139 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.139 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.140 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.141 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.144 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.145 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.146 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.592 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.719 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.009 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.058.011 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.012 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.012 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.012 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.013 I llama_model_loader: - type  f32:  194 tensors
0.00.058.014 I llama_model_loader: - type  f16:   98 tensors
0.00.058.014 I print_info: file format = GGUF V3 (latest)
0.00.058.015 I print_info: file type   = all F32 (guessed)
0.00.058.017 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.070.534 I load: special tokens cache size = 25
0.00.078.766 I load: token to piece cache size = 0.2984 MB
0.00.078.781 I print_info: arch             = gptneox
0.00.078.783 I print_info: vocab_only       = 0
0.00.078.783 I print_info: n_ctx_train      = 2048
0.00.078.783 I print_info: n_embd           = 2048
0.00.078.783 I print_info: n_layer          = 24
0.00.078.786 I print_info: n_head           = 16
0.00.078.787 I print_info: n_head_kv        = 16
0.00.078.787 I print_info: n_rot            = 32
0.00.078.788 I print_info: n_swa            = 0
0.00.078.788 I print_info: n_embd_head_k    = 128
0.00.078.788 I print_info: n_embd_head_v    = 128
0.00.078.789 I print_info: n_gqa            = 1
0.00.078.790 I print_info: n_embd_k_gqa     = 2048
0.00.078.792 I print_info: n_embd_v_gqa     = 2048
0.00.078.793 I print_info: f_norm_eps       = 1.0e-05
0.00.078.795 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.795 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.795 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.795 I print_info: f_logit_scale    = 0.0e+00
0.00.078.796 I print_info: n_ff             = 8192
0.00.078.796 I print_info: n_expert         = 0
0.00.078.796 I print_info: n_expert_used    = 0
0.00.078.797 I print_info: causal attn      = 1
0.00.078.797 I print_info: pooling type     = 0
0.00.078.797 I print_info: rope type        = 2
0.00.078.797 I print_info: rope scaling     = linear
0.00.078.797 I print_info: freq_base_train  = 10000.0
0.00.078.798 I print_info: freq_scale_train = 1
0.00.078.798 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.799 I print_info: rope_finetuned   = unknown
0.00.078.799 I print_info: ssm_d_conv       = 0
0.00.078.803 I print_info: ssm_d_inner      = 0
0.00.078.803 I print_info: ssm_d_state      = 0
0.00.078.803 I print_info: ssm_dt_rank      = 0
0.00.078.803 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.803 I print_info: model type       = 1.4B
0.00.078.805 I print_info: model params     = 1.41 B
0.00.078.805 I print_info: general.name     = 1.4B
0.00.078.806 I print_info: vocab type       = BPE
0.00.078.806 I print_info: n_vocab          = 50304
0.00.078.806 I print_info: n_merges         = 50009
0.00.078.806 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.807 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.807 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.807 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.807 I print_info: LF token         = 187 'Ċ'
0.00.078.808 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.808 I print_info: max token length = 1024
0.00.078.808 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.391.057 I load_tensors: offloading 24 repeating layers to GPU
0.01.391.062 I load_tensors: offloading output layer to GPU
0.01.391.062 I load_tensors: offloaded 25/25 layers to GPU
0.01.391.088 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.391.090 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.391.900 I llama_context: n_seq_max     = 1
0.01.391.901 I llama_context: n_ctx         = 128
0.01.391.901 I llama_context: n_ctx_per_seq = 128
0.01.391.902 I llama_context: n_batch       = 128
0.01.391.902 I llama_context: n_ubatch      = 128
0.01.391.903 I llama_context: flash_attn    = 0
0.01.391.903 I llama_context: freq_base     = 10000.0
0.01.391.903 I llama_context: freq_scale    = 1
0.01.391.904 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.391.904 I ggml_metal_init: allocating
0.01.392.016 I ggml_metal_init: found device: Apple M4
0.01.392.022 I ggml_metal_init: picking default device: Apple M4
0.01.393.236 I ggml_metal_init: using embedded metal library
0.01.397.057 I ggml_metal_init: GPU name:   Apple M4
0.01.397.059 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.397.060 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.397.060 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.397.060 I ggml_metal_init: simdgroup reduction   = true
0.01.397.061 I ggml_metal_init: simdgroup matrix mul. = true
0.01.397.061 I ggml_metal_init: has residency sets    = true
0.01.397.061 I ggml_metal_init: has bfloat            = true
0.01.397.061 I ggml_metal_init: use bfloat            = true
0.01.397.062 I ggml_metal_init: hasUnifiedMemory      = true
0.01.397.062 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.408.053 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.408.056 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.409.799 I init:      Metal KV buffer size =    24.00 MiB
0.01.409.802 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.411.450 I init:      Metal compute buffer size =    25.56 MiB
0.01.411.451 I init:        CPU compute buffer size =     1.06 MiB
0.01.411.451 I init: graph nodes  = 967
0.01.411.452 I init: graph splits = 2
0.01.411.453 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.411.453 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.445.940 I 
0.01.445.965 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.445.968 I perplexity: tokenizing the input ..
0.01.450.996 I perplexity: tokenization took 5.025 ms
0.01.450.999 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.569.321 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.570.630 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.570.659 I llama_perf_context_print:        load time =    1421.24 ms
0.01.570.660 I llama_perf_context_print: prompt eval time =     118.05 ms /   128 tokens (    0.92 ms per token,  1084.26 tokens per second)
0.01.570.660 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.570.661 I llama_perf_context_print:       total time =     124.72 ms /   129 tokens
0.01.571.169 I ggml_metal_free: deallocating

real	0m1.756s
user	0m0.099s
sys	0m0.265s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.061 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.107 I main: llama backend init
0.00.000.109 I main: load the model and apply lora adapter, if any
0.00.009.542 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.485 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.492 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.494 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.501 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.501 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.501 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.502 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.503 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.503 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.504 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.504 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.504 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.505 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.505 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.508 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.508 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.509 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.332 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.342 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.188 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.189 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.190 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.190 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.190 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.191 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.191 I llama_model_loader: - type  f32:  194 tensors
0.00.038.191 I llama_model_loader: - type q8_0:   98 tensors
0.00.038.192 I print_info: file format = GGUF V3 (latest)
0.00.038.192 I print_info: file type   = Q8_0
0.00.038.193 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.151 I load: special tokens cache size = 25
0.00.053.986 I load: token to piece cache size = 0.2984 MB
0.00.054.001 I print_info: arch             = gptneox
0.00.054.002 I print_info: vocab_only       = 0
0.00.054.003 I print_info: n_ctx_train      = 2048
0.00.054.003 I print_info: n_embd           = 2048
0.00.054.003 I print_info: n_layer          = 24
0.00.054.008 I print_info: n_head           = 16
0.00.054.009 I print_info: n_head_kv        = 16
0.00.054.010 I print_info: n_rot            = 32
0.00.054.011 I print_info: n_swa            = 0
0.00.054.013 I print_info: n_embd_head_k    = 128
0.00.054.013 I print_info: n_embd_head_v    = 128
0.00.054.014 I print_info: n_gqa            = 1
0.00.054.015 I print_info: n_embd_k_gqa     = 2048
0.00.054.016 I print_info: n_embd_v_gqa     = 2048
0.00.054.016 I print_info: f_norm_eps       = 1.0e-05
0.00.054.017 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.017 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.017 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.018 I print_info: f_logit_scale    = 0.0e+00
0.00.054.018 I print_info: n_ff             = 8192
0.00.054.020 I print_info: n_expert         = 0
0.00.054.020 I print_info: n_expert_used    = 0
0.00.054.020 I print_info: causal attn      = 1
0.00.054.020 I print_info: pooling type     = 0
0.00.054.020 I print_info: rope type        = 2
0.00.054.020 I print_info: rope scaling     = linear
0.00.054.021 I print_info: freq_base_train  = 10000.0
0.00.054.021 I print_info: freq_scale_train = 1
0.00.054.021 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.022 I print_info: rope_finetuned   = unknown
0.00.054.022 I print_info: ssm_d_conv       = 0
0.00.054.022 I print_info: ssm_d_inner      = 0
0.00.054.022 I print_info: ssm_d_state      = 0
0.00.054.022 I print_info: ssm_dt_rank      = 0
0.00.054.022 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.022 I print_info: model type       = 1.4B
0.00.054.023 I print_info: model params     = 1.41 B
0.00.054.024 I print_info: general.name     = 1.4B
0.00.054.025 I print_info: vocab type       = BPE
0.00.054.026 I print_info: n_vocab          = 50304
0.00.054.026 I print_info: n_merges         = 50009
0.00.054.026 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.026 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.026 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.030 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.031 I print_info: LF token         = 187 'Ċ'
0.00.054.031 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.031 I print_info: max token length = 1024
0.00.054.032 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.339.367 I load_tensors: offloading 24 repeating layers to GPU
0.01.339.373 I load_tensors: offloading output layer to GPU
0.01.339.374 I load_tensors: offloaded 25/25 layers to GPU
0.01.339.400 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.339.403 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.340.800 I llama_context: n_seq_max     = 1
0.01.340.802 I llama_context: n_ctx         = 2048
0.01.340.802 I llama_context: n_ctx_per_seq = 2048
0.01.340.803 I llama_context: n_batch       = 2048
0.01.340.803 I llama_context: n_ubatch      = 512
0.01.340.803 I llama_context: flash_attn    = 0
0.01.340.804 I llama_context: freq_base     = 10000.0
0.01.340.805 I llama_context: freq_scale    = 1
0.01.340.806 I ggml_metal_init: allocating
0.01.340.818 I ggml_metal_init: found device: Apple M4
0.01.340.827 I ggml_metal_init: picking default device: Apple M4
0.01.342.154 I ggml_metal_init: using embedded metal library
0.01.347.912 I ggml_metal_init: GPU name:   Apple M4
0.01.347.915 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.347.916 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.347.917 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.347.917 I ggml_metal_init: simdgroup reduction   = true
0.01.347.917 I ggml_metal_init: simdgroup matrix mul. = true
0.01.347.918 I ggml_metal_init: has residency sets    = true
0.01.347.918 I ggml_metal_init: has bfloat            = true
0.01.347.918 I ggml_metal_init: use bfloat            = true
0.01.347.919 I ggml_metal_init: hasUnifiedMemory      = true
0.01.347.920 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.363.094 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.363.098 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.415.240 I init:      Metal KV buffer size =   384.00 MiB
0.01.415.251 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.419.362 I init:      Metal compute buffer size =   102.25 MiB
0.01.419.365 I init:        CPU compute buffer size =     8.01 MiB
0.01.419.365 I init: graph nodes  = 967
0.01.419.366 I init: graph splits = 2
0.01.419.371 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.419.487 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.419.487 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.473.964 I main: llama threadpool init, n_threads = 4
0.01.474.010 I 
0.01.474.028 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.474.028 I 
0.01.474.204 I sampler seed: 1234
0.01.474.209 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.474.229 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.474.229 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.474.229 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.559.134 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55468.75 tokens per second)
0.02.559.135 I llama_perf_context_print:        load time =    1463.73 ms
0.02.559.136 I llama_perf_context_print: prompt eval time =      48.99 ms /     7 tokens (    7.00 ms per token,   142.88 tokens per second)
0.02.559.137 I llama_perf_context_print:        eval time =    1033.07 ms /    63 runs   (   16.40 ms per token,    60.98 tokens per second)
0.02.559.137 I llama_perf_context_print:       total time =    1085.86 ms /    70 tokens
0.02.562.363 I ggml_metal_free: deallocating

real	0m2.578s
user	0m0.108s
sys	0m0.291s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.143 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.624 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.630 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.632 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.638 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.638 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.639 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.639 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.640 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.640 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.640 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.641 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.641 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.641 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.642 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.644 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.644 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.645 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.590 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.680 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.568 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.569 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.570 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.570 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.570 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.571 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.571 I llama_model_loader: - type  f32:  194 tensors
0.00.025.571 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.572 I print_info: file format = GGUF V3 (latest)
0.00.025.573 I print_info: file type   = Q8_0
0.00.025.574 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.428 I load: special tokens cache size = 25
0.00.040.582 I load: token to piece cache size = 0.2984 MB
0.00.040.599 I print_info: arch             = gptneox
0.00.040.600 I print_info: vocab_only       = 0
0.00.040.600 I print_info: n_ctx_train      = 2048
0.00.040.600 I print_info: n_embd           = 2048
0.00.040.600 I print_info: n_layer          = 24
0.00.040.604 I print_info: n_head           = 16
0.00.040.605 I print_info: n_head_kv        = 16
0.00.040.605 I print_info: n_rot            = 32
0.00.040.605 I print_info: n_swa            = 0
0.00.040.605 I print_info: n_embd_head_k    = 128
0.00.040.605 I print_info: n_embd_head_v    = 128
0.00.040.606 I print_info: n_gqa            = 1
0.00.040.607 I print_info: n_embd_k_gqa     = 2048
0.00.040.607 I print_info: n_embd_v_gqa     = 2048
0.00.040.608 I print_info: f_norm_eps       = 1.0e-05
0.00.040.608 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.608 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.608 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.610 I print_info: f_logit_scale    = 0.0e+00
0.00.040.610 I print_info: n_ff             = 8192
0.00.040.611 I print_info: n_expert         = 0
0.00.040.611 I print_info: n_expert_used    = 0
0.00.040.611 I print_info: causal attn      = 1
0.00.040.611 I print_info: pooling type     = 0
0.00.040.612 I print_info: rope type        = 2
0.00.040.612 I print_info: rope scaling     = linear
0.00.040.612 I print_info: freq_base_train  = 10000.0
0.00.040.612 I print_info: freq_scale_train = 1
0.00.040.613 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.615 I print_info: rope_finetuned   = unknown
0.00.040.615 I print_info: ssm_d_conv       = 0
0.00.040.615 I print_info: ssm_d_inner      = 0
0.00.040.615 I print_info: ssm_d_state      = 0
0.00.040.615 I print_info: ssm_dt_rank      = 0
0.00.040.615 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.616 I print_info: model type       = 1.4B
0.00.040.616 I print_info: model params     = 1.41 B
0.00.040.616 I print_info: general.name     = 1.4B
0.00.040.617 I print_info: vocab type       = BPE
0.00.040.617 I print_info: n_vocab          = 50304
0.00.040.618 I print_info: n_merges         = 50009
0.00.040.618 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.618 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.619 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.619 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.619 I print_info: LF token         = 187 'Ċ'
0.00.040.619 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.621 I print_info: max token length = 1024
0.00.040.621 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.909.099 I load_tensors: offloading 24 repeating layers to GPU
0.00.909.105 I load_tensors: offloading output layer to GPU
0.00.909.106 I load_tensors: offloaded 25/25 layers to GPU
0.00.909.138 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.909.140 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.910.519 I llama_context: n_seq_max     = 1
0.00.910.521 I llama_context: n_ctx         = 128
0.00.910.521 I llama_context: n_ctx_per_seq = 128
0.00.910.522 I llama_context: n_batch       = 128
0.00.910.522 I llama_context: n_ubatch      = 128
0.00.910.522 I llama_context: flash_attn    = 0
0.00.910.524 I llama_context: freq_base     = 10000.0
0.00.910.524 I llama_context: freq_scale    = 1
0.00.910.525 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.910.526 I ggml_metal_init: allocating
0.00.910.605 I ggml_metal_init: found device: Apple M4
0.00.910.616 I ggml_metal_init: picking default device: Apple M4
0.00.912.152 I ggml_metal_init: using embedded metal library
0.00.917.426 I ggml_metal_init: GPU name:   Apple M4
0.00.917.429 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.917.430 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.917.430 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.917.431 I ggml_metal_init: simdgroup reduction   = true
0.00.917.431 I ggml_metal_init: simdgroup matrix mul. = true
0.00.917.431 I ggml_metal_init: has residency sets    = true
0.00.917.432 I ggml_metal_init: has bfloat            = true
0.00.917.432 I ggml_metal_init: use bfloat            = true
0.00.917.433 I ggml_metal_init: hasUnifiedMemory      = true
0.00.917.435 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.932.626 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.932.629 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.935.180 I init:      Metal KV buffer size =    24.00 MiB
0.00.935.183 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.937.521 I init:      Metal compute buffer size =    25.56 MiB
0.00.937.523 I init:        CPU compute buffer size =     1.06 MiB
0.00.937.523 I init: graph nodes  = 967
0.00.937.524 I init: graph splits = 2
0.00.937.526 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.937.526 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.961.711 I 
0.00.961.746 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.961.751 I perplexity: tokenizing the input ..
0.00.967.969 I perplexity: tokenization took 6.217 ms
0.00.967.977 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.105.261 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.106.597 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.106.623 I llama_perf_context_print:        load time =     952.56 ms
0.01.106.624 I llama_perf_context_print: prompt eval time =     136.87 ms /   128 tokens (    1.07 ms per token,   935.19 tokens per second)
0.01.106.625 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.106.625 I llama_perf_context_print:       total time =     144.91 ms /   129 tokens
0.01.107.231 I ggml_metal_free: deallocating

real	0m1.122s
user	0m0.076s
sys	0m0.168s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.010.087 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.844 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.025.851 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.854 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.855 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.855 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.855 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.856 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.857 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.857 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.857 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.858 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.858 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.859 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.860 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.861 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.862 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.862 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.765 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.834 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.767 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.768 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.769 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.769 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.769 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.769 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.034.770 I llama_model_loader: - type  f32:  194 tensors
0.00.034.770 I llama_model_loader: - type q4_0:   97 tensors
0.00.034.770 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.771 I print_info: file format = GGUF V3 (latest)
0.00.034.772 I print_info: file type   = Q4_0
0.00.034.773 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.043.488 I load: special tokens cache size = 25
0.00.049.663 I load: token to piece cache size = 0.2984 MB
0.00.049.673 I print_info: arch             = gptneox
0.00.049.674 I print_info: vocab_only       = 0
0.00.049.674 I print_info: n_ctx_train      = 2048
0.00.049.675 I print_info: n_embd           = 2048
0.00.049.675 I print_info: n_layer          = 24
0.00.049.678 I print_info: n_head           = 16
0.00.049.679 I print_info: n_head_kv        = 16
0.00.049.679 I print_info: n_rot            = 32
0.00.049.680 I print_info: n_swa            = 0
0.00.049.680 I print_info: n_embd_head_k    = 128
0.00.049.680 I print_info: n_embd_head_v    = 128
0.00.049.681 I print_info: n_gqa            = 1
0.00.049.681 I print_info: n_embd_k_gqa     = 2048
0.00.049.682 I print_info: n_embd_v_gqa     = 2048
0.00.049.683 I print_info: f_norm_eps       = 1.0e-05
0.00.049.685 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.685 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.685 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.685 I print_info: f_logit_scale    = 0.0e+00
0.00.049.686 I print_info: n_ff             = 8192
0.00.049.686 I print_info: n_expert         = 0
0.00.049.686 I print_info: n_expert_used    = 0
0.00.049.687 I print_info: causal attn      = 1
0.00.049.687 I print_info: pooling type     = 0
0.00.049.689 I print_info: rope type        = 2
0.00.049.689 I print_info: rope scaling     = linear
0.00.049.689 I print_info: freq_base_train  = 10000.0
0.00.049.689 I print_info: freq_scale_train = 1
0.00.049.690 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.690 I print_info: rope_finetuned   = unknown
0.00.049.690 I print_info: ssm_d_conv       = 0
0.00.049.690 I print_info: ssm_d_inner      = 0
0.00.049.690 I print_info: ssm_d_state      = 0
0.00.049.690 I print_info: ssm_dt_rank      = 0
0.00.049.690 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.691 I print_info: model type       = 1.4B
0.00.049.691 I print_info: model params     = 1.41 B
0.00.049.691 I print_info: general.name     = 1.4B
0.00.049.692 I print_info: vocab type       = BPE
0.00.049.692 I print_info: n_vocab          = 50304
0.00.049.692 I print_info: n_merges         = 50009
0.00.049.696 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.696 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.696 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.696 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.697 I print_info: LF token         = 187 'Ċ'
0.00.049.697 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.698 I print_info: max token length = 1024
0.00.049.698 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.776.818 I load_tensors: offloading 24 repeating layers to GPU
0.00.776.835 I load_tensors: offloading output layer to GPU
0.00.776.835 I load_tensors: offloaded 25/25 layers to GPU
0.00.776.868 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.776.869 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.778.326 I llama_context: n_seq_max     = 1
0.00.778.335 I llama_context: n_ctx         = 2048
0.00.778.335 I llama_context: n_ctx_per_seq = 2048
0.00.778.336 I llama_context: n_batch       = 2048
0.00.778.337 I llama_context: n_ubatch      = 512
0.00.778.337 I llama_context: flash_attn    = 0
0.00.778.339 I llama_context: freq_base     = 10000.0
0.00.778.340 I llama_context: freq_scale    = 1
0.00.778.342 I ggml_metal_init: allocating
0.00.778.412 I ggml_metal_init: found device: Apple M4
0.00.778.425 I ggml_metal_init: picking default device: Apple M4
0.00.780.215 I ggml_metal_init: using embedded metal library
0.00.784.981 I ggml_metal_init: GPU name:   Apple M4
0.00.784.988 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.784.989 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.784.990 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.784.990 I ggml_metal_init: simdgroup reduction   = true
0.00.784.991 I ggml_metal_init: simdgroup matrix mul. = true
0.00.784.991 I ggml_metal_init: has residency sets    = true
0.00.784.991 I ggml_metal_init: has bfloat            = true
0.00.784.992 I ggml_metal_init: use bfloat            = true
0.00.784.993 I ggml_metal_init: hasUnifiedMemory      = true
0.00.784.996 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.799.094 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.799.098 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.832.827 I init:      Metal KV buffer size =   384.00 MiB
0.00.832.834 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.837.702 I init:      Metal compute buffer size =   102.25 MiB
0.00.837.705 I init:        CPU compute buffer size =     8.01 MiB
0.00.837.706 I init: graph nodes  = 967
0.00.837.706 I init: graph splits = 2
0.00.837.712 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.837.842 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.837.843 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.893.226 I main: llama threadpool init, n_threads = 4
0.00.893.267 I 
0.00.893.282 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.893.282 I 
0.00.893.462 I sampler seed: 1234
0.00.893.466 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.893.478 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.893.478 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.893.478 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.576.785 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50823.19 tokens per second)
0.01.576.786 I llama_perf_context_print:        load time =     882.40 ms
0.01.576.786 I llama_perf_context_print: prompt eval time =      49.07 ms /     7 tokens (    7.01 ms per token,   142.66 tokens per second)
0.01.576.787 I llama_perf_context_print:        eval time =     631.36 ms /    63 runs   (   10.02 ms per token,    99.78 tokens per second)
0.01.576.787 I llama_perf_context_print:       total time =     684.30 ms /    70 tokens
0.01.580.437 I ggml_metal_free: deallocating

real	0m1.598s
user	0m0.104s
sys	0m0.195s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.331 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.733 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.739 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.745 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.746 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.746 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.747 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.747 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.748 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.748 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.749 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.749 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.749 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.750 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.751 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.754 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.754 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.755 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.672 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.767 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.685 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.687 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.687 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.688 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.688 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.689 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.689 I llama_model_loader: - type  f32:  194 tensors
0.00.026.690 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.690 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.691 I print_info: file format = GGUF V3 (latest)
0.00.026.691 I print_info: file type   = Q4_0
0.00.026.692 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.250 I load: special tokens cache size = 25
0.00.041.268 I load: token to piece cache size = 0.2984 MB
0.00.041.286 I print_info: arch             = gptneox
0.00.041.286 I print_info: vocab_only       = 0
0.00.041.287 I print_info: n_ctx_train      = 2048
0.00.041.287 I print_info: n_embd           = 2048
0.00.041.287 I print_info: n_layer          = 24
0.00.041.291 I print_info: n_head           = 16
0.00.041.291 I print_info: n_head_kv        = 16
0.00.041.292 I print_info: n_rot            = 32
0.00.041.292 I print_info: n_swa            = 0
0.00.041.292 I print_info: n_embd_head_k    = 128
0.00.041.292 I print_info: n_embd_head_v    = 128
0.00.041.296 I print_info: n_gqa            = 1
0.00.041.296 I print_info: n_embd_k_gqa     = 2048
0.00.041.297 I print_info: n_embd_v_gqa     = 2048
0.00.041.297 I print_info: f_norm_eps       = 1.0e-05
0.00.041.302 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.303 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.303 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.303 I print_info: f_logit_scale    = 0.0e+00
0.00.041.304 I print_info: n_ff             = 8192
0.00.041.304 I print_info: n_expert         = 0
0.00.041.304 I print_info: n_expert_used    = 0
0.00.041.304 I print_info: causal attn      = 1
0.00.041.304 I print_info: pooling type     = 0
0.00.041.304 I print_info: rope type        = 2
0.00.041.305 I print_info: rope scaling     = linear
0.00.041.305 I print_info: freq_base_train  = 10000.0
0.00.041.305 I print_info: freq_scale_train = 1
0.00.041.305 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.306 I print_info: rope_finetuned   = unknown
0.00.041.306 I print_info: ssm_d_conv       = 0
0.00.041.306 I print_info: ssm_d_inner      = 0
0.00.041.306 I print_info: ssm_d_state      = 0
0.00.041.306 I print_info: ssm_dt_rank      = 0
0.00.041.308 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.308 I print_info: model type       = 1.4B
0.00.041.308 I print_info: model params     = 1.41 B
0.00.041.309 I print_info: general.name     = 1.4B
0.00.041.309 I print_info: vocab type       = BPE
0.00.041.309 I print_info: n_vocab          = 50304
0.00.041.309 I print_info: n_merges         = 50009
0.00.041.309 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.310 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.310 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.310 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.310 I print_info: LF token         = 187 'Ċ'
0.00.041.310 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.311 I print_info: max token length = 1024
0.00.041.311 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.628.133 I load_tensors: offloading 24 repeating layers to GPU
0.00.628.146 I load_tensors: offloading output layer to GPU
0.00.628.147 I load_tensors: offloaded 25/25 layers to GPU
0.00.628.186 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.628.188 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.629.811 I llama_context: n_seq_max     = 1
0.00.629.814 I llama_context: n_ctx         = 128
0.00.629.814 I llama_context: n_ctx_per_seq = 128
0.00.629.815 I llama_context: n_batch       = 128
0.00.629.815 I llama_context: n_ubatch      = 128
0.00.629.816 I llama_context: flash_attn    = 0
0.00.629.818 I llama_context: freq_base     = 10000.0
0.00.629.819 I llama_context: freq_scale    = 1
0.00.629.820 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.629.822 I ggml_metal_init: allocating
0.00.629.923 I ggml_metal_init: found device: Apple M4
0.00.629.936 I ggml_metal_init: picking default device: Apple M4
0.00.631.876 I ggml_metal_init: using embedded metal library
0.00.637.377 I ggml_metal_init: GPU name:   Apple M4
0.00.637.385 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.637.386 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.637.387 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.637.388 I ggml_metal_init: simdgroup reduction   = true
0.00.637.388 I ggml_metal_init: simdgroup matrix mul. = true
0.00.637.388 I ggml_metal_init: has residency sets    = true
0.00.637.389 I ggml_metal_init: has bfloat            = true
0.00.637.389 I ggml_metal_init: use bfloat            = true
0.00.637.390 I ggml_metal_init: hasUnifiedMemory      = true
0.00.637.398 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.657.461 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.657.466 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.661.117 I init:      Metal KV buffer size =    24.00 MiB
0.00.661.121 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.664.374 I init:      Metal compute buffer size =    25.56 MiB
0.00.664.376 I init:        CPU compute buffer size =     1.06 MiB
0.00.664.377 I init: graph nodes  = 967
0.00.664.377 I init: graph splits = 2
0.00.664.387 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.664.387 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.903 I 
0.00.689.957 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.964 I perplexity: tokenizing the input ..
0.00.696.791 I perplexity: tokenization took 6.823 ms
0.00.696.796 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.824.899 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.826.302 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.826.323 I llama_perf_context_print:        load time =     679.56 ms
0.00.826.324 I llama_perf_context_print: prompt eval time =     127.27 ms /   128 tokens (    0.99 ms per token,  1005.73 tokens per second)
0.00.826.325 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.826.325 I llama_perf_context_print:       total time =     136.43 ms /   129 tokens
0.00.826.862 I ggml_metal_free: deallocating

real	0m0.841s
user	0m0.080s
sys	0m0.136s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.016.891 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.881 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.026.885 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.891 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.892 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.892 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.892 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.893 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.893 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.894 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.894 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.894 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.894 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.895 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.895 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.896 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.897 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.897 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.821 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.894 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.056 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.058 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.058 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.059 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.059 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.059 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.036.060 I llama_model_loader: - type  f32:  194 tensors
0.00.036.060 I llama_model_loader: - type q4_1:   97 tensors
0.00.036.060 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.061 I print_info: file format = GGUF V3 (latest)
0.00.036.061 I print_info: file type   = Q4_1
0.00.036.062 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.928 I load: special tokens cache size = 25
0.00.052.084 I load: token to piece cache size = 0.2984 MB
0.00.052.098 I print_info: arch             = gptneox
0.00.052.100 I print_info: vocab_only       = 0
0.00.052.100 I print_info: n_ctx_train      = 2048
0.00.052.100 I print_info: n_embd           = 2048
0.00.052.100 I print_info: n_layer          = 24
0.00.052.103 I print_info: n_head           = 16
0.00.052.104 I print_info: n_head_kv        = 16
0.00.052.104 I print_info: n_rot            = 32
0.00.052.105 I print_info: n_swa            = 0
0.00.052.105 I print_info: n_embd_head_k    = 128
0.00.052.105 I print_info: n_embd_head_v    = 128
0.00.052.108 I print_info: n_gqa            = 1
0.00.052.108 I print_info: n_embd_k_gqa     = 2048
0.00.052.109 I print_info: n_embd_v_gqa     = 2048
0.00.052.110 I print_info: f_norm_eps       = 1.0e-05
0.00.052.110 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.112 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.112 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.112 I print_info: f_logit_scale    = 0.0e+00
0.00.052.113 I print_info: n_ff             = 8192
0.00.052.113 I print_info: n_expert         = 0
0.00.052.113 I print_info: n_expert_used    = 0
0.00.052.113 I print_info: causal attn      = 1
0.00.052.114 I print_info: pooling type     = 0
0.00.052.114 I print_info: rope type        = 2
0.00.052.114 I print_info: rope scaling     = linear
0.00.052.115 I print_info: freq_base_train  = 10000.0
0.00.052.115 I print_info: freq_scale_train = 1
0.00.052.116 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.117 I print_info: rope_finetuned   = unknown
0.00.052.117 I print_info: ssm_d_conv       = 0
0.00.052.117 I print_info: ssm_d_inner      = 0
0.00.052.117 I print_info: ssm_d_state      = 0
0.00.052.117 I print_info: ssm_dt_rank      = 0
0.00.052.117 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.119 I print_info: model type       = 1.4B
0.00.052.119 I print_info: model params     = 1.41 B
0.00.052.119 I print_info: general.name     = 1.4B
0.00.052.120 I print_info: vocab type       = BPE
0.00.052.121 I print_info: n_vocab          = 50304
0.00.052.121 I print_info: n_merges         = 50009
0.00.052.121 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.123 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.123 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.123 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.123 I print_info: LF token         = 187 'Ċ'
0.00.052.124 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.124 I print_info: max token length = 1024
0.00.052.124 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.707.761 I load_tensors: offloading 24 repeating layers to GPU
0.00.707.774 I load_tensors: offloading output layer to GPU
0.00.707.774 I load_tensors: offloaded 25/25 layers to GPU
0.00.707.809 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.707.810 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.709.142 I llama_context: n_seq_max     = 1
0.00.709.145 I llama_context: n_ctx         = 2048
0.00.709.145 I llama_context: n_ctx_per_seq = 2048
0.00.709.146 I llama_context: n_batch       = 2048
0.00.709.147 I llama_context: n_ubatch      = 512
0.00.709.147 I llama_context: flash_attn    = 0
0.00.709.149 I llama_context: freq_base     = 10000.0
0.00.709.149 I llama_context: freq_scale    = 1
0.00.709.152 I ggml_metal_init: allocating
0.00.709.208 I ggml_metal_init: found device: Apple M4
0.00.709.221 I ggml_metal_init: picking default device: Apple M4
0.00.711.090 I ggml_metal_init: using embedded metal library
0.00.717.670 I ggml_metal_init: GPU name:   Apple M4
0.00.717.674 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.717.674 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.717.675 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.717.676 I ggml_metal_init: simdgroup reduction   = true
0.00.717.676 I ggml_metal_init: simdgroup matrix mul. = true
0.00.717.677 I ggml_metal_init: has residency sets    = true
0.00.717.677 I ggml_metal_init: has bfloat            = true
0.00.717.677 I ggml_metal_init: use bfloat            = true
0.00.717.678 I ggml_metal_init: hasUnifiedMemory      = true
0.00.717.680 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.735.628 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.735.632 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.790.703 I init:      Metal KV buffer size =   384.00 MiB
0.00.790.711 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.794.986 I init:      Metal compute buffer size =   102.25 MiB
0.00.794.988 I init:        CPU compute buffer size =     8.01 MiB
0.00.794.988 I init: graph nodes  = 967
0.00.794.988 I init: graph splits = 2
0.00.794.997 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.795.124 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.795.125 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.852.946 I main: llama threadpool init, n_threads = 4
0.00.852.993 I 
0.00.853.008 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.853.009 I 
0.00.853.162 I sampler seed: 1234
0.00.853.167 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.853.177 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.853.178 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.853.178 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.577.187 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58006.54 tokens per second)
0.01.577.188 I llama_perf_context_print:        load time =     835.36 ms
0.01.577.190 I llama_perf_context_print: prompt eval time =      44.60 ms /     7 tokens (    6.37 ms per token,   156.94 tokens per second)
0.01.577.191 I llama_perf_context_print:        eval time =     676.72 ms /    63 runs   (   10.74 ms per token,    93.10 tokens per second)
0.01.577.191 I llama_perf_context_print:       total time =     724.94 ms /    70 tokens
0.01.581.235 I ggml_metal_free: deallocating

real	0m1.603s
user	0m0.112s
sys	0m0.225s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.795 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.066 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.070 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.072 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.072 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.073 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.073 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.073 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.074 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.075 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.075 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.075 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.076 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.076 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.077 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.078 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.078 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.079 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.066 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.181 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.030 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.031 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.032 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.037 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.038 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.038 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.038 I llama_model_loader: - type  f32:  194 tensors
0.00.025.039 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.039 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.039 I print_info: file format = GGUF V3 (latest)
0.00.025.040 I print_info: file type   = Q4_1
0.00.025.041 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.406 I load: special tokens cache size = 25
0.00.039.652 I load: token to piece cache size = 0.2984 MB
0.00.039.670 I print_info: arch             = gptneox
0.00.039.671 I print_info: vocab_only       = 0
0.00.039.671 I print_info: n_ctx_train      = 2048
0.00.039.671 I print_info: n_embd           = 2048
0.00.039.671 I print_info: n_layer          = 24
0.00.039.674 I print_info: n_head           = 16
0.00.039.677 I print_info: n_head_kv        = 16
0.00.039.677 I print_info: n_rot            = 32
0.00.039.677 I print_info: n_swa            = 0
0.00.039.677 I print_info: n_embd_head_k    = 128
0.00.039.677 I print_info: n_embd_head_v    = 128
0.00.039.678 I print_info: n_gqa            = 1
0.00.039.678 I print_info: n_embd_k_gqa     = 2048
0.00.039.679 I print_info: n_embd_v_gqa     = 2048
0.00.039.679 I print_info: f_norm_eps       = 1.0e-05
0.00.039.680 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.680 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.680 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.680 I print_info: f_logit_scale    = 0.0e+00
0.00.039.681 I print_info: n_ff             = 8192
0.00.039.681 I print_info: n_expert         = 0
0.00.039.681 I print_info: n_expert_used    = 0
0.00.039.681 I print_info: causal attn      = 1
0.00.039.681 I print_info: pooling type     = 0
0.00.039.682 I print_info: rope type        = 2
0.00.039.682 I print_info: rope scaling     = linear
0.00.039.682 I print_info: freq_base_train  = 10000.0
0.00.039.682 I print_info: freq_scale_train = 1
0.00.039.683 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.683 I print_info: rope_finetuned   = unknown
0.00.039.683 I print_info: ssm_d_conv       = 0
0.00.039.683 I print_info: ssm_d_inner      = 0
0.00.039.683 I print_info: ssm_d_state      = 0
0.00.039.683 I print_info: ssm_dt_rank      = 0
0.00.039.683 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.684 I print_info: model type       = 1.4B
0.00.039.684 I print_info: model params     = 1.41 B
0.00.039.684 I print_info: general.name     = 1.4B
0.00.039.685 I print_info: vocab type       = BPE
0.00.039.685 I print_info: n_vocab          = 50304
0.00.039.685 I print_info: n_merges         = 50009
0.00.039.685 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.685 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.685 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.685 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.686 I print_info: LF token         = 187 'Ċ'
0.00.039.686 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.686 I print_info: max token length = 1024
0.00.039.687 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.672.183 I load_tensors: offloading 24 repeating layers to GPU
0.00.672.199 I load_tensors: offloading output layer to GPU
0.00.672.199 I load_tensors: offloaded 25/25 layers to GPU
0.00.672.230 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.672.231 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.673.921 I llama_context: n_seq_max     = 1
0.00.673.932 I llama_context: n_ctx         = 128
0.00.673.932 I llama_context: n_ctx_per_seq = 128
0.00.673.933 I llama_context: n_batch       = 128
0.00.673.934 I llama_context: n_ubatch      = 128
0.00.673.934 I llama_context: flash_attn    = 0
0.00.673.936 I llama_context: freq_base     = 10000.0
0.00.673.937 I llama_context: freq_scale    = 1
0.00.673.938 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.673.940 I ggml_metal_init: allocating
0.00.674.046 I ggml_metal_init: found device: Apple M4
0.00.674.061 I ggml_metal_init: picking default device: Apple M4
0.00.676.033 I ggml_metal_init: using embedded metal library
0.00.681.737 I ggml_metal_init: GPU name:   Apple M4
0.00.681.745 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.681.745 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.681.746 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.681.747 I ggml_metal_init: simdgroup reduction   = true
0.00.681.747 I ggml_metal_init: simdgroup matrix mul. = true
0.00.681.748 I ggml_metal_init: has residency sets    = true
0.00.681.748 I ggml_metal_init: has bfloat            = true
0.00.681.748 I ggml_metal_init: use bfloat            = true
0.00.681.749 I ggml_metal_init: hasUnifiedMemory      = true
0.00.681.753 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.699.932 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.699.937 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.703.490 I init:      Metal KV buffer size =    24.00 MiB
0.00.703.495 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.706.990 I init:      Metal compute buffer size =    25.56 MiB
0.00.706.992 I init:        CPU compute buffer size =     1.06 MiB
0.00.706.992 I init: graph nodes  = 967
0.00.706.992 I init: graph splits = 2
0.00.706.997 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.706.997 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.735.152 I 
0.00.735.176 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.735.179 I perplexity: tokenizing the input ..
0.00.739.439 I perplexity: tokenization took 4.258 ms
0.00.739.442 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.868.394 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.869.799 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.869.821 I llama_perf_context_print:        load time =     726.35 ms
0.00.869.822 I llama_perf_context_print: prompt eval time =     128.72 ms /   128 tokens (    1.01 ms per token,   994.39 tokens per second)
0.00.869.822 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.869.823 I llama_perf_context_print:       total time =     134.67 ms /   129 tokens
0.00.870.330 I ggml_metal_free: deallocating

real	0m0.887s
user	0m0.076s
sys	0m0.112s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.648 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.152 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.157 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.158 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.159 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.159 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.160 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.160 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.161 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.161 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.161 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.162 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.164 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.164 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.165 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.167 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.168 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.168 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.847 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.895 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.628 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.629 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.629 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.630 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.630 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.630 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.631 I llama_model_loader: - type  f32:  194 tensors
0.00.027.631 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.631 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.632 I print_info: file format = GGUF V3 (latest)
0.00.027.632 I print_info: file type   = Q5_0
0.00.027.633 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.036.163 I load: special tokens cache size = 25
0.00.042.377 I load: token to piece cache size = 0.2984 MB
0.00.042.392 I print_info: arch             = gptneox
0.00.042.393 I print_info: vocab_only       = 0
0.00.042.393 I print_info: n_ctx_train      = 2048
0.00.042.393 I print_info: n_embd           = 2048
0.00.042.394 I print_info: n_layer          = 24
0.00.042.396 I print_info: n_head           = 16
0.00.042.397 I print_info: n_head_kv        = 16
0.00.042.397 I print_info: n_rot            = 32
0.00.042.397 I print_info: n_swa            = 0
0.00.042.398 I print_info: n_embd_head_k    = 128
0.00.042.398 I print_info: n_embd_head_v    = 128
0.00.042.399 I print_info: n_gqa            = 1
0.00.042.399 I print_info: n_embd_k_gqa     = 2048
0.00.042.400 I print_info: n_embd_v_gqa     = 2048
0.00.042.400 I print_info: f_norm_eps       = 1.0e-05
0.00.042.401 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.401 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.401 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.401 I print_info: f_logit_scale    = 0.0e+00
0.00.042.402 I print_info: n_ff             = 8192
0.00.042.402 I print_info: n_expert         = 0
0.00.042.402 I print_info: n_expert_used    = 0
0.00.042.403 I print_info: causal attn      = 1
0.00.042.403 I print_info: pooling type     = 0
0.00.042.405 I print_info: rope type        = 2
0.00.042.406 I print_info: rope scaling     = linear
0.00.042.406 I print_info: freq_base_train  = 10000.0
0.00.042.407 I print_info: freq_scale_train = 1
0.00.042.407 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.407 I print_info: rope_finetuned   = unknown
0.00.042.407 I print_info: ssm_d_conv       = 0
0.00.042.408 I print_info: ssm_d_inner      = 0
0.00.042.408 I print_info: ssm_d_state      = 0
0.00.042.408 I print_info: ssm_dt_rank      = 0
0.00.042.408 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.408 I print_info: model type       = 1.4B
0.00.042.409 I print_info: model params     = 1.41 B
0.00.042.409 I print_info: general.name     = 1.4B
0.00.042.409 I print_info: vocab type       = BPE
0.00.042.409 I print_info: n_vocab          = 50304
0.00.042.410 I print_info: n_merges         = 50009
0.00.042.410 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.410 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.410 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.410 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.411 I print_info: LF token         = 187 'Ċ'
0.00.042.411 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.411 I print_info: max token length = 1024
0.00.042.411 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.741.393 I load_tensors: offloading 24 repeating layers to GPU
0.00.741.409 I load_tensors: offloading output layer to GPU
0.00.741.410 I load_tensors: offloaded 25/25 layers to GPU
0.00.741.445 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.741.446 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.743.022 I llama_context: n_seq_max     = 1
0.00.743.025 I llama_context: n_ctx         = 2048
0.00.743.026 I llama_context: n_ctx_per_seq = 2048
0.00.743.027 I llama_context: n_batch       = 2048
0.00.743.027 I llama_context: n_ubatch      = 512
0.00.743.027 I llama_context: flash_attn    = 0
0.00.743.029 I llama_context: freq_base     = 10000.0
0.00.743.030 I llama_context: freq_scale    = 1
0.00.743.032 I ggml_metal_init: allocating
0.00.743.109 I ggml_metal_init: found device: Apple M4
0.00.743.123 I ggml_metal_init: picking default device: Apple M4
0.00.745.045 I ggml_metal_init: using embedded metal library
0.00.751.714 I ggml_metal_init: GPU name:   Apple M4
0.00.751.718 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.751.719 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.751.719 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.751.720 I ggml_metal_init: simdgroup reduction   = true
0.00.751.720 I ggml_metal_init: simdgroup matrix mul. = true
0.00.751.720 I ggml_metal_init: has residency sets    = true
0.00.751.720 I ggml_metal_init: has bfloat            = true
0.00.751.721 I ggml_metal_init: use bfloat            = true
0.00.751.722 I ggml_metal_init: hasUnifiedMemory      = true
0.00.751.723 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.770.367 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.770.372 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.826.472 I init:      Metal KV buffer size =   384.00 MiB
0.00.826.481 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.830.695 I init:      Metal compute buffer size =   102.25 MiB
0.00.830.698 I init:        CPU compute buffer size =     8.01 MiB
0.00.830.698 I init: graph nodes  = 967
0.00.830.698 I init: graph splits = 2
0.00.830.704 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.830.832 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.830.833 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.888.961 I main: llama threadpool init, n_threads = 4
0.00.889.005 I 
0.00.889.022 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.889.022 I 
0.00.889.191 I sampler seed: 1234
0.00.889.195 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.889.241 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.889.244 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.889.244 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.689.603 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51900.58 tokens per second)
0.01.689.604 I llama_perf_context_print:        load time =     879.61 ms
0.01.689.605 I llama_perf_context_print: prompt eval time =      53.91 ms /     7 tokens (    7.70 ms per token,   129.84 tokens per second)
0.01.689.605 I llama_perf_context_print:        eval time =     743.56 ms /    63 runs   (   11.80 ms per token,    84.73 tokens per second)
0.01.689.606 I llama_perf_context_print:       total time =     801.34 ms /    70 tokens
0.01.693.443 I ggml_metal_free: deallocating

real	0m1.714s
user	0m0.110s
sys	0m0.213s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.732 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.815 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.820 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.825 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.826 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.826 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.827 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.827 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.828 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.828 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.829 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.829 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.829 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.830 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.830 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.832 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.832 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.832 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.649 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.673 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.589 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.590 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.590 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.590 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.591 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.591 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.592 I llama_model_loader: - type  f32:  194 tensors
0.00.024.592 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.592 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.593 I print_info: file format = GGUF V3 (latest)
0.00.024.593 I print_info: file type   = Q5_0
0.00.024.594 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.779 I load: special tokens cache size = 25
0.00.038.888 I load: token to piece cache size = 0.2984 MB
0.00.038.902 I print_info: arch             = gptneox
0.00.038.903 I print_info: vocab_only       = 0
0.00.038.903 I print_info: n_ctx_train      = 2048
0.00.038.903 I print_info: n_embd           = 2048
0.00.038.903 I print_info: n_layer          = 24
0.00.038.906 I print_info: n_head           = 16
0.00.038.907 I print_info: n_head_kv        = 16
0.00.038.907 I print_info: n_rot            = 32
0.00.038.907 I print_info: n_swa            = 0
0.00.038.908 I print_info: n_embd_head_k    = 128
0.00.038.908 I print_info: n_embd_head_v    = 128
0.00.038.908 I print_info: n_gqa            = 1
0.00.038.909 I print_info: n_embd_k_gqa     = 2048
0.00.038.909 I print_info: n_embd_v_gqa     = 2048
0.00.038.910 I print_info: f_norm_eps       = 1.0e-05
0.00.038.910 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.910 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.911 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.911 I print_info: f_logit_scale    = 0.0e+00
0.00.038.911 I print_info: n_ff             = 8192
0.00.038.911 I print_info: n_expert         = 0
0.00.038.912 I print_info: n_expert_used    = 0
0.00.038.912 I print_info: causal attn      = 1
0.00.038.912 I print_info: pooling type     = 0
0.00.038.912 I print_info: rope type        = 2
0.00.038.912 I print_info: rope scaling     = linear
0.00.038.912 I print_info: freq_base_train  = 10000.0
0.00.038.913 I print_info: freq_scale_train = 1
0.00.038.913 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.913 I print_info: rope_finetuned   = unknown
0.00.038.913 I print_info: ssm_d_conv       = 0
0.00.038.913 I print_info: ssm_d_inner      = 0
0.00.038.913 I print_info: ssm_d_state      = 0
0.00.038.914 I print_info: ssm_dt_rank      = 0
0.00.038.914 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.914 I print_info: model type       = 1.4B
0.00.038.914 I print_info: model params     = 1.41 B
0.00.038.914 I print_info: general.name     = 1.4B
0.00.038.915 I print_info: vocab type       = BPE
0.00.038.915 I print_info: n_vocab          = 50304
0.00.038.915 I print_info: n_merges         = 50009
0.00.038.915 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.916 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.916 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.916 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.916 I print_info: LF token         = 187 'Ċ'
0.00.038.916 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.917 I print_info: max token length = 1024
0.00.038.917 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.726.882 I load_tensors: offloading 24 repeating layers to GPU
0.00.726.889 I load_tensors: offloading output layer to GPU
0.00.726.890 I load_tensors: offloaded 25/25 layers to GPU
0.00.726.912 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.726.914 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.728.050 I llama_context: n_seq_max     = 1
0.00.728.055 I llama_context: n_ctx         = 128
0.00.728.056 I llama_context: n_ctx_per_seq = 128
0.00.728.056 I llama_context: n_batch       = 128
0.00.728.056 I llama_context: n_ubatch      = 128
0.00.728.057 I llama_context: flash_attn    = 0
0.00.728.058 I llama_context: freq_base     = 10000.0
0.00.728.059 I llama_context: freq_scale    = 1
0.00.728.059 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.728.061 I ggml_metal_init: allocating
0.00.728.132 I ggml_metal_init: found device: Apple M4
0.00.728.145 I ggml_metal_init: picking default device: Apple M4
0.00.729.924 I ggml_metal_init: using embedded metal library
0.00.737.069 I ggml_metal_init: GPU name:   Apple M4
0.00.737.075 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.737.076 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.737.077 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.737.077 I ggml_metal_init: simdgroup reduction   = true
0.00.737.078 I ggml_metal_init: simdgroup matrix mul. = true
0.00.737.078 I ggml_metal_init: has residency sets    = true
0.00.737.078 I ggml_metal_init: has bfloat            = true
0.00.737.078 I ggml_metal_init: use bfloat            = true
0.00.737.080 I ggml_metal_init: hasUnifiedMemory      = true
0.00.737.082 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.755.435 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.755.439 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.759.058 I init:      Metal KV buffer size =    24.00 MiB
0.00.759.063 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.762.272 I init:      Metal compute buffer size =    25.56 MiB
0.00.762.274 I init:        CPU compute buffer size =     1.06 MiB
0.00.762.275 I init: graph nodes  = 967
0.00.762.275 I init: graph splits = 2
0.00.762.279 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.762.280 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.792.854 I 
0.00.792.930 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.792.939 I perplexity: tokenizing the input ..
0.00.798.083 I perplexity: tokenization took 5.143 ms
0.00.798.087 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.945.236 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.946.543 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.946.572 I llama_perf_context_print:        load time =     784.11 ms
0.00.946.574 I llama_perf_context_print: prompt eval time =     146.92 ms /   128 tokens (    1.15 ms per token,   871.24 tokens per second)
0.00.946.574 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.946.575 I llama_perf_context_print:       total time =     153.72 ms /   129 tokens
0.00.947.188 I ggml_metal_free: deallocating

real	0m0.961s
user	0m0.077s
sys	0m0.127s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.602 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.116 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.120 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.122 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.127 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.127 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.128 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.128 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.129 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.129 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.129 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.130 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.130 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.131 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.131 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.134 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.134 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.135 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.983 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.008 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.787 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.788 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.788 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.788 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.789 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.789 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.789 I llama_model_loader: - type  f32:  194 tensors
0.00.024.790 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.790 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.791 I print_info: file format = GGUF V3 (latest)
0.00.024.791 I print_info: file type   = Q5_1
0.00.024.792 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.653 I load: special tokens cache size = 25
0.00.038.858 I load: token to piece cache size = 0.2984 MB
0.00.038.872 I print_info: arch             = gptneox
0.00.038.873 I print_info: vocab_only       = 0
0.00.038.873 I print_info: n_ctx_train      = 2048
0.00.038.873 I print_info: n_embd           = 2048
0.00.038.873 I print_info: n_layer          = 24
0.00.038.876 I print_info: n_head           = 16
0.00.038.876 I print_info: n_head_kv        = 16
0.00.038.877 I print_info: n_rot            = 32
0.00.038.877 I print_info: n_swa            = 0
0.00.038.877 I print_info: n_embd_head_k    = 128
0.00.038.877 I print_info: n_embd_head_v    = 128
0.00.038.879 I print_info: n_gqa            = 1
0.00.038.880 I print_info: n_embd_k_gqa     = 2048
0.00.038.881 I print_info: n_embd_v_gqa     = 2048
0.00.038.881 I print_info: f_norm_eps       = 1.0e-05
0.00.038.882 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.882 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.882 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.882 I print_info: f_logit_scale    = 0.0e+00
0.00.038.883 I print_info: n_ff             = 8192
0.00.038.883 I print_info: n_expert         = 0
0.00.038.883 I print_info: n_expert_used    = 0
0.00.038.884 I print_info: causal attn      = 1
0.00.038.884 I print_info: pooling type     = 0
0.00.038.884 I print_info: rope type        = 2
0.00.038.884 I print_info: rope scaling     = linear
0.00.038.886 I print_info: freq_base_train  = 10000.0
0.00.038.886 I print_info: freq_scale_train = 1
0.00.038.886 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.886 I print_info: rope_finetuned   = unknown
0.00.038.887 I print_info: ssm_d_conv       = 0
0.00.038.887 I print_info: ssm_d_inner      = 0
0.00.038.887 I print_info: ssm_d_state      = 0
0.00.038.887 I print_info: ssm_dt_rank      = 0
0.00.038.887 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.887 I print_info: model type       = 1.4B
0.00.038.891 I print_info: model params     = 1.41 B
0.00.038.891 I print_info: general.name     = 1.4B
0.00.038.891 I print_info: vocab type       = BPE
0.00.038.892 I print_info: n_vocab          = 50304
0.00.038.892 I print_info: n_merges         = 50009
0.00.038.892 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.893 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.893 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.893 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.893 I print_info: LF token         = 187 'Ċ'
0.00.038.893 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.894 I print_info: max token length = 1024
0.00.038.894 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.616.701 I load_tensors: offloading 24 repeating layers to GPU
0.00.616.717 I load_tensors: offloading output layer to GPU
0.00.616.718 I load_tensors: offloaded 25/25 layers to GPU
0.00.616.753 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.616.754 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.618.259 I llama_context: n_seq_max     = 1
0.00.618.261 I llama_context: n_ctx         = 2048
0.00.618.262 I llama_context: n_ctx_per_seq = 2048
0.00.618.262 I llama_context: n_batch       = 2048
0.00.618.263 I llama_context: n_ubatch      = 512
0.00.618.263 I llama_context: flash_attn    = 0
0.00.618.266 I llama_context: freq_base     = 10000.0
0.00.618.266 I llama_context: freq_scale    = 1
0.00.618.268 I ggml_metal_init: allocating
0.00.618.373 I ggml_metal_init: found device: Apple M4
0.00.618.390 I ggml_metal_init: picking default device: Apple M4
0.00.620.147 I ggml_metal_init: using embedded metal library
0.00.626.568 I ggml_metal_init: GPU name:   Apple M4
0.00.626.572 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.626.572 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.626.573 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.626.574 I ggml_metal_init: simdgroup reduction   = true
0.00.626.574 I ggml_metal_init: simdgroup matrix mul. = true
0.00.626.575 I ggml_metal_init: has residency sets    = true
0.00.626.575 I ggml_metal_init: has bfloat            = true
0.00.626.575 I ggml_metal_init: use bfloat            = true
0.00.626.576 I ggml_metal_init: hasUnifiedMemory      = true
0.00.626.580 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.404 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.643.408 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.695.914 I init:      Metal KV buffer size =   384.00 MiB
0.00.695.922 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.700.856 I init:      Metal compute buffer size =   102.25 MiB
0.00.700.859 I init:        CPU compute buffer size =     8.01 MiB
0.00.700.859 I init: graph nodes  = 967
0.00.700.859 I init: graph splits = 2
0.00.700.866 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.700.994 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.700.995 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.759.723 I main: llama threadpool init, n_threads = 4
0.00.759.768 I 
0.00.759.784 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.759.784 I 
0.00.759.935 I sampler seed: 1234
0.00.759.940 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.759.951 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.759.951 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.759.951 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.613.258 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50497.87 tokens per second)
0.01.613.258 I llama_perf_context_print:        load time =     750.42 ms
0.01.613.259 I llama_perf_context_print: prompt eval time =      51.92 ms /     7 tokens (    7.42 ms per token,   134.81 tokens per second)
0.01.613.260 I llama_perf_context_print:        eval time =     798.72 ms /    63 runs   (   12.68 ms per token,    78.88 tokens per second)
0.01.613.261 I llama_perf_context_print:       total time =     854.23 ms /    70 tokens
0.01.616.056 I ggml_metal_free: deallocating

real	0m1.631s
user	0m0.107s
sys	0m0.210s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.512 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.866 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.023.871 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.873 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.874 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.874 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.874 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.875 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.876 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.876 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.876 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.877 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.877 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.877 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.878 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.880 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.880 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.880 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.704 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.731 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.633 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.635 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.635 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.635 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.636 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.636 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.032.637 I llama_model_loader: - type  f32:  194 tensors
0.00.032.637 I llama_model_loader: - type q5_1:   97 tensors
0.00.032.637 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.638 I print_info: file format = GGUF V3 (latest)
0.00.032.639 I print_info: file type   = Q5_1
0.00.032.640 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.040.644 I load: special tokens cache size = 25
0.00.046.901 I load: token to piece cache size = 0.2984 MB
0.00.046.920 I print_info: arch             = gptneox
0.00.046.920 I print_info: vocab_only       = 0
0.00.046.921 I print_info: n_ctx_train      = 2048
0.00.046.921 I print_info: n_embd           = 2048
0.00.046.921 I print_info: n_layer          = 24
0.00.046.925 I print_info: n_head           = 16
0.00.046.926 I print_info: n_head_kv        = 16
0.00.046.926 I print_info: n_rot            = 32
0.00.046.926 I print_info: n_swa            = 0
0.00.046.926 I print_info: n_embd_head_k    = 128
0.00.046.926 I print_info: n_embd_head_v    = 128
0.00.046.927 I print_info: n_gqa            = 1
0.00.046.927 I print_info: n_embd_k_gqa     = 2048
0.00.046.928 I print_info: n_embd_v_gqa     = 2048
0.00.046.928 I print_info: f_norm_eps       = 1.0e-05
0.00.046.929 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.046.929 I print_info: f_clamp_kqv      = 0.0e+00
0.00.046.929 I print_info: f_max_alibi_bias = 0.0e+00
0.00.046.929 I print_info: f_logit_scale    = 0.0e+00
0.00.046.931 I print_info: n_ff             = 8192
0.00.046.931 I print_info: n_expert         = 0
0.00.046.932 I print_info: n_expert_used    = 0
0.00.046.932 I print_info: causal attn      = 1
0.00.046.932 I print_info: pooling type     = 0
0.00.046.932 I print_info: rope type        = 2
0.00.046.932 I print_info: rope scaling     = linear
0.00.046.933 I print_info: freq_base_train  = 10000.0
0.00.046.933 I print_info: freq_scale_train = 1
0.00.046.933 I print_info: n_ctx_orig_yarn  = 2048
0.00.046.933 I print_info: rope_finetuned   = unknown
0.00.046.933 I print_info: ssm_d_conv       = 0
0.00.046.934 I print_info: ssm_d_inner      = 0
0.00.046.934 I print_info: ssm_d_state      = 0
0.00.046.934 I print_info: ssm_dt_rank      = 0
0.00.046.934 I print_info: ssm_dt_b_c_rms   = 0
0.00.046.934 I print_info: model type       = 1.4B
0.00.046.935 I print_info: model params     = 1.41 B
0.00.046.935 I print_info: general.name     = 1.4B
0.00.046.935 I print_info: vocab type       = BPE
0.00.046.935 I print_info: n_vocab          = 50304
0.00.046.936 I print_info: n_merges         = 50009
0.00.046.936 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.046.936 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.046.936 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.046.936 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.046.937 I print_info: LF token         = 187 'Ċ'
0.00.046.937 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.046.937 I print_info: max token length = 1024
0.00.046.938 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.615.742 I load_tensors: offloading 24 repeating layers to GPU
0.00.615.756 I load_tensors: offloading output layer to GPU
0.00.615.757 I load_tensors: offloaded 25/25 layers to GPU
0.00.615.795 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.615.796 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.617.222 I llama_context: n_seq_max     = 1
0.00.617.225 I llama_context: n_ctx         = 128
0.00.617.225 I llama_context: n_ctx_per_seq = 128
0.00.617.226 I llama_context: n_batch       = 128
0.00.617.227 I llama_context: n_ubatch      = 128
0.00.617.227 I llama_context: flash_attn    = 0
0.00.617.229 I llama_context: freq_base     = 10000.0
0.00.617.230 I llama_context: freq_scale    = 1
0.00.617.230 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.617.235 I ggml_metal_init: allocating
0.00.617.387 I ggml_metal_init: found device: Apple M4
0.00.617.402 I ggml_metal_init: picking default device: Apple M4
0.00.619.263 I ggml_metal_init: using embedded metal library
0.00.625.671 I ggml_metal_init: GPU name:   Apple M4
0.00.625.677 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.625.678 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.625.678 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.625.679 I ggml_metal_init: simdgroup reduction   = true
0.00.625.680 I ggml_metal_init: simdgroup matrix mul. = true
0.00.625.680 I ggml_metal_init: has residency sets    = true
0.00.625.680 I ggml_metal_init: has bfloat            = true
0.00.625.680 I ggml_metal_init: use bfloat            = true
0.00.625.681 I ggml_metal_init: hasUnifiedMemory      = true
0.00.625.685 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.320 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.643.325 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.646.879 I init:      Metal KV buffer size =    24.00 MiB
0.00.646.883 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.649.983 I init:      Metal compute buffer size =    25.56 MiB
0.00.649.985 I init:        CPU compute buffer size =     1.06 MiB
0.00.649.985 I init: graph nodes  = 967
0.00.649.986 I init: graph splits = 2
0.00.649.989 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.649.989 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.944 I 
0.00.678.004 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.678.011 I perplexity: tokenizing the input ..
0.00.685.695 I perplexity: tokenization took 7.681 ms
0.00.685.704 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.821.389 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.822.728 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.822.752 I llama_perf_context_print:        load time =     661.42 ms
0.00.822.755 I llama_perf_context_print: prompt eval time =     134.79 ms /   128 tokens (    1.05 ms per token,   949.61 tokens per second)
0.00.822.759 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.822.760 I llama_perf_context_print:       total time =     144.81 ms /   129 tokens
0.00.823.363 I ggml_metal_free: deallocating

real	0m0.843s
user	0m0.080s
sys	0m0.136s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.010.103 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.895 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.901 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.903 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.903 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.904 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.904 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.909 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.910 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.910 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.911 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.911 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.912 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.912 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.912 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.914 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.915 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.915 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.937 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.052 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.056 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.059 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.059 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.060 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.060 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.061 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.061 I llama_model_loader: - type  f32:  194 tensors
0.00.026.061 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.062 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.062 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.063 I print_info: file format = GGUF V3 (latest)
0.00.026.063 I print_info: file type   = Q2_K - Medium
0.00.026.064 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.356 I load: special tokens cache size = 25
0.00.040.441 I load: token to piece cache size = 0.2984 MB
0.00.040.460 I print_info: arch             = gptneox
0.00.040.460 I print_info: vocab_only       = 0
0.00.040.461 I print_info: n_ctx_train      = 2048
0.00.040.461 I print_info: n_embd           = 2048
0.00.040.461 I print_info: n_layer          = 24
0.00.040.466 I print_info: n_head           = 16
0.00.040.467 I print_info: n_head_kv        = 16
0.00.040.467 I print_info: n_rot            = 32
0.00.040.467 I print_info: n_swa            = 0
0.00.040.467 I print_info: n_embd_head_k    = 128
0.00.040.467 I print_info: n_embd_head_v    = 128
0.00.040.468 I print_info: n_gqa            = 1
0.00.040.469 I print_info: n_embd_k_gqa     = 2048
0.00.040.469 I print_info: n_embd_v_gqa     = 2048
0.00.040.470 I print_info: f_norm_eps       = 1.0e-05
0.00.040.470 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.470 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.470 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.470 I print_info: f_logit_scale    = 0.0e+00
0.00.040.471 I print_info: n_ff             = 8192
0.00.040.471 I print_info: n_expert         = 0
0.00.040.471 I print_info: n_expert_used    = 0
0.00.040.471 I print_info: causal attn      = 1
0.00.040.472 I print_info: pooling type     = 0
0.00.040.473 I print_info: rope type        = 2
0.00.040.475 I print_info: rope scaling     = linear
0.00.040.475 I print_info: freq_base_train  = 10000.0
0.00.040.475 I print_info: freq_scale_train = 1
0.00.040.476 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.476 I print_info: rope_finetuned   = unknown
0.00.040.477 I print_info: ssm_d_conv       = 0
0.00.040.477 I print_info: ssm_d_inner      = 0
0.00.040.477 I print_info: ssm_d_state      = 0
0.00.040.477 I print_info: ssm_dt_rank      = 0
0.00.040.477 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.478 I print_info: model type       = 1.4B
0.00.040.478 I print_info: model params     = 1.41 B
0.00.040.478 I print_info: general.name     = 1.4B
0.00.040.479 I print_info: vocab type       = BPE
0.00.040.479 I print_info: n_vocab          = 50304
0.00.040.479 I print_info: n_merges         = 50009
0.00.040.479 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.480 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.480 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.480 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.480 I print_info: LF token         = 187 'Ċ'
0.00.040.480 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.481 I print_info: max token length = 1024
0.00.040.481 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.325.464 I load_tensors: offloading 24 repeating layers to GPU
0.00.325.481 I load_tensors: offloading output layer to GPU
0.00.325.481 I load_tensors: offloaded 25/25 layers to GPU
0.00.325.512 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.325.513 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.326.788 I llama_context: n_seq_max     = 1
0.00.326.795 I llama_context: n_ctx         = 2048
0.00.326.795 I llama_context: n_ctx_per_seq = 2048
0.00.326.796 I llama_context: n_batch       = 2048
0.00.326.796 I llama_context: n_ubatch      = 512
0.00.326.797 I llama_context: flash_attn    = 0
0.00.326.798 I llama_context: freq_base     = 10000.0
0.00.326.799 I llama_context: freq_scale    = 1
0.00.326.801 I ggml_metal_init: allocating
0.00.326.882 I ggml_metal_init: found device: Apple M4
0.00.326.896 I ggml_metal_init: picking default device: Apple M4
0.00.328.830 I ggml_metal_init: using embedded metal library
0.00.333.559 I ggml_metal_init: GPU name:   Apple M4
0.00.333.571 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.333.572 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.333.573 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.333.573 I ggml_metal_init: simdgroup reduction   = true
0.00.333.574 I ggml_metal_init: simdgroup matrix mul. = true
0.00.333.574 I ggml_metal_init: has residency sets    = true
0.00.333.575 I ggml_metal_init: has bfloat            = true
0.00.333.575 I ggml_metal_init: use bfloat            = true
0.00.333.577 I ggml_metal_init: hasUnifiedMemory      = true
0.00.333.579 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.350.265 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.350.269 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.398.629 I init:      Metal KV buffer size =   384.00 MiB
0.00.398.636 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.403.632 I init:      Metal compute buffer size =   102.25 MiB
0.00.403.634 I init:        CPU compute buffer size =     8.01 MiB
0.00.403.634 I init: graph nodes  = 967
0.00.403.635 I init: graph splits = 2
0.00.403.641 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.403.769 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.403.770 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.462.938 I main: llama threadpool init, n_threads = 4
0.00.462.989 I 
0.00.463.004 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.463.005 I 
0.00.463.180 I sampler seed: 1234
0.00.463.185 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.463.205 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.463.205 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.463.205 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.150.875 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55081.46 tokens per second)
0.01.150.876 I llama_perf_context_print:        load time =     452.12 ms
0.01.150.877 I llama_perf_context_print: prompt eval time =      44.16 ms /     7 tokens (    6.31 ms per token,   158.53 tokens per second)
0.01.150.878 I llama_perf_context_print:        eval time =     640.73 ms /    63 runs   (   10.17 ms per token,    98.33 tokens per second)
0.01.150.879 I llama_perf_context_print:       total time =     688.65 ms /    70 tokens
0.01.154.647 I ggml_metal_free: deallocating

real	0m1.170s
user	0m0.108s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.018 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.113 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.119 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.120 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.124 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.125 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.125 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.125 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.126 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.126 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.127 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.127 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.127 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.128 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.128 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.130 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.130 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.130 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.260 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.325 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.382 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.386 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.386 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.387 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.387 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.387 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.388 I llama_model_loader: - type  f32:  194 tensors
0.00.025.388 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.388 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.389 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.389 I print_info: file format = GGUF V3 (latest)
0.00.025.390 I print_info: file type   = Q2_K - Medium
0.00.025.391 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.614 I load: special tokens cache size = 25
0.00.040.034 I load: token to piece cache size = 0.2984 MB
0.00.040.052 I print_info: arch             = gptneox
0.00.040.053 I print_info: vocab_only       = 0
0.00.040.054 I print_info: n_ctx_train      = 2048
0.00.040.054 I print_info: n_embd           = 2048
0.00.040.054 I print_info: n_layer          = 24
0.00.040.058 I print_info: n_head           = 16
0.00.040.059 I print_info: n_head_kv        = 16
0.00.040.059 I print_info: n_rot            = 32
0.00.040.059 I print_info: n_swa            = 0
0.00.040.060 I print_info: n_embd_head_k    = 128
0.00.040.060 I print_info: n_embd_head_v    = 128
0.00.040.060 I print_info: n_gqa            = 1
0.00.040.061 I print_info: n_embd_k_gqa     = 2048
0.00.040.062 I print_info: n_embd_v_gqa     = 2048
0.00.040.062 I print_info: f_norm_eps       = 1.0e-05
0.00.040.063 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.063 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.063 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.063 I print_info: f_logit_scale    = 0.0e+00
0.00.040.064 I print_info: n_ff             = 8192
0.00.040.064 I print_info: n_expert         = 0
0.00.040.064 I print_info: n_expert_used    = 0
0.00.040.064 I print_info: causal attn      = 1
0.00.040.065 I print_info: pooling type     = 0
0.00.040.065 I print_info: rope type        = 2
0.00.040.066 I print_info: rope scaling     = linear
0.00.040.068 I print_info: freq_base_train  = 10000.0
0.00.040.068 I print_info: freq_scale_train = 1
0.00.040.068 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.069 I print_info: rope_finetuned   = unknown
0.00.040.069 I print_info: ssm_d_conv       = 0
0.00.040.069 I print_info: ssm_d_inner      = 0
0.00.040.069 I print_info: ssm_d_state      = 0
0.00.040.069 I print_info: ssm_dt_rank      = 0
0.00.040.069 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.069 I print_info: model type       = 1.4B
0.00.040.070 I print_info: model params     = 1.41 B
0.00.040.071 I print_info: general.name     = 1.4B
0.00.040.071 I print_info: vocab type       = BPE
0.00.040.071 I print_info: n_vocab          = 50304
0.00.040.072 I print_info: n_merges         = 50009
0.00.040.072 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.072 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.072 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.072 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.072 I print_info: LF token         = 187 'Ċ'
0.00.040.073 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.073 I print_info: max token length = 1024
0.00.040.073 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.352.337 I load_tensors: offloading 24 repeating layers to GPU
0.00.352.349 I load_tensors: offloading output layer to GPU
0.00.352.349 I load_tensors: offloaded 25/25 layers to GPU
0.00.352.384 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.352.389 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.353.799 I llama_context: n_seq_max     = 1
0.00.353.803 I llama_context: n_ctx         = 128
0.00.353.803 I llama_context: n_ctx_per_seq = 128
0.00.353.804 I llama_context: n_batch       = 128
0.00.353.804 I llama_context: n_ubatch      = 128
0.00.353.805 I llama_context: flash_attn    = 0
0.00.353.806 I llama_context: freq_base     = 10000.0
0.00.353.806 I llama_context: freq_scale    = 1
0.00.353.807 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.353.812 I ggml_metal_init: allocating
0.00.353.901 I ggml_metal_init: found device: Apple M4
0.00.353.914 I ggml_metal_init: picking default device: Apple M4
0.00.355.810 I ggml_metal_init: using embedded metal library
0.00.361.246 I ggml_metal_init: GPU name:   Apple M4
0.00.361.261 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.361.262 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.361.262 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.361.263 I ggml_metal_init: simdgroup reduction   = true
0.00.361.263 I ggml_metal_init: simdgroup matrix mul. = true
0.00.361.264 I ggml_metal_init: has residency sets    = true
0.00.361.264 I ggml_metal_init: has bfloat            = true
0.00.361.264 I ggml_metal_init: use bfloat            = true
0.00.361.266 I ggml_metal_init: hasUnifiedMemory      = true
0.00.361.270 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.383.677 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.383.682 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.387.502 I init:      Metal KV buffer size =    24.00 MiB
0.00.387.508 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.391.250 I init:      Metal compute buffer size =    25.56 MiB
0.00.391.253 I init:        CPU compute buffer size =     1.06 MiB
0.00.391.253 I init: graph nodes  = 967
0.00.391.253 I init: graph splits = 2
0.00.391.257 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.391.257 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.421.736 I 
0.00.421.811 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.421.818 I perplexity: tokenizing the input ..
0.00.428.922 I perplexity: tokenization took 7.1 ms
0.00.428.927 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.562.038 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.563.375 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.563.401 I llama_perf_context_print:        load time =     412.71 ms
0.00.563.402 I llama_perf_context_print: prompt eval time =     132.16 ms /   128 tokens (    1.03 ms per token,   968.52 tokens per second)
0.00.563.402 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.563.403 I llama_perf_context_print:       total time =     141.67 ms /   129 tokens
0.00.563.999 I ggml_metal_free: deallocating

real	0m0.578s
user	0m0.083s
sys	0m0.104s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.134 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.575 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.580 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.582 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.586 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.587 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.587 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.588 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.589 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.589 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.589 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.590 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.590 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.590 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.591 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.595 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.595 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.595 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.374 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.378 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.179 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.180 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.180 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.180 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.181 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.184 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.184 I llama_model_loader: - type  f32:  194 tensors
0.00.025.185 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.185 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.185 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.186 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.186 I print_info: file format = GGUF V3 (latest)
0.00.025.187 I print_info: file type   = Q3_K - Medium
0.00.025.188 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.027 I load: special tokens cache size = 25
0.00.039.069 I load: token to piece cache size = 0.2984 MB
0.00.039.082 I print_info: arch             = gptneox
0.00.039.084 I print_info: vocab_only       = 0
0.00.039.084 I print_info: n_ctx_train      = 2048
0.00.039.084 I print_info: n_embd           = 2048
0.00.039.084 I print_info: n_layer          = 24
0.00.039.087 I print_info: n_head           = 16
0.00.039.088 I print_info: n_head_kv        = 16
0.00.039.088 I print_info: n_rot            = 32
0.00.039.088 I print_info: n_swa            = 0
0.00.039.088 I print_info: n_embd_head_k    = 128
0.00.039.088 I print_info: n_embd_head_v    = 128
0.00.039.089 I print_info: n_gqa            = 1
0.00.039.090 I print_info: n_embd_k_gqa     = 2048
0.00.039.091 I print_info: n_embd_v_gqa     = 2048
0.00.039.091 I print_info: f_norm_eps       = 1.0e-05
0.00.039.092 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.092 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.092 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.092 I print_info: f_logit_scale    = 0.0e+00
0.00.039.093 I print_info: n_ff             = 8192
0.00.039.101 I print_info: n_expert         = 0
0.00.039.102 I print_info: n_expert_used    = 0
0.00.039.104 I print_info: causal attn      = 1
0.00.039.105 I print_info: pooling type     = 0
0.00.039.106 I print_info: rope type        = 2
0.00.039.106 I print_info: rope scaling     = linear
0.00.039.106 I print_info: freq_base_train  = 10000.0
0.00.039.106 I print_info: freq_scale_train = 1
0.00.039.107 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.107 I print_info: rope_finetuned   = unknown
0.00.039.107 I print_info: ssm_d_conv       = 0
0.00.039.107 I print_info: ssm_d_inner      = 0
0.00.039.107 I print_info: ssm_d_state      = 0
0.00.039.107 I print_info: ssm_dt_rank      = 0
0.00.039.107 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.108 I print_info: model type       = 1.4B
0.00.039.109 I print_info: model params     = 1.41 B
0.00.039.109 I print_info: general.name     = 1.4B
0.00.039.110 I print_info: vocab type       = BPE
0.00.039.110 I print_info: n_vocab          = 50304
0.00.039.110 I print_info: n_merges         = 50009
0.00.039.110 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.111 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.111 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.111 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.111 I print_info: LF token         = 187 'Ċ'
0.00.039.111 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.112 I print_info: max token length = 1024
0.00.039.112 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.432.521 I load_tensors: offloading 24 repeating layers to GPU
0.00.432.535 I load_tensors: offloading output layer to GPU
0.00.432.535 I load_tensors: offloaded 25/25 layers to GPU
0.00.432.569 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.432.570 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.434.228 I llama_context: n_seq_max     = 1
0.00.434.230 I llama_context: n_ctx         = 2048
0.00.434.231 I llama_context: n_ctx_per_seq = 2048
0.00.434.232 I llama_context: n_batch       = 2048
0.00.434.232 I llama_context: n_ubatch      = 512
0.00.434.232 I llama_context: flash_attn    = 0
0.00.434.234 I llama_context: freq_base     = 10000.0
0.00.434.235 I llama_context: freq_scale    = 1
0.00.434.239 I ggml_metal_init: allocating
0.00.434.313 I ggml_metal_init: found device: Apple M4
0.00.434.327 I ggml_metal_init: picking default device: Apple M4
0.00.436.294 I ggml_metal_init: using embedded metal library
0.00.442.711 I ggml_metal_init: GPU name:   Apple M4
0.00.442.717 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.442.717 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.442.718 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.442.719 I ggml_metal_init: simdgroup reduction   = true
0.00.442.719 I ggml_metal_init: simdgroup matrix mul. = true
0.00.442.720 I ggml_metal_init: has residency sets    = true
0.00.442.720 I ggml_metal_init: has bfloat            = true
0.00.442.720 I ggml_metal_init: use bfloat            = true
0.00.442.721 I ggml_metal_init: hasUnifiedMemory      = true
0.00.442.723 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.461.251 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.461.255 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.517.922 I init:      Metal KV buffer size =   384.00 MiB
0.00.517.930 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.522.984 I init:      Metal compute buffer size =   102.25 MiB
0.00.522.986 I init:        CPU compute buffer size =     8.01 MiB
0.00.522.986 I init: graph nodes  = 967
0.00.522.987 I init: graph splits = 2
0.00.522.992 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.523.117 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.523.118 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.576.747 I main: llama threadpool init, n_threads = 4
0.00.576.788 I 
0.00.576.803 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.576.803 I 
0.00.576.981 I sampler seed: 1234
0.00.576.986 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.577.028 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.577.031 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.577.031 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.311.787 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53544.49 tokens per second)
0.01.311.788 I llama_perf_context_print:        load time =     566.89 ms
0.01.311.789 I llama_perf_context_print: prompt eval time =      40.43 ms /     7 tokens (    5.78 ms per token,   173.14 tokens per second)
0.01.311.791 I llama_perf_context_print:        eval time =     691.45 ms /    63 runs   (   10.98 ms per token,    91.11 tokens per second)
0.01.311.791 I llama_perf_context_print:       total time =     735.76 ms /    70 tokens
0.01.315.768 I ggml_metal_free: deallocating

real	0m1.335s
user	0m0.109s
sys	0m0.183s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.802 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.154 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.160 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.167 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.167 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.167 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.168 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.168 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.169 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.169 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.170 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.170 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.170 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.170 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.172 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.174 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.174 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.174 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.997 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.006 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.895 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.896 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.897 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.897 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.897 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.898 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.898 I llama_model_loader: - type  f32:  194 tensors
0.00.024.899 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.899 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.899 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.899 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.900 I print_info: file format = GGUF V3 (latest)
0.00.024.900 I print_info: file type   = Q3_K - Medium
0.00.024.902 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.005 I load: special tokens cache size = 25
0.00.039.238 I load: token to piece cache size = 0.2984 MB
0.00.039.255 I print_info: arch             = gptneox
0.00.039.255 I print_info: vocab_only       = 0
0.00.039.256 I print_info: n_ctx_train      = 2048
0.00.039.256 I print_info: n_embd           = 2048
0.00.039.256 I print_info: n_layer          = 24
0.00.039.261 I print_info: n_head           = 16
0.00.039.262 I print_info: n_head_kv        = 16
0.00.039.262 I print_info: n_rot            = 32
0.00.039.262 I print_info: n_swa            = 0
0.00.039.262 I print_info: n_embd_head_k    = 128
0.00.039.263 I print_info: n_embd_head_v    = 128
0.00.039.263 I print_info: n_gqa            = 1
0.00.039.270 I print_info: n_embd_k_gqa     = 2048
0.00.039.273 I print_info: n_embd_v_gqa     = 2048
0.00.039.274 I print_info: f_norm_eps       = 1.0e-05
0.00.039.274 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.274 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.274 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.274 I print_info: f_logit_scale    = 0.0e+00
0.00.039.275 I print_info: n_ff             = 8192
0.00.039.275 I print_info: n_expert         = 0
0.00.039.275 I print_info: n_expert_used    = 0
0.00.039.276 I print_info: causal attn      = 1
0.00.039.276 I print_info: pooling type     = 0
0.00.039.276 I print_info: rope type        = 2
0.00.039.276 I print_info: rope scaling     = linear
0.00.039.277 I print_info: freq_base_train  = 10000.0
0.00.039.277 I print_info: freq_scale_train = 1
0.00.039.277 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.277 I print_info: rope_finetuned   = unknown
0.00.039.277 I print_info: ssm_d_conv       = 0
0.00.039.278 I print_info: ssm_d_inner      = 0
0.00.039.278 I print_info: ssm_d_state      = 0
0.00.039.278 I print_info: ssm_dt_rank      = 0
0.00.039.278 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.278 I print_info: model type       = 1.4B
0.00.039.279 I print_info: model params     = 1.41 B
0.00.039.279 I print_info: general.name     = 1.4B
0.00.039.279 I print_info: vocab type       = BPE
0.00.039.280 I print_info: n_vocab          = 50304
0.00.039.280 I print_info: n_merges         = 50009
0.00.039.280 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.281 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.281 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.281 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.281 I print_info: LF token         = 187 'Ċ'
0.00.039.282 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.282 I print_info: max token length = 1024
0.00.039.282 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.432.486 I load_tensors: offloading 24 repeating layers to GPU
0.00.432.499 I load_tensors: offloading output layer to GPU
0.00.432.500 I load_tensors: offloaded 25/25 layers to GPU
0.00.432.535 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.432.536 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.434.195 I llama_context: n_seq_max     = 1
0.00.434.197 I llama_context: n_ctx         = 128
0.00.434.198 I llama_context: n_ctx_per_seq = 128
0.00.434.199 I llama_context: n_batch       = 128
0.00.434.199 I llama_context: n_ubatch      = 128
0.00.434.199 I llama_context: flash_attn    = 0
0.00.434.202 I llama_context: freq_base     = 10000.0
0.00.434.202 I llama_context: freq_scale    = 1
0.00.434.203 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.434.205 I ggml_metal_init: allocating
0.00.434.271 I ggml_metal_init: found device: Apple M4
0.00.434.284 I ggml_metal_init: picking default device: Apple M4
0.00.436.080 I ggml_metal_init: using embedded metal library
0.00.441.732 I ggml_metal_init: GPU name:   Apple M4
0.00.441.740 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.441.741 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.441.742 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.441.746 I ggml_metal_init: simdgroup reduction   = true
0.00.441.746 I ggml_metal_init: simdgroup matrix mul. = true
0.00.441.747 I ggml_metal_init: has residency sets    = true
0.00.441.747 I ggml_metal_init: has bfloat            = true
0.00.441.750 I ggml_metal_init: use bfloat            = true
0.00.441.760 I ggml_metal_init: hasUnifiedMemory      = true
0.00.441.763 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.461.095 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.461.100 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.464.637 I init:      Metal KV buffer size =    24.00 MiB
0.00.464.641 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.467.774 I init:      Metal compute buffer size =    25.56 MiB
0.00.467.776 I init:        CPU compute buffer size =     1.06 MiB
0.00.467.776 I init: graph nodes  = 967
0.00.467.777 I init: graph splits = 2
0.00.467.781 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.467.782 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.494.302 I 
0.00.494.365 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.494.375 I perplexity: tokenizing the input ..
0.00.501.778 I perplexity: tokenization took 7.401 ms
0.00.501.788 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.634.394 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.635.742 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.635.762 I llama_perf_context_print:        load time =     485.49 ms
0.00.635.763 I llama_perf_context_print: prompt eval time =     131.72 ms /   128 tokens (    1.03 ms per token,   971.77 tokens per second)
0.00.635.764 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.635.764 I llama_perf_context_print:       total time =     141.47 ms /   129 tokens
0.00.636.288 I ggml_metal_free: deallocating

real	0m0.649s
user	0m0.080s
sys	0m0.104s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.152 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.872 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.878 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.880 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.881 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.881 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.881 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.882 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.882 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.883 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.883 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.883 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.884 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.884 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.885 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.886 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.886 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.887 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.700 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.746 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.490 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.491 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.491 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.492 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.492 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.492 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.493 I llama_model_loader: - type  f32:  194 tensors
0.00.024.493 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.493 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.493 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.494 I print_info: file format = GGUF V3 (latest)
0.00.024.495 I print_info: file type   = Q4_K - Medium
0.00.024.495 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.755 I load: special tokens cache size = 25
0.00.038.843 I load: token to piece cache size = 0.2984 MB
0.00.038.858 I print_info: arch             = gptneox
0.00.038.859 I print_info: vocab_only       = 0
0.00.038.860 I print_info: n_ctx_train      = 2048
0.00.038.860 I print_info: n_embd           = 2048
0.00.038.860 I print_info: n_layer          = 24
0.00.038.863 I print_info: n_head           = 16
0.00.038.863 I print_info: n_head_kv        = 16
0.00.038.864 I print_info: n_rot            = 32
0.00.038.864 I print_info: n_swa            = 0
0.00.038.864 I print_info: n_embd_head_k    = 128
0.00.038.864 I print_info: n_embd_head_v    = 128
0.00.038.865 I print_info: n_gqa            = 1
0.00.038.868 I print_info: n_embd_k_gqa     = 2048
0.00.038.869 I print_info: n_embd_v_gqa     = 2048
0.00.038.870 I print_info: f_norm_eps       = 1.0e-05
0.00.038.870 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.870 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.870 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.870 I print_info: f_logit_scale    = 0.0e+00
0.00.038.871 I print_info: n_ff             = 8192
0.00.038.871 I print_info: n_expert         = 0
0.00.038.871 I print_info: n_expert_used    = 0
0.00.038.871 I print_info: causal attn      = 1
0.00.038.872 I print_info: pooling type     = 0
0.00.038.872 I print_info: rope type        = 2
0.00.038.872 I print_info: rope scaling     = linear
0.00.038.873 I print_info: freq_base_train  = 10000.0
0.00.038.874 I print_info: freq_scale_train = 1
0.00.038.874 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.874 I print_info: rope_finetuned   = unknown
0.00.038.874 I print_info: ssm_d_conv       = 0
0.00.038.874 I print_info: ssm_d_inner      = 0
0.00.038.874 I print_info: ssm_d_state      = 0
0.00.038.874 I print_info: ssm_dt_rank      = 0
0.00.038.874 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.875 I print_info: model type       = 1.4B
0.00.038.875 I print_info: model params     = 1.41 B
0.00.038.875 I print_info: general.name     = 1.4B
0.00.038.879 I print_info: vocab type       = BPE
0.00.038.880 I print_info: n_vocab          = 50304
0.00.038.880 I print_info: n_merges         = 50009
0.00.038.880 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.880 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.881 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.881 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.882 I print_info: LF token         = 187 'Ċ'
0.00.038.882 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.882 I print_info: max token length = 1024
0.00.038.883 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.528.976 I load_tensors: offloading 24 repeating layers to GPU
0.00.528.992 I load_tensors: offloading output layer to GPU
0.00.528.992 I load_tensors: offloaded 25/25 layers to GPU
0.00.529.035 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.529.037 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.530.505 I llama_context: n_seq_max     = 1
0.00.530.508 I llama_context: n_ctx         = 2048
0.00.530.508 I llama_context: n_ctx_per_seq = 2048
0.00.530.509 I llama_context: n_batch       = 2048
0.00.530.509 I llama_context: n_ubatch      = 512
0.00.530.510 I llama_context: flash_attn    = 0
0.00.530.512 I llama_context: freq_base     = 10000.0
0.00.530.513 I llama_context: freq_scale    = 1
0.00.530.514 I ggml_metal_init: allocating
0.00.530.597 I ggml_metal_init: found device: Apple M4
0.00.530.612 I ggml_metal_init: picking default device: Apple M4
0.00.532.615 I ggml_metal_init: using embedded metal library
0.00.539.241 I ggml_metal_init: GPU name:   Apple M4
0.00.539.245 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.539.246 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.539.246 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.539.247 I ggml_metal_init: simdgroup reduction   = true
0.00.539.247 I ggml_metal_init: simdgroup matrix mul. = true
0.00.539.247 I ggml_metal_init: has residency sets    = true
0.00.539.248 I ggml_metal_init: has bfloat            = true
0.00.539.248 I ggml_metal_init: use bfloat            = true
0.00.539.249 I ggml_metal_init: hasUnifiedMemory      = true
0.00.539.251 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.556.445 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.556.450 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.611.295 I init:      Metal KV buffer size =   384.00 MiB
0.00.611.302 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.615.965 I init:      Metal compute buffer size =   102.25 MiB
0.00.615.968 I init:        CPU compute buffer size =     8.01 MiB
0.00.615.969 I init: graph nodes  = 967
0.00.615.969 I init: graph splits = 2
0.00.615.976 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.616.104 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.616.104 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.673.231 I main: llama threadpool init, n_threads = 4
0.00.673.278 I 
0.00.673.293 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.673.293 I 
0.00.673.475 I sampler seed: 1234
0.00.673.480 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.673.501 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.673.502 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.673.502 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.434.081 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51189.62 tokens per second)
0.01.434.082 I llama_perf_context_print:        load time =     663.36 ms
0.01.434.082 I llama_perf_context_print: prompt eval time =      57.22 ms /     7 tokens (    8.17 ms per token,   122.34 tokens per second)
0.01.434.083 I llama_perf_context_print:        eval time =     700.38 ms /    63 runs   (   11.12 ms per token,    89.95 tokens per second)
0.01.434.083 I llama_perf_context_print:       total time =     761.56 ms /    70 tokens
0.01.437.929 I ggml_metal_free: deallocating

real	0m1.453s
user	0m0.108s
sys	0m0.209s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.860 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.867 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.874 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.875 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.876 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.876 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.877 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.877 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.878 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.878 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.878 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.879 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.879 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.880 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.880 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.883 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.883 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.883 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.758 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.769 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.604 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.606 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.606 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.607 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.607 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.607 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.608 I llama_model_loader: - type  f32:  194 tensors
0.00.024.608 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.609 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.609 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.610 I print_info: file format = GGUF V3 (latest)
0.00.024.610 I print_info: file type   = Q4_K - Medium
0.00.024.612 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.067 I load: special tokens cache size = 25
0.00.039.054 I load: token to piece cache size = 0.2984 MB
0.00.039.071 I print_info: arch             = gptneox
0.00.039.072 I print_info: vocab_only       = 0
0.00.039.072 I print_info: n_ctx_train      = 2048
0.00.039.073 I print_info: n_embd           = 2048
0.00.039.073 I print_info: n_layer          = 24
0.00.039.077 I print_info: n_head           = 16
0.00.039.077 I print_info: n_head_kv        = 16
0.00.039.078 I print_info: n_rot            = 32
0.00.039.078 I print_info: n_swa            = 0
0.00.039.078 I print_info: n_embd_head_k    = 128
0.00.039.078 I print_info: n_embd_head_v    = 128
0.00.039.079 I print_info: n_gqa            = 1
0.00.039.079 I print_info: n_embd_k_gqa     = 2048
0.00.039.080 I print_info: n_embd_v_gqa     = 2048
0.00.039.080 I print_info: f_norm_eps       = 1.0e-05
0.00.039.081 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.081 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.081 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.081 I print_info: f_logit_scale    = 0.0e+00
0.00.039.082 I print_info: n_ff             = 8192
0.00.039.082 I print_info: n_expert         = 0
0.00.039.082 I print_info: n_expert_used    = 0
0.00.039.082 I print_info: causal attn      = 1
0.00.039.085 I print_info: pooling type     = 0
0.00.039.085 I print_info: rope type        = 2
0.00.039.085 I print_info: rope scaling     = linear
0.00.039.086 I print_info: freq_base_train  = 10000.0
0.00.039.086 I print_info: freq_scale_train = 1
0.00.039.086 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.086 I print_info: rope_finetuned   = unknown
0.00.039.086 I print_info: ssm_d_conv       = 0
0.00.039.086 I print_info: ssm_d_inner      = 0
0.00.039.086 I print_info: ssm_d_state      = 0
0.00.039.088 I print_info: ssm_dt_rank      = 0
0.00.039.088 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.088 I print_info: model type       = 1.4B
0.00.039.089 I print_info: model params     = 1.41 B
0.00.039.089 I print_info: general.name     = 1.4B
0.00.039.089 I print_info: vocab type       = BPE
0.00.039.090 I print_info: n_vocab          = 50304
0.00.039.091 I print_info: n_merges         = 50009
0.00.039.091 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.091 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.091 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.091 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.091 I print_info: LF token         = 187 'Ċ'
0.00.039.093 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.093 I print_info: max token length = 1024
0.00.039.093 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.515.423 I load_tensors: offloading 24 repeating layers to GPU
0.00.515.437 I load_tensors: offloading output layer to GPU
0.00.515.437 I load_tensors: offloaded 25/25 layers to GPU
0.00.515.471 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.515.472 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.517.037 I llama_context: n_seq_max     = 1
0.00.517.040 I llama_context: n_ctx         = 128
0.00.517.040 I llama_context: n_ctx_per_seq = 128
0.00.517.041 I llama_context: n_batch       = 128
0.00.517.041 I llama_context: n_ubatch      = 128
0.00.517.042 I llama_context: flash_attn    = 0
0.00.517.044 I llama_context: freq_base     = 10000.0
0.00.517.045 I llama_context: freq_scale    = 1
0.00.517.045 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.517.047 I ggml_metal_init: allocating
0.00.517.132 I ggml_metal_init: found device: Apple M4
0.00.517.147 I ggml_metal_init: picking default device: Apple M4
0.00.519.113 I ggml_metal_init: using embedded metal library
0.00.525.831 I ggml_metal_init: GPU name:   Apple M4
0.00.525.839 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.525.839 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.525.840 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.525.841 I ggml_metal_init: simdgroup reduction   = true
0.00.525.841 I ggml_metal_init: simdgroup matrix mul. = true
0.00.525.841 I ggml_metal_init: has residency sets    = true
0.00.525.842 I ggml_metal_init: has bfloat            = true
0.00.525.842 I ggml_metal_init: use bfloat            = true
0.00.525.843 I ggml_metal_init: hasUnifiedMemory      = true
0.00.525.846 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.543.442 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.543.447 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.546.954 I init:      Metal KV buffer size =    24.00 MiB
0.00.546.957 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.550.309 I init:      Metal compute buffer size =    25.56 MiB
0.00.550.311 I init:        CPU compute buffer size =     1.06 MiB
0.00.550.311 I init: graph nodes  = 967
0.00.550.312 I init: graph splits = 2
0.00.550.315 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.550.315 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.577.190 I 
0.00.577.247 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.577.256 I perplexity: tokenizing the input ..
0.00.584.662 I perplexity: tokenization took 7.403 ms
0.00.584.667 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.718.380 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.719.729 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.719.751 I llama_perf_context_print:        load time =     568.32 ms
0.00.719.754 I llama_perf_context_print: prompt eval time =     132.81 ms /   128 tokens (    1.04 ms per token,   963.80 tokens per second)
0.00.719.754 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.719.755 I llama_perf_context_print:       total time =     142.57 ms /   129 tokens
0.00.720.294 I ggml_metal_free: deallocating

real	0m0.734s
user	0m0.080s
sys	0m0.123s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.009.369 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.992 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.997 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.999 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.999 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.999 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.000 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.000 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.001 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.001 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.002 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.002 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.002 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.005 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.006 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.007 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.007 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.008 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.865 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.909 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.724 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.725 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.726 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.726 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.726 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.726 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.727 I llama_model_loader: - type  f32:  194 tensors
0.00.024.727 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.727 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.728 I print_info: file format = GGUF V3 (latest)
0.00.024.728 I print_info: file type   = Q5_K - Medium
0.00.024.734 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.904 I load: special tokens cache size = 25
0.00.039.228 I load: token to piece cache size = 0.2984 MB
0.00.039.242 I print_info: arch             = gptneox
0.00.039.243 I print_info: vocab_only       = 0
0.00.039.243 I print_info: n_ctx_train      = 2048
0.00.039.244 I print_info: n_embd           = 2048
0.00.039.244 I print_info: n_layer          = 24
0.00.039.247 I print_info: n_head           = 16
0.00.039.247 I print_info: n_head_kv        = 16
0.00.039.248 I print_info: n_rot            = 32
0.00.039.248 I print_info: n_swa            = 0
0.00.039.248 I print_info: n_embd_head_k    = 128
0.00.039.248 I print_info: n_embd_head_v    = 128
0.00.039.249 I print_info: n_gqa            = 1
0.00.039.250 I print_info: n_embd_k_gqa     = 2048
0.00.039.251 I print_info: n_embd_v_gqa     = 2048
0.00.039.251 I print_info: f_norm_eps       = 1.0e-05
0.00.039.252 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.252 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.252 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.252 I print_info: f_logit_scale    = 0.0e+00
0.00.039.253 I print_info: n_ff             = 8192
0.00.039.253 I print_info: n_expert         = 0
0.00.039.253 I print_info: n_expert_used    = 0
0.00.039.253 I print_info: causal attn      = 1
0.00.039.253 I print_info: pooling type     = 0
0.00.039.253 I print_info: rope type        = 2
0.00.039.254 I print_info: rope scaling     = linear
0.00.039.254 I print_info: freq_base_train  = 10000.0
0.00.039.254 I print_info: freq_scale_train = 1
0.00.039.254 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.254 I print_info: rope_finetuned   = unknown
0.00.039.255 I print_info: ssm_d_conv       = 0
0.00.039.255 I print_info: ssm_d_inner      = 0
0.00.039.256 I print_info: ssm_d_state      = 0
0.00.039.256 I print_info: ssm_dt_rank      = 0
0.00.039.256 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.256 I print_info: model type       = 1.4B
0.00.039.256 I print_info: model params     = 1.41 B
0.00.039.257 I print_info: general.name     = 1.4B
0.00.039.257 I print_info: vocab type       = BPE
0.00.039.257 I print_info: n_vocab          = 50304
0.00.039.257 I print_info: n_merges         = 50009
0.00.039.257 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.258 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.258 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.259 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.259 I print_info: LF token         = 187 'Ċ'
0.00.039.259 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.260 I print_info: max token length = 1024
0.00.039.260 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.622.157 I load_tensors: offloading 24 repeating layers to GPU
0.00.622.161 I load_tensors: offloading output layer to GPU
0.00.622.162 I load_tensors: offloaded 25/25 layers to GPU
0.00.622.187 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.622.189 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.623.789 I llama_context: n_seq_max     = 1
0.00.623.792 I llama_context: n_ctx         = 2048
0.00.623.792 I llama_context: n_ctx_per_seq = 2048
0.00.623.793 I llama_context: n_batch       = 2048
0.00.623.793 I llama_context: n_ubatch      = 512
0.00.623.794 I llama_context: flash_attn    = 0
0.00.623.795 I llama_context: freq_base     = 10000.0
0.00.623.795 I llama_context: freq_scale    = 1
0.00.623.796 I ggml_metal_init: allocating
0.00.623.858 I ggml_metal_init: found device: Apple M4
0.00.623.869 I ggml_metal_init: picking default device: Apple M4
0.00.625.448 I ggml_metal_init: using embedded metal library
0.00.631.491 I ggml_metal_init: GPU name:   Apple M4
0.00.631.494 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.631.495 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.631.495 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.631.496 I ggml_metal_init: simdgroup reduction   = true
0.00.631.496 I ggml_metal_init: simdgroup matrix mul. = true
0.00.631.496 I ggml_metal_init: has residency sets    = true
0.00.631.496 I ggml_metal_init: has bfloat            = true
0.00.631.497 I ggml_metal_init: use bfloat            = true
0.00.631.497 I ggml_metal_init: hasUnifiedMemory      = true
0.00.631.498 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.647.944 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.647.948 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.701.834 I init:      Metal KV buffer size =   384.00 MiB
0.00.701.845 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.707.294 I init:      Metal compute buffer size =   102.25 MiB
0.00.707.296 I init:        CPU compute buffer size =     8.01 MiB
0.00.707.296 I init: graph nodes  = 967
0.00.707.297 I init: graph splits = 2
0.00.707.304 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.707.427 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.707.428 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.771.495 I main: llama threadpool init, n_threads = 4
0.00.771.536 I 
0.00.771.550 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.771.551 I 
0.00.771.700 I sampler seed: 1234
0.00.771.704 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.771.724 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.771.724 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.771.724 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.613.150 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53303.30 tokens per second)
0.01.613.152 I llama_perf_context_print:        load time =     761.44 ms
0.01.613.153 I llama_perf_context_print: prompt eval time =      52.60 ms /     7 tokens (    7.51 ms per token,   133.08 tokens per second)
0.01.613.153 I llama_perf_context_print:        eval time =     785.90 ms /    63 runs   (   12.47 ms per token,    80.16 tokens per second)
0.01.613.154 I llama_perf_context_print:       total time =     842.35 ms /    70 tokens
0.01.617.057 I ggml_metal_free: deallocating

real	0m1.635s
user	0m0.107s
sys	0m0.234s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.148 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.072 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.078 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.080 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.081 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.081 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.081 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.082 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.082 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.083 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.083 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.084 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.084 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.087 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.087 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.089 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.089 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.090 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.940 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.994 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.924 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.925 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.926 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.926 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.926 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.927 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.927 I llama_model_loader: - type  f32:  194 tensors
0.00.025.928 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.928 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.929 I print_info: file format = GGUF V3 (latest)
0.00.025.929 I print_info: file type   = Q5_K - Medium
0.00.025.931 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.398 I load: special tokens cache size = 25
0.00.040.818 I load: token to piece cache size = 0.2984 MB
0.00.040.835 I print_info: arch             = gptneox
0.00.040.836 I print_info: vocab_only       = 0
0.00.040.836 I print_info: n_ctx_train      = 2048
0.00.040.836 I print_info: n_embd           = 2048
0.00.040.836 I print_info: n_layer          = 24
0.00.040.839 I print_info: n_head           = 16
0.00.040.840 I print_info: n_head_kv        = 16
0.00.040.840 I print_info: n_rot            = 32
0.00.040.840 I print_info: n_swa            = 0
0.00.040.840 I print_info: n_embd_head_k    = 128
0.00.040.841 I print_info: n_embd_head_v    = 128
0.00.040.844 I print_info: n_gqa            = 1
0.00.040.845 I print_info: n_embd_k_gqa     = 2048
0.00.040.845 I print_info: n_embd_v_gqa     = 2048
0.00.040.846 I print_info: f_norm_eps       = 1.0e-05
0.00.040.846 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.846 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.847 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.847 I print_info: f_logit_scale    = 0.0e+00
0.00.040.849 I print_info: n_ff             = 8192
0.00.040.849 I print_info: n_expert         = 0
0.00.040.849 I print_info: n_expert_used    = 0
0.00.040.849 I print_info: causal attn      = 1
0.00.040.850 I print_info: pooling type     = 0
0.00.040.850 I print_info: rope type        = 2
0.00.040.850 I print_info: rope scaling     = linear
0.00.040.850 I print_info: freq_base_train  = 10000.0
0.00.040.851 I print_info: freq_scale_train = 1
0.00.040.851 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.851 I print_info: rope_finetuned   = unknown
0.00.040.851 I print_info: ssm_d_conv       = 0
0.00.040.851 I print_info: ssm_d_inner      = 0
0.00.040.851 I print_info: ssm_d_state      = 0
0.00.040.851 I print_info: ssm_dt_rank      = 0
0.00.040.852 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.852 I print_info: model type       = 1.4B
0.00.040.852 I print_info: model params     = 1.41 B
0.00.040.853 I print_info: general.name     = 1.4B
0.00.040.854 I print_info: vocab type       = BPE
0.00.040.854 I print_info: n_vocab          = 50304
0.00.040.854 I print_info: n_merges         = 50009
0.00.040.854 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.854 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.855 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.855 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.856 I print_info: LF token         = 187 'Ċ'
0.00.040.856 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.856 I print_info: max token length = 1024
0.00.040.857 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.615.243 I load_tensors: offloading 24 repeating layers to GPU
0.00.615.251 I load_tensors: offloading output layer to GPU
0.00.615.251 I load_tensors: offloaded 25/25 layers to GPU
0.00.615.282 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.615.284 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.616.808 I llama_context: n_seq_max     = 1
0.00.616.812 I llama_context: n_ctx         = 128
0.00.616.813 I llama_context: n_ctx_per_seq = 128
0.00.616.813 I llama_context: n_batch       = 128
0.00.616.813 I llama_context: n_ubatch      = 128
0.00.616.814 I llama_context: flash_attn    = 0
0.00.616.815 I llama_context: freq_base     = 10000.0
0.00.616.815 I llama_context: freq_scale    = 1
0.00.616.816 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.616.822 I ggml_metal_init: allocating
0.00.616.893 I ggml_metal_init: found device: Apple M4
0.00.616.907 I ggml_metal_init: picking default device: Apple M4
0.00.619.014 I ggml_metal_init: using embedded metal library
0.00.625.828 I ggml_metal_init: GPU name:   Apple M4
0.00.625.834 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.625.835 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.625.836 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.625.836 I ggml_metal_init: simdgroup reduction   = true
0.00.625.837 I ggml_metal_init: simdgroup matrix mul. = true
0.00.625.837 I ggml_metal_init: has residency sets    = true
0.00.625.837 I ggml_metal_init: has bfloat            = true
0.00.625.837 I ggml_metal_init: use bfloat            = true
0.00.625.839 I ggml_metal_init: hasUnifiedMemory      = true
0.00.625.842 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.450 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.643.455 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.646.902 I init:      Metal KV buffer size =    24.00 MiB
0.00.646.907 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.650.016 I init:      Metal compute buffer size =    25.56 MiB
0.00.650.018 I init:        CPU compute buffer size =     1.06 MiB
0.00.650.018 I init: graph nodes  = 967
0.00.650.019 I init: graph splits = 2
0.00.650.022 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.650.022 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.688.352 I 
0.00.688.409 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.688.416 I perplexity: tokenizing the input ..
0.00.695.026 I perplexity: tokenization took 6.607 ms
0.00.695.033 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.840.656 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.842.034 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.842.060 I llama_perf_context_print:        load time =     678.20 ms
0.00.842.061 I llama_perf_context_print: prompt eval time =     145.07 ms /   128 tokens (    1.13 ms per token,   882.31 tokens per second)
0.00.842.061 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.842.062 I llama_perf_context_print:       total time =     153.71 ms /   129 tokens
0.00.842.602 I ggml_metal_free: deallocating

real	0m0.858s
user	0m0.079s
sys	0m0.156s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.723 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.437 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.442 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.444 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.444 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.444 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.445 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.445 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.447 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.448 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.448 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.448 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.449 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.449 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.449 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.451 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.451 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.452 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.229 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.201 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.917 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.919 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.919 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.919 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.919 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.920 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.920 I llama_model_loader: - type  f32:  194 tensors
0.00.023.921 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.921 I print_info: file format = GGUF V3 (latest)
0.00.023.922 I print_info: file type   = Q6_K
0.00.023.922 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.095 I load: special tokens cache size = 25
0.00.038.088 I load: token to piece cache size = 0.2984 MB
0.00.038.102 I print_info: arch             = gptneox
0.00.038.103 I print_info: vocab_only       = 0
0.00.038.103 I print_info: n_ctx_train      = 2048
0.00.038.104 I print_info: n_embd           = 2048
0.00.038.104 I print_info: n_layer          = 24
0.00.038.107 I print_info: n_head           = 16
0.00.038.108 I print_info: n_head_kv        = 16
0.00.038.108 I print_info: n_rot            = 32
0.00.038.108 I print_info: n_swa            = 0
0.00.038.108 I print_info: n_embd_head_k    = 128
0.00.038.108 I print_info: n_embd_head_v    = 128
0.00.038.109 I print_info: n_gqa            = 1
0.00.038.110 I print_info: n_embd_k_gqa     = 2048
0.00.038.111 I print_info: n_embd_v_gqa     = 2048
0.00.038.111 I print_info: f_norm_eps       = 1.0e-05
0.00.038.112 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.112 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.112 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.112 I print_info: f_logit_scale    = 0.0e+00
0.00.038.113 I print_info: n_ff             = 8192
0.00.038.113 I print_info: n_expert         = 0
0.00.038.113 I print_info: n_expert_used    = 0
0.00.038.113 I print_info: causal attn      = 1
0.00.038.113 I print_info: pooling type     = 0
0.00.038.113 I print_info: rope type        = 2
0.00.038.114 I print_info: rope scaling     = linear
0.00.038.114 I print_info: freq_base_train  = 10000.0
0.00.038.114 I print_info: freq_scale_train = 1
0.00.038.115 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.115 I print_info: rope_finetuned   = unknown
0.00.038.115 I print_info: ssm_d_conv       = 0
0.00.038.115 I print_info: ssm_d_inner      = 0
0.00.038.115 I print_info: ssm_d_state      = 0
0.00.038.115 I print_info: ssm_dt_rank      = 0
0.00.038.115 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.116 I print_info: model type       = 1.4B
0.00.038.117 I print_info: model params     = 1.41 B
0.00.038.117 I print_info: general.name     = 1.4B
0.00.038.117 I print_info: vocab type       = BPE
0.00.038.117 I print_info: n_vocab          = 50304
0.00.038.118 I print_info: n_merges         = 50009
0.00.038.118 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.118 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.118 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.118 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.119 I print_info: LF token         = 187 'Ċ'
0.00.038.119 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.119 I print_info: max token length = 1024
0.00.038.119 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.666.126 I load_tensors: offloading 24 repeating layers to GPU
0.00.666.130 I load_tensors: offloading output layer to GPU
0.00.666.131 I load_tensors: offloaded 25/25 layers to GPU
0.00.666.153 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.666.156 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.667.705 I llama_context: n_seq_max     = 1
0.00.667.708 I llama_context: n_ctx         = 2048
0.00.667.708 I llama_context: n_ctx_per_seq = 2048
0.00.667.709 I llama_context: n_batch       = 2048
0.00.667.709 I llama_context: n_ubatch      = 512
0.00.667.709 I llama_context: flash_attn    = 0
0.00.667.710 I llama_context: freq_base     = 10000.0
0.00.667.711 I llama_context: freq_scale    = 1
0.00.667.712 I ggml_metal_init: allocating
0.00.667.763 I ggml_metal_init: found device: Apple M4
0.00.667.777 I ggml_metal_init: picking default device: Apple M4
0.00.669.392 I ggml_metal_init: using embedded metal library
0.00.675.442 I ggml_metal_init: GPU name:   Apple M4
0.00.675.445 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.675.446 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.675.447 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.675.447 I ggml_metal_init: simdgroup reduction   = true
0.00.675.448 I ggml_metal_init: simdgroup matrix mul. = true
0.00.675.448 I ggml_metal_init: has residency sets    = true
0.00.675.448 I ggml_metal_init: has bfloat            = true
0.00.675.448 I ggml_metal_init: use bfloat            = true
0.00.675.449 I ggml_metal_init: hasUnifiedMemory      = true
0.00.675.450 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.692.159 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.692.164 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.749.117 I init:      Metal KV buffer size =   384.00 MiB
0.00.749.125 I llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.753.562 I init:      Metal compute buffer size =   102.25 MiB
0.00.753.564 I init:        CPU compute buffer size =     8.01 MiB
0.00.753.564 I init: graph nodes  = 967
0.00.753.564 I init: graph splits = 2
0.00.753.570 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.753.698 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.753.699 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.822.436 I main: llama threadpool init, n_threads = 4
0.00.822.482 I 
0.00.822.498 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.822.498 I 
0.00.822.663 I sampler seed: 1234
0.00.822.667 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.822.709 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.822.712 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.822.712 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.695.814 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52398.52 tokens per second)
0.01.695.815 I llama_perf_context_print:        load time =     813.02 ms
0.01.695.816 I llama_perf_context_print: prompt eval time =      57.79 ms /     7 tokens (    8.26 ms per token,   121.14 tokens per second)
0.01.695.816 I llama_perf_context_print:        eval time =     812.38 ms /    63 runs   (   12.89 ms per token,    77.55 tokens per second)
0.01.695.817 I llama_perf_context_print:       total time =     874.07 ms /    70 tokens
0.01.699.669 I ggml_metal_free: deallocating

real	0m1.715s
user	0m0.107s
sys	0m0.238s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4792 (172f6169) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.954 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.575 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.580 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.587 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.588 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.588 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.588 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.589 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.590 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.590 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.590 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.591 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.591 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.591 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.592 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.593 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.594 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.594 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.466 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.484 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.403 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.405 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.405 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.405 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.406 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.406 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.407 I llama_model_loader: - type  f32:  194 tensors
0.00.024.407 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.408 I print_info: file format = GGUF V3 (latest)
0.00.024.409 I print_info: file type   = Q6_K
0.00.024.410 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.590 I load: special tokens cache size = 25
0.00.038.760 I load: token to piece cache size = 0.2984 MB
0.00.038.775 I print_info: arch             = gptneox
0.00.038.777 I print_info: vocab_only       = 0
0.00.038.777 I print_info: n_ctx_train      = 2048
0.00.038.777 I print_info: n_embd           = 2048
0.00.038.777 I print_info: n_layer          = 24
0.00.038.781 I print_info: n_head           = 16
0.00.038.782 I print_info: n_head_kv        = 16
0.00.038.782 I print_info: n_rot            = 32
0.00.038.783 I print_info: n_swa            = 0
0.00.038.783 I print_info: n_embd_head_k    = 128
0.00.038.783 I print_info: n_embd_head_v    = 128
0.00.038.784 I print_info: n_gqa            = 1
0.00.038.785 I print_info: n_embd_k_gqa     = 2048
0.00.038.785 I print_info: n_embd_v_gqa     = 2048
0.00.038.786 I print_info: f_norm_eps       = 1.0e-05
0.00.038.786 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.786 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.787 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.787 I print_info: f_logit_scale    = 0.0e+00
0.00.038.787 I print_info: n_ff             = 8192
0.00.038.787 I print_info: n_expert         = 0
0.00.038.787 I print_info: n_expert_used    = 0
0.00.038.788 I print_info: causal attn      = 1
0.00.038.788 I print_info: pooling type     = 0
0.00.038.788 I print_info: rope type        = 2
0.00.038.788 I print_info: rope scaling     = linear
0.00.038.788 I print_info: freq_base_train  = 10000.0
0.00.038.789 I print_info: freq_scale_train = 1
0.00.038.789 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.789 I print_info: rope_finetuned   = unknown
0.00.038.789 I print_info: ssm_d_conv       = 0
0.00.038.789 I print_info: ssm_d_inner      = 0
0.00.038.789 I print_info: ssm_d_state      = 0
0.00.038.789 I print_info: ssm_dt_rank      = 0
0.00.038.790 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.790 I print_info: model type       = 1.4B
0.00.038.790 I print_info: model params     = 1.41 B
0.00.038.790 I print_info: general.name     = 1.4B
0.00.038.791 I print_info: vocab type       = BPE
0.00.038.791 I print_info: n_vocab          = 50304
0.00.038.791 I print_info: n_merges         = 50009
0.00.038.791 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.791 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.791 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.792 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.792 I print_info: LF token         = 187 'Ċ'
0.00.038.792 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.792 I print_info: max token length = 1024
0.00.038.792 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.546.720 I load_tensors: offloading 24 repeating layers to GPU
0.00.546.726 I load_tensors: offloading output layer to GPU
0.00.546.727 I load_tensors: offloaded 25/25 layers to GPU
0.00.546.751 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.546.755 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.548.152 I llama_context: n_seq_max     = 1
0.00.548.154 I llama_context: n_ctx         = 128
0.00.548.155 I llama_context: n_ctx_per_seq = 128
0.00.548.155 I llama_context: n_batch       = 128
0.00.548.155 I llama_context: n_ubatch      = 128
0.00.548.156 I llama_context: flash_attn    = 0
0.00.548.157 I llama_context: freq_base     = 10000.0
0.00.548.157 I llama_context: freq_scale    = 1
0.00.548.158 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.548.159 I ggml_metal_init: allocating
0.00.548.176 I ggml_metal_init: found device: Apple M4
0.00.548.184 I ggml_metal_init: picking default device: Apple M4
0.00.549.664 I ggml_metal_init: using embedded metal library
0.00.555.723 I ggml_metal_init: GPU name:   Apple M4
0.00.555.726 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.555.727 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.555.728 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.555.728 I ggml_metal_init: simdgroup reduction   = true
0.00.555.729 I ggml_metal_init: simdgroup matrix mul. = true
0.00.555.729 I ggml_metal_init: has residency sets    = true
0.00.555.729 I ggml_metal_init: has bfloat            = true
0.00.555.729 I ggml_metal_init: use bfloat            = true
0.00.555.730 I ggml_metal_init: hasUnifiedMemory      = true
0.00.555.732 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.572.650 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.572.654 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.576.119 I init:      Metal KV buffer size =    24.00 MiB
0.00.576.123 I llama_context_kv_self: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.579.388 I init:      Metal compute buffer size =    25.56 MiB
0.00.579.390 I init:        CPU compute buffer size =     1.06 MiB
0.00.579.390 I init: graph nodes  = 967
0.00.579.390 I init: graph splits = 2
0.00.579.394 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.579.394 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.616.477 I 
0.00.616.544 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.616.551 I perplexity: tokenizing the input ..
0.00.623.649 I perplexity: tokenization took 7.094 ms
0.00.623.657 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.755.691 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.757.002 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.757.028 I llama_perf_context_print:        load time =     607.51 ms
0.00.757.029 I llama_perf_context_print: prompt eval time =     131.17 ms /   128 tokens (    1.02 ms per token,   975.86 tokens per second)
0.00.757.034 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.757.034 I llama_perf_context_print:       total time =     140.55 ms /   129 tokens
0.00.757.571 I ggml_metal_free: deallocating

real	0m0.772s
user	0m0.079s
sys	0m0.135s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4792 (172f6169)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141a06660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141a06cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141a07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141a09eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141a0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141a0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141a0ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x141a0b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141a0b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141a0bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141a0c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141a0c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141a0d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141a0da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141a0e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141a0e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141a0f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141a0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141a0ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141a106d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141a10df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141a11510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141a11c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141a124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141a12bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141a12eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141a134c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141a14130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141a14670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141a14930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141a14dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141a15090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141a15920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141a15e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141a16120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141a165c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141a16a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141a16f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141a173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141a17840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141a17ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141a18180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141a18620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141a18ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141a18d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141a19390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x141a199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141a1a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141a1a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141a1aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141a1b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x141a1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141a1c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141a1c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141a1cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141a1d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141a1d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141a1db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141a1e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141a1e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141a1ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141a1f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141a1f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141a1f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141a1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141a202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141a20790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141a20c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141a210d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141a21570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141a21a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141a21eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141a22350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x141a228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141a22df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141a23340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141a23890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141a23de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x141a24330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141a24880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141a24dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141a25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141a25870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141a25dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141a26310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141a26860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141a26db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141a27300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141a27850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141a27da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141a282f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141a28840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141a28d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x141a292e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141a29830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141a29d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141a2a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141a19fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141a2a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x141a2aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x141a2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x141a2b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x141a2bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x141a2c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141a2c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141a2ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141a2d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141a2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141a2dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x141a2e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141a2e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141a2eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141a2f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141a2f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141a2fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141a301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141a30680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141a30b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141a30fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141a31460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141a31900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141a31da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141a32240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141a326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141a32b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141a33020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141a334c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141a33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141a33e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141a342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141a34740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141a34be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141a35080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141a35520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141a359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141a35e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141a36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141a367a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141a36c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141a370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141a37580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141a37a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141a37ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141a38360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141a38800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141a38ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141a39140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141a395e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141a39a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141a39f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141a3a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141a3a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141a3ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141a3b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141a3b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141a3bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141a3bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141a3c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141a3c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141a3cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141a3d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141a3d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141a3db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141a3dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141a3e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141a3e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141a3edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141a3f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141a3f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141a3fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141a40040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141a404e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141a40980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141a40e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141a412c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141a41760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141a41c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141a420a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141a42540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141a429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141a42e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141a43320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141a437c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141a43c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141a44100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141a445a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141a44a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141a44ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141a45380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141a45820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141a45cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141a46160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141a46600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141a46b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141a470a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141a475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141a47b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141a47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141a48410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141a48a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141a49030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x141a49820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x141a49cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141a49f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141a4a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141a4aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141a4b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141a4b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141a4bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141a4c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141a4c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141a4ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141a4d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141a4d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141a4de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141a4e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x141a4e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141a4ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141a4f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141a4f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141a4fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141a50390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141a508e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141a50e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141a51380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141a518d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141a51e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141a52370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141a528c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141a52e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141a53360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141a538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141a53e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141a54350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141a548a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141a54df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141a55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141a55890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141a55de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141a56330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141a56880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141a56dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141a57320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141a57870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141a57dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141a58310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141a58860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141a58db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141a59300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141a59850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141a59da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141a5a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x141a5a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141a5ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141a5b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141a5b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141a5bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141a5c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141a5c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141a5cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141a5d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141a5d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141a5dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141a5e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141a5e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141a5ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141a5f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141a5f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141a5fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141a60080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141a60520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141a609c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141a60e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141a61300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141a617a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141a61c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141a620e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141a62580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141a62a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141a62ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141a63360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141a63800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141a63d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141a64470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141a64b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141a652b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141a659d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141a65c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141a66480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141a66740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141a66d50 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 967
init: graph splits = 2
0.00.722.083 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.722.087 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141a66a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141a486d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141a480c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141a48ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141a1bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141a1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141a1ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x141a13170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141a19c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141a1a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141a1ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141a19650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141a1c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141a12170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141a4afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141a05c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141a1c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141a1e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141a2aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141a65f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141a15350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141a15610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141a492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141a13780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141a13a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141a13d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141a671b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141a67470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141a67730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141a679f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141a67cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141a67f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141a68230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141a684f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141a687b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141a68a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141a68d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141a68ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141a692b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141a69570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141a69830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141a69af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141a69db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141a6a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141a6a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141a6a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x141a6a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141a6ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141a6ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141a6b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141a6b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x141a6b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141a6b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141a6bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141a6beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141a6c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141a6c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141a6c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141a6c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141a6cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141a6cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141a6d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141a6d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141a6d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141a6da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141a6dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141a6dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141a6e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141a6e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141a6e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141a6eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141a6ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141a6f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x141a6f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141a6f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141a6f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141a6fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141a6fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x141a700b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141a70370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141a70630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141a708f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141a70bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141a70e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141a71130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141a713f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141a716b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141a71970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141a71c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141a71ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141a721b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x141a72470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141a72730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x141a729f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x141a72cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x141a72f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x141a73230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x141a734f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x141a737b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x141a73a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x141a73d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x141a73ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x141a742b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x141a74570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141a74830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141a74af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141a74db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141a75070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141a75330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x141a755f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141a758b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141a75b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141a75e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141a760f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141a763b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141a76670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141a76930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141a76bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141a76eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141a77170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141a77430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141a776f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141a779b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141a77c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141a77f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141a781f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141a784b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141a78770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141a78a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141a78cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141a78fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141a79270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141a79530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141a797f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141a79ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141a79d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141a7a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141a7a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141a7a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141a7a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141a7ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141a7adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141a7b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141a7b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141a7b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141a7b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141a7bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141a7be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141a7c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141a7c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141a7c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141a7c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141a7cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141a7cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141a7d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141a7d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141a7d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141a7d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141a7dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141a7df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141a7e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141a7e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141a7e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141a7ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141a7ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141a7eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141a7f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141a7f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141a7f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141a7faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141a7fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141a80070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141a80330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141a805f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141a808b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141a80b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141a80e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141a810f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141a813b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141a81670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141a81930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141a81bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141a81eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141a82170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141a82430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141a826f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141a829b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141a82c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141a82f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141a831f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141a834b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141a83770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141a83a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141a83cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141a83fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141a84270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141a84530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141a847f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141a84ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141a84d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141a85030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x141a852f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x141a855b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141a85870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141a85b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141a85df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141a860b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141a86370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141a86630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141a868f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141a86ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141a87180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141a87440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141a87700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141a879c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141a87c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x141a87f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141a88200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141a884c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141a88780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141a88a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141a88d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141a88fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141a89280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141a89540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141a89800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141a89ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141a89d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141a8a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141a8a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141a8a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141a8a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141a8ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141a8ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141a8b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141a8b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141a8b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141a8b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141a8bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141a8be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141a8c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141a8c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141a8c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141a8cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141a8d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141a8d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141a8dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141a8e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141a8e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141a8ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141a8f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141a8f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x141a8fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141a90130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141a90680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141a90bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141a91120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141a91670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141a91bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141a92110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141a92660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141a92bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141a93100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141a93650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141a93ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141a940f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141a94640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141a94ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x141a94f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141a95420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141a958c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141a95d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141a96200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141a966a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141a96b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141a96fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141a97480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141a97920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141a97dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141a98260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141a98700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141a98ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141a990f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141a99810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141a99f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141a9a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141a9ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141a9b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141a9b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141a9bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141a9c0f0 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 967
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13e8044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13e804950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13e804dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13e805230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13e8056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13e805b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13e805f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13e8063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13e806860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13e806db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13e807220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13e8078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13e8083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13e808b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13e809380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13e809aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13e80a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13e80a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13e80b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13e80b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13e80bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e80c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13e80cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13e80d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13e80db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13e80de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e80e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13e80e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e80e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e80ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13e80f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13e80f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13e80fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13e80ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e810380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13e8107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e810c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e8110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13e811540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13e8119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e811e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e812290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e812700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e812b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e812fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e813450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e8138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13e813d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e8141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e814610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13e814a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e814ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13e815360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13e8157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13e815c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13e8160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13e816620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13e816b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13e816f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13e817400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13e817870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13e817ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13e818150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13e8185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e818a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13e818ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e819310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13e819780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13e819bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13e81a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13e81a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13e81a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e81adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e81b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e81b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e81bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e81bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e81c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e81c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e81ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e81d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13e81d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e81da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e81de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13e81e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e81e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e81ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e81f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e81f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13e81f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e81fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e820200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e820670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e820ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e820f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13e8213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e821830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13e821ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e822110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13e822580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13e8229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e822e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13e8232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e823740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13e8240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e824370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13e8247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13e824c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13e8250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13e825530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13e8259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13e825e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e826280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13e8266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13e826b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e826fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13e827440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13e8278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e827d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13e828190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e828600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13e828a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e828ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13e829350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e8297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e829c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13e82a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e82a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e82a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13e82adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e82b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e82b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13e82bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e82bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13e82c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13e82c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13e82cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e82d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13e82d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e82da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e82dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e82e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13e82e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13e82ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13e82f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13e82f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13e82f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13e82fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e830240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e8306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13e830b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13e830f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13e831400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13e831870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13e831ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e832150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13e8325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13e832a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13e832ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e833310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13e833780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13e833bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e834060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e8344d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13e834940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e834db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13e835220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13e835690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e835b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e835f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e8363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13e836850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13e836cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13e837130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13e8375a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13e837a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13e837e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13e8382f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13e838760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13e838bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e839040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13e8394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13e839920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e839d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e83a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13e83a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13e83aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13e83af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13e83b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13e83b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e83bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e83c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e83c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e83c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13e83ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13e83d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e83d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13e83dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13e83e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13e83e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13e83e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e83ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e83f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13e83f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13e83fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13e83ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e8403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13e840930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e840da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e841210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e841d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e842020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13e8422e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13e842750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13e842bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e843030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13e8434a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13e843910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13e843d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13e8441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13e844660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13e844ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13e844f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13e8453b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e845820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e845c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13e846100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e846570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e8469e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13e846e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e8472c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13e847730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e847ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13e848010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e848480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13e8488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e848d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13e8491d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13e849640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13e849ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e849f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e84a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e84a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13e84ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e84b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13e84b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13e84b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13e84be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13e84c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e84c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13e84cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e84cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13e84d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13e84d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13e84dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e84e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e84e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e84ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e84ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e84f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e84f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13e84fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13e8500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13e850530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13e8509a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13e850e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13e851280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13e8516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13e851b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13e851fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13e852440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13e8528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13e852d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13e853190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13e853600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13e853a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13e853ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13e854350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13e8547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13e854c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13e8550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13e855510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13e855980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13e8563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13e856b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13e857230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13e857950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13e857c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13e858080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13e858680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e858c90 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 967
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.769s
user	0m0.273s
sys	0m0.302s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4792 (172f6169)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x150e0d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x150e0e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x150e0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x150e0ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x150e0f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x150e0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x150e0fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x150e10340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x150e108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x150e10df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x150e112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x150e117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x150e12310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x150e12ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x150e132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x150e139f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x150e14110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x150e14830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x150e14f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x150e15720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x150e15e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x150e16560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x150e16c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x150e17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x150e17c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x150e17f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x150e18510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x150e19180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x150e196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x150e19980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x150e19e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x150e1a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x150e1a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x150e1aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x150e1b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x150e1b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x150e1bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x150e1bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x150e1c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x150e1c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x150e1cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x150e1d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x150e1d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x150e1db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x150e1ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x150e1e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x150e1e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x150e1f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x150e1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x150e1ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x150e20540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x150e20b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x150e21160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x150e21770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x150e21f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x150e22400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x150e228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x150e22b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x150e23170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x150e23960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x150e23c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x150e240c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x150e24560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x150e24a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x150e24ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x150e25340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x150e257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x150e25c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x150e26120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x150e265c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x150e26a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x150e26f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x150e273a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x150e278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x150e27e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x150e28390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x150e288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x150e28e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x150e29380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x150e298d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x150e29e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x150e2a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x150e2a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x150e2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x150e2b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x150e2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x150e2be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x150e2c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x150e2c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x150e2cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x150e2d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x150e2d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x150e2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x150e2e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x150e2e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x150e2edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x150e2f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x150e1f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x150e2f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x150e2ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x150e30490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x150e309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x150e30f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x150e31480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x150e319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x150e31f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x150e32470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x150e329c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x150e32f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x150e33460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x150e339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x150e33f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x150e34450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x150e348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x150e34d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x150e35230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x150e356d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x150e35b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x150e36010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x150e364b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x150e36950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x150e36df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x150e37290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x150e37730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x150e37bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x150e38070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x150e38510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x150e389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x150e38e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x150e392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x150e39790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x150e39c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x150e3a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x150e3a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x150e3aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x150e3aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x150e3b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x150e3b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x150e3bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x150e3c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x150e3c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x150e3ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x150e3cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x150e3d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x150e3d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x150e3dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x150e3e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x150e3e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x150e3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x150e3ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x150e3f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x150e3f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x150e3fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x150e401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x150e40690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x150e40b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x150e40fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x150e41470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x150e41910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x150e41db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x150e42250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x150e426f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x150e42b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x150e43030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x150e434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x150e43970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x150e43e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x150e442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x150e44750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x150e44bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x150e45090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x150e45530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x150e459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x150e45e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x150e46310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x150e467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x150e46c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x150e470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x150e47590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x150e47a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x150e47ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x150e48370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x150e48810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x150e48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x150e49150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x150e495f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x150e49a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x150e49f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x150e4a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x150e4a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x150e4ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x150e4b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x150e4b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x150e4bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x150e4c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x150e4c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x150e4cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x150e4ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x150e4d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x150e4da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x150e4e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x150e4e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x150e4ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x150e4efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x150e4f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x150e4fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x150e503e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x150e50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x150e50d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x150e511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x150e51970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x150e51ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x150e52410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x150e52960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x150e52eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x150e53400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x150e53950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x150e53ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x150e543f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x150e54940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x150e54e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x150e553e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x150e55930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x150e55e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x150e563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x150e56920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x150e56e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x150e573c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x150e57910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x150e57e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x150e583b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x150e58900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x150e58e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x150e593a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x150e598f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x150e59e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x150e5a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x150e5a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x150e5ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x150e5b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x150e5b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x150e5be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x150e5c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x150e5c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x150e5ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x150e5d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x150e5d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x150e5de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x150e5e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x150e5e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x150e5edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x150e5f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x150e5f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x150e5fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x150e60330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x150e60880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x150e60dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x150e61320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x150e61870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x150e61dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x150e62310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x150e62860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x150e62db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x150e63300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x150e63850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x150e63da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x150e642f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x150e64790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x150e64c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x150e650d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x150e65570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x150e65a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x150e65eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x150e66350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x150e667f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x150e66c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x150e67130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x150e675d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x150e67a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x150e67f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x150e683b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x150e68850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x150e68da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x150e694c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x150e69be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x150e6a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x150e6aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x150e6ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x150e6b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x150e6b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x150e6bda0 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 872
init: graph splits = 2
0.00.098.329 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.333 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x152006230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1520066a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x152006b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x152006f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1520073f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x152007860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x152007cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x152008140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1520085b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x152008b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x152008f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1520095f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15200a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15200a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15200b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15200b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15200bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15200c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15200cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15200d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15200dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15200e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15200ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15200f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15200f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15200fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15200fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1520102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x152010720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x152010b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x152011000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x152011530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1520119a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152011c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1520120d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152012540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1520129b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152012e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152013290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152013700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152013b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x152013fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152014450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1520148c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x152014d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1520151a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x152015610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x152015a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x152015ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x152016360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1520167d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x152016c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1520170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x152017520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x152017990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x152017e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x152018370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x152018870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x152018ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x152019150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1520195c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x152019a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x152019ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15201a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15201a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15201abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15201b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15201b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15201b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15201bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15201c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15201c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15201cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15201cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15201d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15201d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15201dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15201e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15201e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15201ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15201ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15201f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15201f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15201fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152020040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1520204b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x152020920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x152020d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x152021200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x152021670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x152021ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x152021f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1520223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x152022830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x152022ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x152023110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x152023580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1520239f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x152023e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1520242d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152024740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x152024bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x152025020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x152025490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x152025900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x152025d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1520261e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x152026650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x152026ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x152026f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1520273a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x152027810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x152027c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1520280f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x152028560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1520289d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x152028e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1520292b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x152029720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x152029b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15202a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15202a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15202a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15202ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15202b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15202b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15202baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15202bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15202c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15202c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15202cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15202d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15202d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15202d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15202de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15202e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15202e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15202eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15202efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15202f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15202f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15202fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1520301a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x152030610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x152030a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x152030ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x152031360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1520317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x152031c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1520320b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152032520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x152032990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x152032e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x152033270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1520336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x152033b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x152033fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x152034430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1520348a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x152034d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x152035180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1520355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152035a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152035ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x152036340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1520367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1520373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1520376a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x152037960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x152037dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x152038240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1520386b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x152038b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x152038f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x152039400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152039870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x152039ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15203a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15203a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15203aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15203aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15203b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15203b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15203bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15203c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15203c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15203c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15203cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15203d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15203d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15203db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15203df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15203e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15203e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15203ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15203f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15203f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15203fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15203fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1520402f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x152040760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x152040bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x152041130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x152041640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x152041ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x152041f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x152042390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x152042800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x152042d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x152043230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x152043da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x152044060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x152044620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x152044be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1520451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152045760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x152045d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1520462e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1520468a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x152046e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x152047420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1520479e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x152047fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152048560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x152048b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1520490e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1520496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x152049c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15204a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15204a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15204ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15204b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15204b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15204bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15204c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15204ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15204d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15204d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15204dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15204e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15204e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15204ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15204f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15204f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15204fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1520503e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1520509a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x152050f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x152051520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152051ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1520520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x152052660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x152052c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1520531e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1520537a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x152053d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x152054320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1520548e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x152054ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x152055460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x152055a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x152055fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1520565a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x152056b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x152057120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1520576e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x152057ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x152058260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x152058760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x152058c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x152059160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x152059660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152059b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15205a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15205a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15205aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15205af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15205b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15205b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15205be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15205c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15205c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15205cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15205d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15205de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15205e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15205ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15205ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15205f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15205fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x152060050 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 872
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x150e4d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x150e4f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x150e6ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x150e4d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x150e4dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x150e20e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x150e20800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x150e22e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x150e4f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x150e181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x150e1ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x150e1f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x150e1e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x150e21420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x150e201f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x150e171c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x150e2fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x150e6afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x150e1a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x150e1a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x150e4feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x150e4e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x150e187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x150e18a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x150e18d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x150e6c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x150e6c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x150e6c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x150e6ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x150e6cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x150e6cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x150e6d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x150e6d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x150e6d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x150e6dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x150e6dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x150e6e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x150e6e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x150e6e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x150e6e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x150e6eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x150e6ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x150e6f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x150e6f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x150e6f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x150e6f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x150e6fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x150e6fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x150e70140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x150e70400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x150e706c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x150e70980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x150e70c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x150e70f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x150e711c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x150e71480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x150e71740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x150e71a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x150e71cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x150e71f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x150e72240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x150e72500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x150e727c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x150e72a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x150e72d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x150e73000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x150e732c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x150e73580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x150e73840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x150e73b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x150e73dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x150e74080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x150e74340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x150e74600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x150e748c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x150e74b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x150e74e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x150e75100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x150e753c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x150e75680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x150e75940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x150e75c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x150e75ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x150e76180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x150e76440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x150e76700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x150e769c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x150e76c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x150e76f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x150e77200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x150e774c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x150e77780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x150e77a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x150e77d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x150e77fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x150e78280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x150e78540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x150e78800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x150e78ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x150e78d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x150e79040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x150e79300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x150e795c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x150e79880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x150e79b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x150e79e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x150e7a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x150e7a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x150e7a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x150e7a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x150e7abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x150e7ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x150e7b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x150e7b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x150e7b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x150e7b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x150e7bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x150e7bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x150e7c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x150e7c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x150e7c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x150e7ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x150e7ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x150e7cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x150e7d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x150e7d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x150e7d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x150e7da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x150e7dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x150e7e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x150e7e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x150e7e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x150e7e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x150e7eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x150e7edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x150e7f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x150e7f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x150e7f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x150e7f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x150e7fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x150e7fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x150e80100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x150e803c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x150e80680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x150e80940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x150e80c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x150e80ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x150e81180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x150e81440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x150e81700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x150e819c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x150e81c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x150e81f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x150e82200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x150e824c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x150e82780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x150e82a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x150e82d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x150e82fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x150e83280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x150e83540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x150e83800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x150e83ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x150e83d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x150e84040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x150e84300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x150e845c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x150e84880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x150e84b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x150e84e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x150e850c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x150e85380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x150e85640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x150e85900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x150e85bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x150e85e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x150e86140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x150e86400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x150e866c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x150e86980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x150e86c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x150e86f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x150e871c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x150e87480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x150e87740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x150e87a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x150e87cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x150e87f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x150e88240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x150e88500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x150e887c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x150e88a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x150e88d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x150e89000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x150e892c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x150e89580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x150e89840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x150e89b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x150e89dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x150e8a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x150e8a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x150e8a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x150e8a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x150e8ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x150e8ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x150e8b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x150e8b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x150e8b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x150e8b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x150e8bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x150e8c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x150e8c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x150e8ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x150e8cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x150e8d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x150e8d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x150e8dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x150e8e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x150e8e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x150e8e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x150e8edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x150e8f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x150e8f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x150e8fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x150e8ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x150e90420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x150e90890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x150e90d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x150e91170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x150e915e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x150e91a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x150e91ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x150e92330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x150e927a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x150e92c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x150e93080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x150e934f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x150e93960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x150e93dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x150e94240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x150e946b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x150e94b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x150e94f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x150e95400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x150e95870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x150e95ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x150e96150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x150e965c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x150e96a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x150e96ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x150e97310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x150e97780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x150e97bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x150e98060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x150e984d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x150e98940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x150e98db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x150e99220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x150e99690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x150e99b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x150e99f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x150e9a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x150e9a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x150e9acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x150e9b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x150e9b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x150e9ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x150e9be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x150e9c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x150e9c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x150e9cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x150e9d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x150e9d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x150e9d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x150e9dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x150e9e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x150e9e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x150e9eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x150e9ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x150e9f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x150e9f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x150e9fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x150ea0110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x150ea0b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x150ea12a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x150ea19c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x150ea20e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x150ea23a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x150ea2b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x150ea2e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x150ea3460 | th_max = 1024 | th_width =   32
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context_kv_self: n_ctx = 2048
llama_context_kv_self: n_ctx = 2048 (padded)
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_kv_self: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
init:      Metal compute buffer size =   102.25 MiB
init:        CPU compute buffer size =     8.01 MiB
init: graph nodes  = 872
init: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.955s
user	0m0.231s
sys	0m0.187s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.07 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.50 sec*proc (2 tests)

Total Test time (real) =   1.51 sec
        1.53 real         0.52 user         0.19 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.22 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.53 real         0.12 user         0.08 sys
```
