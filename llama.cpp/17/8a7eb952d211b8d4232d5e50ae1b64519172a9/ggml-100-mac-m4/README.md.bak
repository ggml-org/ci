### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.24 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.65 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.63 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.40 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.27 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.32 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.92 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.31 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.31 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.19 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.25 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.24 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.08 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.28 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    2.81 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.82 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  189.32 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.88 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.14 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.34 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.24 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 234.95 sec*proc (28 tests)

Total Test time (real) = 234.96 sec

real	3m55.075s
user	8m15.899s
sys	0m6.997s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.16 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.21 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.29 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.19 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.42 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.40 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   30.39 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.38 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.07 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  52.81 sec*proc (28 tests)

Total Test time (real) =  52.82 sec

real	0m52.808s
user	1m14.706s
sys	0m6.334s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.068 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.010 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.778 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.019.784 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.787 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.019.787 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.788 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.019.789 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.019.789 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.019.791 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.019.791 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.019.792 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.019.793 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.019.793 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.019.796 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.019.796 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.019.800 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.019.800 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.019.801 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.019.801 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.019.802 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.024.157 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.025.357 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.359 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.025.360 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.025.360 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.025.361 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.025.361 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.025.361 I llama_model_loader: - type  f32:  124 tensors
0.00.025.362 I llama_model_loader: - type  f16:   73 tensors
0.00.025.365 I print_info: file format = GGUF V3 (latest)
0.00.025.365 I print_info: file type   = F16
0.00.025.366 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.029.413 I load: special tokens cache size = 5
0.00.031.627 I load: token to piece cache size = 0.2032 MB
0.00.031.631 I print_info: arch             = bert
0.00.031.631 I print_info: vocab_only       = 0
0.00.031.632 I print_info: n_ctx_train      = 512
0.00.031.632 I print_info: n_embd           = 384
0.00.031.632 I print_info: n_layer          = 12
0.00.031.635 I print_info: n_head           = 12
0.00.031.636 I print_info: n_head_kv        = 12
0.00.031.636 I print_info: n_rot            = 32
0.00.031.637 I print_info: n_swa            = 0
0.00.031.637 I print_info: n_embd_head_k    = 32
0.00.031.637 I print_info: n_embd_head_v    = 32
0.00.031.638 I print_info: n_gqa            = 1
0.00.031.639 I print_info: n_embd_k_gqa     = 384
0.00.031.640 I print_info: n_embd_v_gqa     = 384
0.00.031.641 I print_info: f_norm_eps       = 1.0e-12
0.00.031.641 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.031.642 I print_info: f_clamp_kqv      = 0.0e+00
0.00.031.642 I print_info: f_max_alibi_bias = 0.0e+00
0.00.031.642 I print_info: f_logit_scale    = 0.0e+00
0.00.031.643 I print_info: n_ff             = 1536
0.00.031.643 I print_info: n_expert         = 0
0.00.031.643 I print_info: n_expert_used    = 0
0.00.031.644 I print_info: causal attn      = 0
0.00.031.644 I print_info: pooling type     = 2
0.00.031.644 I print_info: rope type        = 2
0.00.031.644 I print_info: rope scaling     = linear
0.00.031.647 I print_info: freq_base_train  = 10000.0
0.00.031.647 I print_info: freq_scale_train = 1
0.00.031.648 I print_info: n_ctx_orig_yarn  = 512
0.00.031.648 I print_info: rope_finetuned   = unknown
0.00.031.648 I print_info: ssm_d_conv       = 0
0.00.031.648 I print_info: ssm_d_inner      = 0
0.00.031.649 I print_info: ssm_d_state      = 0
0.00.031.649 I print_info: ssm_dt_rank      = 0
0.00.031.649 I print_info: ssm_dt_b_c_rms   = 0
0.00.031.649 I print_info: model type       = 33M
0.00.031.650 I print_info: model params     = 33.21 M
0.00.031.650 I print_info: general.name     = Bge Small
0.00.031.651 I print_info: vocab type       = WPM
0.00.031.651 I print_info: n_vocab          = 30522
0.00.031.651 I print_info: n_merges         = 0
0.00.031.651 I print_info: BOS token        = 101 '[CLS]'
0.00.031.652 I print_info: UNK token        = 100 '[UNK]'
0.00.031.652 I print_info: SEP token        = 102 '[SEP]'
0.00.031.652 I print_info: PAD token        = 0 '[PAD]'
0.00.031.652 I print_info: MASK token       = 103 '[MASK]'
0.00.031.653 I print_info: LF token         = 0 '[PAD]'
0.00.031.653 I print_info: max token length = 21
0.00.034.914 I load_tensors: offloading 12 repeating layers to GPU
0.00.034.916 I load_tensors: offloading output layer to GPU
0.00.034.916 I load_tensors: offloaded 13/13 layers to GPU
0.00.034.940 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.034.942 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.035.184 I llama_init_from_model: n_seq_max     = 1
0.00.035.186 I llama_init_from_model: n_ctx         = 512
0.00.035.186 I llama_init_from_model: n_ctx_per_seq = 512
0.00.035.186 I llama_init_from_model: n_batch       = 2048
0.00.035.186 I llama_init_from_model: n_ubatch      = 2048
0.00.035.187 I llama_init_from_model: flash_attn    = 0
0.00.035.187 I llama_init_from_model: freq_base     = 10000.0
0.00.035.188 I llama_init_from_model: freq_scale    = 1
0.00.035.188 I ggml_metal_init: allocating
0.00.035.193 I ggml_metal_init: found device: Apple M4
0.00.035.196 I ggml_metal_init: picking default device: Apple M4
0.00.035.908 I ggml_metal_init: using embedded metal library
0.00.039.907 I ggml_metal_init: GPU name:   Apple M4
0.00.039.910 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.039.910 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.039.911 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.039.911 I ggml_metal_init: simdgroup reduction   = true
0.00.039.911 I ggml_metal_init: simdgroup matrix mul. = true
0.00.039.912 I ggml_metal_init: has residency sets    = true
0.00.039.912 I ggml_metal_init: has bfloat            = true
0.00.039.912 I ggml_metal_init: use bfloat            = true
0.00.039.912 I ggml_metal_init: hasUnifiedMemory      = true
0.00.039.913 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.051.391 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.052.035 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.052.037 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.052.038 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.053.168 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.053.170 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.053.170 I llama_init_from_model: graph nodes  = 429
0.00.053.170 I llama_init_from_model: graph splits = 2
0.00.053.172 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.053.172 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.058.709 I 
0.00.058.740 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.059.379 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.064.624 I llama_perf_context_print:        load time =      43.69 ms
0.00.064.625 I llama_perf_context_print: prompt eval time =       5.09 ms /     9 tokens (    0.57 ms per token,  1768.17 tokens per second)
0.00.064.626 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.064.626 I llama_perf_context_print:       total time =       5.92 ms /    10 tokens
0.00.064.775 I ggml_metal_free: deallocating

real	0m0.244s
user	0m0.047s
sys	0m0.029s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.039 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.630 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.530 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.534 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.535 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.536 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.536 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.537 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.537 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.538 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.538 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.539 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.539 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.539 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.542 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.542 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.542 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.543 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.543 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.543 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.015.068 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.781 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.782 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.783 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.783 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.783 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.784 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.784 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.784 I llama_model_loader: - type  f32:  124 tensors
0.00.015.785 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.785 I print_info: file format = GGUF V3 (latest)
0.00.015.786 I print_info: file type   = Q8_0
0.00.015.787 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.018.292 I load: special tokens cache size = 5
0.00.019.638 I load: token to piece cache size = 0.2032 MB
0.00.019.643 I print_info: arch             = bert
0.00.019.643 I print_info: vocab_only       = 0
0.00.019.643 I print_info: n_ctx_train      = 512
0.00.019.643 I print_info: n_embd           = 384
0.00.019.644 I print_info: n_layer          = 12
0.00.019.647 I print_info: n_head           = 12
0.00.019.647 I print_info: n_head_kv        = 12
0.00.019.648 I print_info: n_rot            = 32
0.00.019.648 I print_info: n_swa            = 0
0.00.019.648 I print_info: n_embd_head_k    = 32
0.00.019.648 I print_info: n_embd_head_v    = 32
0.00.019.649 I print_info: n_gqa            = 1
0.00.019.649 I print_info: n_embd_k_gqa     = 384
0.00.019.650 I print_info: n_embd_v_gqa     = 384
0.00.019.650 I print_info: f_norm_eps       = 1.0e-12
0.00.019.651 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.651 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.651 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.651 I print_info: f_logit_scale    = 0.0e+00
0.00.019.652 I print_info: n_ff             = 1536
0.00.019.652 I print_info: n_expert         = 0
0.00.019.652 I print_info: n_expert_used    = 0
0.00.019.652 I print_info: causal attn      = 0
0.00.019.652 I print_info: pooling type     = 2
0.00.019.652 I print_info: rope type        = 2
0.00.019.652 I print_info: rope scaling     = linear
0.00.019.653 I print_info: freq_base_train  = 10000.0
0.00.019.653 I print_info: freq_scale_train = 1
0.00.019.653 I print_info: n_ctx_orig_yarn  = 512
0.00.019.653 I print_info: rope_finetuned   = unknown
0.00.019.653 I print_info: ssm_d_conv       = 0
0.00.019.654 I print_info: ssm_d_inner      = 0
0.00.019.656 I print_info: ssm_d_state      = 0
0.00.019.656 I print_info: ssm_dt_rank      = 0
0.00.019.656 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.656 I print_info: model type       = 33M
0.00.019.656 I print_info: model params     = 33.21 M
0.00.019.658 I print_info: general.name     = Bge Small
0.00.019.658 I print_info: vocab type       = WPM
0.00.019.659 I print_info: n_vocab          = 30522
0.00.019.659 I print_info: n_merges         = 0
0.00.019.659 I print_info: BOS token        = 101 '[CLS]'
0.00.019.659 I print_info: UNK token        = 100 '[UNK]'
0.00.019.659 I print_info: SEP token        = 102 '[SEP]'
0.00.019.659 I print_info: PAD token        = 0 '[PAD]'
0.00.019.660 I print_info: MASK token       = 103 '[MASK]'
0.00.019.660 I print_info: LF token         = 0 '[PAD]'
0.00.019.660 I print_info: max token length = 21
0.00.021.540 I load_tensors: offloading 12 repeating layers to GPU
0.00.021.541 I load_tensors: offloading output layer to GPU
0.00.021.542 I load_tensors: offloaded 13/13 layers to GPU
0.00.021.549 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.021.550 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.021.775 I llama_init_from_model: n_seq_max     = 1
0.00.021.776 I llama_init_from_model: n_ctx         = 512
0.00.021.776 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.776 I llama_init_from_model: n_batch       = 2048
0.00.021.776 I llama_init_from_model: n_ubatch      = 2048
0.00.021.776 I llama_init_from_model: flash_attn    = 0
0.00.021.777 I llama_init_from_model: freq_base     = 10000.0
0.00.021.777 I llama_init_from_model: freq_scale    = 1
0.00.021.777 I ggml_metal_init: allocating
0.00.021.790 I ggml_metal_init: found device: Apple M4
0.00.021.793 I ggml_metal_init: picking default device: Apple M4
0.00.022.308 I ggml_metal_init: using embedded metal library
0.00.024.900 I ggml_metal_init: GPU name:   Apple M4
0.00.024.902 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.902 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.903 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.903 I ggml_metal_init: simdgroup reduction   = true
0.00.024.903 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.903 I ggml_metal_init: has residency sets    = true
0.00.024.903 I ggml_metal_init: has bfloat            = true
0.00.024.904 I ggml_metal_init: use bfloat            = true
0.00.024.904 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.905 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.035.264 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.863 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.867 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.870 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.036.875 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.036.877 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.036.877 I llama_init_from_model: graph nodes  = 429
0.00.036.877 I llama_init_from_model: graph splits = 2
0.00.036.878 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.878 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.976 I 
0.00.041.000 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.526 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.976 I llama_perf_context_print:        load time =      31.34 ms
0.00.045.982 I llama_perf_context_print: prompt eval time =       4.31 ms /     9 tokens (    0.48 ms per token,  2088.65 tokens per second)
0.00.045.983 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.983 I llama_perf_context_print:       total time =       5.00 ms /    10 tokens
0.00.046.184 I ggml_metal_free: deallocating

real	0m0.059s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.180 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.772 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.489 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.494 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.496 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.497 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.498 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.498 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.499 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.500 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.501 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.502 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.503 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.503 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.507 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.508 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.508 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.509 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.509 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.593 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.709 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.181 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.183 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.184 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.184 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.185 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.185 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.185 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.186 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.186 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.186 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.187 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.049.187 I llama_model_loader: - type  f32:   40 tensors
0.00.049.188 I llama_model_loader: - type  f16:   30 tensors
0.00.049.188 I print_info: file format = GGUF V3 (latest)
0.00.049.189 I print_info: file type   = F16
0.00.049.190 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.065.650 W load: empty token at index 5
0.00.070.110 W load: model vocab missing newline token, using special_pad_id instead
0.00.071.396 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.071.426 I load: special tokens cache size = 5
0.00.333.233 I load: token to piece cache size = 1.5060 MB
0.00.333.239 I print_info: arch             = jina-bert-v2
0.00.333.239 I print_info: vocab_only       = 0
0.00.333.240 I print_info: n_ctx_train      = 8192
0.00.333.240 I print_info: n_embd           = 384
0.00.333.240 I print_info: n_layer          = 4
0.00.333.249 I print_info: n_head           = 12
0.00.333.250 I print_info: n_head_kv        = 12
0.00.333.250 I print_info: n_rot            = 32
0.00.333.251 I print_info: n_swa            = 0
0.00.333.251 I print_info: n_embd_head_k    = 32
0.00.333.251 I print_info: n_embd_head_v    = 32
0.00.333.252 I print_info: n_gqa            = 1
0.00.333.253 I print_info: n_embd_k_gqa     = 384
0.00.333.253 I print_info: n_embd_v_gqa     = 384
0.00.333.254 I print_info: f_norm_eps       = 1.0e-12
0.00.333.255 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.333.255 I print_info: f_clamp_kqv      = 0.0e+00
0.00.333.256 I print_info: f_max_alibi_bias = 8.0e+00
0.00.333.256 I print_info: f_logit_scale    = 0.0e+00
0.00.333.256 I print_info: n_ff             = 1536
0.00.333.256 I print_info: n_expert         = 0
0.00.333.257 I print_info: n_expert_used    = 0
0.00.333.257 I print_info: causal attn      = 0
0.00.333.257 I print_info: pooling type     = -1
0.00.333.257 I print_info: rope type        = -1
0.00.333.257 I print_info: rope scaling     = linear
0.00.333.258 I print_info: freq_base_train  = 10000.0
0.00.333.258 I print_info: freq_scale_train = 1
0.00.333.258 I print_info: n_ctx_orig_yarn  = 8192
0.00.333.260 I print_info: rope_finetuned   = unknown
0.00.333.260 I print_info: ssm_d_conv       = 0
0.00.333.260 I print_info: ssm_d_inner      = 0
0.00.333.260 I print_info: ssm_d_state      = 0
0.00.333.260 I print_info: ssm_dt_rank      = 0
0.00.333.260 I print_info: ssm_dt_b_c_rms   = 0
0.00.333.261 I print_info: model type       = 33M
0.00.333.261 I print_info: model params     = 32.90 M
0.00.333.262 I print_info: general.name     = Jina Bert Implementation
0.00.333.262 I print_info: vocab type       = BPE
0.00.333.263 I print_info: n_vocab          = 61056
0.00.333.263 I print_info: n_merges         = 39382
0.00.333.263 I print_info: BOS token        = 0 '<s>'
0.00.333.263 I print_info: EOS token        = 2 '</s>'
0.00.333.263 I print_info: UNK token        = 3 '<unk>'
0.00.333.263 I print_info: SEP token        = 2 '</s>'
0.00.333.264 I print_info: PAD token        = 1 '<pad>'
0.00.333.264 I print_info: MASK token       = 4 '<mask>'
0.00.333.264 I print_info: EOG token        = 2 '</s>'
0.00.333.264 I print_info: max token length = 45
0.00.335.548 I load_tensors: offloading 4 repeating layers to GPU
0.00.335.549 I load_tensors: offloading output layer to GPU
0.00.335.549 I load_tensors: offloaded 5/5 layers to GPU
0.00.335.573 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.335.574 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.336.019 I llama_init_from_model: n_seq_max     = 1
0.00.336.020 I llama_init_from_model: n_ctx         = 8192
0.00.336.020 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.336.020 I llama_init_from_model: n_batch       = 2048
0.00.336.020 I llama_init_from_model: n_ubatch      = 2048
0.00.336.021 I llama_init_from_model: flash_attn    = 0
0.00.336.021 I llama_init_from_model: freq_base     = 10000.0
0.00.336.021 I llama_init_from_model: freq_scale    = 1
0.00.336.022 I ggml_metal_init: allocating
0.00.336.026 I ggml_metal_init: found device: Apple M4
0.00.336.027 I ggml_metal_init: picking default device: Apple M4
0.00.336.876 I ggml_metal_init: using embedded metal library
0.00.339.683 I ggml_metal_init: GPU name:   Apple M4
0.00.339.684 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.339.685 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.339.685 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.339.686 I ggml_metal_init: simdgroup reduction   = true
0.00.339.686 I ggml_metal_init: simdgroup matrix mul. = true
0.00.339.686 I ggml_metal_init: has residency sets    = true
0.00.339.686 I ggml_metal_init: has bfloat            = true
0.00.339.686 I ggml_metal_init: use bfloat            = true
0.00.339.687 I ggml_metal_init: hasUnifiedMemory      = true
0.00.339.688 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.349.087 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.352.080 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.352.082 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.352.083 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.357.972 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.357.973 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.357.973 I llama_init_from_model: graph nodes  = 154
0.00.357.974 I llama_init_from_model: graph splits = 2
0.00.357.975 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.357.975 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.365.225 I 
0.00.365.255 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.365.679 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.365.680 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.365.691 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.365.691 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.365.694 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.365.694 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.366.199 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.369.757 I llama_perf_context_print:        load time =     342.45 ms
0.00.369.759 I llama_perf_context_print: prompt eval time =       3.54 ms /    62 tokens (    0.06 ms per token, 17538.90 tokens per second)
0.00.369.760 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.369.760 I llama_perf_context_print:       total time =       4.53 ms /    63 tokens
0.00.370.002 I ggml_metal_free: deallocating

real	0m1.096s
user	0m0.340s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.145 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.326 I main: llama backend init
0.00.000.336 I main: load the model and apply lora adapter, if any
0.00.042.052 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.054.770 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.054.788 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.054.793 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.054.794 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.054.794 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.054.795 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.054.795 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.054.798 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.054.798 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.054.799 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.054.799 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.054.800 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.054.800 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.054.801 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.054.806 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.054.806 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.054.807 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.061.859 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.064.056 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.073.119 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.073.129 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.073.129 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.073.130 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.073.130 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.073.131 I llama_model_loader: - type  f32:  194 tensors
0.00.073.132 I llama_model_loader: - type  f16:   98 tensors
0.00.073.141 I print_info: file format = GGUF V3 (latest)
0.00.073.142 I print_info: file type   = all F32 (guessed)
0.00.073.144 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.106.140 I load: special tokens cache size = 25
0.00.113.404 I load: token to piece cache size = 0.2984 MB
0.00.113.407 I print_info: arch             = gptneox
0.00.113.407 I print_info: vocab_only       = 0
0.00.113.408 I print_info: n_ctx_train      = 2048
0.00.113.408 I print_info: n_embd           = 2048
0.00.113.408 I print_info: n_layer          = 24
0.00.113.411 I print_info: n_head           = 16
0.00.113.412 I print_info: n_head_kv        = 16
0.00.113.412 I print_info: n_rot            = 32
0.00.113.412 I print_info: n_swa            = 0
0.00.113.413 I print_info: n_embd_head_k    = 128
0.00.113.415 I print_info: n_embd_head_v    = 128
0.00.113.415 I print_info: n_gqa            = 1
0.00.113.416 I print_info: n_embd_k_gqa     = 2048
0.00.113.417 I print_info: n_embd_v_gqa     = 2048
0.00.113.417 I print_info: f_norm_eps       = 1.0e-05
0.00.113.419 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.113.419 I print_info: f_clamp_kqv      = 0.0e+00
0.00.113.420 I print_info: f_max_alibi_bias = 0.0e+00
0.00.113.420 I print_info: f_logit_scale    = 0.0e+00
0.00.113.420 I print_info: n_ff             = 8192
0.00.113.421 I print_info: n_expert         = 0
0.00.113.421 I print_info: n_expert_used    = 0
0.00.113.421 I print_info: causal attn      = 1
0.00.113.421 I print_info: pooling type     = 0
0.00.113.422 I print_info: rope type        = 2
0.00.113.422 I print_info: rope scaling     = linear
0.00.113.423 I print_info: freq_base_train  = 10000.0
0.00.113.423 I print_info: freq_scale_train = 1
0.00.113.423 I print_info: n_ctx_orig_yarn  = 2048
0.00.113.423 I print_info: rope_finetuned   = unknown
0.00.113.424 I print_info: ssm_d_conv       = 0
0.00.113.424 I print_info: ssm_d_inner      = 0
0.00.113.425 I print_info: ssm_d_state      = 0
0.00.113.425 I print_info: ssm_dt_rank      = 0
0.00.113.425 I print_info: ssm_dt_b_c_rms   = 0
0.00.113.426 I print_info: model type       = 1.4B
0.00.113.426 I print_info: model params     = 1.41 B
0.00.113.426 I print_info: general.name     = 1.4B
0.00.113.427 I print_info: vocab type       = BPE
0.00.113.427 I print_info: n_vocab          = 50304
0.00.113.427 I print_info: n_merges         = 50009
0.00.113.427 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.113.427 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.113.428 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.113.428 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.113.428 I print_info: LF token         = 128 'Ä'
0.00.113.428 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.113.429 I print_info: max token length = 1024
0.00.149.489 I load_tensors: offloading 24 repeating layers to GPU
0.00.149.493 I load_tensors: offloading output layer to GPU
0.00.149.493 I load_tensors: offloaded 25/25 layers to GPU
0.00.149.516 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.149.517 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.149.800 I llama_init_from_model: n_seq_max     = 1
0.00.149.801 I llama_init_from_model: n_ctx         = 2048
0.00.149.802 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.149.802 I llama_init_from_model: n_batch       = 2048
0.00.149.802 I llama_init_from_model: n_ubatch      = 512
0.00.149.802 I llama_init_from_model: flash_attn    = 0
0.00.149.803 I llama_init_from_model: freq_base     = 10000.0
0.00.149.803 I llama_init_from_model: freq_scale    = 1
0.00.149.803 I ggml_metal_init: allocating
0.00.149.822 I ggml_metal_init: found device: Apple M4
0.00.149.825 I ggml_metal_init: picking default device: Apple M4
0.00.150.562 I ggml_metal_init: using embedded metal library
0.00.159.211 I ggml_metal_init: GPU name:   Apple M4
0.00.159.213 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.159.213 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.159.214 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.159.214 I ggml_metal_init: simdgroup reduction   = true
0.00.159.214 I ggml_metal_init: simdgroup matrix mul. = true
0.00.159.214 I ggml_metal_init: has residency sets    = true
0.00.159.214 I ggml_metal_init: has bfloat            = true
0.00.159.214 I ggml_metal_init: use bfloat            = true
0.00.159.215 I ggml_metal_init: hasUnifiedMemory      = true
0.00.159.216 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.194.341 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.223.870 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.223.876 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.223.898 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.228.098 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.228.100 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.228.100 I llama_init_from_model: graph nodes  = 967
0.00.228.101 I llama_init_from_model: graph splits = 2
0.00.228.104 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.228.224 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.228.224 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.294.271 I main: llama threadpool init, n_threads = 4
0.00.294.312 I 
0.00.294.350 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.294.352 I 
0.00.294.417 I sampler seed: 1234
0.00.294.422 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.294.446 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.294.448 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.294.448 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.143.180 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60425.53 tokens per second)
0.02.143.180 I llama_perf_context_print:        load time =     251.19 ms
0.02.143.181 I llama_perf_context_print: prompt eval time =      53.50 ms /     7 tokens (    7.64 ms per token,   130.85 tokens per second)
0.02.143.182 I llama_perf_context_print:        eval time =    1792.37 ms /    63 runs   (   28.45 ms per token,    35.15 tokens per second)
0.02.143.182 I llama_perf_context_print:       total time =    1849.93 ms /    70 tokens
0.02.143.406 I ggml_metal_free: deallocating

real	0m2.484s
user	0m0.148s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.650 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.028.260 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.042.107 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.042.135 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.139 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.140 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.140 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.141 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.142 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.145 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.146 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.146 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.147 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.151 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.152 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.154 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.158 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.159 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.159 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.051.226 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.053.515 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.565 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.061.568 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.569 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.569 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.570 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.571 I llama_model_loader: - type  f32:  194 tensors
0.00.061.571 I llama_model_loader: - type  f16:   98 tensors
0.00.061.572 I print_info: file format = GGUF V3 (latest)
0.00.061.578 I print_info: file type   = all F32 (guessed)
0.00.061.581 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.091.405 I load: special tokens cache size = 25
0.00.098.660 I load: token to piece cache size = 0.2984 MB
0.00.098.665 I print_info: arch             = gptneox
0.00.098.665 I print_info: vocab_only       = 0
0.00.098.665 I print_info: n_ctx_train      = 2048
0.00.098.666 I print_info: n_embd           = 2048
0.00.098.666 I print_info: n_layer          = 24
0.00.098.670 I print_info: n_head           = 16
0.00.098.671 I print_info: n_head_kv        = 16
0.00.098.671 I print_info: n_rot            = 32
0.00.098.671 I print_info: n_swa            = 0
0.00.098.671 I print_info: n_embd_head_k    = 128
0.00.098.673 I print_info: n_embd_head_v    = 128
0.00.098.673 I print_info: n_gqa            = 1
0.00.098.675 I print_info: n_embd_k_gqa     = 2048
0.00.098.675 I print_info: n_embd_v_gqa     = 2048
0.00.098.676 I print_info: f_norm_eps       = 1.0e-05
0.00.098.676 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.098.677 I print_info: f_clamp_kqv      = 0.0e+00
0.00.098.677 I print_info: f_max_alibi_bias = 0.0e+00
0.00.098.677 I print_info: f_logit_scale    = 0.0e+00
0.00.098.678 I print_info: n_ff             = 8192
0.00.098.678 I print_info: n_expert         = 0
0.00.098.678 I print_info: n_expert_used    = 0
0.00.098.678 I print_info: causal attn      = 1
0.00.098.678 I print_info: pooling type     = 0
0.00.098.678 I print_info: rope type        = 2
0.00.098.679 I print_info: rope scaling     = linear
0.00.098.679 I print_info: freq_base_train  = 10000.0
0.00.098.679 I print_info: freq_scale_train = 1
0.00.098.680 I print_info: n_ctx_orig_yarn  = 2048
0.00.098.680 I print_info: rope_finetuned   = unknown
0.00.098.680 I print_info: ssm_d_conv       = 0
0.00.098.680 I print_info: ssm_d_inner      = 0
0.00.098.681 I print_info: ssm_d_state      = 0
0.00.098.681 I print_info: ssm_dt_rank      = 0
0.00.098.681 I print_info: ssm_dt_b_c_rms   = 0
0.00.098.681 I print_info: model type       = 1.4B
0.00.098.682 I print_info: model params     = 1.41 B
0.00.098.682 I print_info: general.name     = 1.4B
0.00.098.683 I print_info: vocab type       = BPE
0.00.098.683 I print_info: n_vocab          = 50304
0.00.098.683 I print_info: n_merges         = 50009
0.00.098.683 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.098.683 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.098.684 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.098.684 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.098.684 I print_info: LF token         = 128 'Ä'
0.00.098.684 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.098.685 I print_info: max token length = 1024
0.00.964.000 I load_tensors: offloading 24 repeating layers to GPU
0.00.964.005 I load_tensors: offloading output layer to GPU
0.00.964.006 I load_tensors: offloaded 25/25 layers to GPU
0.00.964.031 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.964.033 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.964.800 I llama_init_from_model: n_seq_max     = 1
0.00.964.802 I llama_init_from_model: n_ctx         = 128
0.00.964.802 I llama_init_from_model: n_ctx_per_seq = 128
0.00.964.802 I llama_init_from_model: n_batch       = 128
0.00.964.803 I llama_init_from_model: n_ubatch      = 128
0.00.964.803 I llama_init_from_model: flash_attn    = 0
0.00.964.803 I llama_init_from_model: freq_base     = 10000.0
0.00.964.804 I llama_init_from_model: freq_scale    = 1
0.00.964.804 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.964.805 I ggml_metal_init: allocating
0.00.964.865 I ggml_metal_init: found device: Apple M4
0.00.964.869 I ggml_metal_init: picking default device: Apple M4
0.00.966.022 I ggml_metal_init: using embedded metal library
0.00.970.062 I ggml_metal_init: GPU name:   Apple M4
0.00.970.065 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.970.066 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.970.066 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.970.067 I ggml_metal_init: simdgroup reduction   = true
0.00.970.067 I ggml_metal_init: simdgroup matrix mul. = true
0.00.970.067 I ggml_metal_init: has residency sets    = true
0.00.970.067 I ggml_metal_init: has bfloat            = true
0.00.970.067 I ggml_metal_init: use bfloat            = true
0.00.970.068 I ggml_metal_init: hasUnifiedMemory      = true
0.00.970.069 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.981.377 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.983.191 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.983.193 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.983.208 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.984.929 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.984.930 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.984.930 I llama_init_from_model: graph nodes  = 967
0.00.984.931 I llama_init_from_model: graph splits = 2
0.00.984.932 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.984.932 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.021.725 I 
0.01.021.769 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.021.795 I perplexity: tokenizing the input ..
0.01.031.951 I perplexity: tokenization took 10.153 ms
0.01.031.977 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.150.929 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.152.226 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.152.241 I llama_perf_context_print:        load time =     993.45 ms
0.01.152.242 I llama_perf_context_print: prompt eval time =     118.60 ms /   128 tokens (    0.93 ms per token,  1079.26 tokens per second)
0.01.152.243 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.152.243 I llama_perf_context_print:       total time =     130.52 ms /   129 tokens
0.01.152.687 I ggml_metal_free: deallocating

real	0m1.340s
user	0m0.123s
sys	0m0.208s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.921 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.481 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.488 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.494 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.494 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.495 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.495 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.495 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.496 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.497 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.497 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.497 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.499 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.499 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.500 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.501 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.502 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.502 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.462 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.510 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.403 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.405 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.405 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.406 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.406 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.406 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.407 I llama_model_loader: - type  f32:  194 tensors
0.00.037.407 I llama_model_loader: - type q8_0:   98 tensors
0.00.037.408 I print_info: file format = GGUF V3 (latest)
0.00.037.409 I print_info: file type   = Q8_0
0.00.037.410 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.058.688 I load: special tokens cache size = 25
0.00.065.046 I load: token to piece cache size = 0.2984 MB
0.00.065.050 I print_info: arch             = gptneox
0.00.065.050 I print_info: vocab_only       = 0
0.00.065.050 I print_info: n_ctx_train      = 2048
0.00.065.051 I print_info: n_embd           = 2048
0.00.065.051 I print_info: n_layer          = 24
0.00.065.056 I print_info: n_head           = 16
0.00.065.057 I print_info: n_head_kv        = 16
0.00.065.057 I print_info: n_rot            = 32
0.00.065.057 I print_info: n_swa            = 0
0.00.065.057 I print_info: n_embd_head_k    = 128
0.00.065.058 I print_info: n_embd_head_v    = 128
0.00.065.058 I print_info: n_gqa            = 1
0.00.065.059 I print_info: n_embd_k_gqa     = 2048
0.00.065.059 I print_info: n_embd_v_gqa     = 2048
0.00.065.060 I print_info: f_norm_eps       = 1.0e-05
0.00.065.060 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.065.060 I print_info: f_clamp_kqv      = 0.0e+00
0.00.065.060 I print_info: f_max_alibi_bias = 0.0e+00
0.00.065.061 I print_info: f_logit_scale    = 0.0e+00
0.00.065.062 I print_info: n_ff             = 8192
0.00.065.062 I print_info: n_expert         = 0
0.00.065.062 I print_info: n_expert_used    = 0
0.00.065.062 I print_info: causal attn      = 1
0.00.065.062 I print_info: pooling type     = 0
0.00.065.062 I print_info: rope type        = 2
0.00.065.062 I print_info: rope scaling     = linear
0.00.065.063 I print_info: freq_base_train  = 10000.0
0.00.065.063 I print_info: freq_scale_train = 1
0.00.065.063 I print_info: n_ctx_orig_yarn  = 2048
0.00.065.063 I print_info: rope_finetuned   = unknown
0.00.065.064 I print_info: ssm_d_conv       = 0
0.00.065.064 I print_info: ssm_d_inner      = 0
0.00.065.064 I print_info: ssm_d_state      = 0
0.00.065.064 I print_info: ssm_dt_rank      = 0
0.00.065.064 I print_info: ssm_dt_b_c_rms   = 0
0.00.065.064 I print_info: model type       = 1.4B
0.00.065.065 I print_info: model params     = 1.41 B
0.00.065.065 I print_info: general.name     = 1.4B
0.00.065.065 I print_info: vocab type       = BPE
0.00.065.066 I print_info: n_vocab          = 50304
0.00.065.066 I print_info: n_merges         = 50009
0.00.065.066 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.065.066 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.065.066 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.065.066 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.065.067 I print_info: LF token         = 128 'Ä'
0.00.065.067 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.065.067 I print_info: max token length = 1024
0.01.077.722 I load_tensors: offloading 24 repeating layers to GPU
0.01.077.727 I load_tensors: offloading output layer to GPU
0.01.077.728 I load_tensors: offloaded 25/25 layers to GPU
0.01.077.752 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.077.755 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.078.409 I llama_init_from_model: n_seq_max     = 1
0.01.078.411 I llama_init_from_model: n_ctx         = 2048
0.01.078.411 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.078.412 I llama_init_from_model: n_batch       = 2048
0.01.078.412 I llama_init_from_model: n_ubatch      = 512
0.01.078.412 I llama_init_from_model: flash_attn    = 0
0.01.078.413 I llama_init_from_model: freq_base     = 10000.0
0.01.078.414 I llama_init_from_model: freq_scale    = 1
0.01.078.415 I ggml_metal_init: allocating
0.01.078.425 I ggml_metal_init: found device: Apple M4
0.01.078.429 I ggml_metal_init: picking default device: Apple M4
0.01.079.729 I ggml_metal_init: using embedded metal library
0.01.084.695 I ggml_metal_init: GPU name:   Apple M4
0.01.084.698 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.084.698 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.084.699 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.084.700 I ggml_metal_init: simdgroup reduction   = true
0.01.084.700 I ggml_metal_init: simdgroup matrix mul. = true
0.01.084.700 I ggml_metal_init: has residency sets    = true
0.01.084.700 I ggml_metal_init: has bfloat            = true
0.01.084.700 I ggml_metal_init: use bfloat            = true
0.01.084.701 I ggml_metal_init: hasUnifiedMemory      = true
0.01.084.702 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.100.108 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.154.427 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.154.445 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.154.472 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.159.041 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.159.043 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.159.044 I llama_init_from_model: graph nodes  = 967
0.01.159.044 I llama_init_from_model: graph splits = 2
0.01.159.049 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.159.167 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.159.167 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.215.123 I main: llama threadpool init, n_threads = 4
0.01.215.159 I 
0.01.215.182 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.215.183 I 
0.01.215.410 I sampler seed: 1234
0.01.215.415 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.215.457 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.215.460 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.215.460 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.316.616 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50035.24 tokens per second)
0.02.316.616 I llama_perf_context_print:        load time =    1204.32 ms
0.02.316.619 I llama_perf_context_print: prompt eval time =      44.16 ms /     7 tokens (    6.31 ms per token,   158.50 tokens per second)
0.02.316.619 I llama_perf_context_print:        eval time =    1054.33 ms /    63 runs   (   16.74 ms per token,    59.75 tokens per second)
0.02.316.620 I llama_perf_context_print:       total time =    1102.37 ms /    70 tokens
0.02.316.908 I ggml_metal_free: deallocating

real	0m2.336s
user	0m0.121s
sys	0m0.261s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.810 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.091 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.099 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.101 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.102 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.102 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.105 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.105 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.106 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.107 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.107 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.107 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.108 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.108 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.108 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.110 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.111 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.111 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.941 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.933 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.746 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.748 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.748 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.749 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.749 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.750 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.750 I llama_model_loader: - type  f32:  194 tensors
0.00.025.751 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.751 I print_info: file format = GGUF V3 (latest)
0.00.025.752 I print_info: file type   = Q8_0
0.00.025.753 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.673 I load: special tokens cache size = 25
0.00.054.367 I load: token to piece cache size = 0.2984 MB
0.00.054.370 I print_info: arch             = gptneox
0.00.054.371 I print_info: vocab_only       = 0
0.00.054.371 I print_info: n_ctx_train      = 2048
0.00.054.371 I print_info: n_embd           = 2048
0.00.054.371 I print_info: n_layer          = 24
0.00.054.375 I print_info: n_head           = 16
0.00.054.375 I print_info: n_head_kv        = 16
0.00.054.378 I print_info: n_rot            = 32
0.00.054.378 I print_info: n_swa            = 0
0.00.054.378 I print_info: n_embd_head_k    = 128
0.00.054.379 I print_info: n_embd_head_v    = 128
0.00.054.379 I print_info: n_gqa            = 1
0.00.054.380 I print_info: n_embd_k_gqa     = 2048
0.00.054.381 I print_info: n_embd_v_gqa     = 2048
0.00.054.381 I print_info: f_norm_eps       = 1.0e-05
0.00.054.382 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.382 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.382 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.382 I print_info: f_logit_scale    = 0.0e+00
0.00.054.383 I print_info: n_ff             = 8192
0.00.054.384 I print_info: n_expert         = 0
0.00.054.384 I print_info: n_expert_used    = 0
0.00.054.384 I print_info: causal attn      = 1
0.00.054.384 I print_info: pooling type     = 0
0.00.054.384 I print_info: rope type        = 2
0.00.054.384 I print_info: rope scaling     = linear
0.00.054.385 I print_info: freq_base_train  = 10000.0
0.00.054.385 I print_info: freq_scale_train = 1
0.00.054.385 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.385 I print_info: rope_finetuned   = unknown
0.00.054.385 I print_info: ssm_d_conv       = 0
0.00.054.386 I print_info: ssm_d_inner      = 0
0.00.054.386 I print_info: ssm_d_state      = 0
0.00.054.386 I print_info: ssm_dt_rank      = 0
0.00.054.386 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.386 I print_info: model type       = 1.4B
0.00.054.386 I print_info: model params     = 1.41 B
0.00.054.386 I print_info: general.name     = 1.4B
0.00.054.387 I print_info: vocab type       = BPE
0.00.054.387 I print_info: n_vocab          = 50304
0.00.054.387 I print_info: n_merges         = 50009
0.00.054.388 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.388 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.388 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.388 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.388 I print_info: LF token         = 128 'Ä'
0.00.054.389 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.389 I print_info: max token length = 1024
0.00.775.355 I load_tensors: offloading 24 repeating layers to GPU
0.00.775.364 I load_tensors: offloading output layer to GPU
0.00.775.365 I load_tensors: offloaded 25/25 layers to GPU
0.00.775.391 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.775.393 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.776.336 I llama_init_from_model: n_seq_max     = 1
0.00.776.338 I llama_init_from_model: n_ctx         = 128
0.00.776.339 I llama_init_from_model: n_ctx_per_seq = 128
0.00.776.339 I llama_init_from_model: n_batch       = 128
0.00.776.340 I llama_init_from_model: n_ubatch      = 128
0.00.776.340 I llama_init_from_model: flash_attn    = 0
0.00.776.341 I llama_init_from_model: freq_base     = 10000.0
0.00.776.341 I llama_init_from_model: freq_scale    = 1
0.00.776.342 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.776.343 I ggml_metal_init: allocating
0.00.776.377 I ggml_metal_init: found device: Apple M4
0.00.776.382 I ggml_metal_init: picking default device: Apple M4
0.00.777.603 I ggml_metal_init: using embedded metal library
0.00.783.027 I ggml_metal_init: GPU name:   Apple M4
0.00.783.030 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.783.031 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.783.032 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.783.032 I ggml_metal_init: simdgroup reduction   = true
0.00.783.032 I ggml_metal_init: simdgroup matrix mul. = true
0.00.783.033 I ggml_metal_init: has residency sets    = true
0.00.783.033 I ggml_metal_init: has bfloat            = true
0.00.783.033 I ggml_metal_init: use bfloat            = true
0.00.783.034 I ggml_metal_init: hasUnifiedMemory      = true
0.00.783.035 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.798.988 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.802.203 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.802.206 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.802.228 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.805.191 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.805.193 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.805.193 I llama_init_from_model: graph nodes  = 967
0.00.805.194 I llama_init_from_model: graph splits = 2
0.00.805.197 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.805.197 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.832.358 I 
0.00.832.445 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.832.477 I perplexity: tokenizing the input ..
0.00.845.071 I perplexity: tokenization took 12.592 ms
0.00.845.092 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.978.394 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.979.724 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.979.740 I llama_perf_context_print:        load time =     822.54 ms
0.00.979.741 I llama_perf_context_print: prompt eval time =     132.67 ms /   128 tokens (    1.04 ms per token,   964.79 tokens per second)
0.00.979.741 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.979.742 I llama_perf_context_print:       total time =     147.39 ms /   129 tokens
0.00.980.179 I ggml_metal_free: deallocating

real	0m0.995s
user	0m0.098s
sys	0m0.165s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.011.902 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.693 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.028.700 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.704 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.705 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.705 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.706 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.706 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.707 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.707 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.708 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.708 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.711 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.711 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.711 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.715 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.715 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.716 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.491 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.488 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.563 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.565 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.565 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.565 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.566 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.566 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.037.567 I llama_model_loader: - type  f32:  194 tensors
0.00.037.567 I llama_model_loader: - type q4_0:   97 tensors
0.00.037.567 I llama_model_loader: - type q6_K:    1 tensors
0.00.037.568 I print_info: file format = GGUF V3 (latest)
0.00.037.568 I print_info: file type   = Q4_0
0.00.037.570 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.060.278 I load: special tokens cache size = 25
0.00.066.839 I load: token to piece cache size = 0.2984 MB
0.00.066.842 I print_info: arch             = gptneox
0.00.066.842 I print_info: vocab_only       = 0
0.00.066.843 I print_info: n_ctx_train      = 2048
0.00.066.843 I print_info: n_embd           = 2048
0.00.066.843 I print_info: n_layer          = 24
0.00.066.848 I print_info: n_head           = 16
0.00.066.848 I print_info: n_head_kv        = 16
0.00.066.849 I print_info: n_rot            = 32
0.00.066.849 I print_info: n_swa            = 0
0.00.066.849 I print_info: n_embd_head_k    = 128
0.00.066.849 I print_info: n_embd_head_v    = 128
0.00.066.850 I print_info: n_gqa            = 1
0.00.066.850 I print_info: n_embd_k_gqa     = 2048
0.00.066.851 I print_info: n_embd_v_gqa     = 2048
0.00.066.851 I print_info: f_norm_eps       = 1.0e-05
0.00.066.852 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.066.852 I print_info: f_clamp_kqv      = 0.0e+00
0.00.066.852 I print_info: f_max_alibi_bias = 0.0e+00
0.00.066.852 I print_info: f_logit_scale    = 0.0e+00
0.00.066.853 I print_info: n_ff             = 8192
0.00.066.853 I print_info: n_expert         = 0
0.00.066.853 I print_info: n_expert_used    = 0
0.00.066.853 I print_info: causal attn      = 1
0.00.066.853 I print_info: pooling type     = 0
0.00.066.854 I print_info: rope type        = 2
0.00.066.854 I print_info: rope scaling     = linear
0.00.066.855 I print_info: freq_base_train  = 10000.0
0.00.066.855 I print_info: freq_scale_train = 1
0.00.066.855 I print_info: n_ctx_orig_yarn  = 2048
0.00.066.855 I print_info: rope_finetuned   = unknown
0.00.066.856 I print_info: ssm_d_conv       = 0
0.00.066.856 I print_info: ssm_d_inner      = 0
0.00.066.856 I print_info: ssm_d_state      = 0
0.00.066.857 I print_info: ssm_dt_rank      = 0
0.00.066.858 I print_info: ssm_dt_b_c_rms   = 0
0.00.066.858 I print_info: model type       = 1.4B
0.00.066.858 I print_info: model params     = 1.41 B
0.00.066.858 I print_info: general.name     = 1.4B
0.00.066.859 I print_info: vocab type       = BPE
0.00.066.859 I print_info: n_vocab          = 50304
0.00.066.859 I print_info: n_merges         = 50009
0.00.066.859 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.066.861 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.066.861 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.066.861 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.066.861 I print_info: LF token         = 128 'Ä'
0.00.066.862 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.066.862 I print_info: max token length = 1024
0.00.617.906 I load_tensors: offloading 24 repeating layers to GPU
0.00.617.922 I load_tensors: offloading output layer to GPU
0.00.617.922 I load_tensors: offloaded 25/25 layers to GPU
0.00.617.955 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.617.956 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.619.429 I llama_init_from_model: n_seq_max     = 1
0.00.619.433 I llama_init_from_model: n_ctx         = 2048
0.00.619.434 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.619.434 I llama_init_from_model: n_batch       = 2048
0.00.619.435 I llama_init_from_model: n_ubatch      = 512
0.00.619.435 I llama_init_from_model: flash_attn    = 0
0.00.619.437 I llama_init_from_model: freq_base     = 10000.0
0.00.619.438 I llama_init_from_model: freq_scale    = 1
0.00.619.449 I ggml_metal_init: allocating
0.00.619.520 I ggml_metal_init: found device: Apple M4
0.00.619.530 I ggml_metal_init: picking default device: Apple M4
0.00.621.331 I ggml_metal_init: using embedded metal library
0.00.627.958 I ggml_metal_init: GPU name:   Apple M4
0.00.627.963 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.627.963 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.627.964 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.627.965 I ggml_metal_init: simdgroup reduction   = true
0.00.627.965 I ggml_metal_init: simdgroup matrix mul. = true
0.00.627.965 I ggml_metal_init: has residency sets    = true
0.00.627.965 I ggml_metal_init: has bfloat            = true
0.00.627.966 I ggml_metal_init: use bfloat            = true
0.00.627.967 I ggml_metal_init: hasUnifiedMemory      = true
0.00.627.968 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.646.342 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.702.584 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.702.592 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.702.623 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.706.881 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.706.883 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.706.884 I llama_init_from_model: graph nodes  = 967
0.00.706.884 I llama_init_from_model: graph splits = 2
0.00.706.889 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.707.020 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.707.020 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.823 I main: llama threadpool init, n_threads = 4
0.00.752.863 I 
0.00.752.889 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.889 I 
0.00.753.064 I sampler seed: 1234
0.00.753.069 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.753.087 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.753.087 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.753.087 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.440.081 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50000.00 tokens per second)
0.01.440.082 I llama_perf_context_print:        load time =     740.06 ms
0.01.440.083 I llama_perf_context_print: prompt eval time =      45.13 ms /     7 tokens (    6.45 ms per token,   155.11 tokens per second)
0.01.440.084 I llama_perf_context_print:        eval time =     638.89 ms /    63 runs   (   10.14 ms per token,    98.61 tokens per second)
0.01.440.084 I llama_perf_context_print:       total time =     688.12 ms /    70 tokens
0.01.440.302 I ggml_metal_free: deallocating

real	0m1.460s
user	0m0.126s
sys	0m0.190s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.168 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.524 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.531 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.533 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.534 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.534 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.534 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.535 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.536 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.536 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.537 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.537 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.540 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.540 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.541 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.542 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.543 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.543 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.375 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.427 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.189 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.190 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.191 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.191 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.191 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.192 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.192 I llama_model_loader: - type  f32:  194 tensors
0.00.026.192 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.193 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.193 I print_info: file format = GGUF V3 (latest)
0.00.026.194 I print_info: file type   = Q4_0
0.00.026.195 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.642 I load: special tokens cache size = 25
0.00.051.702 I load: token to piece cache size = 0.2984 MB
0.00.051.705 I print_info: arch             = gptneox
0.00.051.705 I print_info: vocab_only       = 0
0.00.051.705 I print_info: n_ctx_train      = 2048
0.00.051.705 I print_info: n_embd           = 2048
0.00.051.706 I print_info: n_layer          = 24
0.00.051.709 I print_info: n_head           = 16
0.00.051.712 I print_info: n_head_kv        = 16
0.00.051.712 I print_info: n_rot            = 32
0.00.051.712 I print_info: n_swa            = 0
0.00.051.712 I print_info: n_embd_head_k    = 128
0.00.051.712 I print_info: n_embd_head_v    = 128
0.00.051.713 I print_info: n_gqa            = 1
0.00.051.714 I print_info: n_embd_k_gqa     = 2048
0.00.051.715 I print_info: n_embd_v_gqa     = 2048
0.00.051.715 I print_info: f_norm_eps       = 1.0e-05
0.00.051.716 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.716 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.716 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.716 I print_info: f_logit_scale    = 0.0e+00
0.00.051.721 I print_info: n_ff             = 8192
0.00.051.721 I print_info: n_expert         = 0
0.00.051.721 I print_info: n_expert_used    = 0
0.00.051.722 I print_info: causal attn      = 1
0.00.051.722 I print_info: pooling type     = 0
0.00.051.722 I print_info: rope type        = 2
0.00.051.722 I print_info: rope scaling     = linear
0.00.051.723 I print_info: freq_base_train  = 10000.0
0.00.051.723 I print_info: freq_scale_train = 1
0.00.051.723 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.725 I print_info: rope_finetuned   = unknown
0.00.051.725 I print_info: ssm_d_conv       = 0
0.00.051.725 I print_info: ssm_d_inner      = 0
0.00.051.725 I print_info: ssm_d_state      = 0
0.00.051.725 I print_info: ssm_dt_rank      = 0
0.00.051.725 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.726 I print_info: model type       = 1.4B
0.00.051.726 I print_info: model params     = 1.41 B
0.00.051.726 I print_info: general.name     = 1.4B
0.00.051.727 I print_info: vocab type       = BPE
0.00.051.727 I print_info: n_vocab          = 50304
0.00.051.727 I print_info: n_merges         = 50009
0.00.051.727 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.727 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.732 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.733 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.733 I print_info: LF token         = 128 'Ä'
0.00.051.734 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.734 I print_info: max token length = 1024
0.00.606.829 I load_tensors: offloading 24 repeating layers to GPU
0.00.606.839 I load_tensors: offloading output layer to GPU
0.00.606.840 I load_tensors: offloaded 25/25 layers to GPU
0.00.606.871 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.606.872 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.608.131 I llama_init_from_model: n_seq_max     = 1
0.00.608.143 I llama_init_from_model: n_ctx         = 128
0.00.608.144 I llama_init_from_model: n_ctx_per_seq = 128
0.00.608.151 I llama_init_from_model: n_batch       = 128
0.00.608.151 I llama_init_from_model: n_ubatch      = 128
0.00.608.152 I llama_init_from_model: flash_attn    = 0
0.00.608.154 I llama_init_from_model: freq_base     = 10000.0
0.00.608.155 I llama_init_from_model: freq_scale    = 1
0.00.608.155 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.608.158 I ggml_metal_init: allocating
0.00.608.254 I ggml_metal_init: found device: Apple M4
0.00.608.265 I ggml_metal_init: picking default device: Apple M4
0.00.610.116 I ggml_metal_init: using embedded metal library
0.00.615.970 I ggml_metal_init: GPU name:   Apple M4
0.00.616.000 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.616.000 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.616.002 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.616.002 I ggml_metal_init: simdgroup reduction   = true
0.00.616.003 I ggml_metal_init: simdgroup matrix mul. = true
0.00.616.003 I ggml_metal_init: has residency sets    = true
0.00.616.003 I ggml_metal_init: has bfloat            = true
0.00.616.003 I ggml_metal_init: use bfloat            = true
0.00.616.006 I ggml_metal_init: hasUnifiedMemory      = true
0.00.616.012 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.636.970 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.640.387 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.640.395 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.640.423 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.643.655 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.643.656 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.643.657 I llama_init_from_model: graph nodes  = 967
0.00.643.657 I llama_init_from_model: graph splits = 2
0.00.643.661 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.643.661 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.189 I 
0.00.668.247 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.668.261 I perplexity: tokenizing the input ..
0.00.677.791 I perplexity: tokenization took 9.528 ms
0.00.677.804 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.633 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.802.961 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.802.974 I llama_perf_context_print:        load time =     658.01 ms
0.00.802.975 I llama_perf_context_print: prompt eval time =     123.57 ms /   128 tokens (    0.97 ms per token,  1035.86 tokens per second)
0.00.802.975 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.976 I llama_perf_context_print:       total time =     134.79 ms /   129 tokens
0.00.803.383 I ggml_metal_free: deallocating

real	0m0.820s
user	0m0.096s
sys	0m0.129s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.401 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.536 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.024.540 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.547 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.547 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.548 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.548 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.548 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.549 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.550 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.550 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.550 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.551 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.551 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.552 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.553 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.553 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.554 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.347 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.340 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.069 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.070 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.070 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.071 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.071 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.071 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.033.072 I llama_model_loader: - type  f32:  194 tensors
0.00.033.072 I llama_model_loader: - type q4_1:   97 tensors
0.00.033.072 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.073 I print_info: file format = GGUF V3 (latest)
0.00.033.073 I print_info: file type   = Q4_1
0.00.033.074 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.052.563 I load: special tokens cache size = 25
0.00.058.686 I load: token to piece cache size = 0.2984 MB
0.00.058.690 I print_info: arch             = gptneox
0.00.058.690 I print_info: vocab_only       = 0
0.00.058.690 I print_info: n_ctx_train      = 2048
0.00.058.690 I print_info: n_embd           = 2048
0.00.058.690 I print_info: n_layer          = 24
0.00.058.693 I print_info: n_head           = 16
0.00.058.694 I print_info: n_head_kv        = 16
0.00.058.694 I print_info: n_rot            = 32
0.00.058.694 I print_info: n_swa            = 0
0.00.058.694 I print_info: n_embd_head_k    = 128
0.00.058.695 I print_info: n_embd_head_v    = 128
0.00.058.695 I print_info: n_gqa            = 1
0.00.058.696 I print_info: n_embd_k_gqa     = 2048
0.00.058.697 I print_info: n_embd_v_gqa     = 2048
0.00.058.697 I print_info: f_norm_eps       = 1.0e-05
0.00.058.697 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.698 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.698 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.698 I print_info: f_logit_scale    = 0.0e+00
0.00.058.699 I print_info: n_ff             = 8192
0.00.058.699 I print_info: n_expert         = 0
0.00.058.699 I print_info: n_expert_used    = 0
0.00.058.700 I print_info: causal attn      = 1
0.00.058.700 I print_info: pooling type     = 0
0.00.058.700 I print_info: rope type        = 2
0.00.058.702 I print_info: rope scaling     = linear
0.00.058.703 I print_info: freq_base_train  = 10000.0
0.00.058.703 I print_info: freq_scale_train = 1
0.00.058.703 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.703 I print_info: rope_finetuned   = unknown
0.00.058.703 I print_info: ssm_d_conv       = 0
0.00.058.704 I print_info: ssm_d_inner      = 0
0.00.058.704 I print_info: ssm_d_state      = 0
0.00.058.704 I print_info: ssm_dt_rank      = 0
0.00.058.704 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.704 I print_info: model type       = 1.4B
0.00.058.705 I print_info: model params     = 1.41 B
0.00.058.705 I print_info: general.name     = 1.4B
0.00.058.705 I print_info: vocab type       = BPE
0.00.058.705 I print_info: n_vocab          = 50304
0.00.058.707 I print_info: n_merges         = 50009
0.00.058.707 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.707 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.707 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.708 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.708 I print_info: LF token         = 128 'Ä'
0.00.058.708 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.708 I print_info: max token length = 1024
0.00.657.298 I load_tensors: offloading 24 repeating layers to GPU
0.00.657.312 I load_tensors: offloading output layer to GPU
0.00.657.312 I load_tensors: offloaded 25/25 layers to GPU
0.00.657.344 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.657.345 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.658.594 I llama_init_from_model: n_seq_max     = 1
0.00.658.600 I llama_init_from_model: n_ctx         = 2048
0.00.658.600 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.658.601 I llama_init_from_model: n_batch       = 2048
0.00.658.601 I llama_init_from_model: n_ubatch      = 512
0.00.658.602 I llama_init_from_model: flash_attn    = 0
0.00.658.604 I llama_init_from_model: freq_base     = 10000.0
0.00.658.604 I llama_init_from_model: freq_scale    = 1
0.00.658.607 I ggml_metal_init: allocating
0.00.658.684 I ggml_metal_init: found device: Apple M4
0.00.658.694 I ggml_metal_init: picking default device: Apple M4
0.00.660.451 I ggml_metal_init: using embedded metal library
0.00.667.211 I ggml_metal_init: GPU name:   Apple M4
0.00.667.215 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.667.216 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.667.216 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.667.217 I ggml_metal_init: simdgroup reduction   = true
0.00.667.217 I ggml_metal_init: simdgroup matrix mul. = true
0.00.667.218 I ggml_metal_init: has residency sets    = true
0.00.667.218 I ggml_metal_init: has bfloat            = true
0.00.667.218 I ggml_metal_init: use bfloat            = true
0.00.667.219 I ggml_metal_init: hasUnifiedMemory      = true
0.00.667.221 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.685.496 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.743.887 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.743.893 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.743.916 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.748.631 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.748.633 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.748.633 I llama_init_from_model: graph nodes  = 967
0.00.748.634 I llama_init_from_model: graph splits = 2
0.00.748.639 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.748.773 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.748.776 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.800.970 I main: llama threadpool init, n_threads = 4
0.00.801.015 I 
0.00.801.042 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.801.042 I 
0.00.801.259 I sampler seed: 1234
0.00.801.264 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.801.315 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.801.318 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.801.318 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.529.278 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57817.59 tokens per second)
0.01.529.279 I llama_perf_context_print:        load time =     789.68 ms
0.01.529.280 I llama_perf_context_print: prompt eval time =      39.52 ms /     7 tokens (    5.65 ms per token,   177.13 tokens per second)
0.01.529.281 I llama_perf_context_print:        eval time =     685.60 ms /    63 runs   (   10.88 ms per token,    91.89 tokens per second)
0.01.529.281 I llama_perf_context_print:       total time =     729.19 ms /    70 tokens
0.01.529.507 I ggml_metal_free: deallocating

real	0m1.550s
user	0m0.122s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.225 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.506 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.511 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.517 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.518 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.518 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.519 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.519 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.520 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.520 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.521 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.521 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.522 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.522 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.522 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.524 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.525 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.525 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.403 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.449 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.289 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.290 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.290 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.291 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.291 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.291 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.292 I llama_model_loader: - type  f32:  194 tensors
0.00.025.292 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.292 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.293 I print_info: file format = GGUF V3 (latest)
0.00.025.294 I print_info: file type   = Q4_1
0.00.025.294 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.822 I load: special tokens cache size = 25
0.00.050.914 I load: token to piece cache size = 0.2984 MB
0.00.050.917 I print_info: arch             = gptneox
0.00.050.917 I print_info: vocab_only       = 0
0.00.050.917 I print_info: n_ctx_train      = 2048
0.00.050.917 I print_info: n_embd           = 2048
0.00.050.918 I print_info: n_layer          = 24
0.00.050.921 I print_info: n_head           = 16
0.00.050.922 I print_info: n_head_kv        = 16
0.00.050.922 I print_info: n_rot            = 32
0.00.050.922 I print_info: n_swa            = 0
0.00.050.922 I print_info: n_embd_head_k    = 128
0.00.050.923 I print_info: n_embd_head_v    = 128
0.00.050.923 I print_info: n_gqa            = 1
0.00.050.924 I print_info: n_embd_k_gqa     = 2048
0.00.050.926 I print_info: n_embd_v_gqa     = 2048
0.00.050.927 I print_info: f_norm_eps       = 1.0e-05
0.00.050.927 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.928 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.928 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.928 I print_info: f_logit_scale    = 0.0e+00
0.00.050.929 I print_info: n_ff             = 8192
0.00.050.929 I print_info: n_expert         = 0
0.00.050.929 I print_info: n_expert_used    = 0
0.00.050.929 I print_info: causal attn      = 1
0.00.050.929 I print_info: pooling type     = 0
0.00.050.930 I print_info: rope type        = 2
0.00.050.930 I print_info: rope scaling     = linear
0.00.050.930 I print_info: freq_base_train  = 10000.0
0.00.050.931 I print_info: freq_scale_train = 1
0.00.050.931 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.931 I print_info: rope_finetuned   = unknown
0.00.050.931 I print_info: ssm_d_conv       = 0
0.00.050.931 I print_info: ssm_d_inner      = 0
0.00.050.931 I print_info: ssm_d_state      = 0
0.00.050.931 I print_info: ssm_dt_rank      = 0
0.00.050.932 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.932 I print_info: model type       = 1.4B
0.00.050.932 I print_info: model params     = 1.41 B
0.00.050.932 I print_info: general.name     = 1.4B
0.00.050.933 I print_info: vocab type       = BPE
0.00.050.933 I print_info: n_vocab          = 50304
0.00.050.933 I print_info: n_merges         = 50009
0.00.050.934 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.934 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.934 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.934 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.934 I print_info: LF token         = 128 'Ä'
0.00.050.935 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.935 I print_info: max token length = 1024
0.00.664.338 I load_tensors: offloading 24 repeating layers to GPU
0.00.664.350 I load_tensors: offloading output layer to GPU
0.00.664.350 I load_tensors: offloaded 25/25 layers to GPU
0.00.664.384 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.664.386 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.665.913 I llama_init_from_model: n_seq_max     = 1
0.00.665.917 I llama_init_from_model: n_ctx         = 128
0.00.665.918 I llama_init_from_model: n_ctx_per_seq = 128
0.00.665.919 I llama_init_from_model: n_batch       = 128
0.00.665.919 I llama_init_from_model: n_ubatch      = 128
0.00.665.920 I llama_init_from_model: flash_attn    = 0
0.00.665.922 I llama_init_from_model: freq_base     = 10000.0
0.00.665.922 I llama_init_from_model: freq_scale    = 1
0.00.665.923 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.665.937 I ggml_metal_init: allocating
0.00.666.002 I ggml_metal_init: found device: Apple M4
0.00.666.011 I ggml_metal_init: picking default device: Apple M4
0.00.667.693 I ggml_metal_init: using embedded metal library
0.00.673.097 I ggml_metal_init: GPU name:   Apple M4
0.00.673.112 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.673.113 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.673.114 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.673.115 I ggml_metal_init: simdgroup reduction   = true
0.00.673.115 I ggml_metal_init: simdgroup matrix mul. = true
0.00.673.115 I ggml_metal_init: has residency sets    = true
0.00.673.116 I ggml_metal_init: has bfloat            = true
0.00.673.116 I ggml_metal_init: use bfloat            = true
0.00.673.117 I ggml_metal_init: hasUnifiedMemory      = true
0.00.673.122 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.692.981 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.696.732 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.696.739 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.696.784 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.700.072 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.700.074 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.700.075 I llama_init_from_model: graph nodes  = 967
0.00.700.075 I llama_init_from_model: graph splits = 2
0.00.700.079 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.700.079 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.729.897 I 
0.00.729.973 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.729.993 I perplexity: tokenizing the input ..
0.00.741.611 I perplexity: tokenization took 11.616 ms
0.00.741.625 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.870.087 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.871.461 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.871.472 I llama_perf_context_print:        load time =     720.66 ms
0.00.871.473 I llama_perf_context_print: prompt eval time =     128.23 ms /   128 tokens (    1.00 ms per token,   998.18 tokens per second)
0.00.871.474 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.871.474 I llama_perf_context_print:       total time =     141.58 ms /   129 tokens
0.00.871.871 I ggml_metal_free: deallocating

real	0m0.886s
user	0m0.096s
sys	0m0.143s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.012.890 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.012 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.034.017 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.018 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.018 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.019 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.019 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.021 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.022 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.022 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.023 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.023 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.023 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.025 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.026 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.030 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.030 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.030 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.038.791 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.015 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.697 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.044.698 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.699 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.699 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.699 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.700 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.044.700 I llama_model_loader: - type  f32:  194 tensors
0.00.044.700 I llama_model_loader: - type q5_0:   97 tensors
0.00.044.701 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.701 I print_info: file format = GGUF V3 (latest)
0.00.044.701 I print_info: file type   = Q5_0
0.00.044.702 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.075.191 I load: special tokens cache size = 25
0.00.086.300 I load: token to piece cache size = 0.2984 MB
0.00.086.304 I print_info: arch             = gptneox
0.00.086.305 I print_info: vocab_only       = 0
0.00.086.305 I print_info: n_ctx_train      = 2048
0.00.086.305 I print_info: n_embd           = 2048
0.00.086.305 I print_info: n_layer          = 24
0.00.086.309 I print_info: n_head           = 16
0.00.086.310 I print_info: n_head_kv        = 16
0.00.086.310 I print_info: n_rot            = 32
0.00.086.311 I print_info: n_swa            = 0
0.00.086.311 I print_info: n_embd_head_k    = 128
0.00.086.311 I print_info: n_embd_head_v    = 128
0.00.086.312 I print_info: n_gqa            = 1
0.00.086.313 I print_info: n_embd_k_gqa     = 2048
0.00.086.314 I print_info: n_embd_v_gqa     = 2048
0.00.086.314 I print_info: f_norm_eps       = 1.0e-05
0.00.086.315 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.086.315 I print_info: f_clamp_kqv      = 0.0e+00
0.00.086.315 I print_info: f_max_alibi_bias = 0.0e+00
0.00.086.315 I print_info: f_logit_scale    = 0.0e+00
0.00.086.316 I print_info: n_ff             = 8192
0.00.086.316 I print_info: n_expert         = 0
0.00.086.316 I print_info: n_expert_used    = 0
0.00.086.317 I print_info: causal attn      = 1
0.00.086.317 I print_info: pooling type     = 0
0.00.086.319 I print_info: rope type        = 2
0.00.086.321 I print_info: rope scaling     = linear
0.00.086.321 I print_info: freq_base_train  = 10000.0
0.00.086.322 I print_info: freq_scale_train = 1
0.00.086.322 I print_info: n_ctx_orig_yarn  = 2048
0.00.086.322 I print_info: rope_finetuned   = unknown
0.00.086.323 I print_info: ssm_d_conv       = 0
0.00.086.323 I print_info: ssm_d_inner      = 0
0.00.086.323 I print_info: ssm_d_state      = 0
0.00.086.323 I print_info: ssm_dt_rank      = 0
0.00.086.323 I print_info: ssm_dt_b_c_rms   = 0
0.00.086.323 I print_info: model type       = 1.4B
0.00.086.324 I print_info: model params     = 1.41 B
0.00.086.324 I print_info: general.name     = 1.4B
0.00.086.325 I print_info: vocab type       = BPE
0.00.086.325 I print_info: n_vocab          = 50304
0.00.086.325 I print_info: n_merges         = 50009
0.00.086.325 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.086.325 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.086.326 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.086.326 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.086.326 I print_info: LF token         = 128 'Ä'
0.00.086.326 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.086.332 I print_info: max token length = 1024
0.00.710.249 I load_tensors: offloading 24 repeating layers to GPU
0.00.710.266 I load_tensors: offloading output layer to GPU
0.00.710.266 I load_tensors: offloaded 25/25 layers to GPU
0.00.710.299 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.710.301 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.711.594 I llama_init_from_model: n_seq_max     = 1
0.00.711.597 I llama_init_from_model: n_ctx         = 2048
0.00.711.597 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.711.598 I llama_init_from_model: n_batch       = 2048
0.00.711.598 I llama_init_from_model: n_ubatch      = 512
0.00.711.599 I llama_init_from_model: flash_attn    = 0
0.00.711.600 I llama_init_from_model: freq_base     = 10000.0
0.00.711.600 I llama_init_from_model: freq_scale    = 1
0.00.711.605 I ggml_metal_init: allocating
0.00.711.625 I ggml_metal_init: found device: Apple M4
0.00.711.630 I ggml_metal_init: picking default device: Apple M4
0.00.713.068 I ggml_metal_init: using embedded metal library
0.00.719.359 I ggml_metal_init: GPU name:   Apple M4
0.00.719.363 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.719.364 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.719.365 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.719.365 I ggml_metal_init: simdgroup reduction   = true
0.00.719.366 I ggml_metal_init: simdgroup matrix mul. = true
0.00.719.366 I ggml_metal_init: has residency sets    = true
0.00.719.366 I ggml_metal_init: has bfloat            = true
0.00.719.366 I ggml_metal_init: use bfloat            = true
0.00.719.367 I ggml_metal_init: hasUnifiedMemory      = true
0.00.719.369 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.736.243 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.793.162 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.793.171 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.793.200 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.798.087 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.798.090 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.798.090 I llama_init_from_model: graph nodes  = 967
0.00.798.090 I llama_init_from_model: graph splits = 2
0.00.798.096 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.798.220 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.798.220 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.855.305 I main: llama threadpool init, n_threads = 4
0.00.855.349 I 
0.00.855.379 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.855.381 I 
0.00.855.605 I sampler seed: 1234
0.00.855.610 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.855.630 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.855.630 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.855.631 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.647.730 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51523.95 tokens per second)
0.01.647.731 I llama_perf_context_print:        load time =     841.54 ms
0.01.647.731 I llama_perf_context_print: prompt eval time =      42.82 ms /     7 tokens (    6.12 ms per token,   163.49 tokens per second)
0.01.647.732 I llama_perf_context_print:        eval time =     746.34 ms /    63 runs   (   11.85 ms per token,    84.41 tokens per second)
0.01.647.733 I llama_perf_context_print:       total time =     793.30 ms /    70 tokens
0.01.647.986 I ggml_metal_free: deallocating

real	0m1.679s
user	0m0.141s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.068 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.200 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.205 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.207 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.207 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.207 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.208 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.208 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.209 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.209 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.210 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.210 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.211 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.211 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.211 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.213 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.216 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.216 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.999 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.021 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.963 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.964 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.965 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.965 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.965 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.966 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.966 I llama_model_loader: - type  f32:  194 tensors
0.00.025.966 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.967 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.967 I print_info: file format = GGUF V3 (latest)
0.00.025.968 I print_info: file type   = Q5_0
0.00.025.969 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.539 I load: special tokens cache size = 25
0.00.051.726 I load: token to piece cache size = 0.2984 MB
0.00.051.729 I print_info: arch             = gptneox
0.00.051.729 I print_info: vocab_only       = 0
0.00.051.729 I print_info: n_ctx_train      = 2048
0.00.051.730 I print_info: n_embd           = 2048
0.00.051.730 I print_info: n_layer          = 24
0.00.051.733 I print_info: n_head           = 16
0.00.051.734 I print_info: n_head_kv        = 16
0.00.051.734 I print_info: n_rot            = 32
0.00.051.734 I print_info: n_swa            = 0
0.00.051.735 I print_info: n_embd_head_k    = 128
0.00.051.735 I print_info: n_embd_head_v    = 128
0.00.051.736 I print_info: n_gqa            = 1
0.00.051.736 I print_info: n_embd_k_gqa     = 2048
0.00.051.737 I print_info: n_embd_v_gqa     = 2048
0.00.051.737 I print_info: f_norm_eps       = 1.0e-05
0.00.051.738 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.738 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.738 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.738 I print_info: f_logit_scale    = 0.0e+00
0.00.051.739 I print_info: n_ff             = 8192
0.00.051.739 I print_info: n_expert         = 0
0.00.051.739 I print_info: n_expert_used    = 0
0.00.051.740 I print_info: causal attn      = 1
0.00.051.740 I print_info: pooling type     = 0
0.00.051.740 I print_info: rope type        = 2
0.00.051.740 I print_info: rope scaling     = linear
0.00.051.742 I print_info: freq_base_train  = 10000.0
0.00.051.743 I print_info: freq_scale_train = 1
0.00.051.743 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.743 I print_info: rope_finetuned   = unknown
0.00.051.743 I print_info: ssm_d_conv       = 0
0.00.051.743 I print_info: ssm_d_inner      = 0
0.00.051.744 I print_info: ssm_d_state      = 0
0.00.051.744 I print_info: ssm_dt_rank      = 0
0.00.051.744 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.744 I print_info: model type       = 1.4B
0.00.051.745 I print_info: model params     = 1.41 B
0.00.051.745 I print_info: general.name     = 1.4B
0.00.051.746 I print_info: vocab type       = BPE
0.00.051.747 I print_info: n_vocab          = 50304
0.00.051.747 I print_info: n_merges         = 50009
0.00.051.748 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.748 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.748 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.748 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.753 I print_info: LF token         = 128 'Ä'
0.00.051.753 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.754 I print_info: max token length = 1024
0.00.625.253 I load_tensors: offloading 24 repeating layers to GPU
0.00.625.268 I load_tensors: offloading output layer to GPU
0.00.625.268 I load_tensors: offloaded 25/25 layers to GPU
0.00.625.303 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.625.305 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.626.867 I llama_init_from_model: n_seq_max     = 1
0.00.626.871 I llama_init_from_model: n_ctx         = 128
0.00.626.872 I llama_init_from_model: n_ctx_per_seq = 128
0.00.626.873 I llama_init_from_model: n_batch       = 128
0.00.626.874 I llama_init_from_model: n_ubatch      = 128
0.00.626.874 I llama_init_from_model: flash_attn    = 0
0.00.626.876 I llama_init_from_model: freq_base     = 10000.0
0.00.626.877 I llama_init_from_model: freq_scale    = 1
0.00.626.877 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.626.879 I ggml_metal_init: allocating
0.00.626.960 I ggml_metal_init: found device: Apple M4
0.00.626.969 I ggml_metal_init: picking default device: Apple M4
0.00.628.697 I ggml_metal_init: using embedded metal library
0.00.635.246 I ggml_metal_init: GPU name:   Apple M4
0.00.635.250 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.635.251 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.635.252 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.635.252 I ggml_metal_init: simdgroup reduction   = true
0.00.635.253 I ggml_metal_init: simdgroup matrix mul. = true
0.00.635.253 I ggml_metal_init: has residency sets    = true
0.00.635.253 I ggml_metal_init: has bfloat            = true
0.00.635.253 I ggml_metal_init: use bfloat            = true
0.00.635.254 I ggml_metal_init: hasUnifiedMemory      = true
0.00.635.257 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.652.174 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.655.654 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.655.657 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.655.693 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.658.970 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.658.972 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.658.973 I llama_init_from_model: graph nodes  = 967
0.00.658.973 I llama_init_from_model: graph splits = 2
0.00.658.976 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.658.976 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.194 I 
0.00.691.274 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.292 I perplexity: tokenizing the input ..
0.00.701.243 I perplexity: tokenization took 9.949 ms
0.00.701.256 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.837.199 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.838.544 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.838.561 I llama_perf_context_print:        load time =     681.12 ms
0.00.838.562 I llama_perf_context_print: prompt eval time =     135.71 ms /   128 tokens (    1.06 ms per token,   943.20 tokens per second)
0.00.838.562 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.838.563 I llama_perf_context_print:       total time =     147.37 ms /   129 tokens
0.00.838.973 I ggml_metal_free: deallocating

real	0m0.856s
user	0m0.093s
sys	0m0.135s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.011.357 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.070 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.019.075 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.076 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.077 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.077 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.077 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.078 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.079 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.079 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.079 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.080 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.080 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.080 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.081 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.085 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.085 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.085 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.832 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.869 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.524 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.525 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.525 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.525 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.526 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.526 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.527 I llama_model_loader: - type  f32:  194 tensors
0.00.027.527 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.527 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.528 I print_info: file format = GGUF V3 (latest)
0.00.027.528 I print_info: file type   = Q5_1
0.00.027.529 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.046.225 I load: special tokens cache size = 25
0.00.052.318 I load: token to piece cache size = 0.2984 MB
0.00.052.321 I print_info: arch             = gptneox
0.00.052.321 I print_info: vocab_only       = 0
0.00.052.321 I print_info: n_ctx_train      = 2048
0.00.052.322 I print_info: n_embd           = 2048
0.00.052.322 I print_info: n_layer          = 24
0.00.052.326 I print_info: n_head           = 16
0.00.052.326 I print_info: n_head_kv        = 16
0.00.052.326 I print_info: n_rot            = 32
0.00.052.327 I print_info: n_swa            = 0
0.00.052.327 I print_info: n_embd_head_k    = 128
0.00.052.328 I print_info: n_embd_head_v    = 128
0.00.052.329 I print_info: n_gqa            = 1
0.00.052.330 I print_info: n_embd_k_gqa     = 2048
0.00.052.330 I print_info: n_embd_v_gqa     = 2048
0.00.052.331 I print_info: f_norm_eps       = 1.0e-05
0.00.052.331 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.331 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.332 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.332 I print_info: f_logit_scale    = 0.0e+00
0.00.052.332 I print_info: n_ff             = 8192
0.00.052.333 I print_info: n_expert         = 0
0.00.052.333 I print_info: n_expert_used    = 0
0.00.052.333 I print_info: causal attn      = 1
0.00.052.333 I print_info: pooling type     = 0
0.00.052.335 I print_info: rope type        = 2
0.00.052.337 I print_info: rope scaling     = linear
0.00.052.337 I print_info: freq_base_train  = 10000.0
0.00.052.337 I print_info: freq_scale_train = 1
0.00.052.337 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.338 I print_info: rope_finetuned   = unknown
0.00.052.338 I print_info: ssm_d_conv       = 0
0.00.052.338 I print_info: ssm_d_inner      = 0
0.00.052.338 I print_info: ssm_d_state      = 0
0.00.052.338 I print_info: ssm_dt_rank      = 0
0.00.052.338 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.339 I print_info: model type       = 1.4B
0.00.052.339 I print_info: model params     = 1.41 B
0.00.052.339 I print_info: general.name     = 1.4B
0.00.052.339 I print_info: vocab type       = BPE
0.00.052.340 I print_info: n_vocab          = 50304
0.00.052.340 I print_info: n_merges         = 50009
0.00.052.344 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.344 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.344 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.344 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.344 I print_info: LF token         = 128 'Ä'
0.00.052.345 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.346 I print_info: max token length = 1024
0.00.714.149 I load_tensors: offloading 24 repeating layers to GPU
0.00.714.181 I load_tensors: offloading output layer to GPU
0.00.714.183 I load_tensors: offloaded 25/25 layers to GPU
0.00.714.210 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.714.213 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.715.575 I llama_init_from_model: n_seq_max     = 1
0.00.715.577 I llama_init_from_model: n_ctx         = 2048
0.00.715.577 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.715.578 I llama_init_from_model: n_batch       = 2048
0.00.715.578 I llama_init_from_model: n_ubatch      = 512
0.00.715.579 I llama_init_from_model: flash_attn    = 0
0.00.715.579 I llama_init_from_model: freq_base     = 10000.0
0.00.715.580 I llama_init_from_model: freq_scale    = 1
0.00.715.584 I ggml_metal_init: allocating
0.00.715.606 I ggml_metal_init: found device: Apple M4
0.00.715.611 I ggml_metal_init: picking default device: Apple M4
0.00.717.075 I ggml_metal_init: using embedded metal library
0.00.723.196 I ggml_metal_init: GPU name:   Apple M4
0.00.723.200 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.723.201 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.723.202 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.723.203 I ggml_metal_init: simdgroup reduction   = true
0.00.723.203 I ggml_metal_init: simdgroup matrix mul. = true
0.00.723.203 I ggml_metal_init: has residency sets    = true
0.00.723.204 I ggml_metal_init: has bfloat            = true
0.00.723.204 I ggml_metal_init: use bfloat            = true
0.00.723.205 I ggml_metal_init: hasUnifiedMemory      = true
0.00.723.206 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.740.146 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.795.002 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.795.009 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.795.032 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.799.588 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.799.591 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.799.592 I llama_init_from_model: graph nodes  = 967
0.00.799.592 I llama_init_from_model: graph splits = 2
0.00.799.598 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.799.728 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.799.728 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.855.935 I main: llama threadpool init, n_threads = 4
0.00.855.984 I 
0.00.856.010 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.856.010 I 
0.00.856.241 I sampler seed: 1234
0.00.856.245 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.856.256 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.856.257 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.856.257 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.695.774 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53303.30 tokens per second)
0.01.695.775 I llama_perf_context_print:        load time =     843.71 ms
0.01.695.776 I llama_perf_context_print: prompt eval time =      41.90 ms /     7 tokens (    5.99 ms per token,   167.06 tokens per second)
0.01.695.776 I llama_perf_context_print:        eval time =     794.72 ms /    63 runs   (   12.61 ms per token,    79.27 tokens per second)
0.01.695.777 I llama_perf_context_print:       total time =     840.71 ms /    70 tokens
0.01.696.064 I ggml_metal_free: deallocating

real	0m1.715s
user	0m0.119s
sys	0m0.207s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.463 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.359 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.019.363 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.365 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.366 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.366 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.366 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.367 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.368 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.368 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.368 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.369 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.369 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.369 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.370 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.371 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.372 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.372 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.194 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.242 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.055 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.056 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.057 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.057 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.057 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.058 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.028.058 I llama_model_loader: - type  f32:  194 tensors
0.00.028.059 I llama_model_loader: - type q5_1:   97 tensors
0.00.028.059 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.059 I print_info: file format = GGUF V3 (latest)
0.00.028.060 I print_info: file type   = Q5_1
0.00.028.062 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.046.840 I load: special tokens cache size = 25
0.00.052.967 I load: token to piece cache size = 0.2984 MB
0.00.052.970 I print_info: arch             = gptneox
0.00.052.971 I print_info: vocab_only       = 0
0.00.052.971 I print_info: n_ctx_train      = 2048
0.00.052.971 I print_info: n_embd           = 2048
0.00.052.971 I print_info: n_layer          = 24
0.00.052.975 I print_info: n_head           = 16
0.00.052.975 I print_info: n_head_kv        = 16
0.00.052.975 I print_info: n_rot            = 32
0.00.052.976 I print_info: n_swa            = 0
0.00.052.976 I print_info: n_embd_head_k    = 128
0.00.052.976 I print_info: n_embd_head_v    = 128
0.00.052.977 I print_info: n_gqa            = 1
0.00.052.977 I print_info: n_embd_k_gqa     = 2048
0.00.052.979 I print_info: n_embd_v_gqa     = 2048
0.00.052.980 I print_info: f_norm_eps       = 1.0e-05
0.00.052.980 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.989 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.990 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.990 I print_info: f_logit_scale    = 0.0e+00
0.00.052.993 I print_info: n_ff             = 8192
0.00.052.993 I print_info: n_expert         = 0
0.00.052.993 I print_info: n_expert_used    = 0
0.00.052.993 I print_info: causal attn      = 1
0.00.052.993 I print_info: pooling type     = 0
0.00.052.993 I print_info: rope type        = 2
0.00.052.994 I print_info: rope scaling     = linear
0.00.052.994 I print_info: freq_base_train  = 10000.0
0.00.052.994 I print_info: freq_scale_train = 1
0.00.052.994 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.995 I print_info: rope_finetuned   = unknown
0.00.052.995 I print_info: ssm_d_conv       = 0
0.00.052.995 I print_info: ssm_d_inner      = 0
0.00.052.995 I print_info: ssm_d_state      = 0
0.00.052.995 I print_info: ssm_dt_rank      = 0
0.00.052.995 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.996 I print_info: model type       = 1.4B
0.00.052.996 I print_info: model params     = 1.41 B
0.00.052.996 I print_info: general.name     = 1.4B
0.00.052.997 I print_info: vocab type       = BPE
0.00.052.997 I print_info: n_vocab          = 50304
0.00.052.997 I print_info: n_merges         = 50009
0.00.052.997 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.997 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.998 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.998 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.998 I print_info: LF token         = 128 'Ä'
0.00.052.998 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.998 I print_info: max token length = 1024
0.00.686.428 I load_tensors: offloading 24 repeating layers to GPU
0.00.686.443 I load_tensors: offloading output layer to GPU
0.00.686.444 I load_tensors: offloaded 25/25 layers to GPU
0.00.686.477 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.686.478 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.687.895 I llama_init_from_model: n_seq_max     = 1
0.00.687.899 I llama_init_from_model: n_ctx         = 128
0.00.687.899 I llama_init_from_model: n_ctx_per_seq = 128
0.00.687.900 I llama_init_from_model: n_batch       = 128
0.00.687.900 I llama_init_from_model: n_ubatch      = 128
0.00.687.900 I llama_init_from_model: flash_attn    = 0
0.00.687.901 I llama_init_from_model: freq_base     = 10000.0
0.00.687.902 I llama_init_from_model: freq_scale    = 1
0.00.687.903 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.687.904 I ggml_metal_init: allocating
0.00.687.919 I ggml_metal_init: found device: Apple M4
0.00.687.924 I ggml_metal_init: picking default device: Apple M4
0.00.689.204 I ggml_metal_init: using embedded metal library
0.00.695.652 I ggml_metal_init: GPU name:   Apple M4
0.00.695.656 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.695.657 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.695.658 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.695.659 I ggml_metal_init: simdgroup reduction   = true
0.00.695.659 I ggml_metal_init: simdgroup matrix mul. = true
0.00.695.659 I ggml_metal_init: has residency sets    = true
0.00.695.659 I ggml_metal_init: has bfloat            = true
0.00.695.660 I ggml_metal_init: use bfloat            = true
0.00.695.660 I ggml_metal_init: hasUnifiedMemory      = true
0.00.695.662 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.712.381 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.715.777 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.715.787 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.715.820 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.719.031 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.719.033 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.719.034 I llama_init_from_model: graph nodes  = 967
0.00.719.034 I llama_init_from_model: graph splits = 2
0.00.719.037 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.719.038 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.139 I 
0.00.745.216 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.234 I perplexity: tokenizing the input ..
0.00.754.383 I perplexity: tokenization took 9.147 ms
0.00.754.395 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.888.598 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.889.936 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.889.953 I llama_perf_context_print:        load time =     732.67 ms
0.00.889.954 I llama_perf_context_print: prompt eval time =     133.97 ms /   128 tokens (    1.05 ms per token,   955.42 tokens per second)
0.00.889.956 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.889.956 I llama_perf_context_print:       total time =     144.82 ms /   129 tokens
0.00.890.393 I ggml_metal_free: deallocating

real	0m0.905s
user	0m0.091s
sys	0m0.129s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.831 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.356 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.361 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.362 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.363 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.363 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.363 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.364 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.365 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.365 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.365 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.366 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.366 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.366 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.367 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.368 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.368 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.369 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.108 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.180 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.883 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.884 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.884 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.884 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.885 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.885 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.886 I llama_model_loader: - type  f32:  194 tensors
0.00.024.886 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.886 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.886 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.887 I print_info: file format = GGUF V3 (latest)
0.00.024.887 I print_info: file type   = Q2_K - Medium
0.00.024.890 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.658 I load: special tokens cache size = 25
0.00.049.777 I load: token to piece cache size = 0.2984 MB
0.00.049.779 I print_info: arch             = gptneox
0.00.049.780 I print_info: vocab_only       = 0
0.00.049.780 I print_info: n_ctx_train      = 2048
0.00.049.780 I print_info: n_embd           = 2048
0.00.049.780 I print_info: n_layer          = 24
0.00.049.783 I print_info: n_head           = 16
0.00.049.784 I print_info: n_head_kv        = 16
0.00.049.784 I print_info: n_rot            = 32
0.00.049.785 I print_info: n_swa            = 0
0.00.049.785 I print_info: n_embd_head_k    = 128
0.00.049.785 I print_info: n_embd_head_v    = 128
0.00.049.786 I print_info: n_gqa            = 1
0.00.049.786 I print_info: n_embd_k_gqa     = 2048
0.00.049.787 I print_info: n_embd_v_gqa     = 2048
0.00.049.787 I print_info: f_norm_eps       = 1.0e-05
0.00.049.788 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.790 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.790 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.791 I print_info: f_logit_scale    = 0.0e+00
0.00.049.791 I print_info: n_ff             = 8192
0.00.049.791 I print_info: n_expert         = 0
0.00.049.792 I print_info: n_expert_used    = 0
0.00.049.793 I print_info: causal attn      = 1
0.00.049.793 I print_info: pooling type     = 0
0.00.049.793 I print_info: rope type        = 2
0.00.049.794 I print_info: rope scaling     = linear
0.00.049.794 I print_info: freq_base_train  = 10000.0
0.00.049.794 I print_info: freq_scale_train = 1
0.00.049.795 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.795 I print_info: rope_finetuned   = unknown
0.00.049.795 I print_info: ssm_d_conv       = 0
0.00.049.795 I print_info: ssm_d_inner      = 0
0.00.049.795 I print_info: ssm_d_state      = 0
0.00.049.796 I print_info: ssm_dt_rank      = 0
0.00.049.796 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.796 I print_info: model type       = 1.4B
0.00.049.796 I print_info: model params     = 1.41 B
0.00.049.797 I print_info: general.name     = 1.4B
0.00.049.798 I print_info: vocab type       = BPE
0.00.049.798 I print_info: n_vocab          = 50304
0.00.049.798 I print_info: n_merges         = 50009
0.00.049.798 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.799 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.799 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.800 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.801 I print_info: LF token         = 128 'Ä'
0.00.049.801 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.801 I print_info: max token length = 1024
0.00.378.220 I load_tensors: offloading 24 repeating layers to GPU
0.00.378.233 I load_tensors: offloading output layer to GPU
0.00.378.234 I load_tensors: offloaded 25/25 layers to GPU
0.00.378.267 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.378.268 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.379.873 I llama_init_from_model: n_seq_max     = 1
0.00.379.877 I llama_init_from_model: n_ctx         = 2048
0.00.379.878 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.379.878 I llama_init_from_model: n_batch       = 2048
0.00.379.879 I llama_init_from_model: n_ubatch      = 512
0.00.379.879 I llama_init_from_model: flash_attn    = 0
0.00.379.882 I llama_init_from_model: freq_base     = 10000.0
0.00.379.887 I llama_init_from_model: freq_scale    = 1
0.00.379.900 I ggml_metal_init: allocating
0.00.379.979 I ggml_metal_init: found device: Apple M4
0.00.379.988 I ggml_metal_init: picking default device: Apple M4
0.00.381.827 I ggml_metal_init: using embedded metal library
0.00.387.399 I ggml_metal_init: GPU name:   Apple M4
0.00.387.417 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.387.418 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.387.419 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.387.419 I ggml_metal_init: simdgroup reduction   = true
0.00.387.420 I ggml_metal_init: simdgroup matrix mul. = true
0.00.387.420 I ggml_metal_init: has residency sets    = true
0.00.387.420 I ggml_metal_init: has bfloat            = true
0.00.387.421 I ggml_metal_init: use bfloat            = true
0.00.387.422 I ggml_metal_init: hasUnifiedMemory      = true
0.00.387.426 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.408.453 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.458.296 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.458.315 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.458.345 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.463.194 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.463.197 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.463.197 I llama_init_from_model: graph nodes  = 967
0.00.463.197 I llama_init_from_model: graph splits = 2
0.00.463.208 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.463.331 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.463.332 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.521.781 I main: llama threadpool init, n_threads = 4
0.00.521.827 I 
0.00.521.852 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.521.852 I 
0.00.522.080 I sampler seed: 1234
0.00.522.085 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.522.096 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.522.096 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.522.096 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.197.349 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56037.88 tokens per second)
0.01.197.349 I llama_perf_context_print:        load time =     511.08 ms
0.01.197.350 I llama_perf_context_print: prompt eval time =      35.77 ms /     7 tokens (    5.11 ms per token,   195.72 tokens per second)
0.01.197.351 I llama_perf_context_print:        eval time =     636.65 ms /    63 runs   (   10.11 ms per token,    98.96 tokens per second)
0.01.197.351 I llama_perf_context_print:       total time =     676.44 ms /    70 tokens
0.01.197.547 I ggml_metal_free: deallocating

real	0m1.215s
user	0m0.124s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.916 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.818 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.822 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.824 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.824 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.825 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.825 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.825 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.826 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.826 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.827 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.827 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.827 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.828 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.828 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.830 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.830 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.830 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.742 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.765 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.588 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.589 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.589 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.589 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.590 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.590 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.591 I llama_model_loader: - type  f32:  194 tensors
0.00.025.591 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.591 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.591 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.592 I print_info: file format = GGUF V3 (latest)
0.00.025.593 I print_info: file type   = Q2_K - Medium
0.00.025.593 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.329 I load: special tokens cache size = 25
0.00.051.426 I load: token to piece cache size = 0.2984 MB
0.00.051.429 I print_info: arch             = gptneox
0.00.051.429 I print_info: vocab_only       = 0
0.00.051.430 I print_info: n_ctx_train      = 2048
0.00.051.430 I print_info: n_embd           = 2048
0.00.051.430 I print_info: n_layer          = 24
0.00.051.433 I print_info: n_head           = 16
0.00.051.434 I print_info: n_head_kv        = 16
0.00.051.434 I print_info: n_rot            = 32
0.00.051.434 I print_info: n_swa            = 0
0.00.051.434 I print_info: n_embd_head_k    = 128
0.00.051.434 I print_info: n_embd_head_v    = 128
0.00.051.435 I print_info: n_gqa            = 1
0.00.051.436 I print_info: n_embd_k_gqa     = 2048
0.00.051.437 I print_info: n_embd_v_gqa     = 2048
0.00.051.437 I print_info: f_norm_eps       = 1.0e-05
0.00.051.437 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.438 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.438 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.438 I print_info: f_logit_scale    = 0.0e+00
0.00.051.439 I print_info: n_ff             = 8192
0.00.051.439 I print_info: n_expert         = 0
0.00.051.439 I print_info: n_expert_used    = 0
0.00.051.439 I print_info: causal attn      = 1
0.00.051.441 I print_info: pooling type     = 0
0.00.051.441 I print_info: rope type        = 2
0.00.051.442 I print_info: rope scaling     = linear
0.00.051.442 I print_info: freq_base_train  = 10000.0
0.00.051.442 I print_info: freq_scale_train = 1
0.00.051.443 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.443 I print_info: rope_finetuned   = unknown
0.00.051.443 I print_info: ssm_d_conv       = 0
0.00.051.443 I print_info: ssm_d_inner      = 0
0.00.051.443 I print_info: ssm_d_state      = 0
0.00.051.443 I print_info: ssm_dt_rank      = 0
0.00.051.444 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.444 I print_info: model type       = 1.4B
0.00.051.444 I print_info: model params     = 1.41 B
0.00.051.444 I print_info: general.name     = 1.4B
0.00.051.445 I print_info: vocab type       = BPE
0.00.051.445 I print_info: n_vocab          = 50304
0.00.051.445 I print_info: n_merges         = 50009
0.00.051.446 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.446 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.450 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.450 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.450 I print_info: LF token         = 128 'Ä'
0.00.051.451 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.451 I print_info: max token length = 1024
0.00.377.735 I load_tensors: offloading 24 repeating layers to GPU
0.00.377.748 I load_tensors: offloading output layer to GPU
0.00.377.749 I load_tensors: offloaded 25/25 layers to GPU
0.00.377.781 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.377.795 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.379.198 I llama_init_from_model: n_seq_max     = 1
0.00.379.203 I llama_init_from_model: n_ctx         = 128
0.00.379.204 I llama_init_from_model: n_ctx_per_seq = 128
0.00.379.204 I llama_init_from_model: n_batch       = 128
0.00.379.205 I llama_init_from_model: n_ubatch      = 128
0.00.379.205 I llama_init_from_model: flash_attn    = 0
0.00.379.208 I llama_init_from_model: freq_base     = 10000.0
0.00.379.209 I llama_init_from_model: freq_scale    = 1
0.00.379.209 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.379.215 I ggml_metal_init: allocating
0.00.379.306 I ggml_metal_init: found device: Apple M4
0.00.379.315 I ggml_metal_init: picking default device: Apple M4
0.00.381.092 I ggml_metal_init: using embedded metal library
0.00.387.005 I ggml_metal_init: GPU name:   Apple M4
0.00.387.018 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.387.019 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.387.020 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.387.021 I ggml_metal_init: simdgroup reduction   = true
0.00.387.021 I ggml_metal_init: simdgroup matrix mul. = true
0.00.387.021 I ggml_metal_init: has residency sets    = true
0.00.387.022 I ggml_metal_init: has bfloat            = true
0.00.387.022 I ggml_metal_init: use bfloat            = true
0.00.387.024 I ggml_metal_init: hasUnifiedMemory      = true
0.00.387.043 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.408.930 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.412.541 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.412.550 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.412.600 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.415.995 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.415.996 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.415.997 I llama_init_from_model: graph nodes  = 967
0.00.415.997 I llama_init_from_model: graph splits = 2
0.00.416.000 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.416.000 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.447.974 I 
0.00.448.063 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.448.087 I perplexity: tokenizing the input ..
0.00.459.313 I perplexity: tokenization took 11.225 ms
0.00.459.325 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.597.009 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.598.345 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.598.360 I llama_perf_context_print:        load time =     438.05 ms
0.00.598.361 I llama_perf_context_print: prompt eval time =     137.45 ms /   128 tokens (    1.07 ms per token,   931.22 tokens per second)
0.00.598.362 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.598.362 I llama_perf_context_print:       total time =     150.39 ms /   129 tokens
0.00.598.741 I ggml_metal_free: deallocating

real	0m0.615s
user	0m0.097s
sys	0m0.092s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.738 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.164 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.169 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.170 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.171 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.171 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.172 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.172 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.173 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.173 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.173 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.174 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.174 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.175 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.178 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.180 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.181 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.181 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.935 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.926 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.677 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.678 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.678 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.679 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.679 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.679 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.680 I llama_model_loader: - type  f32:  194 tensors
0.00.024.680 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.680 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.681 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.681 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.681 I print_info: file format = GGUF V3 (latest)
0.00.024.682 I print_info: file type   = Q3_K - Medium
0.00.024.683 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.373 I load: special tokens cache size = 25
0.00.049.512 I load: token to piece cache size = 0.2984 MB
0.00.049.515 I print_info: arch             = gptneox
0.00.049.515 I print_info: vocab_only       = 0
0.00.049.515 I print_info: n_ctx_train      = 2048
0.00.049.515 I print_info: n_embd           = 2048
0.00.049.516 I print_info: n_layer          = 24
0.00.049.519 I print_info: n_head           = 16
0.00.049.520 I print_info: n_head_kv        = 16
0.00.049.520 I print_info: n_rot            = 32
0.00.049.520 I print_info: n_swa            = 0
0.00.049.520 I print_info: n_embd_head_k    = 128
0.00.049.520 I print_info: n_embd_head_v    = 128
0.00.049.521 I print_info: n_gqa            = 1
0.00.049.522 I print_info: n_embd_k_gqa     = 2048
0.00.049.524 I print_info: n_embd_v_gqa     = 2048
0.00.049.525 I print_info: f_norm_eps       = 1.0e-05
0.00.049.527 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.527 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.527 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.527 I print_info: f_logit_scale    = 0.0e+00
0.00.049.528 I print_info: n_ff             = 8192
0.00.049.528 I print_info: n_expert         = 0
0.00.049.528 I print_info: n_expert_used    = 0
0.00.049.530 I print_info: causal attn      = 1
0.00.049.531 I print_info: pooling type     = 0
0.00.049.531 I print_info: rope type        = 2
0.00.049.532 I print_info: rope scaling     = linear
0.00.049.532 I print_info: freq_base_train  = 10000.0
0.00.049.532 I print_info: freq_scale_train = 1
0.00.049.532 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.534 I print_info: rope_finetuned   = unknown
0.00.049.534 I print_info: ssm_d_conv       = 0
0.00.049.534 I print_info: ssm_d_inner      = 0
0.00.049.534 I print_info: ssm_d_state      = 0
0.00.049.535 I print_info: ssm_dt_rank      = 0
0.00.049.535 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.535 I print_info: model type       = 1.4B
0.00.049.535 I print_info: model params     = 1.41 B
0.00.049.536 I print_info: general.name     = 1.4B
0.00.049.536 I print_info: vocab type       = BPE
0.00.049.536 I print_info: n_vocab          = 50304
0.00.049.536 I print_info: n_merges         = 50009
0.00.049.537 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.537 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.537 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.537 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.537 I print_info: LF token         = 128 'Ä'
0.00.049.538 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.539 I print_info: max token length = 1024
0.00.445.133 I load_tensors: offloading 24 repeating layers to GPU
0.00.445.148 I load_tensors: offloading output layer to GPU
0.00.445.149 I load_tensors: offloaded 25/25 layers to GPU
0.00.445.180 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.445.187 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.446.787 I llama_init_from_model: n_seq_max     = 1
0.00.446.793 I llama_init_from_model: n_ctx         = 2048
0.00.446.793 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.446.794 I llama_init_from_model: n_batch       = 2048
0.00.446.794 I llama_init_from_model: n_ubatch      = 512
0.00.446.795 I llama_init_from_model: flash_attn    = 0
0.00.446.801 I llama_init_from_model: freq_base     = 10000.0
0.00.446.803 I llama_init_from_model: freq_scale    = 1
0.00.446.806 I ggml_metal_init: allocating
0.00.446.880 I ggml_metal_init: found device: Apple M4
0.00.446.890 I ggml_metal_init: picking default device: Apple M4
0.00.448.698 I ggml_metal_init: using embedded metal library
0.00.454.232 I ggml_metal_init: GPU name:   Apple M4
0.00.454.237 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.454.238 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.454.239 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.454.240 I ggml_metal_init: simdgroup reduction   = true
0.00.454.240 I ggml_metal_init: simdgroup matrix mul. = true
0.00.454.241 I ggml_metal_init: has residency sets    = true
0.00.454.241 I ggml_metal_init: has bfloat            = true
0.00.454.241 I ggml_metal_init: use bfloat            = true
0.00.454.242 I ggml_metal_init: hasUnifiedMemory      = true
0.00.454.248 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.473.607 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.532.719 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.532.727 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.532.753 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.537.023 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.537.025 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.537.025 I llama_init_from_model: graph nodes  = 967
0.00.537.025 I llama_init_from_model: graph splits = 2
0.00.537.032 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.537.155 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.537.156 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.592.021 I main: llama threadpool init, n_threads = 4
0.00.592.075 I 
0.00.592.104 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.592.105 I 
0.00.592.342 I sampler seed: 1234
0.00.592.347 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.592.368 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.592.368 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.592.368 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.335.783 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50750.54 tokens per second)
0.01.335.783 I llama_perf_context_print:        load time =     582.41 ms
0.01.335.784 I llama_perf_context_print: prompt eval time =      40.25 ms /     7 tokens (    5.75 ms per token,   173.90 tokens per second)
0.01.335.785 I llama_perf_context_print:        eval time =     700.17 ms /    63 runs   (   11.11 ms per token,    89.98 tokens per second)
0.01.335.785 I llama_perf_context_print:       total time =     744.63 ms /    70 tokens
0.01.336.075 I ggml_metal_free: deallocating

real	0m1.352s
user	0m0.122s
sys	0m0.177s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.952 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.120 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.125 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.127 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.128 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.128 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.128 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.130 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.131 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.131 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.131 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.132 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.132 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.132 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.133 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.134 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.135 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.135 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.916 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.978 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.812 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.814 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.814 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.814 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.815 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.815 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.815 I llama_model_loader: - type  f32:  194 tensors
0.00.024.816 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.816 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.816 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.816 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.817 I print_info: file format = GGUF V3 (latest)
0.00.024.818 I print_info: file type   = Q3_K - Medium
0.00.024.818 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.232 I load: special tokens cache size = 25
0.00.050.362 I load: token to piece cache size = 0.2984 MB
0.00.050.364 I print_info: arch             = gptneox
0.00.050.365 I print_info: vocab_only       = 0
0.00.050.365 I print_info: n_ctx_train      = 2048
0.00.050.365 I print_info: n_embd           = 2048
0.00.050.365 I print_info: n_layer          = 24
0.00.050.368 I print_info: n_head           = 16
0.00.050.369 I print_info: n_head_kv        = 16
0.00.050.369 I print_info: n_rot            = 32
0.00.050.369 I print_info: n_swa            = 0
0.00.050.370 I print_info: n_embd_head_k    = 128
0.00.050.370 I print_info: n_embd_head_v    = 128
0.00.050.370 I print_info: n_gqa            = 1
0.00.050.371 I print_info: n_embd_k_gqa     = 2048
0.00.050.372 I print_info: n_embd_v_gqa     = 2048
0.00.050.372 I print_info: f_norm_eps       = 1.0e-05
0.00.050.373 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.373 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.373 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.373 I print_info: f_logit_scale    = 0.0e+00
0.00.050.374 I print_info: n_ff             = 8192
0.00.050.374 I print_info: n_expert         = 0
0.00.050.374 I print_info: n_expert_used    = 0
0.00.050.375 I print_info: causal attn      = 1
0.00.050.375 I print_info: pooling type     = 0
0.00.050.375 I print_info: rope type        = 2
0.00.050.375 I print_info: rope scaling     = linear
0.00.050.376 I print_info: freq_base_train  = 10000.0
0.00.050.376 I print_info: freq_scale_train = 1
0.00.050.376 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.376 I print_info: rope_finetuned   = unknown
0.00.050.377 I print_info: ssm_d_conv       = 0
0.00.050.377 I print_info: ssm_d_inner      = 0
0.00.050.377 I print_info: ssm_d_state      = 0
0.00.050.377 I print_info: ssm_dt_rank      = 0
0.00.050.377 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.377 I print_info: model type       = 1.4B
0.00.050.378 I print_info: model params     = 1.41 B
0.00.050.378 I print_info: general.name     = 1.4B
0.00.050.381 I print_info: vocab type       = BPE
0.00.050.381 I print_info: n_vocab          = 50304
0.00.050.381 I print_info: n_merges         = 50009
0.00.050.381 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.381 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.382 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.382 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.382 I print_info: LF token         = 128 'Ä'
0.00.050.382 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.382 I print_info: max token length = 1024
0.00.443.658 I load_tensors: offloading 24 repeating layers to GPU
0.00.443.671 I load_tensors: offloading output layer to GPU
0.00.443.671 I load_tensors: offloaded 25/25 layers to GPU
0.00.443.704 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.443.711 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.445.227 I llama_init_from_model: n_seq_max     = 1
0.00.445.232 I llama_init_from_model: n_ctx         = 128
0.00.445.233 I llama_init_from_model: n_ctx_per_seq = 128
0.00.445.233 I llama_init_from_model: n_batch       = 128
0.00.445.233 I llama_init_from_model: n_ubatch      = 128
0.00.445.234 I llama_init_from_model: flash_attn    = 0
0.00.445.235 I llama_init_from_model: freq_base     = 10000.0
0.00.445.236 I llama_init_from_model: freq_scale    = 1
0.00.445.237 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.445.239 I ggml_metal_init: allocating
0.00.445.310 I ggml_metal_init: found device: Apple M4
0.00.445.318 I ggml_metal_init: picking default device: Apple M4
0.00.447.011 I ggml_metal_init: using embedded metal library
0.00.452.610 I ggml_metal_init: GPU name:   Apple M4
0.00.452.625 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.452.625 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.452.626 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.452.627 I ggml_metal_init: simdgroup reduction   = true
0.00.452.627 I ggml_metal_init: simdgroup matrix mul. = true
0.00.452.628 I ggml_metal_init: has residency sets    = true
0.00.452.628 I ggml_metal_init: has bfloat            = true
0.00.452.628 I ggml_metal_init: use bfloat            = true
0.00.452.630 I ggml_metal_init: hasUnifiedMemory      = true
0.00.452.635 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.472.629 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.476.179 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.476.182 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.476.220 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.479.567 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.479.569 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.479.570 I llama_init_from_model: graph nodes  = 967
0.00.479.570 I llama_init_from_model: graph splits = 2
0.00.479.574 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.479.574 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.504.906 I 
0.00.504.983 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.505.002 I perplexity: tokenizing the input ..
0.00.514.342 I perplexity: tokenization took 9.338 ms
0.00.514.354 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.645.363 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.646.706 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.646.722 I llama_perf_context_print:        load time =     495.95 ms
0.00.646.722 I llama_perf_context_print: prompt eval time =     130.78 ms /   128 tokens (    1.02 ms per token,   978.77 tokens per second)
0.00.646.723 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.646.723 I llama_perf_context_print:       total time =     141.82 ms /   129 tokens
0.00.647.120 I ggml_metal_free: deallocating

real	0m0.661s
user	0m0.093s
sys	0m0.100s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.850 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.352 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.358 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.364 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.365 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.366 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.366 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.366 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.367 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.368 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.368 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.369 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.369 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.369 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.370 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.371 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.372 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.372 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.157 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.228 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.155 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.156 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.157 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.157 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.157 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.158 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.158 I llama_model_loader: - type  f32:  194 tensors
0.00.026.159 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.159 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.159 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.160 I print_info: file format = GGUF V3 (latest)
0.00.026.162 I print_info: file type   = Q4_K - Medium
0.00.026.163 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.046.357 I load: special tokens cache size = 25
0.00.052.599 I load: token to piece cache size = 0.2984 MB
0.00.052.604 I print_info: arch             = gptneox
0.00.052.606 I print_info: vocab_only       = 0
0.00.052.606 I print_info: n_ctx_train      = 2048
0.00.052.606 I print_info: n_embd           = 2048
0.00.052.607 I print_info: n_layer          = 24
0.00.052.611 I print_info: n_head           = 16
0.00.052.612 I print_info: n_head_kv        = 16
0.00.052.613 I print_info: n_rot            = 32
0.00.052.613 I print_info: n_swa            = 0
0.00.052.613 I print_info: n_embd_head_k    = 128
0.00.052.613 I print_info: n_embd_head_v    = 128
0.00.052.614 I print_info: n_gqa            = 1
0.00.052.614 I print_info: n_embd_k_gqa     = 2048
0.00.052.615 I print_info: n_embd_v_gqa     = 2048
0.00.052.615 I print_info: f_norm_eps       = 1.0e-05
0.00.052.616 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.616 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.616 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.616 I print_info: f_logit_scale    = 0.0e+00
0.00.052.617 I print_info: n_ff             = 8192
0.00.052.617 I print_info: n_expert         = 0
0.00.052.617 I print_info: n_expert_used    = 0
0.00.052.617 I print_info: causal attn      = 1
0.00.052.617 I print_info: pooling type     = 0
0.00.052.617 I print_info: rope type        = 2
0.00.052.618 I print_info: rope scaling     = linear
0.00.052.618 I print_info: freq_base_train  = 10000.0
0.00.052.618 I print_info: freq_scale_train = 1
0.00.052.618 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.619 I print_info: rope_finetuned   = unknown
0.00.052.619 I print_info: ssm_d_conv       = 0
0.00.052.619 I print_info: ssm_d_inner      = 0
0.00.052.619 I print_info: ssm_d_state      = 0
0.00.052.619 I print_info: ssm_dt_rank      = 0
0.00.052.619 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.620 I print_info: model type       = 1.4B
0.00.052.620 I print_info: model params     = 1.41 B
0.00.052.620 I print_info: general.name     = 1.4B
0.00.052.620 I print_info: vocab type       = BPE
0.00.052.620 I print_info: n_vocab          = 50304
0.00.052.621 I print_info: n_merges         = 50009
0.00.052.621 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.621 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.621 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.621 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.622 I print_info: LF token         = 128 'Ä'
0.00.052.622 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.622 I print_info: max token length = 1024
0.00.503.783 I load_tensors: offloading 24 repeating layers to GPU
0.00.503.787 I load_tensors: offloading output layer to GPU
0.00.503.788 I load_tensors: offloaded 25/25 layers to GPU
0.00.503.809 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.503.810 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.504.480 I llama_init_from_model: n_seq_max     = 1
0.00.504.484 I llama_init_from_model: n_ctx         = 2048
0.00.504.484 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.504.485 I llama_init_from_model: n_batch       = 2048
0.00.504.485 I llama_init_from_model: n_ubatch      = 512
0.00.504.485 I llama_init_from_model: flash_attn    = 0
0.00.504.486 I llama_init_from_model: freq_base     = 10000.0
0.00.504.487 I llama_init_from_model: freq_scale    = 1
0.00.504.488 I ggml_metal_init: allocating
0.00.504.531 I ggml_metal_init: found device: Apple M4
0.00.504.538 I ggml_metal_init: picking default device: Apple M4
0.00.505.590 I ggml_metal_init: using embedded metal library
0.00.509.780 I ggml_metal_init: GPU name:   Apple M4
0.00.509.788 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.509.788 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.509.789 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.509.789 I ggml_metal_init: simdgroup reduction   = true
0.00.509.790 I ggml_metal_init: simdgroup matrix mul. = true
0.00.509.790 I ggml_metal_init: has residency sets    = true
0.00.509.790 I ggml_metal_init: has bfloat            = true
0.00.509.791 I ggml_metal_init: use bfloat            = true
0.00.509.792 I ggml_metal_init: hasUnifiedMemory      = true
0.00.509.794 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.525.336 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.555.928 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.555.934 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.555.957 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.560.604 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.560.606 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.560.606 I llama_init_from_model: graph nodes  = 967
0.00.560.606 I llama_init_from_model: graph splits = 2
0.00.560.611 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.560.740 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.560.740 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.616.262 I main: llama threadpool init, n_threads = 4
0.00.616.298 I 
0.00.616.319 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.616.319 I 
0.00.616.495 I sampler seed: 1234
0.00.616.499 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.616.510 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.616.510 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.616.510 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.384.817 I llama_perf_sampler_print:    sampling time =       1.51 ms /    71 runs   (    0.02 ms per token, 47144.75 tokens per second)
0.01.384.817 I llama_perf_context_print:        load time =     605.53 ms
0.01.384.818 I llama_perf_context_print: prompt eval time =      51.14 ms /     7 tokens (    7.31 ms per token,   136.87 tokens per second)
0.01.384.819 I llama_perf_context_print:        eval time =     714.61 ms /    63 runs   (   11.34 ms per token,    88.16 tokens per second)
0.01.384.819 I llama_perf_context_print:       total time =     769.44 ms /    70 tokens
0.01.385.118 I ggml_metal_free: deallocating

real	0m1.405s
user	0m0.118s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.089 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.209 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.214 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.221 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.221 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.222 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.222 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.222 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.225 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.225 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.225 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.226 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.226 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.226 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.227 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.233 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.234 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.234 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.990 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.993 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.755 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.756 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.757 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.757 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.757 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.758 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.758 I llama_model_loader: - type  f32:  194 tensors
0.00.024.759 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.759 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.759 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.760 I print_info: file format = GGUF V3 (latest)
0.00.024.760 I print_info: file type   = Q4_K - Medium
0.00.024.761 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.219 I load: special tokens cache size = 25
0.00.050.313 I load: token to piece cache size = 0.2984 MB
0.00.050.316 I print_info: arch             = gptneox
0.00.050.316 I print_info: vocab_only       = 0
0.00.050.317 I print_info: n_ctx_train      = 2048
0.00.050.317 I print_info: n_embd           = 2048
0.00.050.317 I print_info: n_layer          = 24
0.00.050.320 I print_info: n_head           = 16
0.00.050.321 I print_info: n_head_kv        = 16
0.00.050.321 I print_info: n_rot            = 32
0.00.050.321 I print_info: n_swa            = 0
0.00.050.321 I print_info: n_embd_head_k    = 128
0.00.050.322 I print_info: n_embd_head_v    = 128
0.00.050.322 I print_info: n_gqa            = 1
0.00.050.323 I print_info: n_embd_k_gqa     = 2048
0.00.050.324 I print_info: n_embd_v_gqa     = 2048
0.00.050.324 I print_info: f_norm_eps       = 1.0e-05
0.00.050.325 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.326 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.326 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.326 I print_info: f_logit_scale    = 0.0e+00
0.00.050.327 I print_info: n_ff             = 8192
0.00.050.327 I print_info: n_expert         = 0
0.00.050.328 I print_info: n_expert_used    = 0
0.00.050.328 I print_info: causal attn      = 1
0.00.050.328 I print_info: pooling type     = 0
0.00.050.328 I print_info: rope type        = 2
0.00.050.328 I print_info: rope scaling     = linear
0.00.050.329 I print_info: freq_base_train  = 10000.0
0.00.050.329 I print_info: freq_scale_train = 1
0.00.050.329 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.329 I print_info: rope_finetuned   = unknown
0.00.050.329 I print_info: ssm_d_conv       = 0
0.00.050.330 I print_info: ssm_d_inner      = 0
0.00.050.330 I print_info: ssm_d_state      = 0
0.00.050.332 I print_info: ssm_dt_rank      = 0
0.00.050.332 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.332 I print_info: model type       = 1.4B
0.00.050.333 I print_info: model params     = 1.41 B
0.00.050.333 I print_info: general.name     = 1.4B
0.00.050.333 I print_info: vocab type       = BPE
0.00.050.333 I print_info: n_vocab          = 50304
0.00.050.333 I print_info: n_merges         = 50009
0.00.050.334 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.334 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.334 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.334 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.335 I print_info: LF token         = 128 'Ä'
0.00.050.338 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.338 I print_info: max token length = 1024
0.00.546.461 I load_tensors: offloading 24 repeating layers to GPU
0.00.546.469 I load_tensors: offloading output layer to GPU
0.00.546.470 I load_tensors: offloaded 25/25 layers to GPU
0.00.546.503 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.546.504 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.548.060 I llama_init_from_model: n_seq_max     = 1
0.00.548.063 I llama_init_from_model: n_ctx         = 128
0.00.548.064 I llama_init_from_model: n_ctx_per_seq = 128
0.00.548.064 I llama_init_from_model: n_batch       = 128
0.00.548.064 I llama_init_from_model: n_ubatch      = 128
0.00.548.065 I llama_init_from_model: flash_attn    = 0
0.00.548.067 I llama_init_from_model: freq_base     = 10000.0
0.00.548.067 I llama_init_from_model: freq_scale    = 1
0.00.548.067 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.548.070 I ggml_metal_init: allocating
0.00.548.121 I ggml_metal_init: found device: Apple M4
0.00.548.129 I ggml_metal_init: picking default device: Apple M4
0.00.550.087 I ggml_metal_init: using embedded metal library
0.00.557.135 I ggml_metal_init: GPU name:   Apple M4
0.00.557.141 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.557.141 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.557.142 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.557.143 I ggml_metal_init: simdgroup reduction   = true
0.00.557.143 I ggml_metal_init: simdgroup matrix mul. = true
0.00.557.143 I ggml_metal_init: has residency sets    = true
0.00.557.144 I ggml_metal_init: has bfloat            = true
0.00.557.144 I ggml_metal_init: use bfloat            = true
0.00.557.145 I ggml_metal_init: hasUnifiedMemory      = true
0.00.557.147 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.575.497 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.578.998 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.579.010 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.579.053 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.582.233 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.582.234 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.582.235 I llama_init_from_model: graph nodes  = 967
0.00.582.235 I llama_init_from_model: graph splits = 2
0.00.582.239 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.582.242 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.609.691 I 
0.00.609.760 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.609.776 I perplexity: tokenizing the input ..
0.00.618.714 I perplexity: tokenization took 8.935 ms
0.00.618.726 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.751.969 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.753.309 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.753.329 I llama_perf_context_print:        load time =     600.59 ms
0.00.753.330 I llama_perf_context_print: prompt eval time =     133.01 ms /   128 tokens (    1.04 ms per token,   962.33 tokens per second)
0.00.753.330 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.753.331 I llama_perf_context_print:       total time =     143.64 ms /   129 tokens
0.00.753.766 I ggml_metal_free: deallocating

real	0m0.768s
user	0m0.093s
sys	0m0.132s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.678 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.425 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.431 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.433 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.434 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.436 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.436 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.437 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.438 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.438 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.438 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.439 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.439 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.439 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.440 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.442 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.442 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.442 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.261 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.305 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.295 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.296 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.297 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.297 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.297 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.298 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.298 I llama_model_loader: - type  f32:  194 tensors
0.00.025.299 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.299 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.300 I print_info: file format = GGUF V3 (latest)
0.00.025.300 I print_info: file type   = Q5_K - Medium
0.00.025.301 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.845 I load: special tokens cache size = 25
0.00.052.136 I load: token to piece cache size = 0.2984 MB
0.00.052.140 I print_info: arch             = gptneox
0.00.052.141 I print_info: vocab_only       = 0
0.00.052.141 I print_info: n_ctx_train      = 2048
0.00.052.141 I print_info: n_embd           = 2048
0.00.052.141 I print_info: n_layer          = 24
0.00.052.146 I print_info: n_head           = 16
0.00.052.146 I print_info: n_head_kv        = 16
0.00.052.147 I print_info: n_rot            = 32
0.00.052.147 I print_info: n_swa            = 0
0.00.052.147 I print_info: n_embd_head_k    = 128
0.00.052.147 I print_info: n_embd_head_v    = 128
0.00.052.149 I print_info: n_gqa            = 1
0.00.052.150 I print_info: n_embd_k_gqa     = 2048
0.00.052.151 I print_info: n_embd_v_gqa     = 2048
0.00.052.151 I print_info: f_norm_eps       = 1.0e-05
0.00.052.151 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.151 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.154 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.154 I print_info: f_logit_scale    = 0.0e+00
0.00.052.154 I print_info: n_ff             = 8192
0.00.052.155 I print_info: n_expert         = 0
0.00.052.155 I print_info: n_expert_used    = 0
0.00.052.155 I print_info: causal attn      = 1
0.00.052.155 I print_info: pooling type     = 0
0.00.052.155 I print_info: rope type        = 2
0.00.052.155 I print_info: rope scaling     = linear
0.00.052.156 I print_info: freq_base_train  = 10000.0
0.00.052.156 I print_info: freq_scale_train = 1
0.00.052.156 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.156 I print_info: rope_finetuned   = unknown
0.00.052.156 I print_info: ssm_d_conv       = 0
0.00.052.157 I print_info: ssm_d_inner      = 0
0.00.052.157 I print_info: ssm_d_state      = 0
0.00.052.157 I print_info: ssm_dt_rank      = 0
0.00.052.157 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.158 I print_info: model type       = 1.4B
0.00.052.158 I print_info: model params     = 1.41 B
0.00.052.158 I print_info: general.name     = 1.4B
0.00.052.159 I print_info: vocab type       = BPE
0.00.052.159 I print_info: n_vocab          = 50304
0.00.052.159 I print_info: n_merges         = 50009
0.00.052.160 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.160 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.160 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.160 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.160 I print_info: LF token         = 128 'Ä'
0.00.052.161 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.161 I print_info: max token length = 1024
0.00.582.741 I load_tensors: offloading 24 repeating layers to GPU
0.00.582.757 I load_tensors: offloading output layer to GPU
0.00.582.758 I load_tensors: offloaded 25/25 layers to GPU
0.00.582.794 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.582.796 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.583.764 I llama_init_from_model: n_seq_max     = 1
0.00.583.772 I llama_init_from_model: n_ctx         = 2048
0.00.583.773 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.583.774 I llama_init_from_model: n_batch       = 2048
0.00.583.774 I llama_init_from_model: n_ubatch      = 512
0.00.583.775 I llama_init_from_model: flash_attn    = 0
0.00.583.777 I llama_init_from_model: freq_base     = 10000.0
0.00.583.777 I llama_init_from_model: freq_scale    = 1
0.00.583.780 I ggml_metal_init: allocating
0.00.583.863 I ggml_metal_init: found device: Apple M4
0.00.583.872 I ggml_metal_init: picking default device: Apple M4
0.00.585.636 I ggml_metal_init: using embedded metal library
0.00.592.004 I ggml_metal_init: GPU name:   Apple M4
0.00.592.010 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.592.011 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.592.012 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.592.013 I ggml_metal_init: simdgroup reduction   = true
0.00.592.013 I ggml_metal_init: simdgroup matrix mul. = true
0.00.592.013 I ggml_metal_init: has residency sets    = true
0.00.592.013 I ggml_metal_init: has bfloat            = true
0.00.592.014 I ggml_metal_init: use bfloat            = true
0.00.592.014 I ggml_metal_init: hasUnifiedMemory      = true
0.00.592.027 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.609.391 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.663.537 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.663.544 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.663.567 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.668.074 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.668.076 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.668.077 I llama_init_from_model: graph nodes  = 967
0.00.668.077 I llama_init_from_model: graph splits = 2
0.00.668.083 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.668.211 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.668.212 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.939 I main: llama threadpool init, n_threads = 4
0.00.730.992 I 
0.00.731.015 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.731.016 I 
0.00.731.195 I sampler seed: 1234
0.00.731.200 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.731.250 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.731.254 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.731.254 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.610.881 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55555.56 tokens per second)
0.01.610.882 I llama_perf_context_print:        load time =     721.38 ms
0.01.610.883 I llama_perf_context_print: prompt eval time =      51.74 ms /     7 tokens (    7.39 ms per token,   135.29 tokens per second)
0.01.610.884 I llama_perf_context_print:        eval time =     824.97 ms /    63 runs   (   13.09 ms per token,    76.37 tokens per second)
0.01.610.885 I llama_perf_context_print:       total time =     880.82 ms /    70 tokens
0.01.611.201 I ggml_metal_free: deallocating

real	0m1.630s
user	0m0.123s
sys	0m0.191s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.854 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.883 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.887 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.889 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.890 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.890 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.890 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.891 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.892 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.892 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.892 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.893 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.893 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.893 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.894 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.896 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.896 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.897 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.607 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.613 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.427 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.428 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.429 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.429 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.429 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.429 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.430 I llama_model_loader: - type  f32:  194 tensors
0.00.026.430 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.430 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.431 I print_info: file format = GGUF V3 (latest)
0.00.026.431 I print_info: file type   = Q5_K - Medium
0.00.026.432 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.983 I load: special tokens cache size = 25
0.00.051.067 I load: token to piece cache size = 0.2984 MB
0.00.051.070 I print_info: arch             = gptneox
0.00.051.070 I print_info: vocab_only       = 0
0.00.051.070 I print_info: n_ctx_train      = 2048
0.00.051.070 I print_info: n_embd           = 2048
0.00.051.071 I print_info: n_layer          = 24
0.00.051.073 I print_info: n_head           = 16
0.00.051.074 I print_info: n_head_kv        = 16
0.00.051.075 I print_info: n_rot            = 32
0.00.051.076 I print_info: n_swa            = 0
0.00.051.076 I print_info: n_embd_head_k    = 128
0.00.051.078 I print_info: n_embd_head_v    = 128
0.00.051.079 I print_info: n_gqa            = 1
0.00.051.080 I print_info: n_embd_k_gqa     = 2048
0.00.051.081 I print_info: n_embd_v_gqa     = 2048
0.00.051.081 I print_info: f_norm_eps       = 1.0e-05
0.00.051.082 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.082 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.082 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.082 I print_info: f_logit_scale    = 0.0e+00
0.00.051.083 I print_info: n_ff             = 8192
0.00.051.083 I print_info: n_expert         = 0
0.00.051.083 I print_info: n_expert_used    = 0
0.00.051.084 I print_info: causal attn      = 1
0.00.051.084 I print_info: pooling type     = 0
0.00.051.084 I print_info: rope type        = 2
0.00.051.084 I print_info: rope scaling     = linear
0.00.051.084 I print_info: freq_base_train  = 10000.0
0.00.051.085 I print_info: freq_scale_train = 1
0.00.051.085 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.085 I print_info: rope_finetuned   = unknown
0.00.051.085 I print_info: ssm_d_conv       = 0
0.00.051.086 I print_info: ssm_d_inner      = 0
0.00.051.086 I print_info: ssm_d_state      = 0
0.00.051.086 I print_info: ssm_dt_rank      = 0
0.00.051.086 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.086 I print_info: model type       = 1.4B
0.00.051.087 I print_info: model params     = 1.41 B
0.00.051.087 I print_info: general.name     = 1.4B
0.00.051.087 I print_info: vocab type       = BPE
0.00.051.087 I print_info: n_vocab          = 50304
0.00.051.088 I print_info: n_merges         = 50009
0.00.051.088 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.088 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.088 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.088 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.089 I print_info: LF token         = 128 'Ä'
0.00.051.090 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.090 I print_info: max token length = 1024
0.00.598.415 I load_tensors: offloading 24 repeating layers to GPU
0.00.598.430 I load_tensors: offloading output layer to GPU
0.00.598.431 I load_tensors: offloaded 25/25 layers to GPU
0.00.598.466 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.598.467 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.599.450 I llama_init_from_model: n_seq_max     = 1
0.00.599.454 I llama_init_from_model: n_ctx         = 128
0.00.599.454 I llama_init_from_model: n_ctx_per_seq = 128
0.00.599.455 I llama_init_from_model: n_batch       = 128
0.00.599.455 I llama_init_from_model: n_ubatch      = 128
0.00.599.456 I llama_init_from_model: flash_attn    = 0
0.00.599.457 I llama_init_from_model: freq_base     = 10000.0
0.00.599.457 I llama_init_from_model: freq_scale    = 1
0.00.599.458 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.599.459 I ggml_metal_init: allocating
0.00.599.511 I ggml_metal_init: found device: Apple M4
0.00.599.519 I ggml_metal_init: picking default device: Apple M4
0.00.600.998 I ggml_metal_init: using embedded metal library
0.00.607.269 I ggml_metal_init: GPU name:   Apple M4
0.00.607.273 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.607.273 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.607.274 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.607.275 I ggml_metal_init: simdgroup reduction   = true
0.00.607.275 I ggml_metal_init: simdgroup matrix mul. = true
0.00.607.275 I ggml_metal_init: has residency sets    = true
0.00.607.275 I ggml_metal_init: has bfloat            = true
0.00.607.276 I ggml_metal_init: use bfloat            = true
0.00.607.277 I ggml_metal_init: hasUnifiedMemory      = true
0.00.607.278 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.624.177 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.627.651 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.627.658 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.627.690 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.631.097 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.631.098 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.631.099 I llama_init_from_model: graph nodes  = 967
0.00.631.099 I llama_init_from_model: graph splits = 2
0.00.631.103 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.631.103 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.659.783 I 
0.00.659.866 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.659.887 I perplexity: tokenizing the input ..
0.00.667.486 I perplexity: tokenization took 7.597 ms
0.00.667.499 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.807.211 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.808.539 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.808.553 I llama_perf_context_print:        load time =     648.92 ms
0.00.808.554 I llama_perf_context_print: prompt eval time =     139.48 ms /   128 tokens (    1.09 ms per token,   917.70 tokens per second)
0.00.808.555 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.808.555 I llama_perf_context_print:       total time =     148.77 ms /   129 tokens
0.00.808.950 I ggml_metal_free: deallocating

real	0m0.825s
user	0m0.089s
sys	0m0.129s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.010.244 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.340 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.346 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.349 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.349 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.349 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.350 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.350 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.351 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.351 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.352 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.352 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.353 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.353 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.353 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.355 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.356 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.356 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.086 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.083 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.776 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.777 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.777 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.777 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.778 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.778 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.779 I llama_model_loader: - type  f32:  194 tensors
0.00.026.779 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.779 I print_info: file format = GGUF V3 (latest)
0.00.026.780 I print_info: file type   = Q6_K
0.00.026.781 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.611 I load: special tokens cache size = 25
0.00.051.651 I load: token to piece cache size = 0.2984 MB
0.00.051.654 I print_info: arch             = gptneox
0.00.051.654 I print_info: vocab_only       = 0
0.00.051.655 I print_info: n_ctx_train      = 2048
0.00.051.655 I print_info: n_embd           = 2048
0.00.051.655 I print_info: n_layer          = 24
0.00.051.659 I print_info: n_head           = 16
0.00.051.659 I print_info: n_head_kv        = 16
0.00.051.659 I print_info: n_rot            = 32
0.00.051.660 I print_info: n_swa            = 0
0.00.051.660 I print_info: n_embd_head_k    = 128
0.00.051.662 I print_info: n_embd_head_v    = 128
0.00.051.663 I print_info: n_gqa            = 1
0.00.051.664 I print_info: n_embd_k_gqa     = 2048
0.00.051.666 I print_info: n_embd_v_gqa     = 2048
0.00.051.667 I print_info: f_norm_eps       = 1.0e-05
0.00.051.667 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.667 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.667 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.668 I print_info: f_logit_scale    = 0.0e+00
0.00.051.668 I print_info: n_ff             = 8192
0.00.051.668 I print_info: n_expert         = 0
0.00.051.669 I print_info: n_expert_used    = 0
0.00.051.669 I print_info: causal attn      = 1
0.00.051.669 I print_info: pooling type     = 0
0.00.051.669 I print_info: rope type        = 2
0.00.051.669 I print_info: rope scaling     = linear
0.00.051.670 I print_info: freq_base_train  = 10000.0
0.00.051.670 I print_info: freq_scale_train = 1
0.00.051.670 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.670 I print_info: rope_finetuned   = unknown
0.00.051.671 I print_info: ssm_d_conv       = 0
0.00.051.671 I print_info: ssm_d_inner      = 0
0.00.051.671 I print_info: ssm_d_state      = 0
0.00.051.671 I print_info: ssm_dt_rank      = 0
0.00.051.671 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.671 I print_info: model type       = 1.4B
0.00.051.672 I print_info: model params     = 1.41 B
0.00.051.672 I print_info: general.name     = 1.4B
0.00.051.673 I print_info: vocab type       = BPE
0.00.051.673 I print_info: n_vocab          = 50304
0.00.051.673 I print_info: n_merges         = 50009
0.00.051.674 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.675 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.675 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.675 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.675 I print_info: LF token         = 128 'Ä'
0.00.051.675 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.676 I print_info: max token length = 1024
0.00.669.047 I load_tensors: offloading 24 repeating layers to GPU
0.00.669.065 I load_tensors: offloading output layer to GPU
0.00.669.066 I load_tensors: offloaded 25/25 layers to GPU
0.00.669.100 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.669.101 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.670.232 I llama_init_from_model: n_seq_max     = 1
0.00.670.238 I llama_init_from_model: n_ctx         = 2048
0.00.670.239 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.670.239 I llama_init_from_model: n_batch       = 2048
0.00.670.240 I llama_init_from_model: n_ubatch      = 512
0.00.670.240 I llama_init_from_model: flash_attn    = 0
0.00.670.242 I llama_init_from_model: freq_base     = 10000.0
0.00.670.243 I llama_init_from_model: freq_scale    = 1
0.00.670.259 I ggml_metal_init: allocating
0.00.670.336 I ggml_metal_init: found device: Apple M4
0.00.670.346 I ggml_metal_init: picking default device: Apple M4
0.00.672.192 I ggml_metal_init: using embedded metal library
0.00.679.204 I ggml_metal_init: GPU name:   Apple M4
0.00.679.209 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.679.210 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.679.211 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.679.211 I ggml_metal_init: simdgroup reduction   = true
0.00.679.212 I ggml_metal_init: simdgroup matrix mul. = true
0.00.679.212 I ggml_metal_init: has residency sets    = true
0.00.679.212 I ggml_metal_init: has bfloat            = true
0.00.679.213 I ggml_metal_init: use bfloat            = true
0.00.679.214 I ggml_metal_init: hasUnifiedMemory      = true
0.00.679.224 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.698.114 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.744.303 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.744.310 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.744.333 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.749.743 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.749.745 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.749.745 I llama_init_from_model: graph nodes  = 967
0.00.749.746 I llama_init_from_model: graph splits = 2
0.00.749.751 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.749.877 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.749.877 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.817.541 I main: llama threadpool init, n_threads = 4
0.00.817.587 I 
0.00.817.610 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.817.610 I 
0.00.817.774 I sampler seed: 1234
0.00.817.779 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.817.789 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.817.789 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.817.789 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.738.783 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 53992.40 tokens per second)
0.01.738.784 I llama_perf_context_print:        load time =     806.41 ms
0.01.738.784 I llama_perf_context_print: prompt eval time =      54.21 ms /     7 tokens (    7.74 ms per token,   129.13 tokens per second)
0.01.738.786 I llama_perf_context_print:        eval time =     863.77 ms /    63 runs   (   13.71 ms per token,    72.94 tokens per second)
0.01.738.786 I llama_perf_context_print:       total time =     922.12 ms /    70 tokens
0.01.739.053 I ggml_metal_free: deallocating

real	0m1.758s
user	0m0.122s
sys	0m0.211s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4562 (178a7eb9) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.930 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.968 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.972 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.974 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.975 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.975 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.975 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.975 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.976 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.977 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.977 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.978 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.978 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.978 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.980 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.982 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.982 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.982 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.817 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.809 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.571 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.572 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.573 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.573 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.573 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.574 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.574 I llama_model_loader: - type  f32:  194 tensors
0.00.024.574 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.575 I print_info: file format = GGUF V3 (latest)
0.00.024.576 I print_info: file type   = Q6_K
0.00.024.576 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.050 I load: special tokens cache size = 25
0.00.050.078 I load: token to piece cache size = 0.2984 MB
0.00.050.081 I print_info: arch             = gptneox
0.00.050.081 I print_info: vocab_only       = 0
0.00.050.081 I print_info: n_ctx_train      = 2048
0.00.050.081 I print_info: n_embd           = 2048
0.00.050.081 I print_info: n_layer          = 24
0.00.050.085 I print_info: n_head           = 16
0.00.050.085 I print_info: n_head_kv        = 16
0.00.050.086 I print_info: n_rot            = 32
0.00.050.086 I print_info: n_swa            = 0
0.00.050.086 I print_info: n_embd_head_k    = 128
0.00.050.086 I print_info: n_embd_head_v    = 128
0.00.050.088 I print_info: n_gqa            = 1
0.00.050.089 I print_info: n_embd_k_gqa     = 2048
0.00.050.090 I print_info: n_embd_v_gqa     = 2048
0.00.050.090 I print_info: f_norm_eps       = 1.0e-05
0.00.050.090 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.091 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.091 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.091 I print_info: f_logit_scale    = 0.0e+00
0.00.050.092 I print_info: n_ff             = 8192
0.00.050.092 I print_info: n_expert         = 0
0.00.050.092 I print_info: n_expert_used    = 0
0.00.050.092 I print_info: causal attn      = 1
0.00.050.092 I print_info: pooling type     = 0
0.00.050.093 I print_info: rope type        = 2
0.00.050.093 I print_info: rope scaling     = linear
0.00.050.093 I print_info: freq_base_train  = 10000.0
0.00.050.093 I print_info: freq_scale_train = 1
0.00.050.094 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.094 I print_info: rope_finetuned   = unknown
0.00.050.094 I print_info: ssm_d_conv       = 0
0.00.050.094 I print_info: ssm_d_inner      = 0
0.00.050.094 I print_info: ssm_d_state      = 0
0.00.050.094 I print_info: ssm_dt_rank      = 0
0.00.050.094 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.096 I print_info: model type       = 1.4B
0.00.050.097 I print_info: model params     = 1.41 B
0.00.050.097 I print_info: general.name     = 1.4B
0.00.050.097 I print_info: vocab type       = BPE
0.00.050.098 I print_info: n_vocab          = 50304
0.00.050.098 I print_info: n_merges         = 50009
0.00.050.098 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.098 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.098 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.099 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.099 I print_info: LF token         = 128 'Ä'
0.00.050.099 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.103 I print_info: max token length = 1024
0.00.161.581 I load_tensors: offloading 24 repeating layers to GPU
0.00.161.586 I load_tensors: offloading output layer to GPU
0.00.161.587 I load_tensors: offloaded 25/25 layers to GPU
0.00.161.606 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.161.607 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.162.479 I llama_init_from_model: n_seq_max     = 1
0.00.162.481 I llama_init_from_model: n_ctx         = 128
0.00.162.482 I llama_init_from_model: n_ctx_per_seq = 128
0.00.162.482 I llama_init_from_model: n_batch       = 128
0.00.162.484 I llama_init_from_model: n_ubatch      = 128
0.00.162.485 I llama_init_from_model: flash_attn    = 0
0.00.162.485 I llama_init_from_model: freq_base     = 10000.0
0.00.162.486 I llama_init_from_model: freq_scale    = 1
0.00.162.486 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.162.488 I ggml_metal_init: allocating
0.00.162.538 I ggml_metal_init: found device: Apple M4
0.00.162.544 I ggml_metal_init: picking default device: Apple M4
0.00.163.489 I ggml_metal_init: using embedded metal library
0.00.167.826 I ggml_metal_init: GPU name:   Apple M4
0.00.167.829 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.167.830 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.167.831 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.167.831 I ggml_metal_init: simdgroup reduction   = true
0.00.167.831 I ggml_metal_init: simdgroup matrix mul. = true
0.00.167.831 I ggml_metal_init: has residency sets    = true
0.00.167.832 I ggml_metal_init: has bfloat            = true
0.00.167.832 I ggml_metal_init: use bfloat            = true
0.00.167.832 I ggml_metal_init: hasUnifiedMemory      = true
0.00.167.834 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.180.578 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.182.543 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.182.546 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.182.592 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.184.472 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.184.474 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.184.474 I llama_init_from_model: graph nodes  = 967
0.00.184.475 I llama_init_from_model: graph splits = 2
0.00.184.476 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.184.476 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.216.145 I 
0.00.216.180 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.216.189 I perplexity: tokenizing the input ..
0.00.223.972 I perplexity: tokenization took 7.781 ms
0.00.223.989 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.363.178 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.364.588 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.364.601 I llama_perf_context_print:        load time =     207.21 ms
0.00.364.602 I llama_perf_context_print: prompt eval time =     138.95 ms /   128 tokens (    1.09 ms per token,   921.17 tokens per second)
0.00.364.602 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.364.603 I llama_perf_context_print:       total time =     148.46 ms /   129 tokens
0.00.364.971 I ggml_metal_free: deallocating

real	0m0.379s
user	0m0.083s
sys	0m0.054s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4562 (178a7eb9)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x106605120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x106608790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x106608c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x106609070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1066094e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x106609950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x106609dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10660a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10660a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10660abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10660b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10660b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10660c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10660c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10660d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10660d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10660dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10660e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10660ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10660f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10660fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x106610420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x106610b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1066113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x106611b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x106611dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x106612080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1066124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x106612c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x106613080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x106613640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x106613b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x106613fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x106614280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1066146f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x106614b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x106614fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x106615440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1066158b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x106615d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x106616190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x106616600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x106616a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x106616ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x106617350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1066177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x106617c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1066180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x106618830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x106618ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x106619110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x106619580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1066199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x106619e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10661a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10661a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10661ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10661b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10661b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10661bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10661c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10661c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10661c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10661cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10661d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10661d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10661dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10661e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10661e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10661eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10661f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10661f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10661fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10661ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x106620500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x106620ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x106621060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x106621610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x106621bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x106622170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x106622720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x106622cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x106623280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x106623830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x106623de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x106624390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x106624940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x106624ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1066254a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x106625a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x106626000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1066265b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x106626b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x106627110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1066276c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x106627c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x106628220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x106618360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x106628980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x106628df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x106629260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x106629810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x106629dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10662a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10662a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10662aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10662b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10662ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10662bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10662c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10662cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10662d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10662d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10662dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10662e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10662e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10662eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10662f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10662f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10662fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10662ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x106630450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x106630950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x106630e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x106631350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x106631850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x106631d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x106632250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x106632750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x106632c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x106633150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x106633650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x106633b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x106634050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x106634550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x106634a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x106634f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x106635450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x106635950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x106635e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x106636350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x106636850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x106636d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x106637250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x106637750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x106637c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x106638150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x106638650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x106638b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x106639050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x106639550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x106639a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x106639f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10663a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10663a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10663ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10663b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10663b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10663bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10663c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10663c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10663cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10663d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10663d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10663db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10663e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10663e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10663ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10663ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10663f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10663f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10663fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x106640350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x106640850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x106640d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x106641250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x106641750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x106641c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x106642150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x106642650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x106642b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x106643050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x106643550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x106643a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x106643f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x106644450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x106644950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x106644e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x106645350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x106645850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x106645d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x106646250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x106646750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x106646c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x106647200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1066477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x106647d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x106648310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x106648920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x106648f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x106649540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x106649d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10664a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10664a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10664aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10664b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10664b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10664bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10664c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10664c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10664ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10664d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10664d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10664de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10664e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10664e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10664ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10664f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10664f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10664fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x106650350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1066508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x106650df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x106651340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x106651890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x106651de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x106652330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x106652880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x106652dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x106653320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x106653870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x106653dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x106654310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x106654860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x106654db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x106655300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x106655850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x106655da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1066562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x106656840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x106656d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1066572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x106657830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x106657d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1066582d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x106658820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x106658d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1066592c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x106659810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x106659d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10665a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10665a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10665ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10665b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10665b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10665bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10665c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10665c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10665cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10665d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10665d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10665dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10665e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10665e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10665ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10665f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10665f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10665fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1066600f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x106660590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x106660a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x106660ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x106661370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x106661810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x106661cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x106662150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1066625f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x106662a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x106662f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1066633d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x106663870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x106663d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x106664260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x106664980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1066650a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1066657c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x106665ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1066661a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x106666990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x106666c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x106667260 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.640.202 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.640.206 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x104a05450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x104a058c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x104a05d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x104a061a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x104a06610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x104a06a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x104a06ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x104a07360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x104a077d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x104a07c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x104a080b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x104a087a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x104a092c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x104a09a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x104a0a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x104a0a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x104a0b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x104a0b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x104a0bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x104a0c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x104a0cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x104a0d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x104a0db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x104a0e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x104a0e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x104a0ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x104a0ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x104a0f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x104a0f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x104a0fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x104a10110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x104a10640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x104a10ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x104a10d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x104a111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x104a11650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x104a11ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x104a11f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x104a123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x104a12810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x104a12c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x104a130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x104a13560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x104a139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x104a13e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x104a142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x104a14720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x104a14b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x104a15000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x104a15470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x104a158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x104a15d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x104a161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x104a16630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x104a16aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x104a16f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x104a17480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x104a17980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x104a17df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x104a18260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x104a186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x104a18b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x104a18fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x104a19420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x104a19890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x104a19d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x104a1a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x104a1a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x104a1aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x104a1aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x104a1b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x104a1b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x104a1bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x104a1c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x104a1c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x104a1c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x104a1cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x104a1d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x104a1d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x104a1db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x104a1df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x104a1e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x104a1e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x104a1ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x104a1f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x104a1f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x104a1fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x104a1fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x104a20310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x104a20780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x104a20bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x104a21060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x104a214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x104a21940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x104a21db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x104a22220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x104a22690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x104a22b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x104a22f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x104a233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x104a23850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x104a23cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x104a24130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x104a245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x104a24a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x104a24e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x104a252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x104a25760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x104a25bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x104a26040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x104a264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x104a26920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x104a26d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x104a27200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x104a27670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x104a27ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x104a27f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x104a283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x104a28830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x104a28ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x104a29110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x104a29580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x104a299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x104a29e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x104a2a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x104a2a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x104a2abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x104a2b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x104a2b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x104a2b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x104a2bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x104a2c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x104a2c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x104a2cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x104a2cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x104a2d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x104a2d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x104a2dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x104a2e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x104a2e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x104a2e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x104a2ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x104a2f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x104a2f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x104a2fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x104a30000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x104a30470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x104a308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x104a30d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x104a311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x104a31630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x104a31aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x104a31f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x104a32380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x104a327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x104a32c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x104a330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x104a33540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x104a339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x104a33e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x104a34290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x104a34700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x104a34b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x104a34fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x104a35450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x104a358c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x104a364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x104a367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x104a36a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x104a36ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x104a37350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x104a377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x104a37c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x104a380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x104a38510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x104a38980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x104a38df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x104a39260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x104a396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x104a39b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x104a39fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x104a3a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x104a3a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x104a3ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x104a3b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x104a3b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x104a3ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x104a3bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x104a3c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x104a3c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x104a3cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x104a3d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x104a3d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x104a3d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x104a3ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x104a3e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x104a3e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x104a3eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x104a3ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x104a3f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x104a3f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x104a3fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x104a40240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x104a40750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x104a40bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x104a41030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x104a414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x104a41910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x104a41e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x104a42340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x104a42eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x104a43170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x104a43730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x104a43cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x104a442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x104a44870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x104a44e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x104a453f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x104a459b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x104a45f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x104a46530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x104a46af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x104a470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x104a47670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x104a47c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x104a481f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x104a487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x104a48d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x104a49330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x104a498f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x104a49eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x104a4a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x104a4aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x104a4aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x104a4b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x104a4bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x104a4c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x104a4c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x104a4ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x104a4d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x104a4d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x104a4ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x104a4e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x104a4e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x104a4ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x104a4f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x104a4fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x104a50070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x104a50630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x104a50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x104a511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x104a51770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x104a51d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x104a522f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x104a528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x104a52e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x104a53430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x104a539f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x104a53fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x104a54570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x104a54b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x104a550f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x104a556b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x104a55c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x104a56230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x104a567f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x104a56db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x104a57370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x104a57870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x104a57d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x104a58270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x104a58770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x104a58c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x104a59170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x104a59670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x104a59b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x104a5a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x104a5a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x104a5aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x104a5af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x104a5b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x104a5b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x104a5be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x104a5c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x104a5cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x104a5d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x104a5dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x104a5e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x104a5e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x104a5eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x104a5f160 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x104f044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x104f04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x104f04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x104f05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x104f056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x104f05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x104f05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x104f063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x104f06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x104f06db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x104f07220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x104f078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x104f083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x104f08b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x104f09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x104f09aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x104f0a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x104f0a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x104f0b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x104f0b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x104f0bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x104f0c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x104f0cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x104f0d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x104f0db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x104f0de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x104f0e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x104f0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x104f0e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x104f0ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x104f0f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x104f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x104f0fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x104f0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x104f10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x104f107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x104f10c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x104f110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x104f11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x104f119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x104f11e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x104f12290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x104f12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x104f12b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x104f12fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x104f13450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x104f138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x104f13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x104f141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x104f14610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x104f14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x104f14ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x104f15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x104f157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x104f15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x104f160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x104f16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x104f16b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x104f16f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x104f17400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x104f17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x104f17ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x104f18150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x104f185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x104f18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x104f18ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x104f19310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x104f19780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x104f19bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x104f1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x104f1a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x104f1a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x104f1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x104f1b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x104f1b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x104f1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x104f1bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x104f1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x104f1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x104f1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x104f1d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x104f1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x104f1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x104f1de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x104f1e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x104f1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x104f1ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x104f1f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x104f1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x104f1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x104f1fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x104f20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x104f20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x104f20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x104f20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x104f213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x104f21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x104f21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x104f22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x104f22580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x104f229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x104f22e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x104f232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x104f23b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x104f23e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x104f24290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x104f24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x104f24b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x104f24fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x104f25450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x104f258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x104f25d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x104f261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x104f26610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x104f26a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x104f26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x104f27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x104f277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x104f27c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x104f280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x104f28520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x104f28990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x104f28e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x104f29270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x104f296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x104f29b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x104f29fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x104f2a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x104f2a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x104f2ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x104f2b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x104f2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x104f2ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x104f2bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x104f2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x104f2c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x104f2cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x104f2d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x104f2d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x104f2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x104f2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x104f2e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x104f2e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x104f2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x104f2efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x104f2f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x104f2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x104f2fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x104f30160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x104f305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x104f30a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x104f30eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x104f31320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x104f31790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x104f31c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x104f32070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x104f324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x104f32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x104f32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x104f33230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x104f336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x104f33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x104f33f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x104f343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x104f34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x104f34cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x104f35140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x104f355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x104f35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x104f35e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x104f36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x104f36770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x104f36be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x104f37050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x104f374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x104f37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x104f37da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x104f38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x104f38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x104f38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x104f38f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x104f393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x104f39840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x104f39cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x104f3a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x104f3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x104f3aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x104f3ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x104f3b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x104f3b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x104f3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x104f3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x104f3c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x104f3c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x104f3cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x104f3d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x104f3d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x104f3dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x104f3df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x104f3e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x104f3e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x104f3ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x104f3f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x104f3f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x104f3f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x104f3fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x104f402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x104f40730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x104f40ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x104f41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x104f41b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x104f41e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x104f42110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x104f42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x104f429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x104f42e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x104f432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x104f43740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x104f43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x104f44020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x104f44490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x104f44900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x104f44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x104f451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x104f45650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x104f45ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x104f45f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x104f463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x104f46810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x104f46c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x104f470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x104f47560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x104f479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x104f47e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x104f482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x104f48720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x104f48b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x104f49000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x104f49470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x104f498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x104f49d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x104f4a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x104f4a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x104f4aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x104f4af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x104f4b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x104f4b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x104f4bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x104f4c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x104f4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x104f4c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x104f4ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x104f4d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x104f4d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x104f4db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x104f4dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x104f4e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x104f4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x104f4ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x104f4f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x104f4f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x104f4fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x104f4fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x104f50360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x104f507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x104f50c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x104f510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x104f51520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x104f51990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x104f51e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x104f52270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x104f526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x104f52b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x104f52fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x104f53430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x104f538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x104f53d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x104f54180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x104f545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x104f54a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x104f54ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x104f55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x104f557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x104f56220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x104f56940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x104f57060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x104f57780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x104f57a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x104f57eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x104f584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x104f58ac0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.688s
user	0m0.294s
sys	0m0.305s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4562 (178a7eb9)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x148607f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x148608640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x148608bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1486091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x148609750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x148609d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14860a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14860a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14860ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14860b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14860b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14860bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14860c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14860cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14860d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14860df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14860e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14860ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14860f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14860fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x148610360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x148610a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1486111a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x148611a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x148612160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x148612420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x148612a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1486136a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x148613be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x148613ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x148614340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x148614600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x148614e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1486153d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x148615690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x148615b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x148615fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x148616470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x148616910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x148616db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x148617250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1486176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x148617b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x148618030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1486182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x148618900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x148618f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x148619830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x148619e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14861a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14861aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14861b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14861b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14861bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14861c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14861c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14861cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14861d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14861d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14861de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14861e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14861e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14861ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14861ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14861f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14861f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14861fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1486201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x148620640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x148620ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x148620f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x148621420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1486218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x148621e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x148622360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1486228b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x148622e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x148623350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1486238a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x148623df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x148624340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x148624890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x148624de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x148625330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x148625880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x148625dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x148626320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x148626870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x148626dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x148627310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x148627860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x148627db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x148628300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x148628850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x148628da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1486292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x148629840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x148619520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x148629cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14862a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14862a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14862af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14862b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14862b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14862bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14862c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14862c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14862cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14862d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14862d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14862ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14862e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14862e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14862ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14862f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14862f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14862fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x148630090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x148630530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1486309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x148630e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x148631310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1486317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x148631c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1486320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x148632590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x148632a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x148632ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x148633370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x148633810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x148633cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x148634150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1486345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x148634a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x148634f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1486353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x148635870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x148635d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1486361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x148636650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x148636af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x148636f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x148637430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1486378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x148637d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x148638210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1486386b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x148638b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x148638ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x148639490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x148639930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x148639dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14863a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14863a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14863abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14863b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14863b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14863b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14863be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14863c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14863c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14863cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14863d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14863d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14863d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14863de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14863e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14863e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14863ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14863f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14863f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14863fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14863fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x148640390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x148640830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x148640cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x148641170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x148641610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x148641ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x148641f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1486423f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x148642890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x148642d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1486431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x148643670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x148643b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x148643fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x148644450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1486448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x148644d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x148645230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1486456d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x148645b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1486460c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x148646610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x148646b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1486470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x148647370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x148647980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x148647f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1486485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x148648d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x148649230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1486494f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x148649b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14864a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14864a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14864ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14864b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14864b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14864be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14864c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14864c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14864ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14864d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14864d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14864de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14864e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14864e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14864ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14864f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14864f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14864fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1486503a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1486508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x148650e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x148651390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1486518e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x148651e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x148652380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1486528d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x148652e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x148653370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1486538c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x148653e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x148654360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1486548b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x148654e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x148655350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1486558a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x148655df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x148656340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x148656890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x148656de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x148657330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x148657880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x148657dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x148658320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x148658870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x148658dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x148659310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x148659860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x148659db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14865a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14865a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14865ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14865b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14865b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14865bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14865c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14865c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14865cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14865d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14865d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14865dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14865e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14865e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14865ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14865f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14865f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14865fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14865ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1486603d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x148660870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x148660d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1486611b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x148661650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x148661af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x148661f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x148662430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1486628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x148662d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1486632c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1486639e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x148664100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x148664820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x148664f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x148665200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1486659f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x148665cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1486662c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.111.754 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.111.759 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14900a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14900ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14900b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14900b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14900bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14900c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14900caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14900d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14900d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14900db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14900e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14900e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14900f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14900f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14900ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x149010700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x149010e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x149011540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x149011c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x149012430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x149012b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x149013270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x149013990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1490140b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1490147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x149014a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1490150a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1490156b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x149015cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1490164b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x149016950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x149016c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1490174a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1490179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149017ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x149018140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1490185e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149018a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149018f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1490193c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x149019860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x149019d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14901a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14901a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14901a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14901af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14901b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14901bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14901c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14901c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14901cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14901d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14901d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14901df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14901e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14901ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14901f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14901f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14901f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x149020180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x149020620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x149020ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x149020f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x149021400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1490218a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x149021d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1490221e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x149022680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x149022b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x149022fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x149023460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x149023900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x149023da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1490242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x149024840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x149024d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1490252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x149025830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x149025d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1490262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x149026820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x149026d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1490272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x149027810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x149027d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1490282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x149028800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x149028d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1490292a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1490297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x149029d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14902a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14902a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14902ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14902b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14902b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14902bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14902c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14902c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14902cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14902d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14902d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14902dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14902e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14902e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14902ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14902f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14902f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14902fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x149030230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x149030780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x149030cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x149031220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1490316c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x149031b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149032000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1490324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149032940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x149032de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x149033280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x149033720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x149033bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x149034060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x149034500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1490349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x149034e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1490352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x149035780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x149035c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1490360c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x149036560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x149036a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x149036ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x149037340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1490377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x149037c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x149038120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1490385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x149038a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x149038f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1490393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x149039840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x149039ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14903a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14903a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14903aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14903af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14903b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14903b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14903bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14903c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14903c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14903cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14903cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14903d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14903d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14903dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14903e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14903e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14903eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14903f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14903f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14903f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14903fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1490402a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x149040740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x149040be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x149041080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x149041520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1490419c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x149041e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x149042300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1490427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x149042c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1490430e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x149043580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x149043a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149043ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x149044360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x149044800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x149044ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x149045140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1490455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x149045a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x149045f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1490463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x149046860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x149046d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1490471a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x149047640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x149047ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x149047f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x149048420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x149048970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x149048ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x149049410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x149049960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x149049c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14904a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14904a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14904ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14904b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14904bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14904bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14904c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14904c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14904d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14904d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14904daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14904df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14904e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14904ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14904f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14904f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14904fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1490501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x149050720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x149050c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1490511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x149051710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x149051c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1490521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x149052700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x149052c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1490531a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1490536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x149053c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x149054190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1490546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x149054c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x149055180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1490556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x149055c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x149056170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1490566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x149056c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x149057160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1490576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x149057c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x149058150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1490586a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x149058bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x149059140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x149059690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x149059be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14905a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14905a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14905abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14905b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14905b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14905bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14905c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14905c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14905cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14905d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14905d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14905dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14905e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14905e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14905eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14905f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14905f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14905fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1490600d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x149060620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x149060b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1490610c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x149061560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x149061a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x149061ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149062340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1490627e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x149062c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149063120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1490635c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x149063a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x149063f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1490643a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149064840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x149064ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x149065180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x149065620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x149065b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x149066290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1490669b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1490670d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1490677f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x149067ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1490682a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x149068560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x149068b70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x148665f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x148647c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x148647630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x148648250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14861b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14861ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14861d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x148649dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1486126e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1486191d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x148619af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14861a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1486185b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14861a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1486116e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14861d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x148629f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1486654c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1486148c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x148614b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14864a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x148648860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x148612cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x148612fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x148613270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x148666720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1486669e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x148666ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x148666f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x148667220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1486674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1486677a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x148667a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x148667d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x148667fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1486682a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x148668560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x148668820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x148668ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x148668da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x148669060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x148669320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1486695e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1486698a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x148669b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x148669e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14866a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14866a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14866a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14866a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14866abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14866aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14866b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14866b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14866b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14866b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14866bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14866bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14866c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14866c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14866c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14866ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14866cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14866cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14866d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14866d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14866d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14866daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14866dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14866e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14866e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14866e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14866e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14866eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14866ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14866f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14866f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14866f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14866f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14866fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14866fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x148670120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1486703e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1486706a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x148670960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x148670c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x148670ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1486711a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x148671460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x148671720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1486719e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x148671ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x148671f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x148672220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1486724e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1486727a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x148672a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x148672d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x148672fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1486732a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x148673560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x148673820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x148673ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x148673da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x148674060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x148674320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1486745e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1486748a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x148674b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x148674e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1486750e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1486753a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x148675660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x148675920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x148675be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x148675ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x148676160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x148676420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1486766e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1486769a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x148676c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x148676f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1486771e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1486774a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x148677760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x148677a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x148677ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x148677fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x148678260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x148678520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1486787e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x148678aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x148678d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x148679020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1486792e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1486795a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x148679860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x148679b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x148679de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14867a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14867a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14867a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14867a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14867aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14867ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14867b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14867b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14867b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14867b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14867bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14867bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14867c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14867c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14867c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14867c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14867cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14867cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14867d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14867d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14867d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14867da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14867dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14867dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14867e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14867e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14867e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14867eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14867eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14867f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14867f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14867f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14867f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14867fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14867fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1486800e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1486803a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x148680660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x148680920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x148680be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x148680ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x148681160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x148681420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1486816e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1486819a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x148681c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x148681f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1486821e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1486824a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x148682760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x148682a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x148682ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x148682fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x148683260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x148683520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1486837e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x148683aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x148683d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x148684020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1486842e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1486845a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x148684860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x148684b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x148684de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1486850a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x148685360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x148685620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1486858e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x148685ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x148685fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x148686440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x148686bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x148686eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x148687170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1486875e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x148687a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x148687ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x148688330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1486887a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x148688c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x148689080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1486894f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x148689960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x148689dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14868a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14868a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14868ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14868af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14868b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14868b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14868bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14868c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14868c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14868ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14868cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14868d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14868d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14868dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14868e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14868e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14868e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14868edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14868f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14868f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14868fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14868ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1486903e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x148690850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x148690cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x148691130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1486915a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x148691a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x148691e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1486922f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x148692760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x148692bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x148693040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1486934b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x148693920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x148693d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x148694200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x148694670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x148694ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x148694f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1486953c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x148695830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x148695ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x148696110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x148696580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1486969f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x148696e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1486972d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x148697740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x148697bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x148698020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x148698490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x148698900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x148698d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1486991e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x148699650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x148699ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x148699f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14869a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14869a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14869b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14869b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14869c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14869c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14869caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14869d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14869d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14869db60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.984s
user	0m0.250s
sys	0m0.200s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.51 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    1.66 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.17 sec*proc (2 tests)

Total Test time (real) =   2.19 sec
        2.21 real         0.69 user         0.25 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.55 sec*proc (2 tests)

Total Test time (real) =   0.56 sec
        0.57 real         0.15 user         0.08 sys
```
