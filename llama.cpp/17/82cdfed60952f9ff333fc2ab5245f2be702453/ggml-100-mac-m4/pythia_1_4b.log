Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.8s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.049s
user	0m1.036s
sys	0m1.437s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Built target build_info
[  5%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target xxhash
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Built target llava
[ 31%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Linking CXX static library libllava_static.a
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 35%] Built target test-c
[ 35%] Built target llama-simple
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llama-simple-chat
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 36%] Built target common
[ 36%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-log
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-log
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-chat
[ 49%] Built target test-sampling
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-llama-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 54%] Linking CXX executable ../bin/test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Linking CXX executable ../bin/test-gguf
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-arg-parser
[ 62%] Built target test-gguf
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Built target test-backend-ops
[ 62%] Built target test-chat-template
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-autorelease
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 63%] Built target test-barrier
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 64%] Built target test-model-load-cancel
[ 65%] Linking CXX executable ../bin/test-rope
[ 65%] Built target test-quantize-perf
[ 65%] Built target test-quantize-fns
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-embedding
[ 72%] Built target llama-batched
[ 72%] Built target llama-batched-bench
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-gbnf-validator
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-gritlm
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-bench
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Built target llama-infill
[ 75%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Built target llama-imatrix
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-lookahead
[ 78%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Built target llama-bench
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookup-create
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-lookup
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup-stats
[ 80%] Built target llama-lookahead
[ 80%] Generating loading.html.hpp
[ 80%] Built target llama-cli
[ 80%] Built target llama-parallel
[ 81%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Generating index.html.gz.hpp
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Built target llama-passkey
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-save-load-state
[ 83%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 84%] Built target llama-perplexity
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Built target llama-quantize
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-run
[ 90%] Built target llama-save-load-state
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-retrieval
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-speculative
[ 91%] Built target llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-speculative-simple
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-tts
[ 93%] Built target llama-run
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Built target llama-gen-docs
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-export-lora
[ 98%] Built target llama-cvector-generator
[ 98%] Built target llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.197s
user	0m6.635s
sys	0m10.014s

main: quantize time =  4799.74 ms
main:    total time =  4799.74 ms

main: quantize time =  1812.57 ms
main:    total time =  1812.57 ms

main: quantize time =  2785.32 ms
main:    total time =  2785.32 ms

main: quantize time =  2504.48 ms
main:    total time =  2504.48 ms

main: quantize time =  2180.77 ms
main:    total time =  2180.77 ms

main: quantize time =  6313.77 ms
main:    total time =  6313.77 ms

main: quantize time =  5943.53 ms
main:    total time =  5943.53 ms

main: quantize time =  6841.43 ms
main:    total time =  6841.43 ms

main: quantize time =  6197.19 ms
main:    total time =  6197.19 ms

main: quantize time =  4578.03 ms
main:    total time =  4578.03 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.146 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.317 I main: llama backend init
0.00.000.324 I main: load the model and apply lora adapter, if any
0.00.052.521 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.064.857 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.064.870 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.064.874 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.064.879 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.064.879 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.064.880 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.064.880 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.064.894 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.064.894 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.064.895 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.064.896 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.064.896 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.064.897 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.064.898 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.064.903 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.064.904 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.064.904 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.071.706 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.073.873 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.080.589 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.080.598 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.080.599 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.080.599 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.080.600 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.080.601 I llama_model_loader: - type  f32:  194 tensors
0.00.080.602 I llama_model_loader: - type  f16:   98 tensors
0.00.080.604 I print_info: file format = GGUF V3 (latest)
0.00.080.605 I print_info: file type   = all F32 (guessed)
0.00.080.608 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.098.770 I load: special tokens cache size = 25
0.00.108.820 I load: token to piece cache size = 0.2984 MB
0.00.108.825 I print_info: arch             = gptneox
0.00.108.826 I print_info: vocab_only       = 0
0.00.108.826 I print_info: n_ctx_train      = 2048
0.00.108.826 I print_info: n_embd           = 2048
0.00.108.826 I print_info: n_layer          = 24
0.00.108.835 I print_info: n_head           = 16
0.00.108.836 I print_info: n_head_kv        = 16
0.00.108.836 I print_info: n_rot            = 32
0.00.108.837 I print_info: n_swa            = 0
0.00.108.838 I print_info: n_embd_head_k    = 128
0.00.108.838 I print_info: n_embd_head_v    = 128
0.00.108.839 I print_info: n_gqa            = 1
0.00.108.840 I print_info: n_embd_k_gqa     = 2048
0.00.108.841 I print_info: n_embd_v_gqa     = 2048
0.00.108.842 I print_info: f_norm_eps       = 1.0e-05
0.00.108.843 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.108.843 I print_info: f_clamp_kqv      = 0.0e+00
0.00.108.843 I print_info: f_max_alibi_bias = 0.0e+00
0.00.108.843 I print_info: f_logit_scale    = 0.0e+00
0.00.108.844 I print_info: n_ff             = 8192
0.00.108.844 I print_info: n_expert         = 0
0.00.108.845 I print_info: n_expert_used    = 0
0.00.108.845 I print_info: causal attn      = 1
0.00.108.845 I print_info: pooling type     = 0
0.00.108.845 I print_info: rope type        = 2
0.00.108.845 I print_info: rope scaling     = linear
0.00.108.846 I print_info: freq_base_train  = 10000.0
0.00.108.846 I print_info: freq_scale_train = 1
0.00.108.846 I print_info: n_ctx_orig_yarn  = 2048
0.00.108.847 I print_info: rope_finetuned   = unknown
0.00.108.847 I print_info: ssm_d_conv       = 0
0.00.108.847 I print_info: ssm_d_inner      = 0
0.00.108.847 I print_info: ssm_d_state      = 0
0.00.108.847 I print_info: ssm_dt_rank      = 0
0.00.108.847 I print_info: ssm_dt_b_c_rms   = 0
0.00.108.848 I print_info: model type       = 1.4B
0.00.108.848 I print_info: model params     = 1.41 B
0.00.108.848 I print_info: general.name     = 1.4B
0.00.108.849 I print_info: vocab type       = BPE
0.00.108.849 I print_info: n_vocab          = 50304
0.00.108.850 I print_info: n_merges         = 50009
0.00.108.850 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.108.850 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.108.850 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.108.850 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.108.851 I print_info: LF token         = 187 'Ċ'
0.00.108.851 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.108.851 I print_info: max token length = 1024
0.00.108.852 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.176.602 I load_tensors: offloading 24 repeating layers to GPU
0.00.176.606 I load_tensors: offloading output layer to GPU
0.00.176.606 I load_tensors: offloaded 25/25 layers to GPU
0.00.176.633 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.176.635 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.177.254 I llama_init_from_model: n_seq_max     = 1
0.00.177.255 I llama_init_from_model: n_ctx         = 2048
0.00.177.255 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.177.255 I llama_init_from_model: n_batch       = 2048
0.00.177.255 I llama_init_from_model: n_ubatch      = 512
0.00.177.255 I llama_init_from_model: flash_attn    = 0
0.00.177.256 I llama_init_from_model: freq_base     = 10000.0
0.00.177.256 I llama_init_from_model: freq_scale    = 1
0.00.177.258 I ggml_metal_init: allocating
0.00.177.298 I ggml_metal_init: found device: Apple M4
0.00.177.304 I ggml_metal_init: picking default device: Apple M4
0.00.177.940 I ggml_metal_init: using embedded metal library
0.00.187.588 I ggml_metal_init: GPU name:   Apple M4
0.00.187.589 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.187.590 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.187.590 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.187.590 I ggml_metal_init: simdgroup reduction   = true
0.00.187.590 I ggml_metal_init: simdgroup matrix mul. = true
0.00.187.590 I ggml_metal_init: has residency sets    = true
0.00.187.591 I ggml_metal_init: has bfloat            = true
0.00.187.591 I ggml_metal_init: use bfloat            = true
0.00.187.591 I ggml_metal_init: hasUnifiedMemory      = true
0.00.187.592 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.212.603 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.241.489 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.241.496 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.241.541 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.245.077 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.245.079 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.245.079 I llama_init_from_model: graph nodes  = 967
0.00.245.079 I llama_init_from_model: graph splits = 2
0.00.245.088 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.245.203 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.245.204 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.308.581 I main: llama threadpool init, n_threads = 4
0.00.308.620 I 
0.00.308.646 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.308.647 I 
0.00.308.768 I sampler seed: 1234
0.00.308.772 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.308.796 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.308.797 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.308.797 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.149.776 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59216.01 tokens per second)
0.02.149.777 I llama_perf_context_print:        load time =     255.20 ms
0.02.149.778 I llama_perf_context_print: prompt eval time =      43.85 ms /     7 tokens (    6.26 ms per token,   159.65 tokens per second)
0.02.149.779 I llama_perf_context_print:        eval time =    1794.29 ms /    63 runs   (   28.48 ms per token,    35.11 tokens per second)
0.02.149.779 I llama_perf_context_print:       total time =    1842.05 ms /    70 tokens
0.02.150.003 I ggml_metal_free: deallocating

real	0m2.443s
user	0m0.133s
sys	0m0.155s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.010.156 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.744 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.752 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.756 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.756 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.756 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.757 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.757 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.758 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.758 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.759 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.759 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.759 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.768 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.771 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.773 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.775 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.775 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.636 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.643 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.460 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.461 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.462 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.462 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.463 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.463 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.464 I llama_model_loader: - type  f32:  194 tensors
0.00.035.464 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.465 I print_info: file format = GGUF V3 (latest)
0.00.035.466 I print_info: file type   = Q8_0
0.00.035.467 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.044.673 I load: special tokens cache size = 25
0.00.051.176 I load: token to piece cache size = 0.2984 MB
0.00.051.181 I print_info: arch             = gptneox
0.00.051.181 I print_info: vocab_only       = 0
0.00.051.183 I print_info: n_ctx_train      = 2048
0.00.051.183 I print_info: n_embd           = 2048
0.00.051.183 I print_info: n_layer          = 24
0.00.051.187 I print_info: n_head           = 16
0.00.051.187 I print_info: n_head_kv        = 16
0.00.051.188 I print_info: n_rot            = 32
0.00.051.188 I print_info: n_swa            = 0
0.00.051.188 I print_info: n_embd_head_k    = 128
0.00.051.188 I print_info: n_embd_head_v    = 128
0.00.051.189 I print_info: n_gqa            = 1
0.00.051.190 I print_info: n_embd_k_gqa     = 2048
0.00.051.191 I print_info: n_embd_v_gqa     = 2048
0.00.051.191 I print_info: f_norm_eps       = 1.0e-05
0.00.051.192 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.192 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.192 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.192 I print_info: f_logit_scale    = 0.0e+00
0.00.051.193 I print_info: n_ff             = 8192
0.00.051.193 I print_info: n_expert         = 0
0.00.051.193 I print_info: n_expert_used    = 0
0.00.051.193 I print_info: causal attn      = 1
0.00.051.194 I print_info: pooling type     = 0
0.00.051.194 I print_info: rope type        = 2
0.00.051.194 I print_info: rope scaling     = linear
0.00.051.195 I print_info: freq_base_train  = 10000.0
0.00.051.195 I print_info: freq_scale_train = 1
0.00.051.195 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.195 I print_info: rope_finetuned   = unknown
0.00.051.196 I print_info: ssm_d_conv       = 0
0.00.051.196 I print_info: ssm_d_inner      = 0
0.00.051.196 I print_info: ssm_d_state      = 0
0.00.051.196 I print_info: ssm_dt_rank      = 0
0.00.051.196 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.197 I print_info: model type       = 1.4B
0.00.051.197 I print_info: model params     = 1.41 B
0.00.051.197 I print_info: general.name     = 1.4B
0.00.051.198 I print_info: vocab type       = BPE
0.00.051.198 I print_info: n_vocab          = 50304
0.00.051.198 I print_info: n_merges         = 50009
0.00.051.198 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.199 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.199 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.199 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.199 I print_info: LF token         = 187 'Ċ'
0.00.051.200 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.200 I print_info: max token length = 1024
0.00.051.201 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.124.110 I load_tensors: offloading 24 repeating layers to GPU
0.01.124.115 I load_tensors: offloading output layer to GPU
0.01.124.116 I load_tensors: offloaded 25/25 layers to GPU
0.01.124.141 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.124.144 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.124.889 I llama_init_from_model: n_seq_max     = 1
0.01.124.890 I llama_init_from_model: n_ctx         = 2048
0.01.124.891 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.124.891 I llama_init_from_model: n_batch       = 2048
0.01.124.892 I llama_init_from_model: n_ubatch      = 512
0.01.124.892 I llama_init_from_model: flash_attn    = 0
0.01.124.893 I llama_init_from_model: freq_base     = 10000.0
0.01.124.893 I llama_init_from_model: freq_scale    = 1
0.01.124.895 I ggml_metal_init: allocating
0.01.124.904 I ggml_metal_init: found device: Apple M4
0.01.124.911 I ggml_metal_init: picking default device: Apple M4
0.01.126.117 I ggml_metal_init: using embedded metal library
0.01.131.526 I ggml_metal_init: GPU name:   Apple M4
0.01.131.529 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.131.530 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.131.530 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.131.531 I ggml_metal_init: simdgroup reduction   = true
0.01.131.531 I ggml_metal_init: simdgroup matrix mul. = true
0.01.131.531 I ggml_metal_init: has residency sets    = true
0.01.131.531 I ggml_metal_init: has bfloat            = true
0.01.131.532 I ggml_metal_init: use bfloat            = true
0.01.131.532 I ggml_metal_init: hasUnifiedMemory      = true
0.01.131.533 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.150.511 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.202.806 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.202.811 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.202.846 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.206.998 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.207.000 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.207.000 I llama_init_from_model: graph nodes  = 967
0.01.207.000 I llama_init_from_model: graph splits = 2
0.01.207.006 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.207.122 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.207.123 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.261.845 I main: llama threadpool init, n_threads = 4
0.01.261.890 I 
0.01.261.912 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.261.913 I 
0.01.262.069 I sampler seed: 1234
0.01.262.074 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.262.116 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.262.128 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.262.129 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.353.161 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51226.55 tokens per second)
0.02.353.162 I llama_perf_context_print:        load time =    1250.97 ms
0.02.353.163 I llama_perf_context_print: prompt eval time =      44.30 ms /     7 tokens (    6.33 ms per token,   158.01 tokens per second)
0.02.353.163 I llama_perf_context_print:        eval time =    1043.75 ms /    63 runs   (   16.57 ms per token,    60.36 tokens per second)
0.02.353.164 I llama_perf_context_print:       total time =    1092.04 ms /    70 tokens
0.02.353.395 I ggml_metal_free: deallocating

real	0m2.372s
user	0m0.110s
sys	0m0.266s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.011.682 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.985 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.020.990 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.998 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.999 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.999 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.999 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.000 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.001 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.001 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.001 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.002 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.002 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.003 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.003 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.006 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.006 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.006 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.945 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.023 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.947 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.948 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.949 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.949 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.949 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.950 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.029.950 I llama_model_loader: - type  f32:  194 tensors
0.00.029.950 I llama_model_loader: - type q4_0:   97 tensors
0.00.029.951 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.952 I print_info: file format = GGUF V3 (latest)
0.00.029.952 I print_info: file type   = Q4_0
0.00.029.953 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.038.147 I load: special tokens cache size = 25
0.00.044.350 I load: token to piece cache size = 0.2984 MB
0.00.044.356 I print_info: arch             = gptneox
0.00.044.356 I print_info: vocab_only       = 0
0.00.044.356 I print_info: n_ctx_train      = 2048
0.00.044.356 I print_info: n_embd           = 2048
0.00.044.357 I print_info: n_layer          = 24
0.00.044.363 I print_info: n_head           = 16
0.00.044.364 I print_info: n_head_kv        = 16
0.00.044.364 I print_info: n_rot            = 32
0.00.044.364 I print_info: n_swa            = 0
0.00.044.364 I print_info: n_embd_head_k    = 128
0.00.044.364 I print_info: n_embd_head_v    = 128
0.00.044.365 I print_info: n_gqa            = 1
0.00.044.365 I print_info: n_embd_k_gqa     = 2048
0.00.044.366 I print_info: n_embd_v_gqa     = 2048
0.00.044.367 I print_info: f_norm_eps       = 1.0e-05
0.00.044.367 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.368 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.368 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.368 I print_info: f_logit_scale    = 0.0e+00
0.00.044.369 I print_info: n_ff             = 8192
0.00.044.369 I print_info: n_expert         = 0
0.00.044.369 I print_info: n_expert_used    = 0
0.00.044.369 I print_info: causal attn      = 1
0.00.044.369 I print_info: pooling type     = 0
0.00.044.369 I print_info: rope type        = 2
0.00.044.370 I print_info: rope scaling     = linear
0.00.044.370 I print_info: freq_base_train  = 10000.0
0.00.044.370 I print_info: freq_scale_train = 1
0.00.044.370 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.371 I print_info: rope_finetuned   = unknown
0.00.044.371 I print_info: ssm_d_conv       = 0
0.00.044.371 I print_info: ssm_d_inner      = 0
0.00.044.371 I print_info: ssm_d_state      = 0
0.00.044.371 I print_info: ssm_dt_rank      = 0
0.00.044.371 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.372 I print_info: model type       = 1.4B
0.00.044.372 I print_info: model params     = 1.41 B
0.00.044.372 I print_info: general.name     = 1.4B
0.00.044.373 I print_info: vocab type       = BPE
0.00.044.373 I print_info: n_vocab          = 50304
0.00.044.373 I print_info: n_merges         = 50009
0.00.044.373 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.375 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.375 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.375 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.375 I print_info: LF token         = 187 'Ċ'
0.00.044.376 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.376 I print_info: max token length = 1024
0.00.044.376 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.620.229 I load_tensors: offloading 24 repeating layers to GPU
0.00.620.236 I load_tensors: offloading output layer to GPU
0.00.620.236 I load_tensors: offloaded 25/25 layers to GPU
0.00.620.261 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.620.262 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.621.133 I llama_init_from_model: n_seq_max     = 1
0.00.621.136 I llama_init_from_model: n_ctx         = 2048
0.00.621.136 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.621.137 I llama_init_from_model: n_batch       = 2048
0.00.621.137 I llama_init_from_model: n_ubatch      = 512
0.00.621.137 I llama_init_from_model: flash_attn    = 0
0.00.621.139 I llama_init_from_model: freq_base     = 10000.0
0.00.621.139 I llama_init_from_model: freq_scale    = 1
0.00.621.140 I ggml_metal_init: allocating
0.00.621.208 I ggml_metal_init: found device: Apple M4
0.00.621.220 I ggml_metal_init: picking default device: Apple M4
0.00.622.328 I ggml_metal_init: using embedded metal library
0.00.626.557 I ggml_metal_init: GPU name:   Apple M4
0.00.626.564 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.626.564 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.626.565 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.626.566 I ggml_metal_init: simdgroup reduction   = true
0.00.626.566 I ggml_metal_init: simdgroup matrix mul. = true
0.00.626.566 I ggml_metal_init: has residency sets    = true
0.00.626.566 I ggml_metal_init: has bfloat            = true
0.00.626.567 I ggml_metal_init: use bfloat            = true
0.00.626.568 I ggml_metal_init: hasUnifiedMemory      = true
0.00.626.571 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.640.677 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.671.371 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.671.382 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.671.420 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.675.837 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.675.839 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.675.839 I llama_init_from_model: graph nodes  = 967
0.00.675.840 I llama_init_from_model: graph splits = 2
0.00.675.846 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.675.975 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.675.975 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.731.997 I main: llama threadpool init, n_threads = 4
0.00.732.042 I 
0.00.732.065 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.732.065 I 
0.00.732.238 I sampler seed: 1234
0.00.732.243 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.732.268 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.732.269 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.732.270 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.412.932 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50859.60 tokens per second)
0.01.412.933 I llama_perf_context_print:        load time =     719.60 ms
0.01.412.933 I llama_perf_context_print: prompt eval time =      49.32 ms /     7 tokens (    7.05 ms per token,   141.93 tokens per second)
0.01.412.934 I llama_perf_context_print:        eval time =     628.39 ms /    63 runs   (    9.97 ms per token,   100.26 tokens per second)
0.01.412.934 I llama_perf_context_print:       total time =     681.65 ms /    70 tokens
0.01.413.170 I ggml_metal_free: deallocating

real	0m1.433s
user	0m0.104s
sys	0m0.167s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.178 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.160 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.164 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.170 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.171 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.171 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.171 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.172 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.173 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.173 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.173 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.174 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.174 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.176 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.177 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.178 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.178 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.179 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.918 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.927 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.647 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.648 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.648 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.649 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.649 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.649 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.650 I llama_model_loader: - type  f32:  194 tensors
0.00.025.650 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.650 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.651 I print_info: file format = GGUF V3 (latest)
0.00.025.651 I print_info: file type   = Q4_1
0.00.025.656 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.535 I load: special tokens cache size = 25
0.00.039.555 I load: token to piece cache size = 0.2984 MB
0.00.039.558 I print_info: arch             = gptneox
0.00.039.558 I print_info: vocab_only       = 0
0.00.039.558 I print_info: n_ctx_train      = 2048
0.00.039.558 I print_info: n_embd           = 2048
0.00.039.559 I print_info: n_layer          = 24
0.00.039.561 I print_info: n_head           = 16
0.00.039.562 I print_info: n_head_kv        = 16
0.00.039.562 I print_info: n_rot            = 32
0.00.039.564 I print_info: n_swa            = 0
0.00.039.564 I print_info: n_embd_head_k    = 128
0.00.039.564 I print_info: n_embd_head_v    = 128
0.00.039.565 I print_info: n_gqa            = 1
0.00.039.566 I print_info: n_embd_k_gqa     = 2048
0.00.039.571 I print_info: n_embd_v_gqa     = 2048
0.00.039.571 I print_info: f_norm_eps       = 1.0e-05
0.00.039.573 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.573 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.574 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.574 I print_info: f_logit_scale    = 0.0e+00
0.00.039.575 I print_info: n_ff             = 8192
0.00.039.575 I print_info: n_expert         = 0
0.00.039.575 I print_info: n_expert_used    = 0
0.00.039.575 I print_info: causal attn      = 1
0.00.039.575 I print_info: pooling type     = 0
0.00.039.575 I print_info: rope type        = 2
0.00.039.576 I print_info: rope scaling     = linear
0.00.039.576 I print_info: freq_base_train  = 10000.0
0.00.039.576 I print_info: freq_scale_train = 1
0.00.039.576 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.577 I print_info: rope_finetuned   = unknown
0.00.039.577 I print_info: ssm_d_conv       = 0
0.00.039.577 I print_info: ssm_d_inner      = 0
0.00.039.577 I print_info: ssm_d_state      = 0
0.00.039.577 I print_info: ssm_dt_rank      = 0
0.00.039.577 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.577 I print_info: model type       = 1.4B
0.00.039.578 I print_info: model params     = 1.41 B
0.00.039.580 I print_info: general.name     = 1.4B
0.00.039.580 I print_info: vocab type       = BPE
0.00.039.580 I print_info: n_vocab          = 50304
0.00.039.580 I print_info: n_merges         = 50009
0.00.039.581 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.581 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.581 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.581 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.582 I print_info: LF token         = 187 'Ċ'
0.00.039.582 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.582 I print_info: max token length = 1024
0.00.039.583 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.608.445 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.459 I load_tensors: offloading output layer to GPU
0.00.608.460 I load_tensors: offloaded 25/25 layers to GPU
0.00.608.497 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.608.498 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.610.017 I llama_init_from_model: n_seq_max     = 1
0.00.610.019 I llama_init_from_model: n_ctx         = 2048
0.00.610.020 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.610.020 I llama_init_from_model: n_batch       = 2048
0.00.610.021 I llama_init_from_model: n_ubatch      = 512
0.00.610.021 I llama_init_from_model: flash_attn    = 0
0.00.610.024 I llama_init_from_model: freq_base     = 10000.0
0.00.610.024 I llama_init_from_model: freq_scale    = 1
0.00.610.026 I ggml_metal_init: allocating
0.00.610.106 I ggml_metal_init: found device: Apple M4
0.00.610.119 I ggml_metal_init: picking default device: Apple M4
0.00.612.004 I ggml_metal_init: using embedded metal library
0.00.618.807 I ggml_metal_init: GPU name:   Apple M4
0.00.618.812 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.618.813 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.618.814 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.618.814 I ggml_metal_init: simdgroup reduction   = true
0.00.618.815 I ggml_metal_init: simdgroup matrix mul. = true
0.00.618.815 I ggml_metal_init: has residency sets    = true
0.00.618.815 I ggml_metal_init: has bfloat            = true
0.00.618.816 I ggml_metal_init: use bfloat            = true
0.00.618.817 I ggml_metal_init: hasUnifiedMemory      = true
0.00.618.819 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.637.609 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.694.650 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.694.657 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.694.691 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.699.385 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.699.388 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.699.388 I llama_init_from_model: graph nodes  = 967
0.00.699.388 I llama_init_from_model: graph splits = 2
0.00.699.394 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.699.526 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.699.527 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.755.252 I main: llama threadpool init, n_threads = 4
0.00.755.299 I 
0.00.755.321 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.755.323 I 
0.00.755.478 I sampler seed: 1234
0.00.755.483 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.755.504 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.755.504 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.755.504 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.476.515 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54573.41 tokens per second)
0.01.476.516 I llama_perf_context_print:        load time =     745.35 ms
0.01.476.517 I llama_perf_context_print: prompt eval time =      47.05 ms /     7 tokens (    6.72 ms per token,   148.78 tokens per second)
0.01.476.517 I llama_perf_context_print:        eval time =     671.18 ms /    63 runs   (   10.65 ms per token,    93.86 tokens per second)
0.01.476.519 I llama_perf_context_print:       total time =     721.99 ms /    70 tokens
0.01.476.793 I ggml_metal_free: deallocating

real	0m1.493s
user	0m0.109s
sys	0m0.208s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.659 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.538 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.543 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.545 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.545 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.545 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.550 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.550 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.551 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.551 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.553 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.553 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.553 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.554 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.554 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.557 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.557 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.557 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.314 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.357 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.081 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.082 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.082 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.083 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.083 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.083 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.084 I llama_model_loader: - type  f32:  194 tensors
0.00.027.084 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.084 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.085 I print_info: file format = GGUF V3 (latest)
0.00.027.085 I print_info: file type   = Q5_0
0.00.027.089 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.829 I load: special tokens cache size = 25
0.00.040.868 I load: token to piece cache size = 0.2984 MB
0.00.040.871 I print_info: arch             = gptneox
0.00.040.871 I print_info: vocab_only       = 0
0.00.040.872 I print_info: n_ctx_train      = 2048
0.00.040.872 I print_info: n_embd           = 2048
0.00.040.872 I print_info: n_layer          = 24
0.00.040.875 I print_info: n_head           = 16
0.00.040.876 I print_info: n_head_kv        = 16
0.00.040.876 I print_info: n_rot            = 32
0.00.040.876 I print_info: n_swa            = 0
0.00.040.877 I print_info: n_embd_head_k    = 128
0.00.040.877 I print_info: n_embd_head_v    = 128
0.00.040.878 I print_info: n_gqa            = 1
0.00.040.879 I print_info: n_embd_k_gqa     = 2048
0.00.040.879 I print_info: n_embd_v_gqa     = 2048
0.00.040.880 I print_info: f_norm_eps       = 1.0e-05
0.00.040.880 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.881 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.881 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.881 I print_info: f_logit_scale    = 0.0e+00
0.00.040.882 I print_info: n_ff             = 8192
0.00.040.882 I print_info: n_expert         = 0
0.00.040.882 I print_info: n_expert_used    = 0
0.00.040.882 I print_info: causal attn      = 1
0.00.040.882 I print_info: pooling type     = 0
0.00.040.885 I print_info: rope type        = 2
0.00.040.886 I print_info: rope scaling     = linear
0.00.040.886 I print_info: freq_base_train  = 10000.0
0.00.040.886 I print_info: freq_scale_train = 1
0.00.040.886 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.887 I print_info: rope_finetuned   = unknown
0.00.040.887 I print_info: ssm_d_conv       = 0
0.00.040.887 I print_info: ssm_d_inner      = 0
0.00.040.887 I print_info: ssm_d_state      = 0
0.00.040.887 I print_info: ssm_dt_rank      = 0
0.00.040.889 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.889 I print_info: model type       = 1.4B
0.00.040.890 I print_info: model params     = 1.41 B
0.00.040.890 I print_info: general.name     = 1.4B
0.00.040.890 I print_info: vocab type       = BPE
0.00.040.890 I print_info: n_vocab          = 50304
0.00.040.891 I print_info: n_merges         = 50009
0.00.040.891 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.893 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.893 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.893 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.893 I print_info: LF token         = 187 'Ċ'
0.00.040.894 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.894 I print_info: max token length = 1024
0.00.040.894 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.653.955 I load_tensors: offloading 24 repeating layers to GPU
0.00.653.969 I load_tensors: offloading output layer to GPU
0.00.653.970 I load_tensors: offloaded 25/25 layers to GPU
0.00.654.001 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.654.002 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.655.515 I llama_init_from_model: n_seq_max     = 1
0.00.655.521 I llama_init_from_model: n_ctx         = 2048
0.00.655.522 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.655.523 I llama_init_from_model: n_batch       = 2048
0.00.655.523 I llama_init_from_model: n_ubatch      = 512
0.00.655.523 I llama_init_from_model: flash_attn    = 0
0.00.655.524 I llama_init_from_model: freq_base     = 10000.0
0.00.655.525 I llama_init_from_model: freq_scale    = 1
0.00.655.527 I ggml_metal_init: allocating
0.00.655.575 I ggml_metal_init: found device: Apple M4
0.00.655.587 I ggml_metal_init: picking default device: Apple M4
0.00.657.391 I ggml_metal_init: using embedded metal library
0.00.663.135 I ggml_metal_init: GPU name:   Apple M4
0.00.663.140 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.663.141 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.663.142 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.663.143 I ggml_metal_init: simdgroup reduction   = true
0.00.663.143 I ggml_metal_init: simdgroup matrix mul. = true
0.00.663.143 I ggml_metal_init: has residency sets    = true
0.00.663.144 I ggml_metal_init: has bfloat            = true
0.00.663.144 I ggml_metal_init: use bfloat            = true
0.00.663.145 I ggml_metal_init: hasUnifiedMemory      = true
0.00.663.147 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.683.426 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.743.435 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.743.445 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.743.494 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.747.852 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.747.853 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.747.853 I llama_init_from_model: graph nodes  = 967
0.00.747.854 I llama_init_from_model: graph splits = 2
0.00.747.857 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.747.981 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.747.981 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.808.915 I main: llama threadpool init, n_threads = 4
0.00.808.957 I 
0.00.808.979 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.808.980 I 
0.00.809.130 I sampler seed: 1234
0.00.809.134 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.809.145 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.809.145 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.809.145 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.597.570 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51189.62 tokens per second)
0.01.597.570 I llama_perf_context_print:        load time =     797.54 ms
0.01.597.572 I llama_perf_context_print: prompt eval time =      52.89 ms /     7 tokens (    7.56 ms per token,   132.35 tokens per second)
0.01.597.573 I llama_perf_context_print:        eval time =     732.64 ms /    63 runs   (   11.63 ms per token,    85.99 tokens per second)
0.01.597.573 I llama_perf_context_print:       total time =     789.37 ms /    70 tokens
0.01.597.802 I ggml_metal_free: deallocating

real	0m1.617s
user	0m0.111s
sys	0m0.215s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.711 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.233 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.238 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.244 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.245 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.245 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.245 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.246 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.246 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.247 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.247 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.249 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.249 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.250 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.250 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.252 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.253 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.254 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.009 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.055 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.817 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.818 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.819 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.819 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.819 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.819 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.820 I llama_model_loader: - type  f32:  194 tensors
0.00.024.820 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.820 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.821 I print_info: file format = GGUF V3 (latest)
0.00.024.821 I print_info: file type   = Q5_1
0.00.024.822 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.584 I load: special tokens cache size = 25
0.00.038.597 I load: token to piece cache size = 0.2984 MB
0.00.038.600 I print_info: arch             = gptneox
0.00.038.601 I print_info: vocab_only       = 0
0.00.038.601 I print_info: n_ctx_train      = 2048
0.00.038.601 I print_info: n_embd           = 2048
0.00.038.601 I print_info: n_layer          = 24
0.00.038.604 I print_info: n_head           = 16
0.00.038.604 I print_info: n_head_kv        = 16
0.00.038.605 I print_info: n_rot            = 32
0.00.038.605 I print_info: n_swa            = 0
0.00.038.605 I print_info: n_embd_head_k    = 128
0.00.038.607 I print_info: n_embd_head_v    = 128
0.00.038.607 I print_info: n_gqa            = 1
0.00.038.608 I print_info: n_embd_k_gqa     = 2048
0.00.038.609 I print_info: n_embd_v_gqa     = 2048
0.00.038.609 I print_info: f_norm_eps       = 1.0e-05
0.00.038.610 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.610 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.610 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.610 I print_info: f_logit_scale    = 0.0e+00
0.00.038.611 I print_info: n_ff             = 8192
0.00.038.611 I print_info: n_expert         = 0
0.00.038.611 I print_info: n_expert_used    = 0
0.00.038.611 I print_info: causal attn      = 1
0.00.038.611 I print_info: pooling type     = 0
0.00.038.611 I print_info: rope type        = 2
0.00.038.612 I print_info: rope scaling     = linear
0.00.038.612 I print_info: freq_base_train  = 10000.0
0.00.038.612 I print_info: freq_scale_train = 1
0.00.038.613 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.613 I print_info: rope_finetuned   = unknown
0.00.038.613 I print_info: ssm_d_conv       = 0
0.00.038.613 I print_info: ssm_d_inner      = 0
0.00.038.613 I print_info: ssm_d_state      = 0
0.00.038.613 I print_info: ssm_dt_rank      = 0
0.00.038.613 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.614 I print_info: model type       = 1.4B
0.00.038.615 I print_info: model params     = 1.41 B
0.00.038.616 I print_info: general.name     = 1.4B
0.00.038.617 I print_info: vocab type       = BPE
0.00.038.617 I print_info: n_vocab          = 50304
0.00.038.617 I print_info: n_merges         = 50009
0.00.038.617 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.617 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.618 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.619 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.619 I print_info: LF token         = 187 'Ċ'
0.00.038.620 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.620 I print_info: max token length = 1024
0.00.038.620 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.696.363 I load_tensors: offloading 24 repeating layers to GPU
0.00.696.368 I load_tensors: offloading output layer to GPU
0.00.696.370 I load_tensors: offloaded 25/25 layers to GPU
0.00.696.394 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.696.396 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.697.937 I llama_init_from_model: n_seq_max     = 1
0.00.697.940 I llama_init_from_model: n_ctx         = 2048
0.00.697.940 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.697.941 I llama_init_from_model: n_batch       = 2048
0.00.697.941 I llama_init_from_model: n_ubatch      = 512
0.00.697.942 I llama_init_from_model: flash_attn    = 0
0.00.697.943 I llama_init_from_model: freq_base     = 10000.0
0.00.697.943 I llama_init_from_model: freq_scale    = 1
0.00.697.945 I ggml_metal_init: allocating
0.00.697.963 I ggml_metal_init: found device: Apple M4
0.00.697.972 I ggml_metal_init: picking default device: Apple M4
0.00.699.450 I ggml_metal_init: using embedded metal library
0.00.705.565 I ggml_metal_init: GPU name:   Apple M4
0.00.705.568 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.705.569 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.705.570 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.705.570 I ggml_metal_init: simdgroup reduction   = true
0.00.705.571 I ggml_metal_init: simdgroup matrix mul. = true
0.00.705.571 I ggml_metal_init: has residency sets    = true
0.00.705.571 I ggml_metal_init: has bfloat            = true
0.00.705.571 I ggml_metal_init: use bfloat            = true
0.00.705.572 I ggml_metal_init: hasUnifiedMemory      = true
0.00.705.573 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.722.694 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.774.829 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.774.836 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.774.872 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.779.145 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.779.147 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.779.148 I llama_init_from_model: graph nodes  = 967
0.00.779.148 I llama_init_from_model: graph splits = 2
0.00.779.153 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.779.282 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.779.283 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.835.804 I main: llama threadpool init, n_threads = 4
0.00.835.846 I 
0.00.835.866 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.835.866 I 
0.00.835.977 I sampler seed: 1234
0.00.835.981 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.836.017 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.836.021 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.836.021 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.686.963 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53463.86 tokens per second)
0.01.686.964 I llama_perf_context_print:        load time =     826.39 ms
0.01.686.966 I llama_perf_context_print: prompt eval time =      52.62 ms /     7 tokens (    7.52 ms per token,   133.02 tokens per second)
0.01.686.966 I llama_perf_context_print:        eval time =     795.53 ms /    63 runs   (   12.63 ms per token,    79.19 tokens per second)
0.01.686.967 I llama_perf_context_print:       total time =     851.86 ms /    70 tokens
0.01.687.188 I ggml_metal_free: deallocating

real	0m1.703s
user	0m0.108s
sys	0m0.208s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.627 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.397 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.402 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.403 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.404 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.404 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.405 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.405 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.408 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.408 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.408 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.409 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.409 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.409 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.410 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.411 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.412 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.412 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.293 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.261 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.010 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.011 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.012 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.012 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.012 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.013 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.013 I llama_model_loader: - type  f32:  194 tensors
0.00.025.013 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.014 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.014 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.014 I print_info: file format = GGUF V3 (latest)
0.00.025.015 I print_info: file type   = Q2_K - Medium
0.00.025.020 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.142 I load: special tokens cache size = 25
0.00.039.156 I load: token to piece cache size = 0.2984 MB
0.00.039.159 I print_info: arch             = gptneox
0.00.039.159 I print_info: vocab_only       = 0
0.00.039.160 I print_info: n_ctx_train      = 2048
0.00.039.160 I print_info: n_embd           = 2048
0.00.039.160 I print_info: n_layer          = 24
0.00.039.163 I print_info: n_head           = 16
0.00.039.164 I print_info: n_head_kv        = 16
0.00.039.164 I print_info: n_rot            = 32
0.00.039.165 I print_info: n_swa            = 0
0.00.039.165 I print_info: n_embd_head_k    = 128
0.00.039.165 I print_info: n_embd_head_v    = 128
0.00.039.166 I print_info: n_gqa            = 1
0.00.039.167 I print_info: n_embd_k_gqa     = 2048
0.00.039.168 I print_info: n_embd_v_gqa     = 2048
0.00.039.168 I print_info: f_norm_eps       = 1.0e-05
0.00.039.168 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.169 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.170 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.170 I print_info: f_logit_scale    = 0.0e+00
0.00.039.171 I print_info: n_ff             = 8192
0.00.039.171 I print_info: n_expert         = 0
0.00.039.171 I print_info: n_expert_used    = 0
0.00.039.171 I print_info: causal attn      = 1
0.00.039.172 I print_info: pooling type     = 0
0.00.039.173 I print_info: rope type        = 2
0.00.039.174 I print_info: rope scaling     = linear
0.00.039.174 I print_info: freq_base_train  = 10000.0
0.00.039.174 I print_info: freq_scale_train = 1
0.00.039.174 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.175 I print_info: rope_finetuned   = unknown
0.00.039.175 I print_info: ssm_d_conv       = 0
0.00.039.175 I print_info: ssm_d_inner      = 0
0.00.039.175 I print_info: ssm_d_state      = 0
0.00.039.175 I print_info: ssm_dt_rank      = 0
0.00.039.175 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.176 I print_info: model type       = 1.4B
0.00.039.176 I print_info: model params     = 1.41 B
0.00.039.176 I print_info: general.name     = 1.4B
0.00.039.177 I print_info: vocab type       = BPE
0.00.039.181 I print_info: n_vocab          = 50304
0.00.039.181 I print_info: n_merges         = 50009
0.00.039.181 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.182 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.182 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.182 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.183 I print_info: LF token         = 187 'Ċ'
0.00.039.183 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.183 I print_info: max token length = 1024
0.00.039.183 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.362.854 I load_tensors: offloading 24 repeating layers to GPU
0.00.362.869 I load_tensors: offloading output layer to GPU
0.00.362.870 I load_tensors: offloaded 25/25 layers to GPU
0.00.362.905 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.362.907 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.364.653 I llama_init_from_model: n_seq_max     = 1
0.00.364.656 I llama_init_from_model: n_ctx         = 2048
0.00.364.656 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.364.657 I llama_init_from_model: n_batch       = 2048
0.00.364.657 I llama_init_from_model: n_ubatch      = 512
0.00.364.657 I llama_init_from_model: flash_attn    = 0
0.00.364.660 I llama_init_from_model: freq_base     = 10000.0
0.00.364.660 I llama_init_from_model: freq_scale    = 1
0.00.364.662 I ggml_metal_init: allocating
0.00.364.750 I ggml_metal_init: found device: Apple M4
0.00.364.765 I ggml_metal_init: picking default device: Apple M4
0.00.366.657 I ggml_metal_init: using embedded metal library
0.00.372.848 I ggml_metal_init: GPU name:   Apple M4
0.00.372.860 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.372.860 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.372.861 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.372.862 I ggml_metal_init: simdgroup reduction   = true
0.00.372.862 I ggml_metal_init: simdgroup matrix mul. = true
0.00.372.862 I ggml_metal_init: has residency sets    = true
0.00.372.862 I ggml_metal_init: has bfloat            = true
0.00.372.863 I ggml_metal_init: use bfloat            = true
0.00.372.865 I ggml_metal_init: hasUnifiedMemory      = true
0.00.372.869 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.396.313 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.458.362 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.458.369 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.458.403 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.462.704 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.462.706 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.462.706 I llama_init_from_model: graph nodes  = 967
0.00.462.707 I llama_init_from_model: graph splits = 2
0.00.462.712 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.462.840 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.462.840 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.521.919 I main: llama threadpool init, n_threads = 4
0.00.521.965 I 
0.00.521.988 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.521.988 I 
0.00.522.134 I sampler seed: 1234
0.00.522.139 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.522.149 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.522.151 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.522.151 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.206.275 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53828.66 tokens per second)
0.01.206.277 I llama_perf_context_print:        load time =     511.57 ms
0.01.206.277 I llama_perf_context_print: prompt eval time =      44.46 ms /     7 tokens (    6.35 ms per token,   157.44 tokens per second)
0.01.206.278 I llama_perf_context_print:        eval time =     636.85 ms /    63 runs   (   10.11 ms per token,    98.92 tokens per second)
0.01.206.278 I llama_perf_context_print:       total time =     685.07 ms /    70 tokens
0.01.206.489 I ggml_metal_free: deallocating

real	0m1.226s
user	0m0.114s
sys	0m0.170s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.081 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.561 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.566 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.568 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.570 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.570 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.570 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.571 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.571 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.572 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.572 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.572 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.573 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.573 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.574 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.576 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.576 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.577 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.367 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.409 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.130 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.131 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.131 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.132 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.132 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.132 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.133 I llama_model_loader: - type  f32:  194 tensors
0.00.025.133 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.133 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.133 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.133 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.134 I print_info: file format = GGUF V3 (latest)
0.00.025.134 I print_info: file type   = Q3_K - Medium
0.00.025.135 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.941 I load: special tokens cache size = 25
0.00.038.975 I load: token to piece cache size = 0.2984 MB
0.00.038.978 I print_info: arch             = gptneox
0.00.038.978 I print_info: vocab_only       = 0
0.00.038.978 I print_info: n_ctx_train      = 2048
0.00.038.978 I print_info: n_embd           = 2048
0.00.038.978 I print_info: n_layer          = 24
0.00.038.981 I print_info: n_head           = 16
0.00.038.982 I print_info: n_head_kv        = 16
0.00.038.982 I print_info: n_rot            = 32
0.00.038.982 I print_info: n_swa            = 0
0.00.038.982 I print_info: n_embd_head_k    = 128
0.00.038.982 I print_info: n_embd_head_v    = 128
0.00.038.983 I print_info: n_gqa            = 1
0.00.038.984 I print_info: n_embd_k_gqa     = 2048
0.00.038.985 I print_info: n_embd_v_gqa     = 2048
0.00.038.985 I print_info: f_norm_eps       = 1.0e-05
0.00.038.986 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.986 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.986 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.986 I print_info: f_logit_scale    = 0.0e+00
0.00.038.987 I print_info: n_ff             = 8192
0.00.038.987 I print_info: n_expert         = 0
0.00.038.987 I print_info: n_expert_used    = 0
0.00.038.989 I print_info: causal attn      = 1
0.00.038.990 I print_info: pooling type     = 0
0.00.038.990 I print_info: rope type        = 2
0.00.038.990 I print_info: rope scaling     = linear
0.00.038.991 I print_info: freq_base_train  = 10000.0
0.00.038.991 I print_info: freq_scale_train = 1
0.00.038.991 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.992 I print_info: rope_finetuned   = unknown
0.00.038.992 I print_info: ssm_d_conv       = 0
0.00.038.992 I print_info: ssm_d_inner      = 0
0.00.038.992 I print_info: ssm_d_state      = 0
0.00.038.992 I print_info: ssm_dt_rank      = 0
0.00.038.992 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.992 I print_info: model type       = 1.4B
0.00.038.993 I print_info: model params     = 1.41 B
0.00.038.993 I print_info: general.name     = 1.4B
0.00.038.994 I print_info: vocab type       = BPE
0.00.038.994 I print_info: n_vocab          = 50304
0.00.038.994 I print_info: n_merges         = 50009
0.00.038.994 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.994 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.995 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.995 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.995 I print_info: LF token         = 187 'Ċ'
0.00.038.997 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.997 I print_info: max token length = 1024
0.00.038.997 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.441.251 I load_tensors: offloading 24 repeating layers to GPU
0.00.441.266 I load_tensors: offloading output layer to GPU
0.00.441.266 I load_tensors: offloaded 25/25 layers to GPU
0.00.441.299 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.441.301 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.442.826 I llama_init_from_model: n_seq_max     = 1
0.00.442.829 I llama_init_from_model: n_ctx         = 2048
0.00.442.830 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.442.830 I llama_init_from_model: n_batch       = 2048
0.00.442.831 I llama_init_from_model: n_ubatch      = 512
0.00.442.831 I llama_init_from_model: flash_attn    = 0
0.00.442.834 I llama_init_from_model: freq_base     = 10000.0
0.00.442.834 I llama_init_from_model: freq_scale    = 1
0.00.442.836 I ggml_metal_init: allocating
0.00.442.913 I ggml_metal_init: found device: Apple M4
0.00.442.927 I ggml_metal_init: picking default device: Apple M4
0.00.444.821 I ggml_metal_init: using embedded metal library
0.00.451.614 I ggml_metal_init: GPU name:   Apple M4
0.00.451.618 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.451.619 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.451.620 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.451.621 I ggml_metal_init: simdgroup reduction   = true
0.00.451.621 I ggml_metal_init: simdgroup matrix mul. = true
0.00.451.621 I ggml_metal_init: has residency sets    = true
0.00.451.621 I ggml_metal_init: has bfloat            = true
0.00.451.622 I ggml_metal_init: use bfloat            = true
0.00.451.623 I ggml_metal_init: hasUnifiedMemory      = true
0.00.451.624 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.470.466 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.524.628 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.524.634 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.524.668 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.529.277 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.529.279 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.529.279 I llama_init_from_model: graph nodes  = 967
0.00.529.279 I llama_init_from_model: graph splits = 2
0.00.529.285 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.529.414 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.529.415 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.583.950 I main: llama threadpool init, n_threads = 4
0.00.583.997 I 
0.00.584.019 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.584.020 I 
0.00.584.165 I sampler seed: 1234
0.00.584.169 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.584.180 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.584.181 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.584.181 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.317.974 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52205.88 tokens per second)
0.01.317.974 I llama_perf_context_print:        load time =     574.15 ms
0.01.317.975 I llama_perf_context_print: prompt eval time =      40.11 ms /     7 tokens (    5.73 ms per token,   174.51 tokens per second)
0.01.317.979 I llama_perf_context_print:        eval time =     690.89 ms /    63 runs   (   10.97 ms per token,    91.19 tokens per second)
0.01.317.979 I llama_perf_context_print:       total time =     734.74 ms /    70 tokens
0.01.318.205 I ggml_metal_free: deallocating

real	0m1.332s
user	0m0.110s
sys	0m0.184s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.006.729 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.014.429 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.434 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.435 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.436 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.436 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.437 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.437 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.438 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.438 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.438 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.439 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.439 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.439 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.440 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.441 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.442 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.442 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.227 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.287 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.061 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.062 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.063 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.063 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.063 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.064 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.064 I llama_model_loader: - type  f32:  194 tensors
0.00.023.065 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.065 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.065 I llama_model_loader: - type q6_K:   13 tensors
0.00.023.065 I print_info: file format = GGUF V3 (latest)
0.00.023.066 I print_info: file type   = Q4_K - Medium
0.00.023.067 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.030.758 I load: special tokens cache size = 25
0.00.036.822 I load: token to piece cache size = 0.2984 MB
0.00.036.825 I print_info: arch             = gptneox
0.00.036.825 I print_info: vocab_only       = 0
0.00.036.825 I print_info: n_ctx_train      = 2048
0.00.036.825 I print_info: n_embd           = 2048
0.00.036.826 I print_info: n_layer          = 24
0.00.036.828 I print_info: n_head           = 16
0.00.036.829 I print_info: n_head_kv        = 16
0.00.036.829 I print_info: n_rot            = 32
0.00.036.829 I print_info: n_swa            = 0
0.00.036.829 I print_info: n_embd_head_k    = 128
0.00.036.829 I print_info: n_embd_head_v    = 128
0.00.036.832 I print_info: n_gqa            = 1
0.00.036.833 I print_info: n_embd_k_gqa     = 2048
0.00.036.833 I print_info: n_embd_v_gqa     = 2048
0.00.036.834 I print_info: f_norm_eps       = 1.0e-05
0.00.036.834 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.036.834 I print_info: f_clamp_kqv      = 0.0e+00
0.00.036.835 I print_info: f_max_alibi_bias = 0.0e+00
0.00.036.835 I print_info: f_logit_scale    = 0.0e+00
0.00.036.836 I print_info: n_ff             = 8192
0.00.036.836 I print_info: n_expert         = 0
0.00.036.836 I print_info: n_expert_used    = 0
0.00.036.836 I print_info: causal attn      = 1
0.00.036.836 I print_info: pooling type     = 0
0.00.036.836 I print_info: rope type        = 2
0.00.036.837 I print_info: rope scaling     = linear
0.00.036.837 I print_info: freq_base_train  = 10000.0
0.00.036.837 I print_info: freq_scale_train = 1
0.00.036.841 I print_info: n_ctx_orig_yarn  = 2048
0.00.036.841 I print_info: rope_finetuned   = unknown
0.00.036.841 I print_info: ssm_d_conv       = 0
0.00.036.842 I print_info: ssm_d_inner      = 0
0.00.036.842 I print_info: ssm_d_state      = 0
0.00.036.842 I print_info: ssm_dt_rank      = 0
0.00.036.842 I print_info: ssm_dt_b_c_rms   = 0
0.00.036.843 I print_info: model type       = 1.4B
0.00.036.844 I print_info: model params     = 1.41 B
0.00.036.844 I print_info: general.name     = 1.4B
0.00.036.845 I print_info: vocab type       = BPE
0.00.036.845 I print_info: n_vocab          = 50304
0.00.036.845 I print_info: n_merges         = 50009
0.00.036.845 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.036.846 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.036.846 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.036.846 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.036.846 I print_info: LF token         = 187 'Ċ'
0.00.036.847 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.036.847 I print_info: max token length = 1024
0.00.036.847 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.540.441 I load_tensors: offloading 24 repeating layers to GPU
0.00.540.454 I load_tensors: offloading output layer to GPU
0.00.540.454 I load_tensors: offloaded 25/25 layers to GPU
0.00.540.493 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.540.494 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.541.947 I llama_init_from_model: n_seq_max     = 1
0.00.541.950 I llama_init_from_model: n_ctx         = 2048
0.00.541.950 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.541.951 I llama_init_from_model: n_batch       = 2048
0.00.541.951 I llama_init_from_model: n_ubatch      = 512
0.00.541.951 I llama_init_from_model: flash_attn    = 0
0.00.541.954 I llama_init_from_model: freq_base     = 10000.0
0.00.541.954 I llama_init_from_model: freq_scale    = 1
0.00.541.956 I ggml_metal_init: allocating
0.00.542.080 I ggml_metal_init: found device: Apple M4
0.00.542.098 I ggml_metal_init: picking default device: Apple M4
0.00.544.124 I ggml_metal_init: using embedded metal library
0.00.551.456 I ggml_metal_init: GPU name:   Apple M4
0.00.551.460 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.551.461 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.551.462 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.551.462 I ggml_metal_init: simdgroup reduction   = true
0.00.551.463 I ggml_metal_init: simdgroup matrix mul. = true
0.00.551.463 I ggml_metal_init: has residency sets    = true
0.00.551.463 I ggml_metal_init: has bfloat            = true
0.00.551.463 I ggml_metal_init: use bfloat            = true
0.00.551.465 I ggml_metal_init: hasUnifiedMemory      = true
0.00.551.467 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.570.069 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.626.546 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.626.553 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.626.589 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.631.930 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.631.933 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.631.933 I llama_init_from_model: graph nodes  = 967
0.00.631.933 I llama_init_from_model: graph splits = 2
0.00.631.944 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.632.068 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.632.069 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.487 I main: llama threadpool init, n_threads = 4
0.00.692.527 I 
0.00.692.547 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.692.550 I 
0.00.692.702 I sampler seed: 1234
0.00.692.707 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.692.717 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.692.718 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.692.718 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.452.441 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48431.11 tokens per second)
0.01.452.442 I llama_perf_context_print:        load time =     684.97 ms
0.01.452.442 I llama_perf_context_print: prompt eval time =      56.51 ms /     7 tokens (    8.07 ms per token,   123.87 tokens per second)
0.01.452.444 I llama_perf_context_print:        eval time =     700.23 ms /    63 runs   (   11.11 ms per token,    89.97 tokens per second)
0.01.452.444 I llama_perf_context_print:       total time =     760.74 ms /    70 tokens
0.01.452.714 I ggml_metal_free: deallocating

real	0m1.469s
user	0m0.110s
sys	0m0.212s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.994 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.361 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.365 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.367 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.367 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.368 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.368 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.368 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.369 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.369 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.370 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.370 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.370 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.371 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.371 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.374 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.374 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.374 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.179 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.242 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.990 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.991 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.991 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.992 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.992 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.992 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.993 I llama_model_loader: - type  f32:  194 tensors
0.00.026.993 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.994 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.994 I print_info: file format = GGUF V3 (latest)
0.00.026.995 I print_info: file type   = Q5_K - Medium
0.00.026.996 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.856 I load: special tokens cache size = 25
0.00.040.793 I load: token to piece cache size = 0.2984 MB
0.00.040.796 I print_info: arch             = gptneox
0.00.040.796 I print_info: vocab_only       = 0
0.00.040.797 I print_info: n_ctx_train      = 2048
0.00.040.797 I print_info: n_embd           = 2048
0.00.040.797 I print_info: n_layer          = 24
0.00.040.800 I print_info: n_head           = 16
0.00.040.801 I print_info: n_head_kv        = 16
0.00.040.801 I print_info: n_rot            = 32
0.00.040.801 I print_info: n_swa            = 0
0.00.040.801 I print_info: n_embd_head_k    = 128
0.00.040.801 I print_info: n_embd_head_v    = 128
0.00.040.802 I print_info: n_gqa            = 1
0.00.040.803 I print_info: n_embd_k_gqa     = 2048
0.00.040.803 I print_info: n_embd_v_gqa     = 2048
0.00.040.804 I print_info: f_norm_eps       = 1.0e-05
0.00.040.804 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.804 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.804 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.805 I print_info: f_logit_scale    = 0.0e+00
0.00.040.805 I print_info: n_ff             = 8192
0.00.040.805 I print_info: n_expert         = 0
0.00.040.807 I print_info: n_expert_used    = 0
0.00.040.807 I print_info: causal attn      = 1
0.00.040.807 I print_info: pooling type     = 0
0.00.040.810 I print_info: rope type        = 2
0.00.040.810 I print_info: rope scaling     = linear
0.00.040.811 I print_info: freq_base_train  = 10000.0
0.00.040.811 I print_info: freq_scale_train = 1
0.00.040.811 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.811 I print_info: rope_finetuned   = unknown
0.00.040.812 I print_info: ssm_d_conv       = 0
0.00.040.813 I print_info: ssm_d_inner      = 0
0.00.040.813 I print_info: ssm_d_state      = 0
0.00.040.814 I print_info: ssm_dt_rank      = 0
0.00.040.814 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.814 I print_info: model type       = 1.4B
0.00.040.814 I print_info: model params     = 1.41 B
0.00.040.814 I print_info: general.name     = 1.4B
0.00.040.815 I print_info: vocab type       = BPE
0.00.040.815 I print_info: n_vocab          = 50304
0.00.040.815 I print_info: n_merges         = 50009
0.00.040.815 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.816 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.816 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.816 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.816 I print_info: LF token         = 187 'Ċ'
0.00.040.817 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.817 I print_info: max token length = 1024
0.00.040.817 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.591.951 I load_tensors: offloading 24 repeating layers to GPU
0.00.591.966 I load_tensors: offloading output layer to GPU
0.00.591.967 I load_tensors: offloaded 25/25 layers to GPU
0.00.592.000 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.592.001 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.593.550 I llama_init_from_model: n_seq_max     = 1
0.00.593.552 I llama_init_from_model: n_ctx         = 2048
0.00.593.552 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.593.553 I llama_init_from_model: n_batch       = 2048
0.00.593.554 I llama_init_from_model: n_ubatch      = 512
0.00.593.554 I llama_init_from_model: flash_attn    = 0
0.00.593.555 I llama_init_from_model: freq_base     = 10000.0
0.00.593.556 I llama_init_from_model: freq_scale    = 1
0.00.593.557 I ggml_metal_init: allocating
0.00.593.573 I ggml_metal_init: found device: Apple M4
0.00.593.583 I ggml_metal_init: picking default device: Apple M4
0.00.595.096 I ggml_metal_init: using embedded metal library
0.00.601.408 I ggml_metal_init: GPU name:   Apple M4
0.00.601.413 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.601.414 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.601.414 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.601.415 I ggml_metal_init: simdgroup reduction   = true
0.00.601.415 I ggml_metal_init: simdgroup matrix mul. = true
0.00.601.415 I ggml_metal_init: has residency sets    = true
0.00.601.415 I ggml_metal_init: has bfloat            = true
0.00.601.416 I ggml_metal_init: use bfloat            = true
0.00.601.417 I ggml_metal_init: hasUnifiedMemory      = true
0.00.601.418 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.618.646 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.673.123 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.673.129 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.673.166 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.677.023 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.677.025 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.677.025 I llama_init_from_model: graph nodes  = 967
0.00.677.025 I llama_init_from_model: graph splits = 2
0.00.677.030 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.677.159 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.677.159 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.744.057 I main: llama threadpool init, n_threads = 4
0.00.744.097 I 
0.00.744.118 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.744.119 I 
0.00.744.262 I sampler seed: 1234
0.00.744.266 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.744.288 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.744.289 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.744.290 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.604.729 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49408.49 tokens per second)
0.01.604.730 I llama_perf_context_print:        load time =     732.22 ms
0.01.604.731 I llama_perf_context_print: prompt eval time =      58.97 ms /     7 tokens (    8.42 ms per token,   118.70 tokens per second)
0.01.604.733 I llama_perf_context_print:        eval time =     798.92 ms /    63 runs   (   12.68 ms per token,    78.86 tokens per second)
0.01.604.733 I llama_perf_context_print:       total time =     861.51 ms /    70 tokens
0.01.605.063 I ggml_metal_free: deallocating

real	0m1.625s
user	0m0.109s
sys	0m0.211s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.158 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.652 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.022.657 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.659 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.659 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.660 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.660 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.660 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.661 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.661 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.662 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.662 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.662 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.663 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.663 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.667 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.668 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.668 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.612 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.754 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.712 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.714 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.714 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.715 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.715 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.715 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.031.716 I llama_model_loader: - type  f32:  194 tensors
0.00.031.716 I llama_model_loader: - type q6_K:   98 tensors
0.00.031.717 I print_info: file format = GGUF V3 (latest)
0.00.031.718 I print_info: file type   = Q6_K
0.00.031.719 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.039.689 I load: special tokens cache size = 25
0.00.045.804 I load: token to piece cache size = 0.2984 MB
0.00.045.807 I print_info: arch             = gptneox
0.00.045.807 I print_info: vocab_only       = 0
0.00.045.808 I print_info: n_ctx_train      = 2048
0.00.045.808 I print_info: n_embd           = 2048
0.00.045.808 I print_info: n_layer          = 24
0.00.045.812 I print_info: n_head           = 16
0.00.045.812 I print_info: n_head_kv        = 16
0.00.045.813 I print_info: n_rot            = 32
0.00.045.813 I print_info: n_swa            = 0
0.00.045.813 I print_info: n_embd_head_k    = 128
0.00.045.813 I print_info: n_embd_head_v    = 128
0.00.045.814 I print_info: n_gqa            = 1
0.00.045.815 I print_info: n_embd_k_gqa     = 2048
0.00.045.815 I print_info: n_embd_v_gqa     = 2048
0.00.045.816 I print_info: f_norm_eps       = 1.0e-05
0.00.045.816 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.816 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.817 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.817 I print_info: f_logit_scale    = 0.0e+00
0.00.045.817 I print_info: n_ff             = 8192
0.00.045.818 I print_info: n_expert         = 0
0.00.045.818 I print_info: n_expert_used    = 0
0.00.045.818 I print_info: causal attn      = 1
0.00.045.818 I print_info: pooling type     = 0
0.00.045.820 I print_info: rope type        = 2
0.00.045.822 I print_info: rope scaling     = linear
0.00.045.824 I print_info: freq_base_train  = 10000.0
0.00.045.824 I print_info: freq_scale_train = 1
0.00.045.824 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.824 I print_info: rope_finetuned   = unknown
0.00.045.824 I print_info: ssm_d_conv       = 0
0.00.045.825 I print_info: ssm_d_inner      = 0
0.00.045.825 I print_info: ssm_d_state      = 0
0.00.045.825 I print_info: ssm_dt_rank      = 0
0.00.045.825 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.825 I print_info: model type       = 1.4B
0.00.045.825 I print_info: model params     = 1.41 B
0.00.045.825 I print_info: general.name     = 1.4B
0.00.045.826 I print_info: vocab type       = BPE
0.00.045.826 I print_info: n_vocab          = 50304
0.00.045.826 I print_info: n_merges         = 50009
0.00.045.827 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.827 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.827 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.827 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.827 I print_info: LF token         = 187 'Ċ'
0.00.045.827 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.831 I print_info: max token length = 1024
0.00.045.833 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.654.773 I load_tensors: offloading 24 repeating layers to GPU
0.00.654.788 I load_tensors: offloading output layer to GPU
0.00.654.788 I load_tensors: offloaded 25/25 layers to GPU
0.00.654.823 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.654.825 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.655.960 I llama_init_from_model: n_seq_max     = 1
0.00.655.962 I llama_init_from_model: n_ctx         = 2048
0.00.655.963 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.655.963 I llama_init_from_model: n_batch       = 2048
0.00.655.964 I llama_init_from_model: n_ubatch      = 512
0.00.655.964 I llama_init_from_model: flash_attn    = 0
0.00.655.967 I llama_init_from_model: freq_base     = 10000.0
0.00.655.967 I llama_init_from_model: freq_scale    = 1
0.00.655.969 I ggml_metal_init: allocating
0.00.656.085 I ggml_metal_init: found device: Apple M4
0.00.656.100 I ggml_metal_init: picking default device: Apple M4
0.00.657.932 I ggml_metal_init: using embedded metal library
0.00.664.483 I ggml_metal_init: GPU name:   Apple M4
0.00.664.488 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.664.489 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.664.489 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.664.490 I ggml_metal_init: simdgroup reduction   = true
0.00.664.490 I ggml_metal_init: simdgroup matrix mul. = true
0.00.664.490 I ggml_metal_init: has residency sets    = true
0.00.664.491 I ggml_metal_init: has bfloat            = true
0.00.664.491 I ggml_metal_init: use bfloat            = true
0.00.664.492 I ggml_metal_init: hasUnifiedMemory      = true
0.00.664.496 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.682.246 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.736.008 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.736.016 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.736.051 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.740.896 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.740.898 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.740.898 I llama_init_from_model: graph nodes  = 967
0.00.740.898 I llama_init_from_model: graph splits = 2
0.00.740.905 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.741.032 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.741.032 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.794.135 I main: llama threadpool init, n_threads = 4
0.00.794.176 I 
0.00.794.197 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.794.198 I 
0.00.794.331 I sampler seed: 1234
0.00.794.335 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.794.345 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.794.345 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.794.345 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.685.672 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52514.79 tokens per second)
0.01.685.672 I llama_perf_context_print:        load time =     784.28 ms
0.01.685.673 I llama_perf_context_print: prompt eval time =      57.93 ms /     7 tokens (    8.28 ms per token,   120.84 tokens per second)
0.01.685.674 I llama_perf_context_print:        eval time =     830.46 ms /    63 runs   (   13.18 ms per token,    75.86 tokens per second)
0.01.685.674 I llama_perf_context_print:       total time =     892.24 ms /    70 tokens
0.01.685.956 I ggml_metal_free: deallocating

real	0m1.716s
user	0m0.111s
sys	0m0.193s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.568 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.030.832 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.044.109 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.044.123 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.127 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.128 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.128 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.129 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.130 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.134 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.134 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.135 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.135 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.136 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.137 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.138 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.145 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.145 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.146 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.051.030 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.053.167 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.062.120 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.062.126 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.062.127 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.062.127 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.062.128 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.062.129 I llama_model_loader: - type  f32:  194 tensors
0.00.062.129 I llama_model_loader: - type  f16:   98 tensors
0.00.062.130 I print_info: file format = GGUF V3 (latest)
0.00.062.134 I print_info: file type   = all F32 (guessed)
0.00.062.136 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.077.269 I load: special tokens cache size = 25
0.00.086.686 I load: token to piece cache size = 0.2984 MB
0.00.086.691 I print_info: arch             = gptneox
0.00.086.691 I print_info: vocab_only       = 0
0.00.086.691 I print_info: n_ctx_train      = 2048
0.00.086.691 I print_info: n_embd           = 2048
0.00.086.692 I print_info: n_layer          = 24
0.00.086.696 I print_info: n_head           = 16
0.00.086.697 I print_info: n_head_kv        = 16
0.00.086.697 I print_info: n_rot            = 32
0.00.086.697 I print_info: n_swa            = 0
0.00.086.698 I print_info: n_embd_head_k    = 128
0.00.086.698 I print_info: n_embd_head_v    = 128
0.00.086.699 I print_info: n_gqa            = 1
0.00.086.700 I print_info: n_embd_k_gqa     = 2048
0.00.086.701 I print_info: n_embd_v_gqa     = 2048
0.00.086.701 I print_info: f_norm_eps       = 1.0e-05
0.00.086.702 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.086.702 I print_info: f_clamp_kqv      = 0.0e+00
0.00.086.702 I print_info: f_max_alibi_bias = 0.0e+00
0.00.086.703 I print_info: f_logit_scale    = 0.0e+00
0.00.086.704 I print_info: n_ff             = 8192
0.00.086.704 I print_info: n_expert         = 0
0.00.086.704 I print_info: n_expert_used    = 0
0.00.086.704 I print_info: causal attn      = 1
0.00.086.704 I print_info: pooling type     = 0
0.00.086.704 I print_info: rope type        = 2
0.00.086.705 I print_info: rope scaling     = linear
0.00.086.705 I print_info: freq_base_train  = 10000.0
0.00.086.706 I print_info: freq_scale_train = 1
0.00.086.706 I print_info: n_ctx_orig_yarn  = 2048
0.00.086.706 I print_info: rope_finetuned   = unknown
0.00.086.706 I print_info: ssm_d_conv       = 0
0.00.086.707 I print_info: ssm_d_inner      = 0
0.00.086.707 I print_info: ssm_d_state      = 0
0.00.086.707 I print_info: ssm_dt_rank      = 0
0.00.086.707 I print_info: ssm_dt_b_c_rms   = 0
0.00.086.707 I print_info: model type       = 1.4B
0.00.086.708 I print_info: model params     = 1.41 B
0.00.086.708 I print_info: general.name     = 1.4B
0.00.086.709 I print_info: vocab type       = BPE
0.00.086.709 I print_info: n_vocab          = 50304
0.00.086.709 I print_info: n_merges         = 50009
0.00.086.710 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.086.710 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.086.710 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.086.710 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.086.711 I print_info: LF token         = 187 'Ċ'
0.00.086.711 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.086.711 I print_info: max token length = 1024
0.00.086.715 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.586.806 I load_tensors: offloading 24 repeating layers to GPU
0.01.586.810 I load_tensors: offloading output layer to GPU
0.01.586.811 I load_tensors: offloaded 25/25 layers to GPU
0.01.586.835 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.586.836 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.588.097 I llama_init_from_model: n_seq_max     = 1
0.01.588.099 I llama_init_from_model: n_ctx         = 128
0.01.588.099 I llama_init_from_model: n_ctx_per_seq = 128
0.01.588.099 I llama_init_from_model: n_batch       = 128
0.01.588.100 I llama_init_from_model: n_ubatch      = 128
0.01.588.100 I llama_init_from_model: flash_attn    = 0
0.01.588.100 I llama_init_from_model: freq_base     = 10000.0
0.01.588.101 I llama_init_from_model: freq_scale    = 1
0.01.588.101 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.588.102 I ggml_metal_init: allocating
0.01.588.190 I ggml_metal_init: found device: Apple M4
0.01.588.196 I ggml_metal_init: picking default device: Apple M4
0.01.589.419 I ggml_metal_init: using embedded metal library
0.01.593.499 I ggml_metal_init: GPU name:   Apple M4
0.01.593.502 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.593.502 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.593.503 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.593.503 I ggml_metal_init: simdgroup reduction   = true
0.01.593.503 I ggml_metal_init: simdgroup matrix mul. = true
0.01.593.503 I ggml_metal_init: has residency sets    = true
0.01.593.503 I ggml_metal_init: has bfloat            = true
0.01.593.504 I ggml_metal_init: use bfloat            = true
0.01.593.504 I ggml_metal_init: hasUnifiedMemory      = true
0.01.593.505 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.605.275 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.607.055 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.607.058 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.607.083 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.608.814 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.608.816 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.608.816 I llama_init_from_model: graph nodes  = 967
0.01.608.816 I llama_init_from_model: graph splits = 2
0.01.608.818 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.608.818 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.643.847 I 
0.01.643.882 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.643.885 I perplexity: tokenizing the input ..
0.01.649.053 I perplexity: tokenization took 5.166 ms
0.01.649.078 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.768.251 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.771.128 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.771.155 I llama_perf_context_print:        load time =    1613.00 ms
0.01.771.156 I llama_perf_context_print: prompt eval time =     118.91 ms /   128 tokens (    0.93 ms per token,  1076.47 tokens per second)
0.01.771.158 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.771.159 I llama_perf_context_print:       total time =     127.31 ms /   129 tokens
0.01.771.846 I ggml_metal_free: deallocating

real	0m1.960s
user	0m0.109s
sys	0m0.274s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.288 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.960 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.025 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.030 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.035 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.035 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.036 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.036 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.036 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.037 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.038 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.038 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.038 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.039 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.039 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.040 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.042 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.042 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.042 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.869 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.888 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.774 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.776 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.776 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.776 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.777 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.777 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.778 I llama_model_loader: - type  f32:  194 tensors
0.00.025.778 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.779 I print_info: file format = GGUF V3 (latest)
0.00.025.780 I print_info: file type   = Q8_0
0.00.025.781 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.255 I load: special tokens cache size = 25
0.00.040.175 I load: token to piece cache size = 0.2984 MB
0.00.040.179 I print_info: arch             = gptneox
0.00.040.179 I print_info: vocab_only       = 0
0.00.040.180 I print_info: n_ctx_train      = 2048
0.00.040.180 I print_info: n_embd           = 2048
0.00.040.180 I print_info: n_layer          = 24
0.00.040.184 I print_info: n_head           = 16
0.00.040.185 I print_info: n_head_kv        = 16
0.00.040.186 I print_info: n_rot            = 32
0.00.040.186 I print_info: n_swa            = 0
0.00.040.186 I print_info: n_embd_head_k    = 128
0.00.040.187 I print_info: n_embd_head_v    = 128
0.00.040.188 I print_info: n_gqa            = 1
0.00.040.189 I print_info: n_embd_k_gqa     = 2048
0.00.040.189 I print_info: n_embd_v_gqa     = 2048
0.00.040.190 I print_info: f_norm_eps       = 1.0e-05
0.00.040.190 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.191 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.191 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.191 I print_info: f_logit_scale    = 0.0e+00
0.00.040.191 I print_info: n_ff             = 8192
0.00.040.193 I print_info: n_expert         = 0
0.00.040.193 I print_info: n_expert_used    = 0
0.00.040.194 I print_info: causal attn      = 1
0.00.040.194 I print_info: pooling type     = 0
0.00.040.194 I print_info: rope type        = 2
0.00.040.194 I print_info: rope scaling     = linear
0.00.040.194 I print_info: freq_base_train  = 10000.0
0.00.040.195 I print_info: freq_scale_train = 1
0.00.040.195 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.195 I print_info: rope_finetuned   = unknown
0.00.040.195 I print_info: ssm_d_conv       = 0
0.00.040.195 I print_info: ssm_d_inner      = 0
0.00.040.195 I print_info: ssm_d_state      = 0
0.00.040.195 I print_info: ssm_dt_rank      = 0
0.00.040.197 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.197 I print_info: model type       = 1.4B
0.00.040.197 I print_info: model params     = 1.41 B
0.00.040.198 I print_info: general.name     = 1.4B
0.00.040.198 I print_info: vocab type       = BPE
0.00.040.198 I print_info: n_vocab          = 50304
0.00.040.198 I print_info: n_merges         = 50009
0.00.040.199 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.199 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.199 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.199 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.200 I print_info: LF token         = 187 'Ċ'
0.00.040.200 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.201 I print_info: max token length = 1024
0.00.040.202 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.847.356 I load_tensors: offloading 24 repeating layers to GPU
0.00.847.362 I load_tensors: offloading output layer to GPU
0.00.847.363 I load_tensors: offloaded 25/25 layers to GPU
0.00.847.390 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.847.393 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.848.723 I llama_init_from_model: n_seq_max     = 1
0.00.848.725 I llama_init_from_model: n_ctx         = 128
0.00.848.726 I llama_init_from_model: n_ctx_per_seq = 128
0.00.848.726 I llama_init_from_model: n_batch       = 128
0.00.848.726 I llama_init_from_model: n_ubatch      = 128
0.00.848.727 I llama_init_from_model: flash_attn    = 0
0.00.848.728 I llama_init_from_model: freq_base     = 10000.0
0.00.848.729 I llama_init_from_model: freq_scale    = 1
0.00.848.729 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.848.731 I ggml_metal_init: allocating
0.00.848.772 I ggml_metal_init: found device: Apple M4
0.00.848.780 I ggml_metal_init: picking default device: Apple M4
0.00.850.056 I ggml_metal_init: using embedded metal library
0.00.855.522 I ggml_metal_init: GPU name:   Apple M4
0.00.855.525 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.855.526 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.855.526 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.855.527 I ggml_metal_init: simdgroup reduction   = true
0.00.855.527 I ggml_metal_init: simdgroup matrix mul. = true
0.00.855.527 I ggml_metal_init: has residency sets    = true
0.00.855.528 I ggml_metal_init: has bfloat            = true
0.00.855.528 I ggml_metal_init: use bfloat            = true
0.00.855.529 I ggml_metal_init: hasUnifiedMemory      = true
0.00.855.530 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.873.934 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.877.282 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.877.288 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.877.330 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.880.443 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.880.445 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.880.445 I llama_init_from_model: graph nodes  = 967
0.00.880.445 I llama_init_from_model: graph splits = 2
0.00.880.448 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.880.448 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.906.847 I 
0.00.906.928 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.906.936 I perplexity: tokenizing the input ..
0.00.914.200 I perplexity: tokenization took 7.262 ms
0.00.914.221 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.052.742 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.054.290 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.054.307 I llama_perf_context_print:        load time =     896.88 ms
0.01.054.308 I llama_perf_context_print: prompt eval time =     137.56 ms /   128 tokens (    1.07 ms per token,   930.50 tokens per second)
0.01.054.309 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.054.310 I llama_perf_context_print:       total time =     147.46 ms /   129 tokens
0.01.054.693 I ggml_metal_free: deallocating

real	0m1.071s
user	0m0.079s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.284 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.691 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.778 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.784 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.786 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.786 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.787 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.787 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.787 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.789 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.789 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.790 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.790 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.790 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.791 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.791 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.793 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.793 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.793 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.604 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.636 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.453 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.455 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.455 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.455 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.456 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.456 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.457 I llama_model_loader: - type  f32:  194 tensors
0.00.025.457 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.457 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.458 I print_info: file format = GGUF V3 (latest)
0.00.025.459 I print_info: file type   = Q4_0
0.00.025.460 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.473 I load: special tokens cache size = 25
0.00.039.490 I load: token to piece cache size = 0.2984 MB
0.00.039.494 I print_info: arch             = gptneox
0.00.039.495 I print_info: vocab_only       = 0
0.00.039.495 I print_info: n_ctx_train      = 2048
0.00.039.495 I print_info: n_embd           = 2048
0.00.039.495 I print_info: n_layer          = 24
0.00.039.500 I print_info: n_head           = 16
0.00.039.500 I print_info: n_head_kv        = 16
0.00.039.501 I print_info: n_rot            = 32
0.00.039.501 I print_info: n_swa            = 0
0.00.039.501 I print_info: n_embd_head_k    = 128
0.00.039.501 I print_info: n_embd_head_v    = 128
0.00.039.502 I print_info: n_gqa            = 1
0.00.039.503 I print_info: n_embd_k_gqa     = 2048
0.00.039.503 I print_info: n_embd_v_gqa     = 2048
0.00.039.504 I print_info: f_norm_eps       = 1.0e-05
0.00.039.504 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.504 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.505 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.505 I print_info: f_logit_scale    = 0.0e+00
0.00.039.505 I print_info: n_ff             = 8192
0.00.039.506 I print_info: n_expert         = 0
0.00.039.506 I print_info: n_expert_used    = 0
0.00.039.506 I print_info: causal attn      = 1
0.00.039.506 I print_info: pooling type     = 0
0.00.039.506 I print_info: rope type        = 2
0.00.039.506 I print_info: rope scaling     = linear
0.00.039.507 I print_info: freq_base_train  = 10000.0
0.00.039.507 I print_info: freq_scale_train = 1
0.00.039.507 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.507 I print_info: rope_finetuned   = unknown
0.00.039.507 I print_info: ssm_d_conv       = 0
0.00.039.508 I print_info: ssm_d_inner      = 0
0.00.039.508 I print_info: ssm_d_state      = 0
0.00.039.508 I print_info: ssm_dt_rank      = 0
0.00.039.508 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.508 I print_info: model type       = 1.4B
0.00.039.509 I print_info: model params     = 1.41 B
0.00.039.509 I print_info: general.name     = 1.4B
0.00.039.509 I print_info: vocab type       = BPE
0.00.039.509 I print_info: n_vocab          = 50304
0.00.039.510 I print_info: n_merges         = 50009
0.00.039.510 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.510 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.510 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.513 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.514 I print_info: LF token         = 187 'Ċ'
0.00.039.514 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.514 I print_info: max token length = 1024
0.00.039.517 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.534.147 I load_tensors: offloading 24 repeating layers to GPU
0.00.534.162 I load_tensors: offloading output layer to GPU
0.00.534.162 I load_tensors: offloaded 25/25 layers to GPU
0.00.534.198 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.534.200 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.535.738 I llama_init_from_model: n_seq_max     = 1
0.00.535.741 I llama_init_from_model: n_ctx         = 128
0.00.535.742 I llama_init_from_model: n_ctx_per_seq = 128
0.00.535.742 I llama_init_from_model: n_batch       = 128
0.00.535.742 I llama_init_from_model: n_ubatch      = 128
0.00.535.743 I llama_init_from_model: flash_attn    = 0
0.00.535.745 I llama_init_from_model: freq_base     = 10000.0
0.00.535.745 I llama_init_from_model: freq_scale    = 1
0.00.535.746 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.535.749 I ggml_metal_init: allocating
0.00.535.846 I ggml_metal_init: found device: Apple M4
0.00.535.860 I ggml_metal_init: picking default device: Apple M4
0.00.537.726 I ggml_metal_init: using embedded metal library
0.00.543.130 I ggml_metal_init: GPU name:   Apple M4
0.00.543.138 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.543.139 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.543.140 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.543.140 I ggml_metal_init: simdgroup reduction   = true
0.00.543.141 I ggml_metal_init: simdgroup matrix mul. = true
0.00.543.141 I ggml_metal_init: has residency sets    = true
0.00.543.141 I ggml_metal_init: has bfloat            = true
0.00.543.141 I ggml_metal_init: use bfloat            = true
0.00.543.143 I ggml_metal_init: hasUnifiedMemory      = true
0.00.543.148 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.562.750 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.566.344 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.566.354 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.566.405 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.569.636 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.569.638 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.569.639 I llama_init_from_model: graph nodes  = 967
0.00.569.639 I llama_init_from_model: graph splits = 2
0.00.569.642 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.569.642 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.597.424 I 
0.00.597.509 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.597.517 I perplexity: tokenizing the input ..
0.00.604.468 I perplexity: tokenization took 6.947 ms
0.00.604.496 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.737.846 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.739.370 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.739.382 I llama_perf_context_print:        load time =     587.73 ms
0.00.739.383 I llama_perf_context_print: prompt eval time =     132.42 ms /   128 tokens (    1.03 ms per token,   966.64 tokens per second)
0.00.739.384 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.739.384 I llama_perf_context_print:       total time =     141.96 ms /   129 tokens
0.00.739.743 I ggml_metal_free: deallocating

real	0m0.755s
user	0m0.081s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.687 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.910 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.917 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.919 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.919 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.920 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.920 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.920 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.921 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.922 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.922 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.922 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.923 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.924 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.925 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.926 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.927 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.927 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.736 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.771 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.636 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.638 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.638 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.638 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.639 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.639 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.640 I llama_model_loader: - type  f32:  194 tensors
0.00.026.640 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.640 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.641 I print_info: file format = GGUF V3 (latest)
0.00.026.642 I print_info: file type   = Q4_1
0.00.026.643 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.549 I load: special tokens cache size = 25
0.00.040.552 I load: token to piece cache size = 0.2984 MB
0.00.040.556 I print_info: arch             = gptneox
0.00.040.556 I print_info: vocab_only       = 0
0.00.040.557 I print_info: n_ctx_train      = 2048
0.00.040.557 I print_info: n_embd           = 2048
0.00.040.557 I print_info: n_layer          = 24
0.00.040.561 I print_info: n_head           = 16
0.00.040.562 I print_info: n_head_kv        = 16
0.00.040.562 I print_info: n_rot            = 32
0.00.040.562 I print_info: n_swa            = 0
0.00.040.562 I print_info: n_embd_head_k    = 128
0.00.040.562 I print_info: n_embd_head_v    = 128
0.00.040.563 I print_info: n_gqa            = 1
0.00.040.564 I print_info: n_embd_k_gqa     = 2048
0.00.040.564 I print_info: n_embd_v_gqa     = 2048
0.00.040.565 I print_info: f_norm_eps       = 1.0e-05
0.00.040.565 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.565 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.566 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.566 I print_info: f_logit_scale    = 0.0e+00
0.00.040.567 I print_info: n_ff             = 8192
0.00.040.567 I print_info: n_expert         = 0
0.00.040.567 I print_info: n_expert_used    = 0
0.00.040.567 I print_info: causal attn      = 1
0.00.040.567 I print_info: pooling type     = 0
0.00.040.567 I print_info: rope type        = 2
0.00.040.567 I print_info: rope scaling     = linear
0.00.040.568 I print_info: freq_base_train  = 10000.0
0.00.040.568 I print_info: freq_scale_train = 1
0.00.040.568 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.568 I print_info: rope_finetuned   = unknown
0.00.040.568 I print_info: ssm_d_conv       = 0
0.00.040.568 I print_info: ssm_d_inner      = 0
0.00.040.569 I print_info: ssm_d_state      = 0
0.00.040.569 I print_info: ssm_dt_rank      = 0
0.00.040.569 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.569 I print_info: model type       = 1.4B
0.00.040.569 I print_info: model params     = 1.41 B
0.00.040.570 I print_info: general.name     = 1.4B
0.00.040.570 I print_info: vocab type       = BPE
0.00.040.572 I print_info: n_vocab          = 50304
0.00.040.572 I print_info: n_merges         = 50009
0.00.040.573 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.573 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.573 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.573 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.574 I print_info: LF token         = 187 'Ċ'
0.00.040.574 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.574 I print_info: max token length = 1024
0.00.040.574 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.630.282 I load_tensors: offloading 24 repeating layers to GPU
0.00.630.294 I load_tensors: offloading output layer to GPU
0.00.630.295 I load_tensors: offloaded 25/25 layers to GPU
0.00.630.332 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.630.334 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.631.649 I llama_init_from_model: n_seq_max     = 1
0.00.631.651 I llama_init_from_model: n_ctx         = 128
0.00.631.652 I llama_init_from_model: n_ctx_per_seq = 128
0.00.631.653 I llama_init_from_model: n_batch       = 128
0.00.631.653 I llama_init_from_model: n_ubatch      = 128
0.00.631.653 I llama_init_from_model: flash_attn    = 0
0.00.631.655 I llama_init_from_model: freq_base     = 10000.0
0.00.631.656 I llama_init_from_model: freq_scale    = 1
0.00.631.656 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.631.659 I ggml_metal_init: allocating
0.00.631.745 I ggml_metal_init: found device: Apple M4
0.00.631.768 I ggml_metal_init: picking default device: Apple M4
0.00.633.542 I ggml_metal_init: using embedded metal library
0.00.640.513 I ggml_metal_init: GPU name:   Apple M4
0.00.640.520 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.640.521 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.640.522 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.640.525 I ggml_metal_init: simdgroup reduction   = true
0.00.640.526 I ggml_metal_init: simdgroup matrix mul. = true
0.00.640.526 I ggml_metal_init: has residency sets    = true
0.00.640.526 I ggml_metal_init: has bfloat            = true
0.00.640.526 I ggml_metal_init: use bfloat            = true
0.00.640.528 I ggml_metal_init: hasUnifiedMemory      = true
0.00.640.532 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.658.612 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.662.074 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.662.078 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.662.123 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.665.261 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.665.263 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.665.264 I llama_init_from_model: graph nodes  = 967
0.00.665.264 I llama_init_from_model: graph splits = 2
0.00.665.267 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.665.267 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.693.701 I 
0.00.693.795 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.804 I perplexity: tokenizing the input ..
0.00.701.082 I perplexity: tokenization took 7.274 ms
0.00.701.108 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.835.693 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.837.243 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.837.259 I llama_perf_context_print:        load time =     685.00 ms
0.00.837.260 I llama_perf_context_print: prompt eval time =     133.56 ms /   128 tokens (    1.04 ms per token,   958.41 tokens per second)
0.00.837.261 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.837.261 I llama_perf_context_print:       total time =     143.56 ms /   129 tokens
0.00.837.694 I ggml_metal_free: deallocating

real	0m0.852s
user	0m0.082s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.115 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.015 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.362 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.367 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.370 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.370 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.371 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.371 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.371 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.372 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.373 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.373 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.373 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.374 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.375 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.376 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.378 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.378 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.378 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.156 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.166 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.983 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.985 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.985 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.985 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.986 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.987 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.989 I llama_model_loader: - type  f32:  194 tensors
0.00.025.990 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.990 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.991 I print_info: file format = GGUF V3 (latest)
0.00.025.991 I print_info: file type   = Q5_0
0.00.025.992 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.433 I load: special tokens cache size = 25
0.00.040.620 I load: token to piece cache size = 0.2984 MB
0.00.040.624 I print_info: arch             = gptneox
0.00.040.624 I print_info: vocab_only       = 0
0.00.040.624 I print_info: n_ctx_train      = 2048
0.00.040.625 I print_info: n_embd           = 2048
0.00.040.625 I print_info: n_layer          = 24
0.00.040.629 I print_info: n_head           = 16
0.00.040.630 I print_info: n_head_kv        = 16
0.00.040.632 I print_info: n_rot            = 32
0.00.040.632 I print_info: n_swa            = 0
0.00.040.635 I print_info: n_embd_head_k    = 128
0.00.040.635 I print_info: n_embd_head_v    = 128
0.00.040.635 I print_info: n_gqa            = 1
0.00.040.636 I print_info: n_embd_k_gqa     = 2048
0.00.040.637 I print_info: n_embd_v_gqa     = 2048
0.00.040.637 I print_info: f_norm_eps       = 1.0e-05
0.00.040.637 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.638 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.638 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.639 I print_info: f_logit_scale    = 0.0e+00
0.00.040.640 I print_info: n_ff             = 8192
0.00.040.641 I print_info: n_expert         = 0
0.00.040.641 I print_info: n_expert_used    = 0
0.00.040.641 I print_info: causal attn      = 1
0.00.040.641 I print_info: pooling type     = 0
0.00.040.641 I print_info: rope type        = 2
0.00.040.641 I print_info: rope scaling     = linear
0.00.040.642 I print_info: freq_base_train  = 10000.0
0.00.040.642 I print_info: freq_scale_train = 1
0.00.040.642 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.642 I print_info: rope_finetuned   = unknown
0.00.040.642 I print_info: ssm_d_conv       = 0
0.00.040.643 I print_info: ssm_d_inner      = 0
0.00.040.643 I print_info: ssm_d_state      = 0
0.00.040.643 I print_info: ssm_dt_rank      = 0
0.00.040.643 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.643 I print_info: model type       = 1.4B
0.00.040.644 I print_info: model params     = 1.41 B
0.00.040.644 I print_info: general.name     = 1.4B
0.00.040.644 I print_info: vocab type       = BPE
0.00.040.644 I print_info: n_vocab          = 50304
0.00.040.645 I print_info: n_merges         = 50009
0.00.040.646 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.646 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.646 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.647 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.647 I print_info: LF token         = 187 'Ċ'
0.00.040.647 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.647 I print_info: max token length = 1024
0.00.040.648 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.671.460 I load_tensors: offloading 24 repeating layers to GPU
0.00.671.477 I load_tensors: offloading output layer to GPU
0.00.671.477 I load_tensors: offloaded 25/25 layers to GPU
0.00.671.512 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.671.514 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.673.162 I llama_init_from_model: n_seq_max     = 1
0.00.673.166 I llama_init_from_model: n_ctx         = 128
0.00.673.167 I llama_init_from_model: n_ctx_per_seq = 128
0.00.673.168 I llama_init_from_model: n_batch       = 128
0.00.673.168 I llama_init_from_model: n_ubatch      = 128
0.00.673.169 I llama_init_from_model: flash_attn    = 0
0.00.673.171 I llama_init_from_model: freq_base     = 10000.0
0.00.673.171 I llama_init_from_model: freq_scale    = 1
0.00.673.172 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.673.174 I ggml_metal_init: allocating
0.00.673.262 I ggml_metal_init: found device: Apple M4
0.00.673.285 I ggml_metal_init: picking default device: Apple M4
0.00.674.834 I ggml_metal_init: using embedded metal library
0.00.681.382 I ggml_metal_init: GPU name:   Apple M4
0.00.681.386 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.681.387 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.681.387 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.681.388 I ggml_metal_init: simdgroup reduction   = true
0.00.681.388 I ggml_metal_init: simdgroup matrix mul. = true
0.00.681.388 I ggml_metal_init: has residency sets    = true
0.00.681.389 I ggml_metal_init: has bfloat            = true
0.00.681.389 I ggml_metal_init: use bfloat            = true
0.00.681.390 I ggml_metal_init: hasUnifiedMemory      = true
0.00.681.400 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.698.620 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.702.007 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.702.011 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.702.053 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.705.460 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.705.462 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.705.462 I llama_init_from_model: graph nodes  = 967
0.00.705.462 I llama_init_from_model: graph splits = 2
0.00.705.465 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.705.466 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.731.475 I 
0.00.731.548 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.731.554 I perplexity: tokenizing the input ..
0.00.738.560 I perplexity: tokenization took 7.003 ms
0.00.738.583 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.874.587 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.876.105 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.876.126 I llama_perf_context_print:        load time =     721.45 ms
0.00.876.127 I llama_perf_context_print: prompt eval time =     135.10 ms /   128 tokens (    1.06 ms per token,   947.47 tokens per second)
0.00.876.127 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.876.128 I llama_perf_context_print:       total time =     144.65 ms /   129 tokens
0.00.876.502 I ggml_metal_free: deallocating

real	0m0.892s
user	0m0.081s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.970 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.992 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.998 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.000 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.000 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.001 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.001 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.001 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.002 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.003 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.003 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.003 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.004 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.004 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.005 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.007 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.007 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.008 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.741 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.763 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.503 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.504 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.504 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.505 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.505 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.505 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.506 I llama_model_loader: - type  f32:  194 tensors
0.00.024.506 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.507 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.507 I print_info: file format = GGUF V3 (latest)
0.00.024.508 I print_info: file type   = Q5_1
0.00.024.509 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.449 I load: special tokens cache size = 25
0.00.038.487 I load: token to piece cache size = 0.2984 MB
0.00.038.491 I print_info: arch             = gptneox
0.00.038.492 I print_info: vocab_only       = 0
0.00.038.492 I print_info: n_ctx_train      = 2048
0.00.038.492 I print_info: n_embd           = 2048
0.00.038.492 I print_info: n_layer          = 24
0.00.038.497 I print_info: n_head           = 16
0.00.038.498 I print_info: n_head_kv        = 16
0.00.038.499 I print_info: n_rot            = 32
0.00.038.500 I print_info: n_swa            = 0
0.00.038.502 I print_info: n_embd_head_k    = 128
0.00.038.502 I print_info: n_embd_head_v    = 128
0.00.038.503 I print_info: n_gqa            = 1
0.00.038.504 I print_info: n_embd_k_gqa     = 2048
0.00.038.504 I print_info: n_embd_v_gqa     = 2048
0.00.038.505 I print_info: f_norm_eps       = 1.0e-05
0.00.038.506 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.506 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.506 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.506 I print_info: f_logit_scale    = 0.0e+00
0.00.038.507 I print_info: n_ff             = 8192
0.00.038.507 I print_info: n_expert         = 0
0.00.038.507 I print_info: n_expert_used    = 0
0.00.038.507 I print_info: causal attn      = 1
0.00.038.508 I print_info: pooling type     = 0
0.00.038.508 I print_info: rope type        = 2
0.00.038.508 I print_info: rope scaling     = linear
0.00.038.508 I print_info: freq_base_train  = 10000.0
0.00.038.509 I print_info: freq_scale_train = 1
0.00.038.509 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.509 I print_info: rope_finetuned   = unknown
0.00.038.509 I print_info: ssm_d_conv       = 0
0.00.038.510 I print_info: ssm_d_inner      = 0
0.00.038.510 I print_info: ssm_d_state      = 0
0.00.038.510 I print_info: ssm_dt_rank      = 0
0.00.038.510 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.510 I print_info: model type       = 1.4B
0.00.038.511 I print_info: model params     = 1.41 B
0.00.038.511 I print_info: general.name     = 1.4B
0.00.038.511 I print_info: vocab type       = BPE
0.00.038.512 I print_info: n_vocab          = 50304
0.00.038.512 I print_info: n_merges         = 50009
0.00.038.512 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.512 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.512 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.512 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.513 I print_info: LF token         = 187 'Ċ'
0.00.038.513 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.513 I print_info: max token length = 1024
0.00.038.514 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.692.385 I load_tensors: offloading 24 repeating layers to GPU
0.00.692.404 I load_tensors: offloading output layer to GPU
0.00.692.405 I load_tensors: offloaded 25/25 layers to GPU
0.00.692.440 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.692.442 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.694.165 I llama_init_from_model: n_seq_max     = 1
0.00.694.167 I llama_init_from_model: n_ctx         = 128
0.00.694.168 I llama_init_from_model: n_ctx_per_seq = 128
0.00.694.169 I llama_init_from_model: n_batch       = 128
0.00.694.169 I llama_init_from_model: n_ubatch      = 128
0.00.694.169 I llama_init_from_model: flash_attn    = 0
0.00.694.171 I llama_init_from_model: freq_base     = 10000.0
0.00.694.171 I llama_init_from_model: freq_scale    = 1
0.00.694.172 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.694.174 I ggml_metal_init: allocating
0.00.694.253 I ggml_metal_init: found device: Apple M4
0.00.694.265 I ggml_metal_init: picking default device: Apple M4
0.00.695.876 I ggml_metal_init: using embedded metal library
0.00.702.404 I ggml_metal_init: GPU name:   Apple M4
0.00.702.410 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.702.411 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.702.411 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.702.412 I ggml_metal_init: simdgroup reduction   = true
0.00.702.412 I ggml_metal_init: simdgroup matrix mul. = true
0.00.702.413 I ggml_metal_init: has residency sets    = true
0.00.702.413 I ggml_metal_init: has bfloat            = true
0.00.702.413 I ggml_metal_init: use bfloat            = true
0.00.702.414 I ggml_metal_init: hasUnifiedMemory      = true
0.00.702.418 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.720.557 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.724.085 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.724.091 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.724.170 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.727.304 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.727.306 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.727.307 I llama_init_from_model: graph nodes  = 967
0.00.727.307 I llama_init_from_model: graph splits = 2
0.00.727.309 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.727.309 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.163 I 
0.00.754.248 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.754.256 I perplexity: tokenizing the input ..
0.00.761.516 I perplexity: tokenization took 7.257 ms
0.00.761.538 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.897.393 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.899.244 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.899.260 I llama_perf_context_print:        load time =     745.18 ms
0.00.899.264 I llama_perf_context_print: prompt eval time =     134.97 ms /   128 tokens (    1.05 ms per token,   948.32 tokens per second)
0.00.899.266 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.899.266 I llama_perf_context_print:       total time =     145.10 ms /   129 tokens
0.00.899.661 I ggml_metal_free: deallocating

real	0m0.914s
user	0m0.080s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.119 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.079 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.913 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.920 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.922 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.927 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.927 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.928 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.928 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.929 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.929 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.930 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.930 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.930 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.930 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.931 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.934 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.934 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.934 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.654 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.773 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.669 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.671 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.671 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.672 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.672 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.672 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.673 I llama_model_loader: - type  f32:  194 tensors
0.00.025.673 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.674 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.674 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.675 I print_info: file format = GGUF V3 (latest)
0.00.025.675 I print_info: file type   = Q2_K - Medium
0.00.025.676 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.758 I load: special tokens cache size = 25
0.00.039.890 I load: token to piece cache size = 0.2984 MB
0.00.039.896 I print_info: arch             = gptneox
0.00.039.896 I print_info: vocab_only       = 0
0.00.039.900 I print_info: n_ctx_train      = 2048
0.00.039.900 I print_info: n_embd           = 2048
0.00.039.901 I print_info: n_layer          = 24
0.00.039.905 I print_info: n_head           = 16
0.00.039.905 I print_info: n_head_kv        = 16
0.00.039.905 I print_info: n_rot            = 32
0.00.039.905 I print_info: n_swa            = 0
0.00.039.907 I print_info: n_embd_head_k    = 128
0.00.039.907 I print_info: n_embd_head_v    = 128
0.00.039.907 I print_info: n_gqa            = 1
0.00.039.908 I print_info: n_embd_k_gqa     = 2048
0.00.039.908 I print_info: n_embd_v_gqa     = 2048
0.00.039.909 I print_info: f_norm_eps       = 1.0e-05
0.00.039.909 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.909 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.910 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.911 I print_info: f_logit_scale    = 0.0e+00
0.00.039.913 I print_info: n_ff             = 8192
0.00.039.913 I print_info: n_expert         = 0
0.00.039.913 I print_info: n_expert_used    = 0
0.00.039.913 I print_info: causal attn      = 1
0.00.039.922 I print_info: pooling type     = 0
0.00.039.923 I print_info: rope type        = 2
0.00.039.923 I print_info: rope scaling     = linear
0.00.039.926 I print_info: freq_base_train  = 10000.0
0.00.039.926 I print_info: freq_scale_train = 1
0.00.039.926 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.927 I print_info: rope_finetuned   = unknown
0.00.039.927 I print_info: ssm_d_conv       = 0
0.00.039.927 I print_info: ssm_d_inner      = 0
0.00.039.927 I print_info: ssm_d_state      = 0
0.00.039.927 I print_info: ssm_dt_rank      = 0
0.00.039.927 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.928 I print_info: model type       = 1.4B
0.00.039.928 I print_info: model params     = 1.41 B
0.00.039.928 I print_info: general.name     = 1.4B
0.00.039.929 I print_info: vocab type       = BPE
0.00.039.929 I print_info: n_vocab          = 50304
0.00.039.929 I print_info: n_merges         = 50009
0.00.039.929 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.929 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.929 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.930 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.930 I print_info: LF token         = 187 'Ċ'
0.00.039.930 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.930 I print_info: max token length = 1024
0.00.039.931 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.368.656 I load_tensors: offloading 24 repeating layers to GPU
0.00.368.673 I load_tensors: offloading output layer to GPU
0.00.368.674 I load_tensors: offloaded 25/25 layers to GPU
0.00.368.709 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.368.710 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.370.336 I llama_init_from_model: n_seq_max     = 1
0.00.370.339 I llama_init_from_model: n_ctx         = 128
0.00.370.339 I llama_init_from_model: n_ctx_per_seq = 128
0.00.370.340 I llama_init_from_model: n_batch       = 128
0.00.370.340 I llama_init_from_model: n_ubatch      = 128
0.00.370.341 I llama_init_from_model: flash_attn    = 0
0.00.370.343 I llama_init_from_model: freq_base     = 10000.0
0.00.370.344 I llama_init_from_model: freq_scale    = 1
0.00.370.344 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.370.352 I ggml_metal_init: allocating
0.00.370.444 I ggml_metal_init: found device: Apple M4
0.00.370.458 I ggml_metal_init: picking default device: Apple M4
0.00.372.351 I ggml_metal_init: using embedded metal library
0.00.378.029 I ggml_metal_init: GPU name:   Apple M4
0.00.378.042 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.378.043 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.378.044 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.378.045 I ggml_metal_init: simdgroup reduction   = true
0.00.378.045 I ggml_metal_init: simdgroup matrix mul. = true
0.00.378.045 I ggml_metal_init: has residency sets    = true
0.00.378.046 I ggml_metal_init: has bfloat            = true
0.00.378.046 I ggml_metal_init: use bfloat            = true
0.00.378.048 I ggml_metal_init: hasUnifiedMemory      = true
0.00.378.052 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.400.473 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.404.159 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.404.167 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.404.219 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.407.659 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.407.661 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.407.662 I llama_init_from_model: graph nodes  = 967
0.00.407.662 I llama_init_from_model: graph splits = 2
0.00.407.666 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.407.666 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.438.898 I 
0.00.438.972 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.438.981 I perplexity: tokenizing the input ..
0.00.446.184 I perplexity: tokenization took 7.198 ms
0.00.446.207 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.579.055 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.580.579 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.580.594 I llama_perf_context_print:        load time =     428.81 ms
0.00.580.595 I llama_perf_context_print: prompt eval time =     131.99 ms /   128 tokens (    1.03 ms per token,   969.76 tokens per second)
0.00.580.595 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.580.596 I llama_perf_context_print:       total time =     141.70 ms /   129 tokens
0.00.580.978 I ggml_metal_free: deallocating

real	0m0.597s
user	0m0.084s
sys	0m0.095s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.821 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.948 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.954 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.960 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.961 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.961 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.962 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.962 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.963 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.963 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.963 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.964 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.964 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.964 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.965 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.966 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.967 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.967 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.779 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.779 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.591 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.592 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.592 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.593 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.593 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.594 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.594 I llama_model_loader: - type  f32:  194 tensors
0.00.024.595 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.595 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.595 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.595 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.596 I print_info: file format = GGUF V3 (latest)
0.00.024.596 I print_info: file type   = Q3_K - Medium
0.00.024.598 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.751 I load: special tokens cache size = 25
0.00.038.806 I load: token to piece cache size = 0.2984 MB
0.00.038.811 I print_info: arch             = gptneox
0.00.038.811 I print_info: vocab_only       = 0
0.00.038.812 I print_info: n_ctx_train      = 2048
0.00.038.812 I print_info: n_embd           = 2048
0.00.038.812 I print_info: n_layer          = 24
0.00.038.816 I print_info: n_head           = 16
0.00.038.817 I print_info: n_head_kv        = 16
0.00.038.817 I print_info: n_rot            = 32
0.00.038.817 I print_info: n_swa            = 0
0.00.038.817 I print_info: n_embd_head_k    = 128
0.00.038.817 I print_info: n_embd_head_v    = 128
0.00.038.818 I print_info: n_gqa            = 1
0.00.038.819 I print_info: n_embd_k_gqa     = 2048
0.00.038.819 I print_info: n_embd_v_gqa     = 2048
0.00.038.820 I print_info: f_norm_eps       = 1.0e-05
0.00.038.820 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.821 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.821 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.821 I print_info: f_logit_scale    = 0.0e+00
0.00.038.822 I print_info: n_ff             = 8192
0.00.038.822 I print_info: n_expert         = 0
0.00.038.822 I print_info: n_expert_used    = 0
0.00.038.822 I print_info: causal attn      = 1
0.00.038.822 I print_info: pooling type     = 0
0.00.038.822 I print_info: rope type        = 2
0.00.038.823 I print_info: rope scaling     = linear
0.00.038.823 I print_info: freq_base_train  = 10000.0
0.00.038.823 I print_info: freq_scale_train = 1
0.00.038.823 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.824 I print_info: rope_finetuned   = unknown
0.00.038.824 I print_info: ssm_d_conv       = 0
0.00.038.824 I print_info: ssm_d_inner      = 0
0.00.038.824 I print_info: ssm_d_state      = 0
0.00.038.824 I print_info: ssm_dt_rank      = 0
0.00.038.824 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.824 I print_info: model type       = 1.4B
0.00.038.825 I print_info: model params     = 1.41 B
0.00.038.825 I print_info: general.name     = 1.4B
0.00.038.829 I print_info: vocab type       = BPE
0.00.038.829 I print_info: n_vocab          = 50304
0.00.038.829 I print_info: n_merges         = 50009
0.00.038.831 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.831 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.831 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.831 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.831 I print_info: LF token         = 187 'Ċ'
0.00.038.832 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.832 I print_info: max token length = 1024
0.00.038.832 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.435.262 I load_tensors: offloading 24 repeating layers to GPU
0.00.435.277 I load_tensors: offloading output layer to GPU
0.00.435.278 I load_tensors: offloaded 25/25 layers to GPU
0.00.435.315 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.435.317 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.436.965 I llama_init_from_model: n_seq_max     = 1
0.00.436.968 I llama_init_from_model: n_ctx         = 128
0.00.436.968 I llama_init_from_model: n_ctx_per_seq = 128
0.00.436.969 I llama_init_from_model: n_batch       = 128
0.00.436.969 I llama_init_from_model: n_ubatch      = 128
0.00.436.969 I llama_init_from_model: flash_attn    = 0
0.00.436.972 I llama_init_from_model: freq_base     = 10000.0
0.00.436.972 I llama_init_from_model: freq_scale    = 1
0.00.436.973 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.436.975 I ggml_metal_init: allocating
0.00.437.064 I ggml_metal_init: found device: Apple M4
0.00.437.078 I ggml_metal_init: picking default device: Apple M4
0.00.438.896 I ggml_metal_init: using embedded metal library
0.00.444.420 I ggml_metal_init: GPU name:   Apple M4
0.00.444.434 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.444.434 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.444.435 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.444.435 I ggml_metal_init: simdgroup reduction   = true
0.00.444.436 I ggml_metal_init: simdgroup matrix mul. = true
0.00.444.436 I ggml_metal_init: has residency sets    = true
0.00.444.436 I ggml_metal_init: has bfloat            = true
0.00.444.437 I ggml_metal_init: use bfloat            = true
0.00.444.438 I ggml_metal_init: hasUnifiedMemory      = true
0.00.444.442 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.464.715 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.468.298 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.468.310 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.468.360 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.471.691 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.471.693 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.471.693 I llama_init_from_model: graph nodes  = 967
0.00.471.694 I llama_init_from_model: graph splits = 2
0.00.471.697 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.471.697 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.502.189 I 
0.00.502.268 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.502.274 I perplexity: tokenizing the input ..
0.00.509.266 I perplexity: tokenization took 6.989 ms
0.00.509.289 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.647.535 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.649.082 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.649.095 I llama_perf_context_print:        load time =     493.36 ms
0.00.649.096 I llama_perf_context_print: prompt eval time =     137.44 ms /   128 tokens (    1.07 ms per token,   931.30 tokens per second)
0.00.649.096 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.649.097 I llama_perf_context_print:       total time =     146.91 ms /   129 tokens
0.00.649.475 I ggml_metal_free: deallocating

real	0m0.663s
user	0m0.081s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.874 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.452 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.459 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.461 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.461 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.461 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.462 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.462 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.463 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.463 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.464 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.464 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.464 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.465 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.465 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.467 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.468 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.468 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.295 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.312 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.201 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.203 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.203 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.203 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.204 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.204 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.205 I llama_model_loader: - type  f32:  194 tensors
0.00.025.205 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.205 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.206 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.206 I print_info: file format = GGUF V3 (latest)
0.00.025.207 I print_info: file type   = Q4_K - Medium
0.00.025.210 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.857 I load: special tokens cache size = 25
0.00.039.754 I load: token to piece cache size = 0.2984 MB
0.00.039.758 I print_info: arch             = gptneox
0.00.039.758 I print_info: vocab_only       = 0
0.00.039.758 I print_info: n_ctx_train      = 2048
0.00.039.759 I print_info: n_embd           = 2048
0.00.039.759 I print_info: n_layer          = 24
0.00.039.763 I print_info: n_head           = 16
0.00.039.764 I print_info: n_head_kv        = 16
0.00.039.764 I print_info: n_rot            = 32
0.00.039.764 I print_info: n_swa            = 0
0.00.039.765 I print_info: n_embd_head_k    = 128
0.00.039.765 I print_info: n_embd_head_v    = 128
0.00.039.767 I print_info: n_gqa            = 1
0.00.039.768 I print_info: n_embd_k_gqa     = 2048
0.00.039.768 I print_info: n_embd_v_gqa     = 2048
0.00.039.769 I print_info: f_norm_eps       = 1.0e-05
0.00.039.769 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.769 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.772 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.772 I print_info: f_logit_scale    = 0.0e+00
0.00.039.772 I print_info: n_ff             = 8192
0.00.039.773 I print_info: n_expert         = 0
0.00.039.773 I print_info: n_expert_used    = 0
0.00.039.773 I print_info: causal attn      = 1
0.00.039.773 I print_info: pooling type     = 0
0.00.039.773 I print_info: rope type        = 2
0.00.039.773 I print_info: rope scaling     = linear
0.00.039.774 I print_info: freq_base_train  = 10000.0
0.00.039.774 I print_info: freq_scale_train = 1
0.00.039.774 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.774 I print_info: rope_finetuned   = unknown
0.00.039.774 I print_info: ssm_d_conv       = 0
0.00.039.774 I print_info: ssm_d_inner      = 0
0.00.039.775 I print_info: ssm_d_state      = 0
0.00.039.775 I print_info: ssm_dt_rank      = 0
0.00.039.775 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.775 I print_info: model type       = 1.4B
0.00.039.776 I print_info: model params     = 1.41 B
0.00.039.776 I print_info: general.name     = 1.4B
0.00.039.777 I print_info: vocab type       = BPE
0.00.039.777 I print_info: n_vocab          = 50304
0.00.039.777 I print_info: n_merges         = 50009
0.00.039.778 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.778 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.778 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.778 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.778 I print_info: LF token         = 187 'Ċ'
0.00.039.778 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.779 I print_info: max token length = 1024
0.00.039.779 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.511.339 I load_tensors: offloading 24 repeating layers to GPU
0.00.511.356 I load_tensors: offloading output layer to GPU
0.00.511.357 I load_tensors: offloaded 25/25 layers to GPU
0.00.511.391 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.511.393 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.512.951 I llama_init_from_model: n_seq_max     = 1
0.00.512.953 I llama_init_from_model: n_ctx         = 128
0.00.512.954 I llama_init_from_model: n_ctx_per_seq = 128
0.00.512.954 I llama_init_from_model: n_batch       = 128
0.00.512.954 I llama_init_from_model: n_ubatch      = 128
0.00.512.955 I llama_init_from_model: flash_attn    = 0
0.00.512.957 I llama_init_from_model: freq_base     = 10000.0
0.00.512.958 I llama_init_from_model: freq_scale    = 1
0.00.512.959 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.512.961 I ggml_metal_init: allocating
0.00.513.045 I ggml_metal_init: found device: Apple M4
0.00.513.058 I ggml_metal_init: picking default device: Apple M4
0.00.514.853 I ggml_metal_init: using embedded metal library
0.00.521.624 I ggml_metal_init: GPU name:   Apple M4
0.00.521.632 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.521.633 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.521.634 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.521.634 I ggml_metal_init: simdgroup reduction   = true
0.00.521.635 I ggml_metal_init: simdgroup matrix mul. = true
0.00.521.635 I ggml_metal_init: has residency sets    = true
0.00.521.635 I ggml_metal_init: has bfloat            = true
0.00.521.636 I ggml_metal_init: use bfloat            = true
0.00.521.637 I ggml_metal_init: hasUnifiedMemory      = true
0.00.521.642 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.540.031 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.543.575 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.543.579 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.543.625 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.546.745 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.546.747 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.546.747 I llama_init_from_model: graph nodes  = 967
0.00.546.748 I llama_init_from_model: graph splits = 2
0.00.546.750 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.546.750 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.575.442 I 
0.00.575.520 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.575.527 I perplexity: tokenizing the input ..
0.00.582.768 I perplexity: tokenization took 7.238 ms
0.00.582.791 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.729.436 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.731.199 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.731.218 I llama_perf_context_print:        load time =     566.56 ms
0.00.731.221 I llama_perf_context_print: prompt eval time =     145.70 ms /   128 tokens (    1.14 ms per token,   878.51 tokens per second)
0.00.731.221 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.731.222 I llama_perf_context_print:       total time =     155.78 ms /   129 tokens
0.00.731.616 I ggml_metal_free: deallocating

real	0m0.745s
user	0m0.082s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.737 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.728 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.735 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.736 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.737 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.737 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.737 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.738 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.739 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.739 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.739 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.741 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.741 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.742 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.742 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.745 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.745 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.745 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.590 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.598 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.440 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.442 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.442 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.442 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.443 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.443 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.444 I llama_model_loader: - type  f32:  194 tensors
0.00.025.444 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.444 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.445 I print_info: file format = GGUF V3 (latest)
0.00.025.446 I print_info: file type   = Q5_K - Medium
0.00.025.447 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.729 I load: special tokens cache size = 25
0.00.039.734 I load: token to piece cache size = 0.2984 MB
0.00.039.738 I print_info: arch             = gptneox
0.00.039.738 I print_info: vocab_only       = 0
0.00.039.738 I print_info: n_ctx_train      = 2048
0.00.039.739 I print_info: n_embd           = 2048
0.00.039.739 I print_info: n_layer          = 24
0.00.039.743 I print_info: n_head           = 16
0.00.039.744 I print_info: n_head_kv        = 16
0.00.039.744 I print_info: n_rot            = 32
0.00.039.745 I print_info: n_swa            = 0
0.00.039.745 I print_info: n_embd_head_k    = 128
0.00.039.745 I print_info: n_embd_head_v    = 128
0.00.039.746 I print_info: n_gqa            = 1
0.00.039.747 I print_info: n_embd_k_gqa     = 2048
0.00.039.747 I print_info: n_embd_v_gqa     = 2048
0.00.039.748 I print_info: f_norm_eps       = 1.0e-05
0.00.039.750 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.750 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.751 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.751 I print_info: f_logit_scale    = 0.0e+00
0.00.039.754 I print_info: n_ff             = 8192
0.00.039.754 I print_info: n_expert         = 0
0.00.039.754 I print_info: n_expert_used    = 0
0.00.039.754 I print_info: causal attn      = 1
0.00.039.754 I print_info: pooling type     = 0
0.00.039.754 I print_info: rope type        = 2
0.00.039.755 I print_info: rope scaling     = linear
0.00.039.755 I print_info: freq_base_train  = 10000.0
0.00.039.756 I print_info: freq_scale_train = 1
0.00.039.756 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.757 I print_info: rope_finetuned   = unknown
0.00.039.757 I print_info: ssm_d_conv       = 0
0.00.039.757 I print_info: ssm_d_inner      = 0
0.00.039.757 I print_info: ssm_d_state      = 0
0.00.039.757 I print_info: ssm_dt_rank      = 0
0.00.039.757 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.758 I print_info: model type       = 1.4B
0.00.039.758 I print_info: model params     = 1.41 B
0.00.039.758 I print_info: general.name     = 1.4B
0.00.039.760 I print_info: vocab type       = BPE
0.00.039.760 I print_info: n_vocab          = 50304
0.00.039.760 I print_info: n_merges         = 50009
0.00.039.760 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.760 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.760 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.760 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.761 I print_info: LF token         = 187 'Ċ'
0.00.039.761 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.761 I print_info: max token length = 1024
0.00.039.762 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.619.200 I load_tensors: offloading 24 repeating layers to GPU
0.00.619.214 I load_tensors: offloading output layer to GPU
0.00.619.214 I load_tensors: offloaded 25/25 layers to GPU
0.00.619.251 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.619.253 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.620.995 I llama_init_from_model: n_seq_max     = 1
0.00.620.999 I llama_init_from_model: n_ctx         = 128
0.00.620.999 I llama_init_from_model: n_ctx_per_seq = 128
0.00.620.999 I llama_init_from_model: n_batch       = 128
0.00.621.000 I llama_init_from_model: n_ubatch      = 128
0.00.621.000 I llama_init_from_model: flash_attn    = 0
0.00.621.003 I llama_init_from_model: freq_base     = 10000.0
0.00.621.003 I llama_init_from_model: freq_scale    = 1
0.00.621.004 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.621.006 I ggml_metal_init: allocating
0.00.621.048 I ggml_metal_init: found device: Apple M4
0.00.621.061 I ggml_metal_init: picking default device: Apple M4
0.00.622.492 I ggml_metal_init: using embedded metal library
0.00.628.933 I ggml_metal_init: GPU name:   Apple M4
0.00.628.937 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.628.938 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.628.938 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.628.939 I ggml_metal_init: simdgroup reduction   = true
0.00.628.939 I ggml_metal_init: simdgroup matrix mul. = true
0.00.628.939 I ggml_metal_init: has residency sets    = true
0.00.628.940 I ggml_metal_init: has bfloat            = true
0.00.628.940 I ggml_metal_init: use bfloat            = true
0.00.628.941 I ggml_metal_init: hasUnifiedMemory      = true
0.00.628.943 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.646.827 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.650.340 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.650.349 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.650.400 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.653.520 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.653.522 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.653.523 I llama_init_from_model: graph nodes  = 967
0.00.653.523 I llama_init_from_model: graph splits = 2
0.00.653.525 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.653.525 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.886 I 
0.00.684.975 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.983 I perplexity: tokenizing the input ..
0.00.691.972 I perplexity: tokenization took 6.986 ms
0.00.691.997 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.828.882 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.830.411 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.830.426 I llama_perf_context_print:        load time =     675.14 ms
0.00.830.427 I llama_perf_context_print: prompt eval time =     136.50 ms /   128 tokens (    1.07 ms per token,   937.72 tokens per second)
0.00.830.427 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.830.428 I llama_perf_context_print:       total time =     145.55 ms /   129 tokens
0.00.830.787 I ggml_metal_free: deallocating

real	0m0.846s
user	0m0.081s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.068 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.026 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.032 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.036 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.036 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.037 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.037 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.037 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.038 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.038 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.039 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.041 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.042 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.042 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.042 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.044 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.044 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.045 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.877 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.945 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.760 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.762 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.763 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.763 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.763 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.764 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.764 I llama_model_loader: - type  f32:  194 tensors
0.00.024.765 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.765 I print_info: file format = GGUF V3 (latest)
0.00.024.766 I print_info: file type   = Q6_K
0.00.024.767 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.824 I load: special tokens cache size = 25
0.00.038.854 I load: token to piece cache size = 0.2984 MB
0.00.038.858 I print_info: arch             = gptneox
0.00.038.858 I print_info: vocab_only       = 0
0.00.038.859 I print_info: n_ctx_train      = 2048
0.00.038.859 I print_info: n_embd           = 2048
0.00.038.859 I print_info: n_layer          = 24
0.00.038.863 I print_info: n_head           = 16
0.00.038.864 I print_info: n_head_kv        = 16
0.00.038.864 I print_info: n_rot            = 32
0.00.038.865 I print_info: n_swa            = 0
0.00.038.865 I print_info: n_embd_head_k    = 128
0.00.038.865 I print_info: n_embd_head_v    = 128
0.00.038.868 I print_info: n_gqa            = 1
0.00.038.869 I print_info: n_embd_k_gqa     = 2048
0.00.038.870 I print_info: n_embd_v_gqa     = 2048
0.00.038.870 I print_info: f_norm_eps       = 1.0e-05
0.00.038.871 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.871 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.871 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.871 I print_info: f_logit_scale    = 0.0e+00
0.00.038.872 I print_info: n_ff             = 8192
0.00.038.872 I print_info: n_expert         = 0
0.00.038.872 I print_info: n_expert_used    = 0
0.00.038.872 I print_info: causal attn      = 1
0.00.038.873 I print_info: pooling type     = 0
0.00.038.873 I print_info: rope type        = 2
0.00.038.874 I print_info: rope scaling     = linear
0.00.038.874 I print_info: freq_base_train  = 10000.0
0.00.038.874 I print_info: freq_scale_train = 1
0.00.038.875 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.876 I print_info: rope_finetuned   = unknown
0.00.038.876 I print_info: ssm_d_conv       = 0
0.00.038.876 I print_info: ssm_d_inner      = 0
0.00.038.876 I print_info: ssm_d_state      = 0
0.00.038.876 I print_info: ssm_dt_rank      = 0
0.00.038.876 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.876 I print_info: model type       = 1.4B
0.00.038.877 I print_info: model params     = 1.41 B
0.00.038.877 I print_info: general.name     = 1.4B
0.00.038.877 I print_info: vocab type       = BPE
0.00.038.877 I print_info: n_vocab          = 50304
0.00.038.878 I print_info: n_merges         = 50009
0.00.038.878 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.878 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.878 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.878 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.879 I print_info: LF token         = 187 'Ċ'
0.00.038.879 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.879 I print_info: max token length = 1024
0.00.038.880 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.596.144 I load_tensors: offloading 24 repeating layers to GPU
0.00.596.152 I load_tensors: offloading output layer to GPU
0.00.596.153 I load_tensors: offloaded 25/25 layers to GPU
0.00.596.184 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.596.187 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.597.713 I llama_init_from_model: n_seq_max     = 1
0.00.597.716 I llama_init_from_model: n_ctx         = 128
0.00.597.716 I llama_init_from_model: n_ctx_per_seq = 128
0.00.597.716 I llama_init_from_model: n_batch       = 128
0.00.597.717 I llama_init_from_model: n_ubatch      = 128
0.00.597.717 I llama_init_from_model: flash_attn    = 0
0.00.597.718 I llama_init_from_model: freq_base     = 10000.0
0.00.597.719 I llama_init_from_model: freq_scale    = 1
0.00.597.719 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.597.721 I ggml_metal_init: allocating
0.00.597.775 I ggml_metal_init: found device: Apple M4
0.00.597.787 I ggml_metal_init: picking default device: Apple M4
0.00.599.256 I ggml_metal_init: using embedded metal library
0.00.605.207 I ggml_metal_init: GPU name:   Apple M4
0.00.605.211 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.605.212 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.605.213 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.605.213 I ggml_metal_init: simdgroup reduction   = true
0.00.605.213 I ggml_metal_init: simdgroup matrix mul. = true
0.00.605.213 I ggml_metal_init: has residency sets    = true
0.00.605.214 I ggml_metal_init: has bfloat            = true
0.00.605.214 I ggml_metal_init: use bfloat            = true
0.00.605.215 I ggml_metal_init: hasUnifiedMemory      = true
0.00.605.217 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.621.965 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.625.453 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.625.457 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.625.531 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.628.738 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.628.740 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.628.740 I llama_init_from_model: graph nodes  = 967
0.00.628.741 I llama_init_from_model: graph splits = 2
0.00.628.743 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.628.744 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.580 I 
0.00.666.665 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.672 I perplexity: tokenizing the input ..
0.00.673.098 I perplexity: tokenization took 6.424 ms
0.00.673.114 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.812.838 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.814.390 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.814.414 I llama_perf_context_print:        load time =     657.50 ms
0.00.814.415 I llama_perf_context_print: prompt eval time =     139.43 ms /   128 tokens (    1.09 ms per token,   918.00 tokens per second)
0.00.814.416 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.814.417 I llama_perf_context_print:       total time =     147.84 ms /   129 tokens
0.00.814.811 I ggml_metal_free: deallocating

real	0m0.829s
user	0m0.078s
sys	0m0.132s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.319 I build: 4798 (1782cdfe) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.257 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.776 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.782 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.784 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.789 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.789 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.789 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.790 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.791 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.792 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.792 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.792 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.793 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.793 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.794 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.796 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.796 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.796 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.185 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.178 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.715 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.717 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.717 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.718 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.718 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.718 I llama_model_loader: - type  f32:  194 tensors
0.00.050.719 I llama_model_loader: - type  f16:   98 tensors
0.00.050.720 I print_info: file format = GGUF V3 (latest)
0.00.050.721 I print_info: file type   = all F32 (guessed)
0.00.050.722 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.062.086 I load: special tokens cache size = 25
0.00.069.754 I load: token to piece cache size = 0.2984 MB
0.00.069.757 I print_info: arch             = gptneox
0.00.069.757 I print_info: vocab_only       = 0
0.00.069.757 I print_info: n_ctx_train      = 2048
0.00.069.757 I print_info: n_embd           = 2048
0.00.069.758 I print_info: n_layer          = 24
0.00.069.761 I print_info: n_head           = 16
0.00.069.762 I print_info: n_head_kv        = 16
0.00.069.762 I print_info: n_rot            = 32
0.00.069.762 I print_info: n_swa            = 0
0.00.069.762 I print_info: n_embd_head_k    = 128
0.00.069.763 I print_info: n_embd_head_v    = 128
0.00.069.763 I print_info: n_gqa            = 1
0.00.069.764 I print_info: n_embd_k_gqa     = 2048
0.00.069.765 I print_info: n_embd_v_gqa     = 2048
0.00.069.765 I print_info: f_norm_eps       = 1.0e-05
0.00.069.766 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.766 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.766 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.766 I print_info: f_logit_scale    = 0.0e+00
0.00.069.767 I print_info: n_ff             = 8192
0.00.069.767 I print_info: n_expert         = 0
0.00.069.767 I print_info: n_expert_used    = 0
0.00.069.767 I print_info: causal attn      = 1
0.00.069.767 I print_info: pooling type     = 0
0.00.069.767 I print_info: rope type        = 2
0.00.069.768 I print_info: rope scaling     = linear
0.00.069.768 I print_info: freq_base_train  = 10000.0
0.00.069.768 I print_info: freq_scale_train = 1
0.00.069.768 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.769 I print_info: rope_finetuned   = unknown
0.00.069.769 I print_info: ssm_d_conv       = 0
0.00.069.769 I print_info: ssm_d_inner      = 0
0.00.069.769 I print_info: ssm_d_state      = 0
0.00.069.769 I print_info: ssm_dt_rank      = 0
0.00.069.769 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.772 I print_info: model type       = 1.4B
0.00.069.772 I print_info: model params     = 1.41 B
0.00.069.772 I print_info: general.name     = 1.4B
0.00.069.773 I print_info: vocab type       = BPE
0.00.069.773 I print_info: n_vocab          = 50304
0.00.069.773 I print_info: n_merges         = 50009
0.00.069.773 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.773 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.774 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.774 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.774 I print_info: LF token         = 187 'Ċ'
0.00.069.774 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.774 I print_info: max token length = 1024
0.00.069.775 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.336.795 I load_tensors: offloading 24 repeating layers to GPU
0.01.336.798 I load_tensors: offloading output layer to GPU
0.01.336.799 I load_tensors: offloaded 25/25 layers to GPU
0.01.336.832 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.336.834 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.337.710 I llama_init_from_model: n_seq_max     = 1
0.01.337.712 I llama_init_from_model: n_ctx         = 128
0.01.337.712 I llama_init_from_model: n_ctx_per_seq = 128
0.01.337.712 I llama_init_from_model: n_batch       = 128
0.01.337.712 I llama_init_from_model: n_ubatch      = 128
0.01.337.713 I llama_init_from_model: flash_attn    = 0
0.01.337.713 I llama_init_from_model: freq_base     = 10000.0
0.01.337.714 I llama_init_from_model: freq_scale    = 1
0.01.337.714 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.337.718 I ggml_metal_init: allocating
0.01.337.782 I ggml_metal_init: found device: Apple M4
0.01.337.791 I ggml_metal_init: picking default device: Apple M4
0.01.338.956 I ggml_metal_init: using embedded metal library
0.01.343.013 I ggml_metal_init: GPU name:   Apple M4
0.01.343.015 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.343.016 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.343.016 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.343.017 I ggml_metal_init: simdgroup reduction   = true
0.01.343.017 I ggml_metal_init: simdgroup matrix mul. = true
0.01.343.017 I ggml_metal_init: has residency sets    = true
0.01.343.017 I ggml_metal_init: has bfloat            = true
0.01.343.017 I ggml_metal_init: use bfloat            = true
0.01.343.018 I ggml_metal_init: hasUnifiedMemory      = true
0.01.343.019 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.354.996 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.356.848 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.356.850 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.356.878 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.358.666 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.358.668 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.358.668 I llama_init_from_model: graph nodes  = 967
0.01.358.668 I llama_init_from_model: graph splits = 2
0.01.358.670 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.358.670 I 
0.01.358.712 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.358.713 I compute_imatrix: tokenizing the input ..
0.01.363.897 I compute_imatrix: tokenization took 5.181 ms
0.01.363.901 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.628.316 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.633.153 I llama_perf_context_print:        load time =    1609.06 ms
0.01.633.154 I llama_perf_context_print: prompt eval time =     262.21 ms /   128 tokens (    2.05 ms per token,   488.16 tokens per second)
0.01.633.155 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.633.157 I llama_perf_context_print:       total time =    1613.89 ms /   129 tokens
0.01.633.796 I ggml_metal_free: deallocating

real	0m1.818s
user	0m0.127s
sys	0m0.273s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4798 (1782cdfe)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x112b04a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x112b05160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x112b05710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x112b05cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x112b06270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x112b06820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x112b06dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x112b07380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x112b07930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x112b07e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x112b08330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x112b08830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x112b09350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x112b09b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x112b0a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x112b0aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x112b0b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x112b0b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x112b0bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x112b0c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x112b0ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x112b0d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x112b0dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x112b0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x112b0ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x112b0ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x112b0f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x112b101c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x112b10700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x112b109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x112b10e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x112b11120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x112b119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x112b11ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x112b121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x112b12650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x112b12af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x112b12f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x112b13430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x112b138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x112b13d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x112b14210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x112b146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x112b14b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x112b14e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x112b15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x112b15a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x112b16350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x112b16960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x112b16f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x112b17580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x112b17b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x112b181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x112b187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x112b18fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x112b19440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x112b198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x112b19ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x112b1a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x112b1a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x112b1ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x112b1b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x112b1b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x112b1ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x112b1bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x112b1c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x112b1c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x112b1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x112b1d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x112b1d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x112b1daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x112b1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x112b1e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x112b1e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x112b1ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x112b1f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x112b1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x112b1fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x112b203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x112b20910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x112b20e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x112b213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x112b21900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x112b21e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x112b223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x112b228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x112b22e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x112b23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x112b238e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x112b23e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x112b24380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x112b248d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x112b24e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x112b25370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x112b258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x112b25e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x112b26360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x112b16040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x112b267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x112b26f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x112b274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x112b27a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x112b27f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x112b284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x112b28a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x112b28f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x112b294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x112b29a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x112b29f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x112b2a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x112b2a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x112b2af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x112b2b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x112b2b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x112b2bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x112b2c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x112b2c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x112b2cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x112b2d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x112b2d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x112b2d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x112b2de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x112b2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x112b2e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x112b2ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x112b2f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x112b2f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x112b2f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x112b2fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x112b30330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x112b307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x112b30c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x112b31110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x112b315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x112b31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x112b31ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x112b32390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x112b32830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x112b32cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x112b33170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x112b33610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x112b33ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x112b33f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x112b343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x112b34890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x112b34d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x112b351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x112b35670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x112b35b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x112b35fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x112b36450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x112b368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x112b36d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x112b37230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x112b376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x112b37b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x112b38010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x112b384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x112b38950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x112b38df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x112b39290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x112b39730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x112b39bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x112b3a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x112b3a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x112b3a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x112b3ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x112b3b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x112b3b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x112b3bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x112b3c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x112b3c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x112b3ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x112b3ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x112b3d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x112b3d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x112b3dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x112b3e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x112b3e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x112b3ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x112b3ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x112b3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x112b3f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x112b3fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x112b40190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x112b40630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x112b40ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x112b40f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x112b41410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x112b418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x112b41d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x112b421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x112b42690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x112b42be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x112b43130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x112b43680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x112b43bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x112b43e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x112b444a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x112b44ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x112b450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x112b458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x112b45d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x112b46010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x112b46620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x112b46c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x112b47420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x112b478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x112b47d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x112b48200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x112b489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x112b48f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x112b49450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x112b499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x112b49ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x112b4a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x112b4a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x112b4aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x112b4b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x112b4b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x112b4bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x112b4c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x112b4c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x112b4cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x112b4d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x112b4d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x112b4deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x112b4e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x112b4e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x112b4eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x112b4f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x112b4f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x112b4fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x112b503e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x112b50930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x112b50e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x112b513d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x112b51920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x112b51e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x112b523c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x112b52910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x112b52e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x112b533b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x112b53900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x112b53e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x112b543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x112b548f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x112b54e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x112b55390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x112b558e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x112b55e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x112b56380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x112b568d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x112b56e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x112b57370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x112b578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x112b57e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x112b58360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x112b588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x112b58e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x112b59350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x112b598a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x112b59df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x112b5a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x112b5a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x112b5ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x112b5b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x112b5b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x112b5bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x112b5c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x112b5c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x112b5ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x112b5cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x112b5d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x112b5d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x112b5dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x112b5e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x112b5e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x112b5eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x112b5ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x112b5f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x112b5f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x112b5fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x112b601d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x112b60670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x112b60b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x112b60fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x112b61450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x112b618f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x112b61d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x112b62230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x112b626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x112b62c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x112b63340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x112b63a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x112b64180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x112b648a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x112b64b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x112b65350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x112b65610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x112b65c20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.687.189 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.687.193 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a60be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a60c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a60c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a60cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a60cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a60d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a60d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a60dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a60e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a60e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a60ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a60f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a60fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a610480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a610c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a6113b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a611ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a6121f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a612910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a613040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a613760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a613e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a6145a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a614cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a6153e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a6156a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a615960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a615dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a616240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a6166b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a616bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a6170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a617530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a6177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a617c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a6180d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a618630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a618b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a619030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a619530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a619a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a619f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a61a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a61a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a61ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a61b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a61b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a61bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a61bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a61c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a61c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a61cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a61d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a61d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a61da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a61e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a61e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a61e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a61efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a61f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a61fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a620100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a6205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a620a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a620ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a621380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a621820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a621cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a622160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a622600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a622aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a622f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a6233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13a623930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13a623e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13a6243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13a624920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13a624e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13a6253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13a625910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13a625e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13a6263b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13a626900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13a626e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13a6273a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13a6278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13a627e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13a628390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13a6288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13a628e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13a629380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13a6298d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13a629e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13a62a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13a62a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13a62ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13a62b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13a62b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13a62be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13a62c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13a62c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13a62cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13a62d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13a62d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13a62dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13a62e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13a62e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13a62edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13a62f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13a62f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13a62fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13a630310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13a630860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a630d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a6311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a631640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a631ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a631f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a632420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a6328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a632d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a633200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a6336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a633b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a633fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a634480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a634920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a634dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a635260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a635700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a635ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a636040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a6364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a636980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a636e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a6372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a637760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a637c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a6380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a638540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a6389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a638e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a639320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a6397c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a639c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a63a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a63a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a63aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a63aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a63b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a63b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a63bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a63c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a63c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a63caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a63cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a63d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a63d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a63dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a63e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a63e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a63eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a63efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a63f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a63f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a63fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a640220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a6406c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a640b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a641000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a6414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a641940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a641de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a642280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a642720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a642bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a643060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a643500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a6439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a643e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a6442e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a644780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a644c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a6450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a645560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a645a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a645ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a646340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a6467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a646c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a647120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a6475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a647a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a647fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a648500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a648a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a648fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a649260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a649870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a649e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a64a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13a64ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13a64b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a64b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a64b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13a64c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a64c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a64cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a64d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a64d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a64dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a64e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a64e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a64ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a64f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a64f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a64fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a6502b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a650800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a650d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a6512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a6517f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a651d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a652290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a6527e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a652d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a653280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a6537d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a653d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a654270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a6547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a654d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a655260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a6557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a655d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a656250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a6567a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a656cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a657240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a657790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a657ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a658230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a658780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a658cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a659220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a659770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a659cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a65a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a65a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a65acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a65b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a65b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a65bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a65c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a65c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a65cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a65d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a65d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a65dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a65e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a65e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a65ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a65f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a65f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a65fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a6601b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a660700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13a660ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13a661040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a6614e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a661980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a661e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a6622c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a662760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a662c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a6630a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a663540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a6639e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a663e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a664320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a6647c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a664c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13a665100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13a6655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13a665a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13a665ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13a666380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13a666820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13a666cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13a667160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13a667600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13a667aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a667ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a668710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a668e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a669550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a669c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a669f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13a66a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a66a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a66aff0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12a7044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12a704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12a704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12a705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12a7056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12a705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12a705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12a7063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12a706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12a706cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12a707140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12a707860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12a708380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12a708b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12a709340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12a709a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12a70a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12a70a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12a70afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12a70b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12a70be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12a70c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12a70cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12a70d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12a70da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12a70dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12a70e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12a70e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12a70e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12a70ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12a70f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12a70f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12a70fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12a70fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12a7102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12a710710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12a710b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12a710ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12a711460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12a7118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12a711d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12a7121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12a712620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12a712a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12a712f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12a713370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12a7137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12a713c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12a7140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12a714530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12a7149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12a714e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12a715280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12a7156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12a715b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12a715fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12a716540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12a716a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12a716eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12a717320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12a717790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12a717c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12a718070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12a7184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12a718950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12a718dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12a719230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12a7196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12a719b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12a719f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12a71a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12a71a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12a71acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12a71b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12a71b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12a71ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12a71be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12a71c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12a71c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12a71cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12a71d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12a71d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12a71d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12a71dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12a71e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12a71e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12a71eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12a71ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12a71f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12a71f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12a71fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12a720120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12a720590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12a720a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12a720e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12a7212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12a721750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12a721bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12a722420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12a722940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12a722ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12a7234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12a723a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12a724000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12a7245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12a724b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12a725110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12a7256c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12a725c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12a726220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12a7267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12a726d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12a727330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12a7278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12a727de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12a7282e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12a7287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12a728ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12a7291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12a7296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12a729be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12a72a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12a72a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12a72aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12a72afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12a72b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12a72b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12a72bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12a72c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12a72c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12a72cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12a72d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12a72d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12a72dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12a72e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12a72e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12a72ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12a72f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12a72f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12a72fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12a72ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12a7304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12a7309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12a730ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12a7313e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12a7318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12a731de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12a7322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12a7327e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12a732ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12a7331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12a7336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12a733be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12a7340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12a7345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12a734ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12a734fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12a7354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12a7359e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12a735ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12a7363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12a7368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12a736de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12a7372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12a7377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12a737ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12a7381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12a7386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12a738be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12a7390e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12a7395e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12a739ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12a739fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12a73a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12a73a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12a73aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12a73b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12a73b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12a73bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12a73c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12a73c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12a73cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12a73d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12a73d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12a73dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12a73e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12a73e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12a73eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12a73efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12a73f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12a73f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12a73fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12a7403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12a7408e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12a740e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12a741440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12a7419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12a741fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12a7425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12a742bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12a7431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12a7439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12a743e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12a744120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12a744730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12a744d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12a745530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12a7459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12a745e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12a746310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12a746ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12a747010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12a747560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12a747ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12a748000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12a748550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12a748aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12a748ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12a749540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12a749a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12a749fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12a74a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12a74aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12a74afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12a74b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12a74ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12a74bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12a74c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12a74ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12a74cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12a74d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12a74da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12a74dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12a74e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12a74ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12a74ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12a74f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12a74fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12a74ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12a7504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12a750a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12a750f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12a7514c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12a751a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12a751f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12a7524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12a752a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12a752f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12a7534a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12a7539f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12a753f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12a754490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12a7549e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12a754f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12a755480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12a7559d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12a755f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12a756470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12a7569c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12a756f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12a757460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12a7579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12a757f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12a758450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12a7589a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12a758ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12a759440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12a7598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12a759d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12a75a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12a75a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12a75ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12a75b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12a75b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12a75b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12a75bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12a75c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12a75c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12a75cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12a75d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12a75d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12a75d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12a75de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12a75e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12a75e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12a75ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12a75f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12a75f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12a75fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12a75fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12a760340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12a7607e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12a760d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12a761450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12a761b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12a762290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12a7629b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12a762c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12a763460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12a763720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12a763d30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.739s
user	0m0.280s
sys	0m0.320s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4798 (1782cdfe)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157f105c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157f10cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157f11270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157f11820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157f11dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157f12380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157f12930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x157f12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157f13490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x157f13990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157f13e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157f14390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157f14eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x157f15660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157f15e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x157f16590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x157f16cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x157f173d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x157f17af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x157f182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x157f189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x157f19100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157f19820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x157f1a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157f1a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157f1aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x157f1b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x157f1bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x157f1c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157f1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x157f1c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x157f1cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157f1d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157f1da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157f1dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157f1e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157f1e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157f1eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157f1ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157f1f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157f1f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157f1fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157f20210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157f206b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157f20970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157f20f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157f21590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157f21eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157f224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157f22ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157f230e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157f236f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157f23d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157f24310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157f24b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x157f24fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157f25440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157f25700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157f25d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157f26500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157f267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157f26c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157f27100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157f275a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157f27a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157f27ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157f28380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x157f28820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157f28cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157f29160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x157f29600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x157f29aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x157f29f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x157f2a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x157f2a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x157f2af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x157f2b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x157f2b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x157f2bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x157f2c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x157f2c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x157f2cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x157f2d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x157f2d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x157f2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x157f2e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x157f2e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x157f2eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x157f2f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x157f2f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x157f2fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157f30430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157f30980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157f30ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157f31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157f31970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157f31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157f21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157f32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157f32ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157f33030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157f33580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x157f33ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157f34020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x157f34570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x157f34ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157f35010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157f35560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x157f35ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157f36000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x157f36550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x157f36aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157f36ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157f37490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157f37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157f37dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157f38270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157f38710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157f38bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157f39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157f394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157f39990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157f39e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157f3a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157f3a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157f3ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157f3b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x157f3b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x157f3b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x157f3be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x157f3c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x157f3c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157f3cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157f3d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x157f3d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x157f3da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x157f3def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x157f3e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x157f3e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x157f3ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x157f3f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x157f3f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x157f3fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x157f3ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x157f403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x157f40890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x157f40d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x157f411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157f41670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157f41b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157f41fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157f42450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157f428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157f42d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157f43230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157f436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157f43b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157f44010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157f444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157f44950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157f44df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157f45290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157f45730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157f45bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157f46070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157f46510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157f469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157f46e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157f472f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157f47790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157f47c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157f480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157f48570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157f48a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x157f48eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157f49350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157f497f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157f49c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157f4a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157f4a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157f4aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157f4af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157f4b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x157f4b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x157f4bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x157f4c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x157f4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x157f4cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x157f4cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157f4d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x157f4d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x157f4dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x157f4e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x157f4e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157f4ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x157f4f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x157f4f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x157f4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x157f50000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x157f50610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x157f50c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x157f51410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x157f518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157f51b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157f52180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157f52790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x157f52f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157f53420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157f538c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157f53d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157f54510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157f54a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157f54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157f55500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157f55a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157f55fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157f564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157f56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x157f56f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157f574e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157f57a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157f57f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157f584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157f58a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157f58f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157f594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157f59a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x157f59f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157f5a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157f5aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157f5af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157f5b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157f5b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157f5bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x157f5c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157f5c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157f5cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x157f5d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157f5d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x157f5df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157f5e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x157f5e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x157f5ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x157f5f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x157f5f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x157f5ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x157f60450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157f609a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x157f60ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x157f61440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157f61990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157f61ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x157f62430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x157f62980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157f62ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157f63420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x157f63970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157f63ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157f64410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x157f64960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157f64eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157f65400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157f65950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157f65ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157f663f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157f66940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x157f66e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157f67330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157f677d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157f67c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157f68110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157f685b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157f68a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157f68ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157f69390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157f69830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157f69cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157f6a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157f6a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157f6aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x157f6af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157f6b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x157f6b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x157f6bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x157f6c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x157f6c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x157f6cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x157f6cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x157f6d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x157f6d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x157f6dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x157f6e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x157f6e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x157f6eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x157f6f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157f6fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157f70400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x157f706c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x157f70eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x157f71170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x157f71780 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.096.897 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.902 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157e05bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157e06020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157e06490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157e06900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157e06d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157e071e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157e07650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x157e07ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157e07f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x157e083a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157e08810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157e08e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157e09990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x157e0a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157e0a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x157e0b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x157e0b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x157e0beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x157e0c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x157e0cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x157e0d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x157e0dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157e0e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x157e0ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157e0f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157e0f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x157e0f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x157e0fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x157e0ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157e10410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x157e10880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x157e10db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157e11220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157e114e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157e11950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157e11dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157e12230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157e126a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157e12b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157e12f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157e133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157e13860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157e13cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157e14140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157e145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157e14a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157e14e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157e15300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157e15770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157e15be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157e16050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157e164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157e16930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157e16da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157e17210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x157e17680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157e17bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157e180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157e18560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157e189d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157e18e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157e192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157e19720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157e19b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157e1a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157e1a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157e1a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x157e1ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157e1b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157e1b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x157e1baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x157e1bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x157e1c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x157e1c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x157e1cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x157e1d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x157e1d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x157e1d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x157e1de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x157e1e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x157e1e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x157e1eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x157e1efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x157e1f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x157e1f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x157e1fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x157e201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x157e20610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x157e20a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x157e20ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x157e21360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157e217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157e21c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157e220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157e22520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157e22990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157e22e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157e23270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157e236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157e23b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157e23fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157e24430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x157e248a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157e24d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x157e25180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x157e255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157e25a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157e25ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x157e26340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157e267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x157e26c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x157e27090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157e27500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157e27970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157e27de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157e28250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157e286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157e28b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157e28fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157e29410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157e29880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157e29cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157e2a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157e2a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157e2aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157e2aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157e2b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x157e2b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x157e2bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x157e2c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x157e2c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x157e2c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157e2cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157e2d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x157e2d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x157e2db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x157e2df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x157e2e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x157e2e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x157e2ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x157e2f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x157e2f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x157e2fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x157e2fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x157e30300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x157e30770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x157e30be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x157e31050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157e314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157e31930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157e31da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157e32210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157e32680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157e32af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157e32f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157e333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157e33840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157e33cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157e34120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157e34590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157e34a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157e34e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157e352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157e35750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157e35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157e36030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157e367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157e36aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157e36f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157e37380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157e377f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157e37c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157e380d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157e38540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x157e389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157e38e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157e39290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157e39700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157e39b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157e39fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157e3a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157e3a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157e3ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x157e3b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x157e3b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x157e3ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x157e3bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x157e3c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x157e3c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157e3cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x157e3d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x157e3d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x157e3d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x157e3de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157e3e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x157e3e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x157e3eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x157e3efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x157e3f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x157e3f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x157e3fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x157e40180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x157e405f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157e40a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157e40ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157e41340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x157e417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157e41c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157e42090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157e42500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157e430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157e43370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157e43630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157e43aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157e43f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157e44380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157e447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157e44c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x157e450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157e45540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157e459b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157e45e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157e46290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157e46700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157e46b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157e46fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157e47450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x157e478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157e47d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157e481a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157e48610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157e48a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157e48ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157e49360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x157e497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157e49c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157e4a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x157e4a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157e4a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x157e4ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157e4b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x157e4b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x157e4bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x157e4bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x157e4c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x157e4c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x157e4cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157e4d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x157e4d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x157e4da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157e4ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157e4e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x157e4e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x157e4ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157e4f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157e4f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x157e4f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157e4fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157e50250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x157e506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157e50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157e50fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157e51410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157e51880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157e51cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157e52160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x157e525d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157e52a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157e52eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157e53320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157e53790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157e53c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157e54070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157e544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157e54950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157e54dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157e55230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157e556a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157e55b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157e55f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x157e563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157e56860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x157e56cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x157e57140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x157e575b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x157e57a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x157e57e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x157e58300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x157e58770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x157e58be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x157e59050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x157e594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x157e59930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x157e5a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x157e5ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157e5b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157e5b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x157e5bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x157e5bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x157e5c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x157e5c7a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x157f71430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157f51e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157f4fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157f508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x157f239b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157f233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157f259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x157f52440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157f1ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x157f21850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157f22170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157f22780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x157f20c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x157f22d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x157f19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x157f25fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x157f325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x157f70980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x157f1cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x157f1d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x157f52a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x157f50ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x157f1b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x157f1b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x157f1b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x157f71be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x157f71ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x157f72160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x157f72420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x157f726e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x157f729a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x157f72c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x157f72f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157f731e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x157f734a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157f73760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157f73a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x157f73ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157f73fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157f74260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157f74520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x157f747e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157f74aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157f74d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157f75020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157f752e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157f755a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157f75860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x157f75b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157f75de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157f760a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157f76360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157f76620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x157f768e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157f76ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x157f76e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157f77120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157f773e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x157f776a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157f77960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157f77c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157f77ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157f781a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x157f78460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157f78720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x157f789e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157f78ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x157f78f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157f79220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x157f794e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x157f797a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x157f79a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x157f79d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x157f79fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x157f7a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x157f7a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x157f7a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x157f7aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x157f7ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x157f7b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x157f7b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x157f7b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x157f7b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x157f7bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x157f7be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x157f7c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x157f7c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x157f7c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x157f7c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x157f7cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x157f7cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157f7d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157f7d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157f7d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157f7d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x157f7dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157f7df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157f7e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157f7e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x157f7e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157f7ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157f7ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x157f7efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157f7f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x157f7f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x157f7f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157f7faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157f7fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x157f80020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157f802e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x157f805a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x157f80860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x157f80b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157f80de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157f810a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157f81360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157f81620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x157f818e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157f81ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x157f81e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157f82120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157f823e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157f826a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157f82960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157f82c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157f82ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x157f831a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x157f83460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x157f83720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x157f839e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x157f83ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x157f83f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x157f84220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x157f844e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x157f847a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x157f84a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x157f84d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x157f84fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x157f852a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x157f85560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x157f85820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x157f85ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x157f85da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x157f86060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x157f86320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x157f865e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x157f868a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x157f86b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157f86e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x157f870e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157f873a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x157f87660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157f87920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x157f87be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157f87ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157f88160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157f88420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157f886e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157f889a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157f88c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x157f88f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157f891e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x157f894a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157f89760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157f89a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157f89ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157f89fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x157f8a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157f8a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x157f8a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157f8aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157f8ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157f8b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157f8b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x157f8b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157f8b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157f8bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157f8bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x157f8c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157f8c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157f8c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x157f8c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157f8cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x157f8ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x157f8d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x157f8d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x157f8d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x157f8d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x157f8dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x157f8dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x157f8e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x157f8e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x157f8e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x157f8e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x157f8eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x157f8ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x157f8f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x157f8f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x157f8f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x157f8fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x157f8fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x157f8ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x157f902a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x157f90560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x157f90820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157f90ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x157f90da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157f91060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157f91320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157f915e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157f91bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157f91e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157f92130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x157f923f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157f926b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157f92970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x157f92c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157f92ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x157f93440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157f93990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x157f93ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157f94430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157f94980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157f94ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157f95420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157f95970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157f95ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x157f96410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157f96960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157f96eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157f97400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157f97950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157f97ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157f983f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x157f98940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157f98e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157f993e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x157f99930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157f99e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x157f9a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157f9a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x157f9ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x157f9b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x157f9b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x157f9be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x157f9c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x157f9c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x157f9ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x157f9d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x157f9d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x157f9de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x157f9e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x157f9e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x157f9ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x157f9f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x157f9f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x157f9fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x157fa0370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x157fa08c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x157fa0e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x157fa1360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x157fa18b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157fa1e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157fa2350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x157fa28a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157fa2df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x157fa3340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157fa3600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157fa38c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157fa3b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157fa3ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157fa4460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157fa48d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x157fa4d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157fa51b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157fa5620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157fa5a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x157fa5f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157fa6370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157fa67e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x157fa6c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157fa70c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x157fa7530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x157fa79a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x157fa7e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x157fa8280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x157fa86f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x157fa8b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x157fa8fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x157fa9440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x157fa98b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x157fa9d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x157faa190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x157faabf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x157fab310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157faba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x157fac150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x157fac410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x157fac880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x157face80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x157fad490 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.950s
user	0m0.229s
sys	0m0.185s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
