Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.8s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.033s
user	0m1.015s
sys	0m1.452s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Built target build_info
[  4%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target xxhash
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Built target ggml-cpu
[ 12%] Built target ggml-blas
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Built target llama-gguf-hash
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 30%] Linking CXX executable ../../bin/llama-simple-chat
[ 30%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llama-simple
[ 35%] Built target test-c
[ 35%] Built target llama-quantize-stats
[ 35%] Built target llama-simple-chat
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-sampling
[ 44%] Linking CXX executable ../bin/test-chat
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-chat
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-sampling
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-grammar-integration
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Built target test-log
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-arg-parser
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-gguf
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Linking CXX executable ../bin/test-quantize-fns
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Built target test-arg-parser
[ 61%] Built target test-gguf
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Built target test-autorelease
[ 62%] Built target test-backend-ops
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-quantize-fns
[ 62%] Built target test-model-load-cancel
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-barrier
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Linking CXX executable ../bin/test-rope
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Built target test-quantize-perf
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target test-rope
[ 71%] Built target llama-batched-bench
[ 71%] Built target llama-batched
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-gbnf-validator
[ 71%] Built target llama-embedding
[ 71%] Built target llama-gguf-split
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-gritlm
[ 71%] Built target llama-imatrix
[ 71%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-bench
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookahead
[ 79%] Linking CXX executable ../../bin/llama-lookup
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 79%] Built target llama-bench
[ 79%] Built target llama-infill
[ 79%] Built target llama-lookup
[ 79%] Built target llama-lookahead
[ 79%] Built target llama-lookup-create
[ 79%] Built target llama-lookup-stats
[ 79%] Built target llama-parallel
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 79%] Built target llama-cli
[ 79%] Built target llama-lookup-merge
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Generating loading.html.hpp
[ 81%] Generating index.html.gz.hpp
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-passkey
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 82%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Linking CXX executable ../../bin/llama-speculative-simple
[ 86%] Linking CXX executable ../../bin/llama-run
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Built target llama-quantize
[ 88%] Built target llama-retrieval
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 89%] Built target llama-perplexity
[ 89%] Built target llama-save-load-state
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Built target llama-tokenize
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-speculative-simple
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-speculative
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Built target llama-run
[ 93%] Built target llama-tts
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 96%] Built target llama-gen-docs
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-export-lora
[ 98%] Built target llama-cvector-generator
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-llava-cli
[ 98%] Built target llama-qwen2vl-cli
[ 98%] Built target llama-minicpmv-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.291s
user	0m6.393s
sys	0m9.818s

main: quantize time =  3564.88 ms
main:    total time =  3564.88 ms

main: quantize time =  1992.03 ms
main:    total time =  1992.03 ms

main: quantize time =  2085.49 ms
main:    total time =  2085.49 ms

main: quantize time =  1727.09 ms
main:    total time =  1727.09 ms

main: quantize time =  4024.91 ms
main:    total time =  4024.91 ms

main: quantize time =  5259.28 ms
main:    total time =  5259.28 ms

main: quantize time =  6369.65 ms
main:    total time =  6369.65 ms

main: quantize time =  7194.24 ms
main:    total time =  7194.24 ms

main: quantize time =  6459.91 ms
main:    total time =  6459.91 ms

main: quantize time =  4595.59 ms
main:    total time =  4595.59 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.183 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.355 I main: llama backend init
0.00.000.361 I main: load the model and apply lora adapter, if any
0.00.045.680 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.058.375 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.058.395 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.058.400 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.058.400 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.058.401 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.058.402 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.058.402 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.058.406 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.058.407 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.058.408 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.058.408 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.058.409 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.058.409 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.058.410 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.058.415 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.058.416 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.058.416 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.067.158 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.069.425 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.077.347 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.077.350 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.077.351 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.077.352 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.077.352 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.077.353 I llama_model_loader: - type  f32:  194 tensors
0.00.077.354 I llama_model_loader: - type  f16:   98 tensors
0.00.077.358 I print_info: file format = GGUF V3 (latest)
0.00.077.360 I print_info: file type   = all F32 (guessed)
0.00.077.361 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.091.828 I load: special tokens cache size = 25
0.00.100.336 I load: token to piece cache size = 0.2984 MB
0.00.100.339 I print_info: arch             = gptneox
0.00.100.339 I print_info: vocab_only       = 0
0.00.100.339 I print_info: n_ctx_train      = 2048
0.00.100.340 I print_info: n_embd           = 2048
0.00.100.340 I print_info: n_layer          = 24
0.00.100.343 I print_info: n_head           = 16
0.00.100.344 I print_info: n_head_kv        = 16
0.00.100.344 I print_info: n_rot            = 32
0.00.100.344 I print_info: n_swa            = 0
0.00.100.345 I print_info: n_embd_head_k    = 128
0.00.100.345 I print_info: n_embd_head_v    = 128
0.00.100.346 I print_info: n_gqa            = 1
0.00.100.347 I print_info: n_embd_k_gqa     = 2048
0.00.100.347 I print_info: n_embd_v_gqa     = 2048
0.00.100.348 I print_info: f_norm_eps       = 1.0e-05
0.00.100.348 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.100.348 I print_info: f_clamp_kqv      = 0.0e+00
0.00.100.351 I print_info: f_max_alibi_bias = 0.0e+00
0.00.100.351 I print_info: f_logit_scale    = 0.0e+00
0.00.100.352 I print_info: n_ff             = 8192
0.00.100.352 I print_info: n_expert         = 0
0.00.100.353 I print_info: n_expert_used    = 0
0.00.100.353 I print_info: causal attn      = 1
0.00.100.353 I print_info: pooling type     = 0
0.00.100.353 I print_info: rope type        = 2
0.00.100.354 I print_info: rope scaling     = linear
0.00.100.354 I print_info: freq_base_train  = 10000.0
0.00.100.355 I print_info: freq_scale_train = 1
0.00.100.355 I print_info: n_ctx_orig_yarn  = 2048
0.00.100.356 I print_info: rope_finetuned   = unknown
0.00.100.356 I print_info: ssm_d_conv       = 0
0.00.100.356 I print_info: ssm_d_inner      = 0
0.00.100.356 I print_info: ssm_d_state      = 0
0.00.100.357 I print_info: ssm_dt_rank      = 0
0.00.100.357 I print_info: ssm_dt_b_c_rms   = 0
0.00.100.357 I print_info: model type       = 1.4B
0.00.100.358 I print_info: model params     = 1.41 B
0.00.100.358 I print_info: general.name     = 1.4B
0.00.100.360 I print_info: vocab type       = BPE
0.00.100.360 I print_info: n_vocab          = 50304
0.00.100.360 I print_info: n_merges         = 50009
0.00.100.360 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.100.361 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.100.361 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.100.361 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.100.361 I print_info: LF token         = 187 'Ċ'
0.00.100.362 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.100.362 I print_info: max token length = 1024
0.00.100.362 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.153.671 I load_tensors: offloading 24 repeating layers to GPU
0.00.153.675 I load_tensors: offloading output layer to GPU
0.00.153.675 I load_tensors: offloaded 25/25 layers to GPU
0.00.153.701 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.153.703 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.154.166 I llama_init_from_model: n_seq_max     = 1
0.00.154.167 I llama_init_from_model: n_ctx         = 2048
0.00.154.167 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.154.167 I llama_init_from_model: n_batch       = 2048
0.00.154.167 I llama_init_from_model: n_ubatch      = 512
0.00.154.167 I llama_init_from_model: flash_attn    = 0
0.00.154.168 I llama_init_from_model: freq_base     = 10000.0
0.00.154.168 I llama_init_from_model: freq_scale    = 1
0.00.154.171 I ggml_metal_init: allocating
0.00.154.233 I ggml_metal_init: found device: Apple M4
0.00.154.244 I ggml_metal_init: picking default device: Apple M4
0.00.155.004 I ggml_metal_init: using embedded metal library
0.00.164.761 I ggml_metal_init: GPU name:   Apple M4
0.00.164.762 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.164.763 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.164.763 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.164.763 I ggml_metal_init: simdgroup reduction   = true
0.00.164.764 I ggml_metal_init: simdgroup matrix mul. = true
0.00.164.764 I ggml_metal_init: has residency sets    = true
0.00.164.764 I ggml_metal_init: has bfloat            = true
0.00.164.764 I ggml_metal_init: use bfloat            = true
0.00.164.764 I ggml_metal_init: hasUnifiedMemory      = true
0.00.164.765 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.217.236 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.245.714 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.245.721 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.245.763 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.249.816 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.249.818 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.249.819 I llama_init_from_model: graph nodes  = 967
0.00.249.819 I llama_init_from_model: graph splits = 2
0.00.249.826 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.249.955 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.249.956 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.315.938 I main: llama threadpool init, n_threads = 4
0.00.315.980 I 
0.00.316.011 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.316.012 I 
0.00.316.193 I sampler seed: 1234
0.00.316.199 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.316.223 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.316.225 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.316.225 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.154.425 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54783.95 tokens per second)
0.02.154.426 I llama_perf_context_print:        load time =     269.38 ms
0.02.154.427 I llama_perf_context_print: prompt eval time =      43.69 ms /     7 tokens (    6.24 ms per token,   160.24 tokens per second)
0.02.154.428 I llama_perf_context_print:        eval time =    1791.64 ms /    63 runs   (   28.44 ms per token,    35.16 tokens per second)
0.02.154.429 I llama_perf_context_print:       total time =    1839.36 ms /    70 tokens
0.02.154.721 I ggml_metal_free: deallocating

real	0m2.473s
user	0m0.133s
sys	0m0.149s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.097 I main: llama backend init
0.00.000.099 I main: load the model and apply lora adapter, if any
0.00.009.861 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.187 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.194 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.196 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.197 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.197 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.197 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.198 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.199 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.199 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.200 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.200 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.200 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.201 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.201 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.203 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.203 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.204 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.990 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.039 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.788 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.789 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.790 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.790 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.790 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.791 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.791 I llama_model_loader: - type  f32:  194 tensors
0.00.026.792 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.793 I print_info: file format = GGUF V3 (latest)
0.00.026.794 I print_info: file type   = Q8_0
0.00.026.795 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.241 I load: special tokens cache size = 25
0.00.041.393 I load: token to piece cache size = 0.2984 MB
0.00.041.397 I print_info: arch             = gptneox
0.00.041.398 I print_info: vocab_only       = 0
0.00.041.398 I print_info: n_ctx_train      = 2048
0.00.041.400 I print_info: n_embd           = 2048
0.00.041.400 I print_info: n_layer          = 24
0.00.041.407 I print_info: n_head           = 16
0.00.041.408 I print_info: n_head_kv        = 16
0.00.041.408 I print_info: n_rot            = 32
0.00.041.408 I print_info: n_swa            = 0
0.00.041.408 I print_info: n_embd_head_k    = 128
0.00.041.408 I print_info: n_embd_head_v    = 128
0.00.041.409 I print_info: n_gqa            = 1
0.00.041.411 I print_info: n_embd_k_gqa     = 2048
0.00.041.411 I print_info: n_embd_v_gqa     = 2048
0.00.041.412 I print_info: f_norm_eps       = 1.0e-05
0.00.041.413 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.413 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.413 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.413 I print_info: f_logit_scale    = 0.0e+00
0.00.041.414 I print_info: n_ff             = 8192
0.00.041.414 I print_info: n_expert         = 0
0.00.041.414 I print_info: n_expert_used    = 0
0.00.041.415 I print_info: causal attn      = 1
0.00.041.415 I print_info: pooling type     = 0
0.00.041.415 I print_info: rope type        = 2
0.00.041.415 I print_info: rope scaling     = linear
0.00.041.416 I print_info: freq_base_train  = 10000.0
0.00.041.416 I print_info: freq_scale_train = 1
0.00.041.419 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.419 I print_info: rope_finetuned   = unknown
0.00.041.419 I print_info: ssm_d_conv       = 0
0.00.041.419 I print_info: ssm_d_inner      = 0
0.00.041.419 I print_info: ssm_d_state      = 0
0.00.041.419 I print_info: ssm_dt_rank      = 0
0.00.041.420 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.420 I print_info: model type       = 1.4B
0.00.041.420 I print_info: model params     = 1.41 B
0.00.041.420 I print_info: general.name     = 1.4B
0.00.041.421 I print_info: vocab type       = BPE
0.00.041.421 I print_info: n_vocab          = 50304
0.00.041.421 I print_info: n_merges         = 50009
0.00.041.422 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.422 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.422 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.422 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.422 I print_info: LF token         = 187 'Ċ'
0.00.041.423 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.423 I print_info: max token length = 1024
0.00.041.423 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.046.599 I load_tensors: offloading 24 repeating layers to GPU
0.01.046.605 I load_tensors: offloading output layer to GPU
0.01.046.607 I load_tensors: offloaded 25/25 layers to GPU
0.01.046.628 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.046.630 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.047.516 I llama_init_from_model: n_seq_max     = 1
0.01.047.517 I llama_init_from_model: n_ctx         = 2048
0.01.047.517 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.047.518 I llama_init_from_model: n_batch       = 2048
0.01.047.518 I llama_init_from_model: n_ubatch      = 512
0.01.047.518 I llama_init_from_model: flash_attn    = 0
0.01.047.519 I llama_init_from_model: freq_base     = 10000.0
0.01.047.520 I llama_init_from_model: freq_scale    = 1
0.01.047.521 I ggml_metal_init: allocating
0.01.047.539 I ggml_metal_init: found device: Apple M4
0.01.047.547 I ggml_metal_init: picking default device: Apple M4
0.01.048.878 I ggml_metal_init: using embedded metal library
0.01.054.093 I ggml_metal_init: GPU name:   Apple M4
0.01.054.096 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.054.097 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.054.098 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.054.098 I ggml_metal_init: simdgroup reduction   = true
0.01.054.098 I ggml_metal_init: simdgroup matrix mul. = true
0.01.054.099 I ggml_metal_init: has residency sets    = true
0.01.054.099 I ggml_metal_init: has bfloat            = true
0.01.054.099 I ggml_metal_init: use bfloat            = true
0.01.054.100 I ggml_metal_init: hasUnifiedMemory      = true
0.01.054.101 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.070.875 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.130.643 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.130.649 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.130.726 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.135.316 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.135.318 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.135.318 I llama_init_from_model: graph nodes  = 967
0.01.135.318 I llama_init_from_model: graph splits = 2
0.01.135.324 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.135.452 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.135.453 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.193.236 I main: llama threadpool init, n_threads = 4
0.01.193.276 I 
0.01.193.300 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.193.302 I 
0.01.193.453 I sampler seed: 1234
0.01.193.457 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.193.468 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.193.468 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.193.470 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.289.613 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50283.29 tokens per second)
0.02.289.614 I llama_perf_context_print:        load time =    1182.66 ms
0.02.289.614 I llama_perf_context_print: prompt eval time =      48.39 ms /     7 tokens (    6.91 ms per token,   144.66 tokens per second)
0.02.289.615 I llama_perf_context_print:        eval time =    1045.00 ms /    63 runs   (   16.59 ms per token,    60.29 tokens per second)
0.02.289.617 I llama_perf_context_print:       total time =    1097.09 ms /    70 tokens
0.02.289.893 I ggml_metal_free: deallocating

real	0m2.310s
user	0m0.109s
sys	0m0.280s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.011.275 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.468 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.474 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.476 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.477 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.477 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.478 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.478 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.479 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.479 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.480 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.480 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.480 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.481 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.481 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.483 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.483 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.484 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.361 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.376 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.190 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.191 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.191 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.192 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.192 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.192 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.193 I llama_model_loader: - type  f32:  194 tensors
0.00.028.193 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.194 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.194 I print_info: file format = GGUF V3 (latest)
0.00.028.195 I print_info: file type   = Q4_0
0.00.028.196 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.507 I load: special tokens cache size = 25
0.00.042.505 I load: token to piece cache size = 0.2984 MB
0.00.042.509 I print_info: arch             = gptneox
0.00.042.509 I print_info: vocab_only       = 0
0.00.042.509 I print_info: n_ctx_train      = 2048
0.00.042.510 I print_info: n_embd           = 2048
0.00.042.510 I print_info: n_layer          = 24
0.00.042.514 I print_info: n_head           = 16
0.00.042.515 I print_info: n_head_kv        = 16
0.00.042.515 I print_info: n_rot            = 32
0.00.042.515 I print_info: n_swa            = 0
0.00.042.515 I print_info: n_embd_head_k    = 128
0.00.042.515 I print_info: n_embd_head_v    = 128
0.00.042.516 I print_info: n_gqa            = 1
0.00.042.517 I print_info: n_embd_k_gqa     = 2048
0.00.042.517 I print_info: n_embd_v_gqa     = 2048
0.00.042.518 I print_info: f_norm_eps       = 1.0e-05
0.00.042.518 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.518 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.519 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.519 I print_info: f_logit_scale    = 0.0e+00
0.00.042.519 I print_info: n_ff             = 8192
0.00.042.520 I print_info: n_expert         = 0
0.00.042.520 I print_info: n_expert_used    = 0
0.00.042.520 I print_info: causal attn      = 1
0.00.042.520 I print_info: pooling type     = 0
0.00.042.522 I print_info: rope type        = 2
0.00.042.522 I print_info: rope scaling     = linear
0.00.042.522 I print_info: freq_base_train  = 10000.0
0.00.042.523 I print_info: freq_scale_train = 1
0.00.042.525 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.525 I print_info: rope_finetuned   = unknown
0.00.042.525 I print_info: ssm_d_conv       = 0
0.00.042.526 I print_info: ssm_d_inner      = 0
0.00.042.526 I print_info: ssm_d_state      = 0
0.00.042.526 I print_info: ssm_dt_rank      = 0
0.00.042.526 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.526 I print_info: model type       = 1.4B
0.00.042.527 I print_info: model params     = 1.41 B
0.00.042.527 I print_info: general.name     = 1.4B
0.00.042.527 I print_info: vocab type       = BPE
0.00.042.528 I print_info: n_vocab          = 50304
0.00.042.528 I print_info: n_merges         = 50009
0.00.042.528 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.528 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.528 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.532 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.533 I print_info: LF token         = 187 'Ċ'
0.00.042.533 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.533 I print_info: max token length = 1024
0.00.042.533 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.617.930 I load_tensors: offloading 24 repeating layers to GPU
0.00.617.940 I load_tensors: offloading output layer to GPU
0.00.617.941 I load_tensors: offloaded 25/25 layers to GPU
0.00.617.974 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.617.975 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.619.432 I llama_init_from_model: n_seq_max     = 1
0.00.619.435 I llama_init_from_model: n_ctx         = 2048
0.00.619.435 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.619.436 I llama_init_from_model: n_batch       = 2048
0.00.619.436 I llama_init_from_model: n_ubatch      = 512
0.00.619.437 I llama_init_from_model: flash_attn    = 0
0.00.619.439 I llama_init_from_model: freq_base     = 10000.0
0.00.619.440 I llama_init_from_model: freq_scale    = 1
0.00.619.442 I ggml_metal_init: allocating
0.00.619.494 I ggml_metal_init: found device: Apple M4
0.00.619.506 I ggml_metal_init: picking default device: Apple M4
0.00.621.312 I ggml_metal_init: using embedded metal library
0.00.626.955 I ggml_metal_init: GPU name:   Apple M4
0.00.626.968 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.626.969 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.626.970 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.626.970 I ggml_metal_init: simdgroup reduction   = true
0.00.626.970 I ggml_metal_init: simdgroup matrix mul. = true
0.00.626.971 I ggml_metal_init: has residency sets    = true
0.00.626.971 I ggml_metal_init: has bfloat            = true
0.00.627.000 I ggml_metal_init: use bfloat            = true
0.00.627.012 I ggml_metal_init: hasUnifiedMemory      = true
0.00.627.020 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.646.659 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.703.396 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.703.404 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.703.449 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.708.392 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.708.394 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.708.394 I llama_init_from_model: graph nodes  = 967
0.00.708.394 I llama_init_from_model: graph splits = 2
0.00.708.400 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.708.524 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.708.524 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.764.371 I main: llama threadpool init, n_threads = 4
0.00.764.418 I 
0.00.764.442 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.764.444 I 
0.00.764.621 I sampler seed: 1234
0.00.764.625 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.764.644 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.764.644 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.764.644 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.442.533 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 49134.95 tokens per second)
0.01.442.534 I llama_perf_context_print:        load time =     752.38 ms
0.01.442.535 I llama_perf_context_print: prompt eval time =      49.10 ms /     7 tokens (    7.01 ms per token,   142.57 tokens per second)
0.01.442.535 I llama_perf_context_print:        eval time =     625.85 ms /    63 runs   (    9.93 ms per token,   100.66 tokens per second)
0.01.442.536 I llama_perf_context_print:       total time =     678.88 ms /    70 tokens
0.01.442.798 I ggml_metal_free: deallocating

real	0m1.461s
user	0m0.110s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.386 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.210 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.214 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.216 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.216 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.217 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.217 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.217 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.218 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.219 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.219 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.219 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.220 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.220 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.220 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.223 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.224 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.224 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.883 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.882 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.539 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.541 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.541 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.541 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.541 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.542 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.542 I llama_model_loader: - type  f32:  194 tensors
0.00.025.543 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.543 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.544 I print_info: file format = GGUF V3 (latest)
0.00.025.544 I print_info: file type   = Q4_1
0.00.025.545 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.296 I load: special tokens cache size = 25
0.00.039.391 I load: token to piece cache size = 0.2984 MB
0.00.039.394 I print_info: arch             = gptneox
0.00.039.395 I print_info: vocab_only       = 0
0.00.039.395 I print_info: n_ctx_train      = 2048
0.00.039.395 I print_info: n_embd           = 2048
0.00.039.395 I print_info: n_layer          = 24
0.00.039.398 I print_info: n_head           = 16
0.00.039.399 I print_info: n_head_kv        = 16
0.00.039.400 I print_info: n_rot            = 32
0.00.039.401 I print_info: n_swa            = 0
0.00.039.401 I print_info: n_embd_head_k    = 128
0.00.039.401 I print_info: n_embd_head_v    = 128
0.00.039.402 I print_info: n_gqa            = 1
0.00.039.403 I print_info: n_embd_k_gqa     = 2048
0.00.039.403 I print_info: n_embd_v_gqa     = 2048
0.00.039.404 I print_info: f_norm_eps       = 1.0e-05
0.00.039.404 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.404 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.404 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.405 I print_info: f_logit_scale    = 0.0e+00
0.00.039.405 I print_info: n_ff             = 8192
0.00.039.406 I print_info: n_expert         = 0
0.00.039.406 I print_info: n_expert_used    = 0
0.00.039.406 I print_info: causal attn      = 1
0.00.039.406 I print_info: pooling type     = 0
0.00.039.408 I print_info: rope type        = 2
0.00.039.409 I print_info: rope scaling     = linear
0.00.039.409 I print_info: freq_base_train  = 10000.0
0.00.039.410 I print_info: freq_scale_train = 1
0.00.039.410 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.410 I print_info: rope_finetuned   = unknown
0.00.039.410 I print_info: ssm_d_conv       = 0
0.00.039.410 I print_info: ssm_d_inner      = 0
0.00.039.410 I print_info: ssm_d_state      = 0
0.00.039.411 I print_info: ssm_dt_rank      = 0
0.00.039.411 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.411 I print_info: model type       = 1.4B
0.00.039.411 I print_info: model params     = 1.41 B
0.00.039.412 I print_info: general.name     = 1.4B
0.00.039.412 I print_info: vocab type       = BPE
0.00.039.412 I print_info: n_vocab          = 50304
0.00.039.412 I print_info: n_merges         = 50009
0.00.039.413 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.413 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.413 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.413 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.414 I print_info: LF token         = 187 'Ċ'
0.00.039.414 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.414 I print_info: max token length = 1024
0.00.039.414 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.685.979 I load_tensors: offloading 24 repeating layers to GPU
0.00.685.994 I load_tensors: offloading output layer to GPU
0.00.685.995 I load_tensors: offloaded 25/25 layers to GPU
0.00.686.028 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.686.029 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.687.753 I llama_init_from_model: n_seq_max     = 1
0.00.687.757 I llama_init_from_model: n_ctx         = 2048
0.00.687.757 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.687.758 I llama_init_from_model: n_batch       = 2048
0.00.687.759 I llama_init_from_model: n_ubatch      = 512
0.00.687.759 I llama_init_from_model: flash_attn    = 0
0.00.687.761 I llama_init_from_model: freq_base     = 10000.0
0.00.687.761 I llama_init_from_model: freq_scale    = 1
0.00.687.763 I ggml_metal_init: allocating
0.00.687.851 I ggml_metal_init: found device: Apple M4
0.00.687.864 I ggml_metal_init: picking default device: Apple M4
0.00.689.806 I ggml_metal_init: using embedded metal library
0.00.695.881 I ggml_metal_init: GPU name:   Apple M4
0.00.695.886 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.695.887 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.695.888 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.695.889 I ggml_metal_init: simdgroup reduction   = true
0.00.695.889 I ggml_metal_init: simdgroup matrix mul. = true
0.00.695.890 I ggml_metal_init: has residency sets    = true
0.00.695.890 I ggml_metal_init: has bfloat            = true
0.00.695.890 I ggml_metal_init: use bfloat            = true
0.00.695.891 I ggml_metal_init: hasUnifiedMemory      = true
0.00.695.893 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.714.632 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.772.439 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.772.448 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.772.488 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.777.223 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.777.226 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.777.226 I llama_init_from_model: graph nodes  = 967
0.00.777.226 I llama_init_from_model: graph splits = 2
0.00.777.231 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.777.360 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.777.360 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.834.299 I main: llama threadpool init, n_threads = 4
0.00.834.342 I 
0.00.834.366 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.834.366 I 
0.00.834.531 I sampler seed: 1234
0.00.834.536 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.834.580 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.834.583 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.834.583 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.559.688 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54573.41 tokens per second)
0.01.559.689 I llama_perf_context_print:        load time =     824.21 ms
0.01.559.689 I llama_perf_context_print: prompt eval time =      49.34 ms /     7 tokens (    7.05 ms per token,   141.86 tokens per second)
0.01.559.690 I llama_perf_context_print:        eval time =     672.96 ms /    63 runs   (   10.68 ms per token,    93.62 tokens per second)
0.01.559.690 I llama_perf_context_print:       total time =     726.09 ms /    70 tokens
0.01.559.919 I ggml_metal_free: deallocating

real	0m1.578s
user	0m0.109s
sys	0m0.222s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.011.298 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.072 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.077 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.078 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.079 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.079 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.079 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.079 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.080 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.081 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.081 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.081 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.082 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.084 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.084 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.088 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.088 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.089 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.839 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.872 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.580 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.581 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.582 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.582 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.582 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.583 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.583 I llama_model_loader: - type  f32:  194 tensors
0.00.027.583 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.584 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.584 I print_info: file format = GGUF V3 (latest)
0.00.027.585 I print_info: file type   = Q5_0
0.00.027.585 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.629 I load: special tokens cache size = 25
0.00.041.453 I load: token to piece cache size = 0.2984 MB
0.00.041.456 I print_info: arch             = gptneox
0.00.041.456 I print_info: vocab_only       = 0
0.00.041.456 I print_info: n_ctx_train      = 2048
0.00.041.456 I print_info: n_embd           = 2048
0.00.041.456 I print_info: n_layer          = 24
0.00.041.459 I print_info: n_head           = 16
0.00.041.460 I print_info: n_head_kv        = 16
0.00.041.460 I print_info: n_rot            = 32
0.00.041.460 I print_info: n_swa            = 0
0.00.041.462 I print_info: n_embd_head_k    = 128
0.00.041.462 I print_info: n_embd_head_v    = 128
0.00.041.463 I print_info: n_gqa            = 1
0.00.041.464 I print_info: n_embd_k_gqa     = 2048
0.00.041.465 I print_info: n_embd_v_gqa     = 2048
0.00.041.465 I print_info: f_norm_eps       = 1.0e-05
0.00.041.465 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.466 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.466 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.466 I print_info: f_logit_scale    = 0.0e+00
0.00.041.467 I print_info: n_ff             = 8192
0.00.041.467 I print_info: n_expert         = 0
0.00.041.467 I print_info: n_expert_used    = 0
0.00.041.467 I print_info: causal attn      = 1
0.00.041.467 I print_info: pooling type     = 0
0.00.041.469 I print_info: rope type        = 2
0.00.041.470 I print_info: rope scaling     = linear
0.00.041.471 I print_info: freq_base_train  = 10000.0
0.00.041.471 I print_info: freq_scale_train = 1
0.00.041.471 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.471 I print_info: rope_finetuned   = unknown
0.00.041.472 I print_info: ssm_d_conv       = 0
0.00.041.472 I print_info: ssm_d_inner      = 0
0.00.041.472 I print_info: ssm_d_state      = 0
0.00.041.472 I print_info: ssm_dt_rank      = 0
0.00.041.472 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.472 I print_info: model type       = 1.4B
0.00.041.477 I print_info: model params     = 1.41 B
0.00.041.477 I print_info: general.name     = 1.4B
0.00.041.478 I print_info: vocab type       = BPE
0.00.041.482 I print_info: n_vocab          = 50304
0.00.041.483 I print_info: n_merges         = 50009
0.00.041.483 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.483 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.484 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.484 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.484 I print_info: LF token         = 187 'Ċ'
0.00.041.484 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.485 I print_info: max token length = 1024
0.00.041.485 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.719.927 I load_tensors: offloading 24 repeating layers to GPU
0.00.719.941 I load_tensors: offloading output layer to GPU
0.00.719.942 I load_tensors: offloaded 25/25 layers to GPU
0.00.719.975 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.719.977 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.721.260 I llama_init_from_model: n_seq_max     = 1
0.00.721.262 I llama_init_from_model: n_ctx         = 2048
0.00.721.263 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.721.263 I llama_init_from_model: n_batch       = 2048
0.00.721.264 I llama_init_from_model: n_ubatch      = 512
0.00.721.264 I llama_init_from_model: flash_attn    = 0
0.00.721.265 I llama_init_from_model: freq_base     = 10000.0
0.00.721.265 I llama_init_from_model: freq_scale    = 1
0.00.721.266 I ggml_metal_init: allocating
0.00.721.281 I ggml_metal_init: found device: Apple M4
0.00.721.292 I ggml_metal_init: picking default device: Apple M4
0.00.722.759 I ggml_metal_init: using embedded metal library
0.00.728.964 I ggml_metal_init: GPU name:   Apple M4
0.00.728.968 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.728.969 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.728.970 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.728.970 I ggml_metal_init: simdgroup reduction   = true
0.00.728.970 I ggml_metal_init: simdgroup matrix mul. = true
0.00.728.970 I ggml_metal_init: has residency sets    = true
0.00.728.971 I ggml_metal_init: has bfloat            = true
0.00.728.971 I ggml_metal_init: use bfloat            = true
0.00.728.972 I ggml_metal_init: hasUnifiedMemory      = true
0.00.728.973 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.745.671 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.798.509 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.798.515 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.798.545 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.803.857 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.803.860 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.803.860 I llama_init_from_model: graph nodes  = 967
0.00.803.860 I llama_init_from_model: graph splits = 2
0.00.803.867 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.803.990 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.803.991 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.863.320 I main: llama threadpool init, n_threads = 4
0.00.863.363 I 
0.00.863.383 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.863.384 I 
0.00.863.554 I sampler seed: 1234
0.00.863.558 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.863.570 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.863.570 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.863.570 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.657.560 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51300.58 tokens per second)
0.01.657.561 I llama_perf_context_print:        load time =     851.30 ms
0.01.657.562 I llama_perf_context_print: prompt eval time =      53.58 ms /     7 tokens (    7.65 ms per token,   130.65 tokens per second)
0.01.657.562 I llama_perf_context_print:        eval time =     737.49 ms /    63 runs   (   11.71 ms per token,    85.43 tokens per second)
0.01.657.563 I llama_perf_context_print:       total time =     794.96 ms /    70 tokens
0.01.657.819 I ggml_metal_free: deallocating

real	0m1.676s
user	0m0.107s
sys	0m0.219s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.834 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.560 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.565 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.566 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.567 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.567 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.567 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.568 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.569 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.569 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.569 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.570 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.570 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.571 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.571 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.576 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.576 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.577 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.305 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.371 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.107 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.108 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.108 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.109 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.109 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.109 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.110 I llama_model_loader: - type  f32:  194 tensors
0.00.025.110 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.110 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.110 I print_info: file format = GGUF V3 (latest)
0.00.025.111 I print_info: file type   = Q5_1
0.00.025.112 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.195 I load: special tokens cache size = 25
0.00.039.412 I load: token to piece cache size = 0.2984 MB
0.00.039.415 I print_info: arch             = gptneox
0.00.039.415 I print_info: vocab_only       = 0
0.00.039.415 I print_info: n_ctx_train      = 2048
0.00.039.415 I print_info: n_embd           = 2048
0.00.039.416 I print_info: n_layer          = 24
0.00.039.418 I print_info: n_head           = 16
0.00.039.419 I print_info: n_head_kv        = 16
0.00.039.419 I print_info: n_rot            = 32
0.00.039.419 I print_info: n_swa            = 0
0.00.039.420 I print_info: n_embd_head_k    = 128
0.00.039.420 I print_info: n_embd_head_v    = 128
0.00.039.421 I print_info: n_gqa            = 1
0.00.039.421 I print_info: n_embd_k_gqa     = 2048
0.00.039.422 I print_info: n_embd_v_gqa     = 2048
0.00.039.424 I print_info: f_norm_eps       = 1.0e-05
0.00.039.425 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.425 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.425 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.425 I print_info: f_logit_scale    = 0.0e+00
0.00.039.426 I print_info: n_ff             = 8192
0.00.039.426 I print_info: n_expert         = 0
0.00.039.426 I print_info: n_expert_used    = 0
0.00.039.426 I print_info: causal attn      = 1
0.00.039.427 I print_info: pooling type     = 0
0.00.039.428 I print_info: rope type        = 2
0.00.039.429 I print_info: rope scaling     = linear
0.00.039.430 I print_info: freq_base_train  = 10000.0
0.00.039.430 I print_info: freq_scale_train = 1
0.00.039.430 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.430 I print_info: rope_finetuned   = unknown
0.00.039.431 I print_info: ssm_d_conv       = 0
0.00.039.431 I print_info: ssm_d_inner      = 0
0.00.039.431 I print_info: ssm_d_state      = 0
0.00.039.431 I print_info: ssm_dt_rank      = 0
0.00.039.431 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.431 I print_info: model type       = 1.4B
0.00.039.432 I print_info: model params     = 1.41 B
0.00.039.432 I print_info: general.name     = 1.4B
0.00.039.436 I print_info: vocab type       = BPE
0.00.039.436 I print_info: n_vocab          = 50304
0.00.039.436 I print_info: n_merges         = 50009
0.00.039.437 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.438 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.438 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.438 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.438 I print_info: LF token         = 187 'Ċ'
0.00.039.438 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.439 I print_info: max token length = 1024
0.00.039.439 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.616.872 I load_tensors: offloading 24 repeating layers to GPU
0.00.616.887 I load_tensors: offloading output layer to GPU
0.00.616.888 I load_tensors: offloaded 25/25 layers to GPU
0.00.616.927 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.616.928 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.618.616 I llama_init_from_model: n_seq_max     = 1
0.00.618.619 I llama_init_from_model: n_ctx         = 2048
0.00.618.620 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.618.620 I llama_init_from_model: n_batch       = 2048
0.00.618.621 I llama_init_from_model: n_ubatch      = 512
0.00.618.621 I llama_init_from_model: flash_attn    = 0
0.00.618.622 I llama_init_from_model: freq_base     = 10000.0
0.00.618.623 I llama_init_from_model: freq_scale    = 1
0.00.618.624 I ggml_metal_init: allocating
0.00.618.647 I ggml_metal_init: found device: Apple M4
0.00.618.656 I ggml_metal_init: picking default device: Apple M4
0.00.620.205 I ggml_metal_init: using embedded metal library
0.00.626.393 I ggml_metal_init: GPU name:   Apple M4
0.00.626.397 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.626.398 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.626.398 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.626.399 I ggml_metal_init: simdgroup reduction   = true
0.00.626.399 I ggml_metal_init: simdgroup matrix mul. = true
0.00.626.399 I ggml_metal_init: has residency sets    = true
0.00.626.400 I ggml_metal_init: has bfloat            = true
0.00.626.400 I ggml_metal_init: use bfloat            = true
0.00.626.401 I ggml_metal_init: hasUnifiedMemory      = true
0.00.626.402 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.147 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.699.972 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.699.983 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.700.024 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.704.325 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.704.327 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.704.327 I llama_init_from_model: graph nodes  = 967
0.00.704.327 I llama_init_from_model: graph splits = 2
0.00.704.333 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.704.464 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.704.465 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.764.832 I main: llama threadpool init, n_threads = 4
0.00.764.879 I 
0.00.764.901 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.764.903 I 
0.00.765.068 I sampler seed: 1234
0.00.765.072 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.765.083 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.765.084 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.765.084 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.615.211 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51263.54 tokens per second)
0.01.615.212 I llama_perf_context_print:        load time =     755.28 ms
0.01.615.212 I llama_perf_context_print: prompt eval time =      51.89 ms /     7 tokens (    7.41 ms per token,   134.90 tokens per second)
0.01.615.213 I llama_perf_context_print:        eval time =     795.23 ms /    63 runs   (   12.62 ms per token,    79.22 tokens per second)
0.01.615.214 I llama_perf_context_print:       total time =     851.09 ms /    70 tokens
0.01.615.404 I ggml_metal_free: deallocating

real	0m1.633s
user	0m0.108s
sys	0m0.216s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.309 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.059 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.064 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.066 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.067 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.067 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.067 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.068 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.069 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.069 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.069 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.070 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.070 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.071 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.071 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.072 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.073 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.073 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.786 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.839 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.550 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.552 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.552 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.552 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.552 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.553 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.553 I llama_model_loader: - type  f32:  194 tensors
0.00.024.554 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.554 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.554 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.555 I print_info: file format = GGUF V3 (latest)
0.00.024.555 I print_info: file type   = Q2_K - Medium
0.00.024.556 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.357 I load: special tokens cache size = 25
0.00.038.390 I load: token to piece cache size = 0.2984 MB
0.00.038.392 I print_info: arch             = gptneox
0.00.038.393 I print_info: vocab_only       = 0
0.00.038.393 I print_info: n_ctx_train      = 2048
0.00.038.393 I print_info: n_embd           = 2048
0.00.038.393 I print_info: n_layer          = 24
0.00.038.397 I print_info: n_head           = 16
0.00.038.397 I print_info: n_head_kv        = 16
0.00.038.398 I print_info: n_rot            = 32
0.00.038.398 I print_info: n_swa            = 0
0.00.038.398 I print_info: n_embd_head_k    = 128
0.00.038.398 I print_info: n_embd_head_v    = 128
0.00.038.399 I print_info: n_gqa            = 1
0.00.038.399 I print_info: n_embd_k_gqa     = 2048
0.00.038.400 I print_info: n_embd_v_gqa     = 2048
0.00.038.401 I print_info: f_norm_eps       = 1.0e-05
0.00.038.406 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.406 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.406 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.407 I print_info: f_logit_scale    = 0.0e+00
0.00.038.409 I print_info: n_ff             = 8192
0.00.038.409 I print_info: n_expert         = 0
0.00.038.409 I print_info: n_expert_used    = 0
0.00.038.410 I print_info: causal attn      = 1
0.00.038.410 I print_info: pooling type     = 0
0.00.038.410 I print_info: rope type        = 2
0.00.038.410 I print_info: rope scaling     = linear
0.00.038.411 I print_info: freq_base_train  = 10000.0
0.00.038.411 I print_info: freq_scale_train = 1
0.00.038.411 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.411 I print_info: rope_finetuned   = unknown
0.00.038.411 I print_info: ssm_d_conv       = 0
0.00.038.411 I print_info: ssm_d_inner      = 0
0.00.038.412 I print_info: ssm_d_state      = 0
0.00.038.415 I print_info: ssm_dt_rank      = 0
0.00.038.415 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.415 I print_info: model type       = 1.4B
0.00.038.415 I print_info: model params     = 1.41 B
0.00.038.416 I print_info: general.name     = 1.4B
0.00.038.417 I print_info: vocab type       = BPE
0.00.038.417 I print_info: n_vocab          = 50304
0.00.038.417 I print_info: n_merges         = 50009
0.00.038.417 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.417 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.417 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.417 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.418 I print_info: LF token         = 187 'Ċ'
0.00.038.418 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.418 I print_info: max token length = 1024
0.00.038.419 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.340.110 I load_tensors: offloading 24 repeating layers to GPU
0.00.340.124 I load_tensors: offloading output layer to GPU
0.00.340.124 I load_tensors: offloaded 25/25 layers to GPU
0.00.340.157 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.340.158 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.341.926 I llama_init_from_model: n_seq_max     = 1
0.00.341.932 I llama_init_from_model: n_ctx         = 2048
0.00.341.933 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.341.933 I llama_init_from_model: n_batch       = 2048
0.00.341.933 I llama_init_from_model: n_ubatch      = 512
0.00.341.934 I llama_init_from_model: flash_attn    = 0
0.00.341.936 I llama_init_from_model: freq_base     = 10000.0
0.00.341.936 I llama_init_from_model: freq_scale    = 1
0.00.341.938 I ggml_metal_init: allocating
0.00.342.044 I ggml_metal_init: found device: Apple M4
0.00.342.057 I ggml_metal_init: picking default device: Apple M4
0.00.343.997 I ggml_metal_init: using embedded metal library
0.00.349.512 I ggml_metal_init: GPU name:   Apple M4
0.00.349.525 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.349.525 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.349.526 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.349.527 I ggml_metal_init: simdgroup reduction   = true
0.00.349.527 I ggml_metal_init: simdgroup matrix mul. = true
0.00.349.527 I ggml_metal_init: has residency sets    = true
0.00.349.528 I ggml_metal_init: has bfloat            = true
0.00.349.528 I ggml_metal_init: use bfloat            = true
0.00.349.532 I ggml_metal_init: hasUnifiedMemory      = true
0.00.349.537 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.370.351 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.427.871 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.427.880 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.427.925 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.432.065 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.432.067 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.432.067 I llama_init_from_model: graph nodes  = 967
0.00.432.067 I llama_init_from_model: graph splits = 2
0.00.432.073 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.432.205 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.432.206 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.487.746 I main: llama threadpool init, n_threads = 4
0.00.487.795 I 
0.00.487.817 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.487.819 I 
0.00.487.980 I sampler seed: 1234
0.00.487.985 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.487.996 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.487.996 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.487.997 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.160.170 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 53992.40 tokens per second)
0.01.160.171 I llama_perf_context_print:        load time =     477.68 ms
0.01.160.172 I llama_perf_context_print: prompt eval time =      35.48 ms /     7 tokens (    5.07 ms per token,   197.27 tokens per second)
0.01.160.173 I llama_perf_context_print:        eval time =     633.96 ms /    63 runs   (   10.06 ms per token,    99.37 tokens per second)
0.01.160.174 I llama_perf_context_print:       total time =     673.18 ms /    70 tokens
0.01.160.400 I ggml_metal_free: deallocating

real	0m1.178s
user	0m0.110s
sys	0m0.165s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.749 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.227 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.232 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.234 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.238 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.239 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.239 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.240 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.241 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.242 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.242 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.243 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.243 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.243 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.244 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.246 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.247 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.247 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.945 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.008 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.652 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.653 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.653 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.654 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.654 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.654 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.655 I llama_model_loader: - type  f32:  194 tensors
0.00.026.655 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.655 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.656 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.656 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.656 I print_info: file format = GGUF V3 (latest)
0.00.026.657 I print_info: file type   = Q3_K - Medium
0.00.026.658 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.433 I load: special tokens cache size = 25
0.00.040.391 I load: token to piece cache size = 0.2984 MB
0.00.040.394 I print_info: arch             = gptneox
0.00.040.394 I print_info: vocab_only       = 0
0.00.040.394 I print_info: n_ctx_train      = 2048
0.00.040.395 I print_info: n_embd           = 2048
0.00.040.395 I print_info: n_layer          = 24
0.00.040.397 I print_info: n_head           = 16
0.00.040.398 I print_info: n_head_kv        = 16
0.00.040.398 I print_info: n_rot            = 32
0.00.040.398 I print_info: n_swa            = 0
0.00.040.398 I print_info: n_embd_head_k    = 128
0.00.040.399 I print_info: n_embd_head_v    = 128
0.00.040.401 I print_info: n_gqa            = 1
0.00.040.402 I print_info: n_embd_k_gqa     = 2048
0.00.040.402 I print_info: n_embd_v_gqa     = 2048
0.00.040.403 I print_info: f_norm_eps       = 1.0e-05
0.00.040.404 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.405 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.406 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.406 I print_info: f_logit_scale    = 0.0e+00
0.00.040.406 I print_info: n_ff             = 8192
0.00.040.407 I print_info: n_expert         = 0
0.00.040.407 I print_info: n_expert_used    = 0
0.00.040.408 I print_info: causal attn      = 1
0.00.040.409 I print_info: pooling type     = 0
0.00.040.409 I print_info: rope type        = 2
0.00.040.410 I print_info: rope scaling     = linear
0.00.040.410 I print_info: freq_base_train  = 10000.0
0.00.040.410 I print_info: freq_scale_train = 1
0.00.040.411 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.411 I print_info: rope_finetuned   = unknown
0.00.040.411 I print_info: ssm_d_conv       = 0
0.00.040.413 I print_info: ssm_d_inner      = 0
0.00.040.413 I print_info: ssm_d_state      = 0
0.00.040.413 I print_info: ssm_dt_rank      = 0
0.00.040.413 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.413 I print_info: model type       = 1.4B
0.00.040.414 I print_info: model params     = 1.41 B
0.00.040.414 I print_info: general.name     = 1.4B
0.00.040.415 I print_info: vocab type       = BPE
0.00.040.415 I print_info: n_vocab          = 50304
0.00.040.415 I print_info: n_merges         = 50009
0.00.040.415 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.415 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.416 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.416 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.416 I print_info: LF token         = 187 'Ċ'
0.00.040.416 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.416 I print_info: max token length = 1024
0.00.040.417 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.441.826 I load_tensors: offloading 24 repeating layers to GPU
0.00.441.840 I load_tensors: offloading output layer to GPU
0.00.441.841 I load_tensors: offloaded 25/25 layers to GPU
0.00.441.874 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.441.876 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.443.513 I llama_init_from_model: n_seq_max     = 1
0.00.443.517 I llama_init_from_model: n_ctx         = 2048
0.00.443.518 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.443.518 I llama_init_from_model: n_batch       = 2048
0.00.443.518 I llama_init_from_model: n_ubatch      = 512
0.00.443.519 I llama_init_from_model: flash_attn    = 0
0.00.443.521 I llama_init_from_model: freq_base     = 10000.0
0.00.443.522 I llama_init_from_model: freq_scale    = 1
0.00.443.525 I ggml_metal_init: allocating
0.00.443.600 I ggml_metal_init: found device: Apple M4
0.00.443.614 I ggml_metal_init: picking default device: Apple M4
0.00.445.571 I ggml_metal_init: using embedded metal library
0.00.451.124 I ggml_metal_init: GPU name:   Apple M4
0.00.451.141 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.451.142 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.451.142 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.451.143 I ggml_metal_init: simdgroup reduction   = true
0.00.451.143 I ggml_metal_init: simdgroup matrix mul. = true
0.00.451.143 I ggml_metal_init: has residency sets    = true
0.00.451.144 I ggml_metal_init: has bfloat            = true
0.00.451.144 I ggml_metal_init: use bfloat            = true
0.00.451.145 I ggml_metal_init: hasUnifiedMemory      = true
0.00.451.150 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.472.001 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.529.560 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.529.568 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.529.606 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.534.019 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.534.021 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.534.021 I llama_init_from_model: graph nodes  = 967
0.00.534.022 I llama_init_from_model: graph splits = 2
0.00.534.028 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.534.168 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.534.168 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.590.028 I main: llama threadpool init, n_threads = 4
0.00.590.069 I 
0.00.590.093 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.590.095 I 
0.00.590.251 I sampler seed: 1234
0.00.590.256 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.590.299 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.590.302 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.590.302 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.334.927 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49615.65 tokens per second)
0.01.334.927 I llama_perf_context_print:        load time =     578.50 ms
0.01.334.928 I llama_perf_context_print: prompt eval time =      49.83 ms /     7 tokens (    7.12 ms per token,   140.48 tokens per second)
0.01.334.929 I llama_perf_context_print:        eval time =     691.81 ms /    63 runs   (   10.98 ms per token,    91.07 tokens per second)
0.01.334.929 I llama_perf_context_print:       total time =     745.67 ms /    70 tokens
0.01.335.141 I ggml_metal_free: deallocating

real	0m1.350s
user	0m0.111s
sys	0m0.183s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.827 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.862 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.867 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.872 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.873 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.875 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.875 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.875 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.876 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.876 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.877 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.880 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.880 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.881 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.882 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.883 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.884 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.884 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.590 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.584 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.222 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.224 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.224 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.224 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.225 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.225 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.225 I llama_model_loader: - type  f32:  194 tensors
0.00.025.226 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.226 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.226 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.227 I print_info: file format = GGUF V3 (latest)
0.00.025.227 I print_info: file type   = Q4_K - Medium
0.00.025.228 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.028 I load: special tokens cache size = 25
0.00.039.080 I load: token to piece cache size = 0.2984 MB
0.00.039.083 I print_info: arch             = gptneox
0.00.039.083 I print_info: vocab_only       = 0
0.00.039.083 I print_info: n_ctx_train      = 2048
0.00.039.083 I print_info: n_embd           = 2048
0.00.039.084 I print_info: n_layer          = 24
0.00.039.086 I print_info: n_head           = 16
0.00.039.087 I print_info: n_head_kv        = 16
0.00.039.087 I print_info: n_rot            = 32
0.00.039.087 I print_info: n_swa            = 0
0.00.039.087 I print_info: n_embd_head_k    = 128
0.00.039.087 I print_info: n_embd_head_v    = 128
0.00.039.088 I print_info: n_gqa            = 1
0.00.039.089 I print_info: n_embd_k_gqa     = 2048
0.00.039.090 I print_info: n_embd_v_gqa     = 2048
0.00.039.091 I print_info: f_norm_eps       = 1.0e-05
0.00.039.091 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.093 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.093 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.093 I print_info: f_logit_scale    = 0.0e+00
0.00.039.094 I print_info: n_ff             = 8192
0.00.039.094 I print_info: n_expert         = 0
0.00.039.094 I print_info: n_expert_used    = 0
0.00.039.095 I print_info: causal attn      = 1
0.00.039.095 I print_info: pooling type     = 0
0.00.039.096 I print_info: rope type        = 2
0.00.039.096 I print_info: rope scaling     = linear
0.00.039.096 I print_info: freq_base_train  = 10000.0
0.00.039.097 I print_info: freq_scale_train = 1
0.00.039.097 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.097 I print_info: rope_finetuned   = unknown
0.00.039.097 I print_info: ssm_d_conv       = 0
0.00.039.097 I print_info: ssm_d_inner      = 0
0.00.039.097 I print_info: ssm_d_state      = 0
0.00.039.097 I print_info: ssm_dt_rank      = 0
0.00.039.098 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.098 I print_info: model type       = 1.4B
0.00.039.098 I print_info: model params     = 1.41 B
0.00.039.099 I print_info: general.name     = 1.4B
0.00.039.099 I print_info: vocab type       = BPE
0.00.039.101 I print_info: n_vocab          = 50304
0.00.039.101 I print_info: n_merges         = 50009
0.00.039.101 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.101 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.101 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.102 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.102 I print_info: LF token         = 187 'Ċ'
0.00.039.102 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.102 I print_info: max token length = 1024
0.00.039.103 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.541.832 I load_tensors: offloading 24 repeating layers to GPU
0.00.541.848 I load_tensors: offloading output layer to GPU
0.00.541.849 I load_tensors: offloaded 25/25 layers to GPU
0.00.541.886 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.541.887 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.543.362 I llama_init_from_model: n_seq_max     = 1
0.00.543.366 I llama_init_from_model: n_ctx         = 2048
0.00.543.366 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.543.367 I llama_init_from_model: n_batch       = 2048
0.00.543.367 I llama_init_from_model: n_ubatch      = 512
0.00.543.367 I llama_init_from_model: flash_attn    = 0
0.00.543.370 I llama_init_from_model: freq_base     = 10000.0
0.00.543.370 I llama_init_from_model: freq_scale    = 1
0.00.543.373 I ggml_metal_init: allocating
0.00.543.453 I ggml_metal_init: found device: Apple M4
0.00.543.466 I ggml_metal_init: picking default device: Apple M4
0.00.545.430 I ggml_metal_init: using embedded metal library
0.00.552.026 I ggml_metal_init: GPU name:   Apple M4
0.00.552.030 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.552.031 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.552.032 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.552.032 I ggml_metal_init: simdgroup reduction   = true
0.00.552.033 I ggml_metal_init: simdgroup matrix mul. = true
0.00.552.033 I ggml_metal_init: has residency sets    = true
0.00.552.033 I ggml_metal_init: has bfloat            = true
0.00.552.033 I ggml_metal_init: use bfloat            = true
0.00.552.034 I ggml_metal_init: hasUnifiedMemory      = true
0.00.552.035 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.570.117 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.624.755 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.624.763 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.624.798 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.628.966 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.628.968 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.628.968 I llama_init_from_model: graph nodes  = 967
0.00.628.969 I llama_init_from_model: graph splits = 2
0.00.628.975 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.629.101 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.629.101 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.687.547 I main: llama threadpool init, n_threads = 4
0.00.687.592 I 
0.00.687.615 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.687.615 I 
0.00.687.768 I sampler seed: 1234
0.00.687.772 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.687.796 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.687.797 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.687.797 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.448.525 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47811.45 tokens per second)
0.01.448.526 I llama_perf_context_print:        load time =     678.01 ms
0.01.448.527 I llama_perf_context_print: prompt eval time =      57.96 ms /     7 tokens (    8.28 ms per token,   120.78 tokens per second)
0.01.448.527 I llama_perf_context_print:        eval time =     699.68 ms /    63 runs   (   11.11 ms per token,    90.04 tokens per second)
0.01.448.529 I llama_perf_context_print:       total time =     761.69 ms /    70 tokens
0.01.448.756 I ggml_metal_free: deallocating

real	0m1.466s
user	0m0.109s
sys	0m0.211s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.011.396 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.818 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.823 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.825 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.825 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.827 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.829 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.829 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.833 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.834 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.834 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.834 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.835 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.835 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.836 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.837 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.837 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.838 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.617 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.646 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.341 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.342 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.342 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.342 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.343 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.343 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.343 I llama_model_loader: - type  f32:  194 tensors
0.00.027.344 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.344 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.344 I print_info: file format = GGUF V3 (latest)
0.00.027.345 I print_info: file type   = Q5_K - Medium
0.00.027.348 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.115 I load: special tokens cache size = 25
0.00.041.084 I load: token to piece cache size = 0.2984 MB
0.00.041.086 I print_info: arch             = gptneox
0.00.041.087 I print_info: vocab_only       = 0
0.00.041.087 I print_info: n_ctx_train      = 2048
0.00.041.087 I print_info: n_embd           = 2048
0.00.041.087 I print_info: n_layer          = 24
0.00.041.090 I print_info: n_head           = 16
0.00.041.091 I print_info: n_head_kv        = 16
0.00.041.091 I print_info: n_rot            = 32
0.00.041.091 I print_info: n_swa            = 0
0.00.041.091 I print_info: n_embd_head_k    = 128
0.00.041.091 I print_info: n_embd_head_v    = 128
0.00.041.092 I print_info: n_gqa            = 1
0.00.041.093 I print_info: n_embd_k_gqa     = 2048
0.00.041.093 I print_info: n_embd_v_gqa     = 2048
0.00.041.094 I print_info: f_norm_eps       = 1.0e-05
0.00.041.094 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.095 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.095 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.095 I print_info: f_logit_scale    = 0.0e+00
0.00.041.096 I print_info: n_ff             = 8192
0.00.041.096 I print_info: n_expert         = 0
0.00.041.096 I print_info: n_expert_used    = 0
0.00.041.096 I print_info: causal attn      = 1
0.00.041.096 I print_info: pooling type     = 0
0.00.041.098 I print_info: rope type        = 2
0.00.041.098 I print_info: rope scaling     = linear
0.00.041.099 I print_info: freq_base_train  = 10000.0
0.00.041.099 I print_info: freq_scale_train = 1
0.00.041.099 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.099 I print_info: rope_finetuned   = unknown
0.00.041.101 I print_info: ssm_d_conv       = 0
0.00.041.101 I print_info: ssm_d_inner      = 0
0.00.041.101 I print_info: ssm_d_state      = 0
0.00.041.101 I print_info: ssm_dt_rank      = 0
0.00.041.102 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.102 I print_info: model type       = 1.4B
0.00.041.102 I print_info: model params     = 1.41 B
0.00.041.102 I print_info: general.name     = 1.4B
0.00.041.103 I print_info: vocab type       = BPE
0.00.041.103 I print_info: n_vocab          = 50304
0.00.041.103 I print_info: n_merges         = 50009
0.00.041.103 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.103 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.104 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.104 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.104 I print_info: LF token         = 187 'Ċ'
0.00.041.104 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.105 I print_info: max token length = 1024
0.00.041.105 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.590.584 I load_tensors: offloading 24 repeating layers to GPU
0.00.590.599 I load_tensors: offloading output layer to GPU
0.00.590.600 I load_tensors: offloaded 25/25 layers to GPU
0.00.590.632 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.590.633 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.591.921 I llama_init_from_model: n_seq_max     = 1
0.00.591.925 I llama_init_from_model: n_ctx         = 2048
0.00.591.925 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.591.926 I llama_init_from_model: n_batch       = 2048
0.00.591.926 I llama_init_from_model: n_ubatch      = 512
0.00.591.926 I llama_init_from_model: flash_attn    = 0
0.00.591.928 I llama_init_from_model: freq_base     = 10000.0
0.00.591.929 I llama_init_from_model: freq_scale    = 1
0.00.591.931 I ggml_metal_init: allocating
0.00.591.977 I ggml_metal_init: found device: Apple M4
0.00.591.990 I ggml_metal_init: picking default device: Apple M4
0.00.593.698 I ggml_metal_init: using embedded metal library
0.00.599.753 I ggml_metal_init: GPU name:   Apple M4
0.00.599.758 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.759 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.760 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.761 I ggml_metal_init: simdgroup reduction   = true
0.00.599.761 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.761 I ggml_metal_init: has residency sets    = true
0.00.599.762 I ggml_metal_init: has bfloat            = true
0.00.599.762 I ggml_metal_init: use bfloat            = true
0.00.599.763 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.773 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.618.486 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.679.591 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.679.600 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.679.632 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.685.346 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.685.348 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.685.348 I llama_init_from_model: graph nodes  = 967
0.00.685.348 I llama_init_from_model: graph splits = 2
0.00.685.355 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.685.485 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.685.486 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.819 I main: llama threadpool init, n_threads = 4
0.00.747.858 I 
0.00.747.883 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.884 I 
0.00.748.023 I sampler seed: 1234
0.00.748.027 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.748.047 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.748.047 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.748.047 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.591.229 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50426.14 tokens per second)
0.01.591.229 I llama_perf_context_print:        load time =     735.67 ms
0.01.591.230 I llama_perf_context_print: prompt eval time =      52.61 ms /     7 tokens (    7.52 ms per token,   133.06 tokens per second)
0.01.591.233 I llama_perf_context_print:        eval time =     788.18 ms /    63 runs   (   12.51 ms per token,    79.93 tokens per second)
0.01.591.234 I llama_perf_context_print:       total time =     844.16 ms /    70 tokens
0.01.591.467 I ggml_metal_free: deallocating

real	0m1.609s
user	0m0.111s
sys	0m0.211s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.427 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.128 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.134 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.136 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.136 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.137 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.137 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.142 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.143 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.144 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.144 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.145 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.145 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.147 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.148 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.150 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.151 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.151 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.857 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.890 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.743 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.744 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.745 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.745 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.745 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.746 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.746 I llama_model_loader: - type  f32:  194 tensors
0.00.025.747 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.748 I print_info: file format = GGUF V3 (latest)
0.00.025.748 I print_info: file type   = Q6_K
0.00.025.750 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.066 I load: special tokens cache size = 25
0.00.040.312 I load: token to piece cache size = 0.2984 MB
0.00.040.316 I print_info: arch             = gptneox
0.00.040.316 I print_info: vocab_only       = 0
0.00.040.317 I print_info: n_ctx_train      = 2048
0.00.040.317 I print_info: n_embd           = 2048
0.00.040.317 I print_info: n_layer          = 24
0.00.040.321 I print_info: n_head           = 16
0.00.040.325 I print_info: n_head_kv        = 16
0.00.040.327 I print_info: n_rot            = 32
0.00.040.327 I print_info: n_swa            = 0
0.00.040.327 I print_info: n_embd_head_k    = 128
0.00.040.327 I print_info: n_embd_head_v    = 128
0.00.040.328 I print_info: n_gqa            = 1
0.00.040.328 I print_info: n_embd_k_gqa     = 2048
0.00.040.329 I print_info: n_embd_v_gqa     = 2048
0.00.040.330 I print_info: f_norm_eps       = 1.0e-05
0.00.040.331 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.331 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.331 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.333 I print_info: f_logit_scale    = 0.0e+00
0.00.040.334 I print_info: n_ff             = 8192
0.00.040.334 I print_info: n_expert         = 0
0.00.040.334 I print_info: n_expert_used    = 0
0.00.040.335 I print_info: causal attn      = 1
0.00.040.335 I print_info: pooling type     = 0
0.00.040.335 I print_info: rope type        = 2
0.00.040.335 I print_info: rope scaling     = linear
0.00.040.335 I print_info: freq_base_train  = 10000.0
0.00.040.336 I print_info: freq_scale_train = 1
0.00.040.336 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.336 I print_info: rope_finetuned   = unknown
0.00.040.336 I print_info: ssm_d_conv       = 0
0.00.040.336 I print_info: ssm_d_inner      = 0
0.00.040.336 I print_info: ssm_d_state      = 0
0.00.040.336 I print_info: ssm_dt_rank      = 0
0.00.040.336 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.337 I print_info: model type       = 1.4B
0.00.040.337 I print_info: model params     = 1.41 B
0.00.040.337 I print_info: general.name     = 1.4B
0.00.040.338 I print_info: vocab type       = BPE
0.00.040.338 I print_info: n_vocab          = 50304
0.00.040.338 I print_info: n_merges         = 50009
0.00.040.338 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.340 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.341 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.341 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.341 I print_info: LF token         = 187 'Ċ'
0.00.040.341 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.341 I print_info: max token length = 1024
0.00.040.342 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.657.176 I load_tensors: offloading 24 repeating layers to GPU
0.00.657.181 I load_tensors: offloading output layer to GPU
0.00.657.182 I load_tensors: offloaded 25/25 layers to GPU
0.00.657.200 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.657.201 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.657.990 I llama_init_from_model: n_seq_max     = 1
0.00.657.994 I llama_init_from_model: n_ctx         = 2048
0.00.657.995 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.657.995 I llama_init_from_model: n_batch       = 2048
0.00.657.996 I llama_init_from_model: n_ubatch      = 512
0.00.657.996 I llama_init_from_model: flash_attn    = 0
0.00.657.997 I llama_init_from_model: freq_base     = 10000.0
0.00.657.998 I llama_init_from_model: freq_scale    = 1
0.00.657.999 I ggml_metal_init: allocating
0.00.658.037 I ggml_metal_init: found device: Apple M4
0.00.658.048 I ggml_metal_init: picking default device: Apple M4
0.00.659.190 I ggml_metal_init: using embedded metal library
0.00.663.493 I ggml_metal_init: GPU name:   Apple M4
0.00.663.501 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.663.502 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.663.502 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.663.503 I ggml_metal_init: simdgroup reduction   = true
0.00.663.503 I ggml_metal_init: simdgroup matrix mul. = true
0.00.663.503 I ggml_metal_init: has residency sets    = true
0.00.663.503 I ggml_metal_init: has bfloat            = true
0.00.663.504 I ggml_metal_init: use bfloat            = true
0.00.663.505 I ggml_metal_init: hasUnifiedMemory      = true
0.00.663.507 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.677.693 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.708.586 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.708.592 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.708.629 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.712.926 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.712.928 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.712.928 I llama_init_from_model: graph nodes  = 967
0.00.712.929 I llama_init_from_model: graph splits = 2
0.00.712.935 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.713.060 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.713.061 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.782.920 I main: llama threadpool init, n_threads = 4
0.00.782.964 I 
0.00.782.988 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.782.989 I 
0.00.783.170 I sampler seed: 1234
0.00.783.175 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.783.186 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.783.186 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.783.186 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.656.258 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53223.39 tokens per second)
0.01.656.259 I llama_perf_context_print:        load time =     772.78 ms
0.01.656.259 I llama_perf_context_print: prompt eval time =      57.77 ms /     7 tokens (    8.25 ms per token,   121.18 tokens per second)
0.01.656.261 I llama_perf_context_print:        eval time =     812.52 ms /    63 runs   (   12.90 ms per token,    77.54 tokens per second)
0.01.656.262 I llama_perf_context_print:       total time =     874.05 ms /    70 tokens
0.01.656.466 I ggml_metal_free: deallocating

real	0m1.675s
user	0m0.105s
sys	0m0.168s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.569 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.014 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.549 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.554 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.556 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.557 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.557 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.558 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.558 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.559 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.560 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.560 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.561 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.561 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.562 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.565 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.567 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.567 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.567 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.501 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.390 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.742 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.744 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.744 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.745 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.745 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.746 I llama_model_loader: - type  f32:  194 tensors
0.00.055.747 I llama_model_loader: - type  f16:   98 tensors
0.00.055.747 I print_info: file format = GGUF V3 (latest)
0.00.055.748 I print_info: file type   = all F32 (guessed)
0.00.055.750 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.672 I load: special tokens cache size = 25
0.00.075.536 I load: token to piece cache size = 0.2984 MB
0.00.075.539 I print_info: arch             = gptneox
0.00.075.540 I print_info: vocab_only       = 0
0.00.075.540 I print_info: n_ctx_train      = 2048
0.00.075.540 I print_info: n_embd           = 2048
0.00.075.540 I print_info: n_layer          = 24
0.00.075.543 I print_info: n_head           = 16
0.00.075.544 I print_info: n_head_kv        = 16
0.00.075.544 I print_info: n_rot            = 32
0.00.075.546 I print_info: n_swa            = 0
0.00.075.546 I print_info: n_embd_head_k    = 128
0.00.075.546 I print_info: n_embd_head_v    = 128
0.00.075.547 I print_info: n_gqa            = 1
0.00.075.548 I print_info: n_embd_k_gqa     = 2048
0.00.075.549 I print_info: n_embd_v_gqa     = 2048
0.00.075.549 I print_info: f_norm_eps       = 1.0e-05
0.00.075.549 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.550 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.550 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.550 I print_info: f_logit_scale    = 0.0e+00
0.00.075.551 I print_info: n_ff             = 8192
0.00.075.551 I print_info: n_expert         = 0
0.00.075.551 I print_info: n_expert_used    = 0
0.00.075.551 I print_info: causal attn      = 1
0.00.075.551 I print_info: pooling type     = 0
0.00.075.551 I print_info: rope type        = 2
0.00.075.551 I print_info: rope scaling     = linear
0.00.075.556 I print_info: freq_base_train  = 10000.0
0.00.075.556 I print_info: freq_scale_train = 1
0.00.075.556 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.557 I print_info: rope_finetuned   = unknown
0.00.075.557 I print_info: ssm_d_conv       = 0
0.00.075.557 I print_info: ssm_d_inner      = 0
0.00.075.557 I print_info: ssm_d_state      = 0
0.00.075.557 I print_info: ssm_dt_rank      = 0
0.00.075.557 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.558 I print_info: model type       = 1.4B
0.00.075.558 I print_info: model params     = 1.41 B
0.00.075.559 I print_info: general.name     = 1.4B
0.00.075.559 I print_info: vocab type       = BPE
0.00.075.559 I print_info: n_vocab          = 50304
0.00.075.559 I print_info: n_merges         = 50009
0.00.075.560 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.560 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.560 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.568 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.572 I print_info: LF token         = 187 'Ċ'
0.00.075.574 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.574 I print_info: max token length = 1024
0.00.075.576 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.455.386 I load_tensors: offloading 24 repeating layers to GPU
0.01.455.388 I load_tensors: offloading output layer to GPU
0.01.455.389 I load_tensors: offloaded 25/25 layers to GPU
0.01.455.413 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.455.414 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.456.527 I llama_init_from_model: n_seq_max     = 1
0.01.456.528 I llama_init_from_model: n_ctx         = 128
0.01.456.528 I llama_init_from_model: n_ctx_per_seq = 128
0.01.456.529 I llama_init_from_model: n_batch       = 128
0.01.456.529 I llama_init_from_model: n_ubatch      = 128
0.01.456.529 I llama_init_from_model: flash_attn    = 0
0.01.456.530 I llama_init_from_model: freq_base     = 10000.0
0.01.456.530 I llama_init_from_model: freq_scale    = 1
0.01.456.530 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.456.534 I ggml_metal_init: allocating
0.01.456.587 I ggml_metal_init: found device: Apple M4
0.01.456.593 I ggml_metal_init: picking default device: Apple M4
0.01.457.671 I ggml_metal_init: using embedded metal library
0.01.461.462 I ggml_metal_init: GPU name:   Apple M4
0.01.461.465 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.461.465 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.461.466 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.461.466 I ggml_metal_init: simdgroup reduction   = true
0.01.461.466 I ggml_metal_init: simdgroup matrix mul. = true
0.01.461.466 I ggml_metal_init: has residency sets    = true
0.01.461.466 I ggml_metal_init: has bfloat            = true
0.01.461.467 I ggml_metal_init: use bfloat            = true
0.01.461.467 I ggml_metal_init: hasUnifiedMemory      = true
0.01.461.468 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.472.599 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.474.350 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.474.353 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.474.378 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.476.066 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.476.067 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.476.068 I llama_init_from_model: graph nodes  = 967
0.01.476.068 I llama_init_from_model: graph splits = 2
0.01.476.070 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.476.070 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.510.577 I 
0.01.510.615 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.510.620 I perplexity: tokenizing the input ..
0.01.515.665 I perplexity: tokenization took 5.043 ms
0.01.515.670 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.633.997 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.635.349 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.635.379 I llama_perf_context_print:        load time =    1485.55 ms
0.01.635.380 I llama_perf_context_print: prompt eval time =     118.06 ms /   128 tokens (    0.92 ms per token,  1084.21 tokens per second)
0.01.635.380 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.635.381 I llama_perf_context_print:       total time =     124.80 ms /   129 tokens
0.01.635.763 I ggml_metal_free: deallocating

real	0m1.823s
user	0m0.096s
sys	0m0.264s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.246 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.312 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.319 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.321 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.326 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.327 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.327 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.327 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.328 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.329 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.329 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.329 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.331 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.331 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.332 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.334 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.334 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.334 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.083 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.104 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.894 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.895 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.896 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.896 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.896 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.897 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.024.897 I llama_model_loader: - type  f32:  194 tensors
0.00.024.898 I llama_model_loader: - type q8_0:   98 tensors
0.00.024.898 I print_info: file format = GGUF V3 (latest)
0.00.024.899 I print_info: file type   = Q8_0
0.00.024.900 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.032.859 I load: special tokens cache size = 25
0.00.038.926 I load: token to piece cache size = 0.2984 MB
0.00.038.930 I print_info: arch             = gptneox
0.00.038.930 I print_info: vocab_only       = 0
0.00.038.931 I print_info: n_ctx_train      = 2048
0.00.038.931 I print_info: n_embd           = 2048
0.00.038.931 I print_info: n_layer          = 24
0.00.038.935 I print_info: n_head           = 16
0.00.038.936 I print_info: n_head_kv        = 16
0.00.038.936 I print_info: n_rot            = 32
0.00.038.937 I print_info: n_swa            = 0
0.00.038.937 I print_info: n_embd_head_k    = 128
0.00.038.937 I print_info: n_embd_head_v    = 128
0.00.038.938 I print_info: n_gqa            = 1
0.00.038.939 I print_info: n_embd_k_gqa     = 2048
0.00.038.939 I print_info: n_embd_v_gqa     = 2048
0.00.038.940 I print_info: f_norm_eps       = 1.0e-05
0.00.038.940 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.940 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.940 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.941 I print_info: f_logit_scale    = 0.0e+00
0.00.038.941 I print_info: n_ff             = 8192
0.00.038.941 I print_info: n_expert         = 0
0.00.038.941 I print_info: n_expert_used    = 0
0.00.038.942 I print_info: causal attn      = 1
0.00.038.942 I print_info: pooling type     = 0
0.00.038.942 I print_info: rope type        = 2
0.00.038.942 I print_info: rope scaling     = linear
0.00.038.942 I print_info: freq_base_train  = 10000.0
0.00.038.943 I print_info: freq_scale_train = 1
0.00.038.943 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.943 I print_info: rope_finetuned   = unknown
0.00.038.943 I print_info: ssm_d_conv       = 0
0.00.038.943 I print_info: ssm_d_inner      = 0
0.00.038.943 I print_info: ssm_d_state      = 0
0.00.038.943 I print_info: ssm_dt_rank      = 0
0.00.038.945 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.945 I print_info: model type       = 1.4B
0.00.038.945 I print_info: model params     = 1.41 B
0.00.038.945 I print_info: general.name     = 1.4B
0.00.038.946 I print_info: vocab type       = BPE
0.00.038.946 I print_info: n_vocab          = 50304
0.00.038.946 I print_info: n_merges         = 50009
0.00.038.947 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.947 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.947 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.947 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.947 I print_info: LF token         = 187 'Ċ'
0.00.038.947 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.948 I print_info: max token length = 1024
0.00.038.950 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.912.113 I load_tensors: offloading 24 repeating layers to GPU
0.00.912.119 I load_tensors: offloading output layer to GPU
0.00.912.120 I load_tensors: offloaded 25/25 layers to GPU
0.00.912.148 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.912.150 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.913.580 I llama_init_from_model: n_seq_max     = 1
0.00.913.583 I llama_init_from_model: n_ctx         = 128
0.00.913.583 I llama_init_from_model: n_ctx_per_seq = 128
0.00.913.584 I llama_init_from_model: n_batch       = 128
0.00.913.584 I llama_init_from_model: n_ubatch      = 128
0.00.913.584 I llama_init_from_model: flash_attn    = 0
0.00.913.586 I llama_init_from_model: freq_base     = 10000.0
0.00.913.586 I llama_init_from_model: freq_scale    = 1
0.00.913.587 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.913.589 I ggml_metal_init: allocating
0.00.913.637 I ggml_metal_init: found device: Apple M4
0.00.913.646 I ggml_metal_init: picking default device: Apple M4
0.00.914.965 I ggml_metal_init: using embedded metal library
0.00.920.087 I ggml_metal_init: GPU name:   Apple M4
0.00.920.091 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.920.091 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.920.092 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.920.093 I ggml_metal_init: simdgroup reduction   = true
0.00.920.093 I ggml_metal_init: simdgroup matrix mul. = true
0.00.920.093 I ggml_metal_init: has residency sets    = true
0.00.920.093 I ggml_metal_init: has bfloat            = true
0.00.920.093 I ggml_metal_init: use bfloat            = true
0.00.920.094 I ggml_metal_init: hasUnifiedMemory      = true
0.00.920.096 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.934.892 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.938.193 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.938.203 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.938.261 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.941.323 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.941.325 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.941.325 I llama_init_from_model: graph nodes  = 967
0.00.941.326 I llama_init_from_model: graph splits = 2
0.00.941.329 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.941.329 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.965.778 I 
0.00.965.836 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.965.841 I perplexity: tokenizing the input ..
0.00.971.981 I perplexity: tokenization took 6.137 ms
0.00.971.988 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.096.010 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.097.339 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.097.365 I llama_perf_context_print:        load time =     956.52 ms
0.01.097.366 I llama_perf_context_print: prompt eval time =     123.47 ms /   128 tokens (    0.96 ms per token,  1036.71 tokens per second)
0.01.097.367 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.097.367 I llama_perf_context_print:       total time =     131.59 ms /   129 tokens
0.01.097.761 I ggml_metal_free: deallocating

real	0m1.112s
user	0m0.074s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.794 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.966 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.971 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.975 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.975 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.976 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.976 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.976 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.977 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.978 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.978 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.979 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.980 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.980 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.980 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.982 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.982 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.983 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.689 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.718 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.464 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.466 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.466 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.467 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.467 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.467 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.468 I llama_model_loader: - type  f32:  194 tensors
0.00.027.468 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.469 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.469 I print_info: file format = GGUF V3 (latest)
0.00.027.470 I print_info: file type   = Q4_0
0.00.027.471 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.389 I load: special tokens cache size = 25
0.00.041.370 I load: token to piece cache size = 0.2984 MB
0.00.041.374 I print_info: arch             = gptneox
0.00.041.374 I print_info: vocab_only       = 0
0.00.041.374 I print_info: n_ctx_train      = 2048
0.00.041.374 I print_info: n_embd           = 2048
0.00.041.375 I print_info: n_layer          = 24
0.00.041.379 I print_info: n_head           = 16
0.00.041.380 I print_info: n_head_kv        = 16
0.00.041.380 I print_info: n_rot            = 32
0.00.041.380 I print_info: n_swa            = 0
0.00.041.380 I print_info: n_embd_head_k    = 128
0.00.041.380 I print_info: n_embd_head_v    = 128
0.00.041.381 I print_info: n_gqa            = 1
0.00.041.382 I print_info: n_embd_k_gqa     = 2048
0.00.041.383 I print_info: n_embd_v_gqa     = 2048
0.00.041.383 I print_info: f_norm_eps       = 1.0e-05
0.00.041.384 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.384 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.384 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.384 I print_info: f_logit_scale    = 0.0e+00
0.00.041.385 I print_info: n_ff             = 8192
0.00.041.385 I print_info: n_expert         = 0
0.00.041.385 I print_info: n_expert_used    = 0
0.00.041.385 I print_info: causal attn      = 1
0.00.041.385 I print_info: pooling type     = 0
0.00.041.385 I print_info: rope type        = 2
0.00.041.386 I print_info: rope scaling     = linear
0.00.041.386 I print_info: freq_base_train  = 10000.0
0.00.041.386 I print_info: freq_scale_train = 1
0.00.041.389 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.389 I print_info: rope_finetuned   = unknown
0.00.041.389 I print_info: ssm_d_conv       = 0
0.00.041.389 I print_info: ssm_d_inner      = 0
0.00.041.389 I print_info: ssm_d_state      = 0
0.00.041.389 I print_info: ssm_dt_rank      = 0
0.00.041.390 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.390 I print_info: model type       = 1.4B
0.00.041.390 I print_info: model params     = 1.41 B
0.00.041.390 I print_info: general.name     = 1.4B
0.00.041.391 I print_info: vocab type       = BPE
0.00.041.391 I print_info: n_vocab          = 50304
0.00.041.391 I print_info: n_merges         = 50009
0.00.041.391 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.391 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.392 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.392 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.392 I print_info: LF token         = 187 'Ċ'
0.00.041.392 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.392 I print_info: max token length = 1024
0.00.041.393 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.633.425 I load_tensors: offloading 24 repeating layers to GPU
0.00.633.439 I load_tensors: offloading output layer to GPU
0.00.633.440 I load_tensors: offloaded 25/25 layers to GPU
0.00.633.473 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.633.475 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.635.097 I llama_init_from_model: n_seq_max     = 1
0.00.635.100 I llama_init_from_model: n_ctx         = 128
0.00.635.101 I llama_init_from_model: n_ctx_per_seq = 128
0.00.635.102 I llama_init_from_model: n_batch       = 128
0.00.635.102 I llama_init_from_model: n_ubatch      = 128
0.00.635.102 I llama_init_from_model: flash_attn    = 0
0.00.635.104 I llama_init_from_model: freq_base     = 10000.0
0.00.635.105 I llama_init_from_model: freq_scale    = 1
0.00.635.105 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.635.108 I ggml_metal_init: allocating
0.00.635.203 I ggml_metal_init: found device: Apple M4
0.00.635.217 I ggml_metal_init: picking default device: Apple M4
0.00.637.046 I ggml_metal_init: using embedded metal library
0.00.643.562 I ggml_metal_init: GPU name:   Apple M4
0.00.643.572 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.643.573 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.643.573 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.643.574 I ggml_metal_init: simdgroup reduction   = true
0.00.643.574 I ggml_metal_init: simdgroup matrix mul. = true
0.00.643.575 I ggml_metal_init: has residency sets    = true
0.00.643.575 I ggml_metal_init: has bfloat            = true
0.00.643.575 I ggml_metal_init: use bfloat            = true
0.00.643.577 I ggml_metal_init: hasUnifiedMemory      = true
0.00.643.581 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.662.064 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.665.642 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.665.646 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.665.689 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.669.126 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.669.127 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.669.128 I llama_init_from_model: graph nodes  = 967
0.00.669.128 I llama_init_from_model: graph splits = 2
0.00.669.131 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.669.131 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.697.834 I 
0.00.697.916 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.697.923 I perplexity: tokenizing the input ..
0.00.705.316 I perplexity: tokenization took 7.389 ms
0.00.705.324 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.840.992 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.842.331 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.842.358 I llama_perf_context_print:        load time =     688.03 ms
0.00.842.359 I llama_perf_context_print: prompt eval time =     134.74 ms /   128 tokens (    1.05 ms per token,   949.96 tokens per second)
0.00.842.359 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.842.360 I llama_perf_context_print:       total time =     144.53 ms /   129 tokens
0.00.842.734 I ggml_metal_free: deallocating

real	0m0.858s
user	0m0.082s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.859 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.784 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.790 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.793 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.794 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.794 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.794 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.795 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.796 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.796 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.796 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.797 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.797 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.800 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.800 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.802 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.802 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.802 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.529 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.561 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.326 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.327 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.328 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.328 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.328 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.329 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.329 I llama_model_loader: - type  f32:  194 tensors
0.00.024.330 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.330 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.331 I print_info: file format = GGUF V3 (latest)
0.00.024.331 I print_info: file type   = Q4_1
0.00.024.332 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.336 I load: special tokens cache size = 25
0.00.038.508 I load: token to piece cache size = 0.2984 MB
0.00.038.512 I print_info: arch             = gptneox
0.00.038.512 I print_info: vocab_only       = 0
0.00.038.512 I print_info: n_ctx_train      = 2048
0.00.038.513 I print_info: n_embd           = 2048
0.00.038.513 I print_info: n_layer          = 24
0.00.038.517 I print_info: n_head           = 16
0.00.038.518 I print_info: n_head_kv        = 16
0.00.038.518 I print_info: n_rot            = 32
0.00.038.518 I print_info: n_swa            = 0
0.00.038.518 I print_info: n_embd_head_k    = 128
0.00.038.518 I print_info: n_embd_head_v    = 128
0.00.038.519 I print_info: n_gqa            = 1
0.00.038.520 I print_info: n_embd_k_gqa     = 2048
0.00.038.520 I print_info: n_embd_v_gqa     = 2048
0.00.038.521 I print_info: f_norm_eps       = 1.0e-05
0.00.038.521 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.521 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.522 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.522 I print_info: f_logit_scale    = 0.0e+00
0.00.038.522 I print_info: n_ff             = 8192
0.00.038.523 I print_info: n_expert         = 0
0.00.038.523 I print_info: n_expert_used    = 0
0.00.038.523 I print_info: causal attn      = 1
0.00.038.523 I print_info: pooling type     = 0
0.00.038.523 I print_info: rope type        = 2
0.00.038.523 I print_info: rope scaling     = linear
0.00.038.524 I print_info: freq_base_train  = 10000.0
0.00.038.524 I print_info: freq_scale_train = 1
0.00.038.527 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.527 I print_info: rope_finetuned   = unknown
0.00.038.527 I print_info: ssm_d_conv       = 0
0.00.038.527 I print_info: ssm_d_inner      = 0
0.00.038.528 I print_info: ssm_d_state      = 0
0.00.038.528 I print_info: ssm_dt_rank      = 0
0.00.038.528 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.528 I print_info: model type       = 1.4B
0.00.038.528 I print_info: model params     = 1.41 B
0.00.038.529 I print_info: general.name     = 1.4B
0.00.038.529 I print_info: vocab type       = BPE
0.00.038.529 I print_info: n_vocab          = 50304
0.00.038.530 I print_info: n_merges         = 50009
0.00.038.531 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.531 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.531 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.532 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.532 I print_info: LF token         = 187 'Ċ'
0.00.038.532 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.533 I print_info: max token length = 1024
0.00.038.533 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.662.721 I load_tensors: offloading 24 repeating layers to GPU
0.00.662.737 I load_tensors: offloading output layer to GPU
0.00.662.738 I load_tensors: offloaded 25/25 layers to GPU
0.00.662.775 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.662.777 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.664.370 I llama_init_from_model: n_seq_max     = 1
0.00.664.373 I llama_init_from_model: n_ctx         = 128
0.00.664.373 I llama_init_from_model: n_ctx_per_seq = 128
0.00.664.374 I llama_init_from_model: n_batch       = 128
0.00.664.374 I llama_init_from_model: n_ubatch      = 128
0.00.664.374 I llama_init_from_model: flash_attn    = 0
0.00.664.377 I llama_init_from_model: freq_base     = 10000.0
0.00.664.377 I llama_init_from_model: freq_scale    = 1
0.00.664.378 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.664.380 I ggml_metal_init: allocating
0.00.664.459 I ggml_metal_init: found device: Apple M4
0.00.664.472 I ggml_metal_init: picking default device: Apple M4
0.00.666.289 I ggml_metal_init: using embedded metal library
0.00.673.158 I ggml_metal_init: GPU name:   Apple M4
0.00.673.166 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.673.167 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.673.168 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.673.169 I ggml_metal_init: simdgroup reduction   = true
0.00.673.169 I ggml_metal_init: simdgroup matrix mul. = true
0.00.673.169 I ggml_metal_init: has residency sets    = true
0.00.673.169 I ggml_metal_init: has bfloat            = true
0.00.673.170 I ggml_metal_init: use bfloat            = true
0.00.673.171 I ggml_metal_init: hasUnifiedMemory      = true
0.00.673.174 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.691.464 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.694.964 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.694.970 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.695.025 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.698.224 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.698.226 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.698.226 I llama_init_from_model: graph nodes  = 967
0.00.698.226 I llama_init_from_model: graph splits = 2
0.00.698.230 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.698.230 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.724.299 I 
0.00.724.372 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.724.378 I perplexity: tokenizing the input ..
0.00.731.714 I perplexity: tokenization took 7.333 ms
0.00.731.723 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.868.426 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.869.763 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.869.784 I llama_perf_context_print:        load time =     715.43 ms
0.00.869.785 I llama_perf_context_print: prompt eval time =     135.76 ms /   128 tokens (    1.06 ms per token,   942.87 tokens per second)
0.00.869.787 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.869.787 I llama_perf_context_print:       total time =     145.49 ms /   129 tokens
0.00.870.151 I ggml_metal_free: deallocating

real	0m0.885s
user	0m0.080s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.909 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.925 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.931 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.933 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.939 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.939 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.939 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.940 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.941 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.941 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.941 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.942 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.942 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.942 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.943 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.944 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.945 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.945 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.672 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.674 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.422 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.424 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.424 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.425 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.425 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.425 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.426 I llama_model_loader: - type  f32:  194 tensors
0.00.025.426 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.426 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.427 I print_info: file format = GGUF V3 (latest)
0.00.025.427 I print_info: file type   = Q5_0
0.00.025.429 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.475 I load: special tokens cache size = 25
0.00.039.557 I load: token to piece cache size = 0.2984 MB
0.00.039.561 I print_info: arch             = gptneox
0.00.039.562 I print_info: vocab_only       = 0
0.00.039.562 I print_info: n_ctx_train      = 2048
0.00.039.562 I print_info: n_embd           = 2048
0.00.039.562 I print_info: n_layer          = 24
0.00.039.567 I print_info: n_head           = 16
0.00.039.567 I print_info: n_head_kv        = 16
0.00.039.568 I print_info: n_rot            = 32
0.00.039.569 I print_info: n_swa            = 0
0.00.039.569 I print_info: n_embd_head_k    = 128
0.00.039.570 I print_info: n_embd_head_v    = 128
0.00.039.572 I print_info: n_gqa            = 1
0.00.039.573 I print_info: n_embd_k_gqa     = 2048
0.00.039.574 I print_info: n_embd_v_gqa     = 2048
0.00.039.574 I print_info: f_norm_eps       = 1.0e-05
0.00.039.575 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.576 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.576 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.576 I print_info: f_logit_scale    = 0.0e+00
0.00.039.577 I print_info: n_ff             = 8192
0.00.039.578 I print_info: n_expert         = 0
0.00.039.578 I print_info: n_expert_used    = 0
0.00.039.578 I print_info: causal attn      = 1
0.00.039.578 I print_info: pooling type     = 0
0.00.039.579 I print_info: rope type        = 2
0.00.039.579 I print_info: rope scaling     = linear
0.00.039.580 I print_info: freq_base_train  = 10000.0
0.00.039.580 I print_info: freq_scale_train = 1
0.00.039.580 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.580 I print_info: rope_finetuned   = unknown
0.00.039.580 I print_info: ssm_d_conv       = 0
0.00.039.580 I print_info: ssm_d_inner      = 0
0.00.039.580 I print_info: ssm_d_state      = 0
0.00.039.582 I print_info: ssm_dt_rank      = 0
0.00.039.582 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.582 I print_info: model type       = 1.4B
0.00.039.582 I print_info: model params     = 1.41 B
0.00.039.582 I print_info: general.name     = 1.4B
0.00.039.583 I print_info: vocab type       = BPE
0.00.039.583 I print_info: n_vocab          = 50304
0.00.039.583 I print_info: n_merges         = 50009
0.00.039.583 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.583 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.584 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.584 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.584 I print_info: LF token         = 187 'Ċ'
0.00.039.584 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.584 I print_info: max token length = 1024
0.00.039.585 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.711.760 I load_tensors: offloading 24 repeating layers to GPU
0.00.711.775 I load_tensors: offloading output layer to GPU
0.00.711.776 I load_tensors: offloaded 25/25 layers to GPU
0.00.711.809 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.711.811 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.713.593 I llama_init_from_model: n_seq_max     = 1
0.00.713.597 I llama_init_from_model: n_ctx         = 128
0.00.713.597 I llama_init_from_model: n_ctx_per_seq = 128
0.00.713.598 I llama_init_from_model: n_batch       = 128
0.00.713.598 I llama_init_from_model: n_ubatch      = 128
0.00.713.598 I llama_init_from_model: flash_attn    = 0
0.00.713.600 I llama_init_from_model: freq_base     = 10000.0
0.00.713.601 I llama_init_from_model: freq_scale    = 1
0.00.713.602 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.713.604 I ggml_metal_init: allocating
0.00.713.643 I ggml_metal_init: found device: Apple M4
0.00.713.654 I ggml_metal_init: picking default device: Apple M4
0.00.715.133 I ggml_metal_init: using embedded metal library
0.00.721.583 I ggml_metal_init: GPU name:   Apple M4
0.00.721.587 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.721.588 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.721.589 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.721.589 I ggml_metal_init: simdgroup reduction   = true
0.00.721.590 I ggml_metal_init: simdgroup matrix mul. = true
0.00.721.590 I ggml_metal_init: has residency sets    = true
0.00.721.590 I ggml_metal_init: has bfloat            = true
0.00.721.590 I ggml_metal_init: use bfloat            = true
0.00.721.591 I ggml_metal_init: hasUnifiedMemory      = true
0.00.721.594 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.738.449 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.741.957 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.741.965 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.742.020 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.745.265 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.745.266 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.745.267 I llama_init_from_model: graph nodes  = 967
0.00.745.267 I llama_init_from_model: graph splits = 2
0.00.745.270 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.745.270 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.774.019 I 
0.00.774.100 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.774.114 I perplexity: tokenizing the input ..
0.00.780.954 I perplexity: tokenization took 6.844 ms
0.00.780.960 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.915.855 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.917.282 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.917.307 I llama_perf_context_print:        load time =     764.10 ms
0.00.917.308 I llama_perf_context_print: prompt eval time =     134.49 ms /   128 tokens (    1.05 ms per token,   951.72 tokens per second)
0.00.917.309 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.917.310 I llama_perf_context_print:       total time =     143.29 ms /   129 tokens
0.00.917.682 I ggml_metal_free: deallocating

real	0m0.934s
user	0m0.078s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.114 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.129 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.051 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.057 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.059 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.059 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.060 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.060 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.060 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.061 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.062 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.062 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.063 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.063 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.063 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.064 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.066 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.066 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.066 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.881 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.923 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.732 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.733 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.734 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.734 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.734 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.735 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.735 I llama_model_loader: - type  f32:  194 tensors
0.00.024.736 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.736 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.737 I print_info: file format = GGUF V3 (latest)
0.00.024.737 I print_info: file type   = Q5_1
0.00.024.738 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.137 I load: special tokens cache size = 25
0.00.039.251 I load: token to piece cache size = 0.2984 MB
0.00.039.255 I print_info: arch             = gptneox
0.00.039.255 I print_info: vocab_only       = 0
0.00.039.255 I print_info: n_ctx_train      = 2048
0.00.039.256 I print_info: n_embd           = 2048
0.00.039.256 I print_info: n_layer          = 24
0.00.039.261 I print_info: n_head           = 16
0.00.039.261 I print_info: n_head_kv        = 16
0.00.039.262 I print_info: n_rot            = 32
0.00.039.262 I print_info: n_swa            = 0
0.00.039.262 I print_info: n_embd_head_k    = 128
0.00.039.262 I print_info: n_embd_head_v    = 128
0.00.039.263 I print_info: n_gqa            = 1
0.00.039.264 I print_info: n_embd_k_gqa     = 2048
0.00.039.264 I print_info: n_embd_v_gqa     = 2048
0.00.039.265 I print_info: f_norm_eps       = 1.0e-05
0.00.039.265 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.265 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.265 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.266 I print_info: f_logit_scale    = 0.0e+00
0.00.039.266 I print_info: n_ff             = 8192
0.00.039.267 I print_info: n_expert         = 0
0.00.039.267 I print_info: n_expert_used    = 0
0.00.039.267 I print_info: causal attn      = 1
0.00.039.267 I print_info: pooling type     = 0
0.00.039.267 I print_info: rope type        = 2
0.00.039.267 I print_info: rope scaling     = linear
0.00.039.268 I print_info: freq_base_train  = 10000.0
0.00.039.268 I print_info: freq_scale_train = 1
0.00.039.268 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.268 I print_info: rope_finetuned   = unknown
0.00.039.268 I print_info: ssm_d_conv       = 0
0.00.039.269 I print_info: ssm_d_inner      = 0
0.00.039.269 I print_info: ssm_d_state      = 0
0.00.039.269 I print_info: ssm_dt_rank      = 0
0.00.039.269 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.269 I print_info: model type       = 1.4B
0.00.039.270 I print_info: model params     = 1.41 B
0.00.039.270 I print_info: general.name     = 1.4B
0.00.039.270 I print_info: vocab type       = BPE
0.00.039.270 I print_info: n_vocab          = 50304
0.00.039.271 I print_info: n_merges         = 50009
0.00.039.271 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.271 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.271 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.271 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.272 I print_info: LF token         = 187 'Ċ'
0.00.039.272 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.272 I print_info: max token length = 1024
0.00.039.274 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.616.230 I load_tensors: offloading 24 repeating layers to GPU
0.00.616.247 I load_tensors: offloading output layer to GPU
0.00.616.248 I load_tensors: offloaded 25/25 layers to GPU
0.00.616.286 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.616.287 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.617.973 I llama_init_from_model: n_seq_max     = 1
0.00.617.976 I llama_init_from_model: n_ctx         = 128
0.00.617.976 I llama_init_from_model: n_ctx_per_seq = 128
0.00.617.977 I llama_init_from_model: n_batch       = 128
0.00.617.977 I llama_init_from_model: n_ubatch      = 128
0.00.617.978 I llama_init_from_model: flash_attn    = 0
0.00.617.980 I llama_init_from_model: freq_base     = 10000.0
0.00.617.981 I llama_init_from_model: freq_scale    = 1
0.00.617.981 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.617.984 I ggml_metal_init: allocating
0.00.618.075 I ggml_metal_init: found device: Apple M4
0.00.618.089 I ggml_metal_init: picking default device: Apple M4
0.00.619.634 I ggml_metal_init: using embedded metal library
0.00.626.132 I ggml_metal_init: GPU name:   Apple M4
0.00.626.137 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.626.138 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.626.139 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.626.140 I ggml_metal_init: simdgroup reduction   = true
0.00.626.140 I ggml_metal_init: simdgroup matrix mul. = true
0.00.626.140 I ggml_metal_init: has residency sets    = true
0.00.626.141 I ggml_metal_init: has bfloat            = true
0.00.626.141 I ggml_metal_init: use bfloat            = true
0.00.626.142 I ggml_metal_init: hasUnifiedMemory      = true
0.00.626.146 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.643.542 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.647.108 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.647.112 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.647.152 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.650.495 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.650.497 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.650.497 I llama_init_from_model: graph nodes  = 967
0.00.650.497 I llama_init_from_model: graph splits = 2
0.00.650.500 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.650.501 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.712 I 
0.00.684.802 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.811 I perplexity: tokenizing the input ..
0.00.691.839 I perplexity: tokenization took 7.024 ms
0.00.691.851 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.839.206 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.840.550 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.840.580 I llama_perf_context_print:        load time =     675.57 ms
0.00.840.581 I llama_perf_context_print: prompt eval time =     146.52 ms /   128 tokens (    1.14 ms per token,   873.61 tokens per second)
0.00.840.582 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.840.583 I llama_perf_context_print:       total time =     155.87 ms /   129 tokens
0.00.840.935 I ggml_metal_free: deallocating

real	0m0.856s
user	0m0.079s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.527 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.346 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.352 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.353 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.354 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.354 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.355 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.355 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.356 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.358 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.358 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.358 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.359 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.359 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.360 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.361 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.362 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.362 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.070 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.161 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.925 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.926 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.926 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.927 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.927 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.927 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.928 I llama_model_loader: - type  f32:  194 tensors
0.00.025.928 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.928 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.929 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.929 I print_info: file format = GGUF V3 (latest)
0.00.025.930 I print_info: file type   = Q2_K - Medium
0.00.025.931 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.958 I load: special tokens cache size = 25
0.00.039.867 I load: token to piece cache size = 0.2984 MB
0.00.039.871 I print_info: arch             = gptneox
0.00.039.871 I print_info: vocab_only       = 0
0.00.039.871 I print_info: n_ctx_train      = 2048
0.00.039.871 I print_info: n_embd           = 2048
0.00.039.872 I print_info: n_layer          = 24
0.00.039.876 I print_info: n_head           = 16
0.00.039.877 I print_info: n_head_kv        = 16
0.00.039.877 I print_info: n_rot            = 32
0.00.039.877 I print_info: n_swa            = 0
0.00.039.878 I print_info: n_embd_head_k    = 128
0.00.039.881 I print_info: n_embd_head_v    = 128
0.00.039.881 I print_info: n_gqa            = 1
0.00.039.882 I print_info: n_embd_k_gqa     = 2048
0.00.039.883 I print_info: n_embd_v_gqa     = 2048
0.00.039.884 I print_info: f_norm_eps       = 1.0e-05
0.00.039.884 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.884 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.884 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.884 I print_info: f_logit_scale    = 0.0e+00
0.00.039.885 I print_info: n_ff             = 8192
0.00.039.886 I print_info: n_expert         = 0
0.00.039.886 I print_info: n_expert_used    = 0
0.00.039.886 I print_info: causal attn      = 1
0.00.039.886 I print_info: pooling type     = 0
0.00.039.886 I print_info: rope type        = 2
0.00.039.887 I print_info: rope scaling     = linear
0.00.039.887 I print_info: freq_base_train  = 10000.0
0.00.039.887 I print_info: freq_scale_train = 1
0.00.039.887 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.888 I print_info: rope_finetuned   = unknown
0.00.039.888 I print_info: ssm_d_conv       = 0
0.00.039.888 I print_info: ssm_d_inner      = 0
0.00.039.888 I print_info: ssm_d_state      = 0
0.00.039.890 I print_info: ssm_dt_rank      = 0
0.00.039.890 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.890 I print_info: model type       = 1.4B
0.00.039.891 I print_info: model params     = 1.41 B
0.00.039.891 I print_info: general.name     = 1.4B
0.00.039.891 I print_info: vocab type       = BPE
0.00.039.891 I print_info: n_vocab          = 50304
0.00.039.892 I print_info: n_merges         = 50009
0.00.039.892 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.892 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.892 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.892 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.892 I print_info: LF token         = 187 'Ċ'
0.00.039.893 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.893 I print_info: max token length = 1024
0.00.039.893 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.340.711 I load_tensors: offloading 24 repeating layers to GPU
0.00.340.728 I load_tensors: offloading output layer to GPU
0.00.340.729 I load_tensors: offloaded 25/25 layers to GPU
0.00.340.762 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.340.763 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.342.433 I llama_init_from_model: n_seq_max     = 1
0.00.342.436 I llama_init_from_model: n_ctx         = 128
0.00.342.436 I llama_init_from_model: n_ctx_per_seq = 128
0.00.342.437 I llama_init_from_model: n_batch       = 128
0.00.342.437 I llama_init_from_model: n_ubatch      = 128
0.00.342.438 I llama_init_from_model: flash_attn    = 0
0.00.342.440 I llama_init_from_model: freq_base     = 10000.0
0.00.342.441 I llama_init_from_model: freq_scale    = 1
0.00.342.442 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.342.444 I ggml_metal_init: allocating
0.00.342.515 I ggml_metal_init: found device: Apple M4
0.00.342.529 I ggml_metal_init: picking default device: Apple M4
0.00.344.370 I ggml_metal_init: using embedded metal library
0.00.349.655 I ggml_metal_init: GPU name:   Apple M4
0.00.349.668 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.349.669 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.349.670 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.349.671 I ggml_metal_init: simdgroup reduction   = true
0.00.349.671 I ggml_metal_init: simdgroup matrix mul. = true
0.00.349.671 I ggml_metal_init: has residency sets    = true
0.00.349.672 I ggml_metal_init: has bfloat            = true
0.00.349.672 I ggml_metal_init: use bfloat            = true
0.00.349.673 I ggml_metal_init: hasUnifiedMemory      = true
0.00.349.678 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.371.142 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.374.810 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.374.814 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.374.862 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.378.252 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.378.254 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.378.255 I llama_init_from_model: graph nodes  = 967
0.00.378.255 I llama_init_from_model: graph splits = 2
0.00.378.258 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.378.258 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.411.394 I 
0.00.411.475 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.411.488 I perplexity: tokenizing the input ..
0.00.418.519 I perplexity: tokenization took 7.028 ms
0.00.418.528 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.560.508 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.561.860 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.561.886 I llama_perf_context_print:        load time =     400.86 ms
0.00.561.887 I llama_perf_context_print: prompt eval time =     141.72 ms /   128 tokens (    1.11 ms per token,   903.19 tokens per second)
0.00.561.888 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.561.888 I llama_perf_context_print:       total time =     150.50 ms /   129 tokens
0.00.562.255 I ggml_metal_free: deallocating

real	0m0.578s
user	0m0.080s
sys	0m0.090s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.806 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.092 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.098 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.106 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.106 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.107 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.107 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.107 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.108 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.108 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.109 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.109 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.109 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.110 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.110 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.112 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.112 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.112 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.884 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.884 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.662 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.664 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.664 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.664 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.665 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.665 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.666 I llama_model_loader: - type  f32:  194 tensors
0.00.024.666 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.666 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.667 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.667 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.667 I print_info: file format = GGUF V3 (latest)
0.00.024.668 I print_info: file type   = Q3_K - Medium
0.00.024.669 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.551 I load: special tokens cache size = 25
0.00.038.698 I load: token to piece cache size = 0.2984 MB
0.00.038.702 I print_info: arch             = gptneox
0.00.038.702 I print_info: vocab_only       = 0
0.00.038.702 I print_info: n_ctx_train      = 2048
0.00.038.703 I print_info: n_embd           = 2048
0.00.038.703 I print_info: n_layer          = 24
0.00.038.707 I print_info: n_head           = 16
0.00.038.708 I print_info: n_head_kv        = 16
0.00.038.710 I print_info: n_rot            = 32
0.00.038.711 I print_info: n_swa            = 0
0.00.038.711 I print_info: n_embd_head_k    = 128
0.00.038.711 I print_info: n_embd_head_v    = 128
0.00.038.712 I print_info: n_gqa            = 1
0.00.038.712 I print_info: n_embd_k_gqa     = 2048
0.00.038.713 I print_info: n_embd_v_gqa     = 2048
0.00.038.714 I print_info: f_norm_eps       = 1.0e-05
0.00.038.714 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.714 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.715 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.715 I print_info: f_logit_scale    = 0.0e+00
0.00.038.715 I print_info: n_ff             = 8192
0.00.038.716 I print_info: n_expert         = 0
0.00.038.716 I print_info: n_expert_used    = 0
0.00.038.716 I print_info: causal attn      = 1
0.00.038.716 I print_info: pooling type     = 0
0.00.038.716 I print_info: rope type        = 2
0.00.038.717 I print_info: rope scaling     = linear
0.00.038.717 I print_info: freq_base_train  = 10000.0
0.00.038.719 I print_info: freq_scale_train = 1
0.00.038.720 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.720 I print_info: rope_finetuned   = unknown
0.00.038.720 I print_info: ssm_d_conv       = 0
0.00.038.720 I print_info: ssm_d_inner      = 0
0.00.038.720 I print_info: ssm_d_state      = 0
0.00.038.720 I print_info: ssm_dt_rank      = 0
0.00.038.720 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.721 I print_info: model type       = 1.4B
0.00.038.721 I print_info: model params     = 1.41 B
0.00.038.722 I print_info: general.name     = 1.4B
0.00.038.722 I print_info: vocab type       = BPE
0.00.038.722 I print_info: n_vocab          = 50304
0.00.038.724 I print_info: n_merges         = 50009
0.00.038.724 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.724 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.725 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.725 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.725 I print_info: LF token         = 187 'Ċ'
0.00.038.725 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.726 I print_info: max token length = 1024
0.00.038.726 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.441.911 I load_tensors: offloading 24 repeating layers to GPU
0.00.441.924 I load_tensors: offloading output layer to GPU
0.00.441.925 I load_tensors: offloaded 25/25 layers to GPU
0.00.441.958 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.441.959 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.443.572 I llama_init_from_model: n_seq_max     = 1
0.00.443.575 I llama_init_from_model: n_ctx         = 128
0.00.443.575 I llama_init_from_model: n_ctx_per_seq = 128
0.00.443.576 I llama_init_from_model: n_batch       = 128
0.00.443.576 I llama_init_from_model: n_ubatch      = 128
0.00.443.576 I llama_init_from_model: flash_attn    = 0
0.00.443.578 I llama_init_from_model: freq_base     = 10000.0
0.00.443.579 I llama_init_from_model: freq_scale    = 1
0.00.443.579 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.443.583 I ggml_metal_init: allocating
0.00.443.656 I ggml_metal_init: found device: Apple M4
0.00.443.669 I ggml_metal_init: picking default device: Apple M4
0.00.445.499 I ggml_metal_init: using embedded metal library
0.00.451.121 I ggml_metal_init: GPU name:   Apple M4
0.00.451.127 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.451.127 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.451.129 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.451.129 I ggml_metal_init: simdgroup reduction   = true
0.00.451.130 I ggml_metal_init: simdgroup matrix mul. = true
0.00.451.130 I ggml_metal_init: has residency sets    = true
0.00.451.130 I ggml_metal_init: has bfloat            = true
0.00.451.131 I ggml_metal_init: use bfloat            = true
0.00.451.132 I ggml_metal_init: hasUnifiedMemory      = true
0.00.451.134 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.471.947 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.475.589 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.475.596 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.475.638 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.478.741 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.478.743 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.478.744 I llama_init_from_model: graph nodes  = 967
0.00.478.744 I llama_init_from_model: graph splits = 2
0.00.478.747 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.478.747 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.505.634 I 
0.00.505.710 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.505.718 I perplexity: tokenizing the input ..
0.00.513.091 I perplexity: tokenization took 7.37 ms
0.00.513.097 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.657.300 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.658.716 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.658.734 I llama_perf_context_print:        load time =     496.82 ms
0.00.658.734 I llama_perf_context_print: prompt eval time =     143.25 ms /   128 tokens (    1.12 ms per token,   893.52 tokens per second)
0.00.658.735 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.658.736 I llama_perf_context_print:       total time =     153.10 ms /   129 tokens
0.00.659.096 I ggml_metal_free: deallocating

real	0m0.672s
user	0m0.080s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.784 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.756 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.763 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.764 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.765 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.765 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.765 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.766 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.767 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.767 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.770 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.771 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.771 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.771 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.772 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.773 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.774 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.774 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.468 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.440 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.108 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.109 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.110 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.110 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.111 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.111 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.111 I llama_model_loader: - type  f32:  194 tensors
0.00.024.112 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.112 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.112 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.113 I print_info: file format = GGUF V3 (latest)
0.00.024.114 I print_info: file type   = Q4_K - Medium
0.00.024.115 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.125 I load: special tokens cache size = 25
0.00.038.118 I load: token to piece cache size = 0.2984 MB
0.00.038.122 I print_info: arch             = gptneox
0.00.038.123 I print_info: vocab_only       = 0
0.00.038.123 I print_info: n_ctx_train      = 2048
0.00.038.123 I print_info: n_embd           = 2048
0.00.038.123 I print_info: n_layer          = 24
0.00.038.128 I print_info: n_head           = 16
0.00.038.129 I print_info: n_head_kv        = 16
0.00.038.129 I print_info: n_rot            = 32
0.00.038.129 I print_info: n_swa            = 0
0.00.038.129 I print_info: n_embd_head_k    = 128
0.00.038.130 I print_info: n_embd_head_v    = 128
0.00.038.130 I print_info: n_gqa            = 1
0.00.038.131 I print_info: n_embd_k_gqa     = 2048
0.00.038.132 I print_info: n_embd_v_gqa     = 2048
0.00.038.132 I print_info: f_norm_eps       = 1.0e-05
0.00.038.133 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.133 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.134 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.134 I print_info: f_logit_scale    = 0.0e+00
0.00.038.134 I print_info: n_ff             = 8192
0.00.038.135 I print_info: n_expert         = 0
0.00.038.135 I print_info: n_expert_used    = 0
0.00.038.135 I print_info: causal attn      = 1
0.00.038.135 I print_info: pooling type     = 0
0.00.038.135 I print_info: rope type        = 2
0.00.038.136 I print_info: rope scaling     = linear
0.00.038.136 I print_info: freq_base_train  = 10000.0
0.00.038.136 I print_info: freq_scale_train = 1
0.00.038.137 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.137 I print_info: rope_finetuned   = unknown
0.00.038.137 I print_info: ssm_d_conv       = 0
0.00.038.137 I print_info: ssm_d_inner      = 0
0.00.038.137 I print_info: ssm_d_state      = 0
0.00.038.137 I print_info: ssm_dt_rank      = 0
0.00.038.138 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.138 I print_info: model type       = 1.4B
0.00.038.141 I print_info: model params     = 1.41 B
0.00.038.141 I print_info: general.name     = 1.4B
0.00.038.142 I print_info: vocab type       = BPE
0.00.038.143 I print_info: n_vocab          = 50304
0.00.038.143 I print_info: n_merges         = 50009
0.00.038.143 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.143 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.143 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.144 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.144 I print_info: LF token         = 187 'Ċ'
0.00.038.144 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.144 I print_info: max token length = 1024
0.00.038.147 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.544.156 I load_tensors: offloading 24 repeating layers to GPU
0.00.544.169 I load_tensors: offloading output layer to GPU
0.00.544.170 I load_tensors: offloaded 25/25 layers to GPU
0.00.544.207 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.544.208 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.545.974 I llama_init_from_model: n_seq_max     = 1
0.00.545.977 I llama_init_from_model: n_ctx         = 128
0.00.545.977 I llama_init_from_model: n_ctx_per_seq = 128
0.00.545.978 I llama_init_from_model: n_batch       = 128
0.00.545.978 I llama_init_from_model: n_ubatch      = 128
0.00.545.978 I llama_init_from_model: flash_attn    = 0
0.00.545.981 I llama_init_from_model: freq_base     = 10000.0
0.00.545.981 I llama_init_from_model: freq_scale    = 1
0.00.545.982 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.545.985 I ggml_metal_init: allocating
0.00.546.069 I ggml_metal_init: found device: Apple M4
0.00.546.082 I ggml_metal_init: picking default device: Apple M4
0.00.547.924 I ggml_metal_init: using embedded metal library
0.00.554.966 I ggml_metal_init: GPU name:   Apple M4
0.00.554.974 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.554.974 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.554.975 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.554.976 I ggml_metal_init: simdgroup reduction   = true
0.00.554.976 I ggml_metal_init: simdgroup matrix mul. = true
0.00.554.977 I ggml_metal_init: has residency sets    = true
0.00.554.977 I ggml_metal_init: has bfloat            = true
0.00.554.977 I ggml_metal_init: use bfloat            = true
0.00.554.978 I ggml_metal_init: hasUnifiedMemory      = true
0.00.554.983 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.574.386 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.578.042 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.578.047 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.578.088 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.581.401 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.581.403 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.581.404 I llama_init_from_model: graph nodes  = 967
0.00.581.404 I llama_init_from_model: graph splits = 2
0.00.581.407 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.581.407 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.612.231 I 
0.00.612.314 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.612.321 I perplexity: tokenizing the input ..
0.00.619.644 I perplexity: tokenization took 7.32 ms
0.00.619.653 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.766.071 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.767.437 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.767.464 I llama_perf_context_print:        load time =     603.44 ms
0.00.767.465 I llama_perf_context_print: prompt eval time =     145.53 ms /   128 tokens (    1.14 ms per token,   879.55 tokens per second)
0.00.767.466 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.767.466 I llama_perf_context_print:       total time =     155.24 ms /   129 tokens
0.00.767.829 I ggml_metal_free: deallocating

real	0m0.782s
user	0m0.080s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.032 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.049 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.055 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.061 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.062 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.062 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.063 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.063 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.064 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.064 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.064 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.065 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.065 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.065 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.066 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.068 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.068 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.068 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.969 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.957 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.629 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.631 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.631 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.631 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.632 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.632 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.633 I llama_model_loader: - type  f32:  194 tensors
0.00.025.633 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.633 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.634 I print_info: file format = GGUF V3 (latest)
0.00.025.639 I print_info: file type   = Q5_K - Medium
0.00.025.640 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.629 I load: special tokens cache size = 25
0.00.039.413 I load: token to piece cache size = 0.2984 MB
0.00.039.417 I print_info: arch             = gptneox
0.00.039.418 I print_info: vocab_only       = 0
0.00.039.418 I print_info: n_ctx_train      = 2048
0.00.039.418 I print_info: n_embd           = 2048
0.00.039.418 I print_info: n_layer          = 24
0.00.039.422 I print_info: n_head           = 16
0.00.039.423 I print_info: n_head_kv        = 16
0.00.039.423 I print_info: n_rot            = 32
0.00.039.423 I print_info: n_swa            = 0
0.00.039.424 I print_info: n_embd_head_k    = 128
0.00.039.424 I print_info: n_embd_head_v    = 128
0.00.039.426 I print_info: n_gqa            = 1
0.00.039.427 I print_info: n_embd_k_gqa     = 2048
0.00.039.428 I print_info: n_embd_v_gqa     = 2048
0.00.039.428 I print_info: f_norm_eps       = 1.0e-05
0.00.039.429 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.429 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.429 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.429 I print_info: f_logit_scale    = 0.0e+00
0.00.039.430 I print_info: n_ff             = 8192
0.00.039.430 I print_info: n_expert         = 0
0.00.039.430 I print_info: n_expert_used    = 0
0.00.039.430 I print_info: causal attn      = 1
0.00.039.431 I print_info: pooling type     = 0
0.00.039.431 I print_info: rope type        = 2
0.00.039.433 I print_info: rope scaling     = linear
0.00.039.433 I print_info: freq_base_train  = 10000.0
0.00.039.433 I print_info: freq_scale_train = 1
0.00.039.434 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.434 I print_info: rope_finetuned   = unknown
0.00.039.434 I print_info: ssm_d_conv       = 0
0.00.039.434 I print_info: ssm_d_inner      = 0
0.00.039.434 I print_info: ssm_d_state      = 0
0.00.039.435 I print_info: ssm_dt_rank      = 0
0.00.039.435 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.435 I print_info: model type       = 1.4B
0.00.039.435 I print_info: model params     = 1.41 B
0.00.039.436 I print_info: general.name     = 1.4B
0.00.039.436 I print_info: vocab type       = BPE
0.00.039.436 I print_info: n_vocab          = 50304
0.00.039.436 I print_info: n_merges         = 50009
0.00.039.437 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.437 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.437 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.437 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.437 I print_info: LF token         = 187 'Ċ'
0.00.039.439 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.439 I print_info: max token length = 1024
0.00.039.439 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.590.424 I load_tensors: offloading 24 repeating layers to GPU
0.00.590.431 I load_tensors: offloading output layer to GPU
0.00.590.433 I load_tensors: offloaded 25/25 layers to GPU
0.00.590.468 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.590.473 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.592.094 I llama_init_from_model: n_seq_max     = 1
0.00.592.097 I llama_init_from_model: n_ctx         = 128
0.00.592.097 I llama_init_from_model: n_ctx_per_seq = 128
0.00.592.097 I llama_init_from_model: n_batch       = 128
0.00.592.098 I llama_init_from_model: n_ubatch      = 128
0.00.592.098 I llama_init_from_model: flash_attn    = 0
0.00.592.099 I llama_init_from_model: freq_base     = 10000.0
0.00.592.100 I llama_init_from_model: freq_scale    = 1
0.00.592.101 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.592.103 I ggml_metal_init: allocating
0.00.592.181 I ggml_metal_init: found device: Apple M4
0.00.592.193 I ggml_metal_init: picking default device: Apple M4
0.00.593.780 I ggml_metal_init: using embedded metal library
0.00.599.884 I ggml_metal_init: GPU name:   Apple M4
0.00.599.888 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.889 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.890 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.891 I ggml_metal_init: simdgroup reduction   = true
0.00.599.891 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.891 I ggml_metal_init: has residency sets    = true
0.00.599.891 I ggml_metal_init: has bfloat            = true
0.00.599.892 I ggml_metal_init: use bfloat            = true
0.00.599.892 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.897 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.617.099 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.620.601 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.620.605 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.620.645 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.624.122 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.624.124 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.624.124 I llama_init_from_model: graph nodes  = 967
0.00.624.124 I llama_init_from_model: graph splits = 2
0.00.624.127 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.624.127 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.883 I 
0.00.653.965 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.653.972 I perplexity: tokenizing the input ..
0.00.661.656 I perplexity: tokenization took 7.68 ms
0.00.661.664 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.882 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.801.224 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.801.250 I llama_perf_context_print:        load time =     643.84 ms
0.00.801.252 I llama_perf_context_print: prompt eval time =     137.28 ms /   128 tokens (    1.07 ms per token,   932.43 tokens per second)
0.00.801.252 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.801.253 I llama_perf_context_print:       total time =     147.37 ms /   129 tokens
0.00.801.625 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.079s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.896 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.873 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.879 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.881 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.882 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.882 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.882 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.883 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.883 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.884 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.884 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.884 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.885 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.885 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.885 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.887 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.887 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.887 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.710 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.713 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.507 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.508 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.509 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.509 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.509 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.510 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.510 I llama_model_loader: - type  f32:  194 tensors
0.00.024.511 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.511 I print_info: file format = GGUF V3 (latest)
0.00.024.512 I print_info: file type   = Q6_K
0.00.024.513 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.886 I load: special tokens cache size = 25
0.00.038.990 I load: token to piece cache size = 0.2984 MB
0.00.038.994 I print_info: arch             = gptneox
0.00.038.994 I print_info: vocab_only       = 0
0.00.038.994 I print_info: n_ctx_train      = 2048
0.00.038.995 I print_info: n_embd           = 2048
0.00.038.995 I print_info: n_layer          = 24
0.00.038.999 I print_info: n_head           = 16
0.00.039.000 I print_info: n_head_kv        = 16
0.00.039.000 I print_info: n_rot            = 32
0.00.039.000 I print_info: n_swa            = 0
0.00.039.000 I print_info: n_embd_head_k    = 128
0.00.039.003 I print_info: n_embd_head_v    = 128
0.00.039.004 I print_info: n_gqa            = 1
0.00.039.005 I print_info: n_embd_k_gqa     = 2048
0.00.039.005 I print_info: n_embd_v_gqa     = 2048
0.00.039.006 I print_info: f_norm_eps       = 1.0e-05
0.00.039.006 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.006 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.006 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.006 I print_info: f_logit_scale    = 0.0e+00
0.00.039.007 I print_info: n_ff             = 8192
0.00.039.007 I print_info: n_expert         = 0
0.00.039.007 I print_info: n_expert_used    = 0
0.00.039.010 I print_info: causal attn      = 1
0.00.039.010 I print_info: pooling type     = 0
0.00.039.010 I print_info: rope type        = 2
0.00.039.010 I print_info: rope scaling     = linear
0.00.039.010 I print_info: freq_base_train  = 10000.0
0.00.039.011 I print_info: freq_scale_train = 1
0.00.039.011 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.011 I print_info: rope_finetuned   = unknown
0.00.039.011 I print_info: ssm_d_conv       = 0
0.00.039.011 I print_info: ssm_d_inner      = 0
0.00.039.011 I print_info: ssm_d_state      = 0
0.00.039.011 I print_info: ssm_dt_rank      = 0
0.00.039.012 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.040 I print_info: model type       = 1.4B
0.00.039.043 I print_info: model params     = 1.41 B
0.00.039.043 I print_info: general.name     = 1.4B
0.00.039.044 I print_info: vocab type       = BPE
0.00.039.044 I print_info: n_vocab          = 50304
0.00.039.044 I print_info: n_merges         = 50009
0.00.039.044 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.045 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.045 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.045 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.045 I print_info: LF token         = 187 'Ċ'
0.00.039.046 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.046 I print_info: max token length = 1024
0.00.039.049 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.601.058 I load_tensors: offloading 24 repeating layers to GPU
0.00.601.063 I load_tensors: offloading output layer to GPU
0.00.601.064 I load_tensors: offloaded 25/25 layers to GPU
0.00.601.095 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.601.099 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.602.541 I llama_init_from_model: n_seq_max     = 1
0.00.602.543 I llama_init_from_model: n_ctx         = 128
0.00.602.544 I llama_init_from_model: n_ctx_per_seq = 128
0.00.602.544 I llama_init_from_model: n_batch       = 128
0.00.602.544 I llama_init_from_model: n_ubatch      = 128
0.00.602.545 I llama_init_from_model: flash_attn    = 0
0.00.602.546 I llama_init_from_model: freq_base     = 10000.0
0.00.602.547 I llama_init_from_model: freq_scale    = 1
0.00.602.547 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.602.549 I ggml_metal_init: allocating
0.00.602.609 I ggml_metal_init: found device: Apple M4
0.00.602.621 I ggml_metal_init: picking default device: Apple M4
0.00.604.000 I ggml_metal_init: using embedded metal library
0.00.609.673 I ggml_metal_init: GPU name:   Apple M4
0.00.609.677 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.609.677 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.609.678 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.609.679 I ggml_metal_init: simdgroup reduction   = true
0.00.609.679 I ggml_metal_init: simdgroup matrix mul. = true
0.00.609.679 I ggml_metal_init: has residency sets    = true
0.00.609.679 I ggml_metal_init: has bfloat            = true
0.00.609.680 I ggml_metal_init: use bfloat            = true
0.00.609.681 I ggml_metal_init: hasUnifiedMemory      = true
0.00.609.682 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.625.930 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.629.338 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.629.345 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.629.387 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.632.560 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.632.562 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.632.562 I llama_init_from_model: graph nodes  = 967
0.00.632.563 I llama_init_from_model: graph splits = 2
0.00.632.567 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.632.569 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.863 I 
0.00.667.949 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.955 I perplexity: tokenizing the input ..
0.00.674.952 I perplexity: tokenization took 6.995 ms
0.00.674.960 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.807.984 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.809.390 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.809.418 I llama_perf_context_print:        load time =     658.96 ms
0.00.809.419 I llama_perf_context_print: prompt eval time =     132.02 ms /   128 tokens (    1.03 ms per token,   969.56 tokens per second)
0.00.809.420 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.809.420 I llama_perf_context_print:       total time =     141.56 ms /   129 tokens
0.00.809.769 I ggml_metal_free: deallocating

real	0m0.824s
user	0m0.077s
sys	0m0.137s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.368 I build: 4754 (de8b5a36) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.881 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.585 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.590 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.592 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.593 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.593 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.595 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.596 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.598 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.598 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.599 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.599 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.599 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.600 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.601 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.607 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.607 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.608 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.048 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.758 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.203 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.052.205 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.205 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.206 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.206 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.207 I llama_model_loader: - type  f32:  194 tensors
0.00.052.207 I llama_model_loader: - type  f16:   98 tensors
0.00.052.208 I print_info: file format = GGUF V3 (latest)
0.00.052.208 I print_info: file type   = all F32 (guessed)
0.00.052.210 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.063.902 I load: special tokens cache size = 25
0.00.071.273 I load: token to piece cache size = 0.2984 MB
0.00.071.276 I print_info: arch             = gptneox
0.00.071.276 I print_info: vocab_only       = 0
0.00.071.276 I print_info: n_ctx_train      = 2048
0.00.071.276 I print_info: n_embd           = 2048
0.00.071.277 I print_info: n_layer          = 24
0.00.071.279 I print_info: n_head           = 16
0.00.071.280 I print_info: n_head_kv        = 16
0.00.071.280 I print_info: n_rot            = 32
0.00.071.280 I print_info: n_swa            = 0
0.00.071.281 I print_info: n_embd_head_k    = 128
0.00.071.281 I print_info: n_embd_head_v    = 128
0.00.071.282 I print_info: n_gqa            = 1
0.00.071.282 I print_info: n_embd_k_gqa     = 2048
0.00.071.283 I print_info: n_embd_v_gqa     = 2048
0.00.071.284 I print_info: f_norm_eps       = 1.0e-05
0.00.071.285 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.071.285 I print_info: f_clamp_kqv      = 0.0e+00
0.00.071.285 I print_info: f_max_alibi_bias = 0.0e+00
0.00.071.285 I print_info: f_logit_scale    = 0.0e+00
0.00.071.286 I print_info: n_ff             = 8192
0.00.071.286 I print_info: n_expert         = 0
0.00.071.286 I print_info: n_expert_used    = 0
0.00.071.287 I print_info: causal attn      = 1
0.00.071.287 I print_info: pooling type     = 0
0.00.071.287 I print_info: rope type        = 2
0.00.071.287 I print_info: rope scaling     = linear
0.00.071.289 I print_info: freq_base_train  = 10000.0
0.00.071.290 I print_info: freq_scale_train = 1
0.00.071.290 I print_info: n_ctx_orig_yarn  = 2048
0.00.071.290 I print_info: rope_finetuned   = unknown
0.00.071.290 I print_info: ssm_d_conv       = 0
0.00.071.290 I print_info: ssm_d_inner      = 0
0.00.071.290 I print_info: ssm_d_state      = 0
0.00.071.290 I print_info: ssm_dt_rank      = 0
0.00.071.291 I print_info: ssm_dt_b_c_rms   = 0
0.00.071.291 I print_info: model type       = 1.4B
0.00.071.291 I print_info: model params     = 1.41 B
0.00.071.291 I print_info: general.name     = 1.4B
0.00.071.292 I print_info: vocab type       = BPE
0.00.071.292 I print_info: n_vocab          = 50304
0.00.071.292 I print_info: n_merges         = 50009
0.00.071.292 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.071.293 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.071.293 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.071.293 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.071.293 I print_info: LF token         = 187 'Ċ'
0.00.071.294 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.071.294 I print_info: max token length = 1024
0.00.071.298 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.320.658 I load_tensors: offloading 24 repeating layers to GPU
0.01.320.662 I load_tensors: offloading output layer to GPU
0.01.320.663 I load_tensors: offloaded 25/25 layers to GPU
0.01.320.682 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.320.684 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.321.847 I llama_init_from_model: n_seq_max     = 1
0.01.321.848 I llama_init_from_model: n_ctx         = 128
0.01.321.848 I llama_init_from_model: n_ctx_per_seq = 128
0.01.321.848 I llama_init_from_model: n_batch       = 128
0.01.321.849 I llama_init_from_model: n_ubatch      = 128
0.01.321.849 I llama_init_from_model: flash_attn    = 0
0.01.321.849 I llama_init_from_model: freq_base     = 10000.0
0.01.321.850 I llama_init_from_model: freq_scale    = 1
0.01.321.850 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.321.851 I ggml_metal_init: allocating
0.01.321.907 I ggml_metal_init: found device: Apple M4
0.01.321.913 I ggml_metal_init: picking default device: Apple M4
0.01.323.112 I ggml_metal_init: using embedded metal library
0.01.326.953 I ggml_metal_init: GPU name:   Apple M4
0.01.326.956 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.326.956 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.326.957 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.326.957 I ggml_metal_init: simdgroup reduction   = true
0.01.326.958 I ggml_metal_init: simdgroup matrix mul. = true
0.01.326.958 I ggml_metal_init: has residency sets    = true
0.01.326.958 I ggml_metal_init: has bfloat            = true
0.01.326.958 I ggml_metal_init: use bfloat            = true
0.01.326.958 I ggml_metal_init: hasUnifiedMemory      = true
0.01.326.962 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.337.943 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.339.671 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.339.673 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.339.699 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.341.400 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.341.402 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.341.402 I llama_init_from_model: graph nodes  = 967
0.01.341.402 I llama_init_from_model: graph splits = 2
0.01.341.404 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.341.404 I 
0.01.341.439 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.341.441 I compute_imatrix: tokenizing the input ..
0.01.345.592 I compute_imatrix: tokenization took 4.151 ms
0.01.345.594 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.610.103 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.612.446 I llama_perf_context_print:        load time =    1589.22 ms
0.01.612.447 I llama_perf_context_print: prompt eval time =     262.77 ms /   128 tokens (    2.05 ms per token,   487.12 tokens per second)
0.01.612.448 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.612.448 I llama_perf_context_print:       total time =    1591.56 ms /   129 tokens
0.01.612.931 I ggml_metal_free: deallocating

real	0m1.801s
user	0m0.124s
sys	0m0.244s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4754 (de8b5a36)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15cc079a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15cc07e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15cc082d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15cc08740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15cc08bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15cc09020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15cc09490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15cc09900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15cc09d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15cc0a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15cc0a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15cc0acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15cc0b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15cc0bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15cc0c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15cc0cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15cc0d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15cc0dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15cc0e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15cc0ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15cc0f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15cc0fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15cc10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15cc10a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15cc11140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15cc11400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15cc116c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15cc11b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15cc12250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15cc126c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15cc12c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15cc13190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15cc13600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15cc138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15cc13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15cc141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15cc14610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15cc14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15cc14ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15cc15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15cc157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15cc15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15cc160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15cc16520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15cc16990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15cc16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15cc17270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15cc176e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15cc17e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15cc182e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15cc18750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15cc18bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15cc19030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15cc194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15cc19910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15cc1a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15cc1a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15cc1a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15cc1ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15cc1b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15cc1b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15cc1b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15cc1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15cc1c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15cc1c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15cc1cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15cc1d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15cc1d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15cc1dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15cc1e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15cc1e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15cc1eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15cc1f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15cc1f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15cc1fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15cc200f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15cc206a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15cc20c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15cc21200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15cc217b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15cc21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15cc22310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15cc228c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15cc22e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15cc23420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15cc239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15cc23f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15cc24530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15cc24ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15cc25090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15cc25640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15cc25bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15cc261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15cc26750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15cc26d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15cc272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15cc27860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15cc179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15cc27fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15cc28430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15cc288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15cc28e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15cc29400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15cc299b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15cc29f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15cc2a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15cc2aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15cc2b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15cc2b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15cc2bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15ca04390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15ca04800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15ca04c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15ca050e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15ca05550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15ca059c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15ca05e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15ca062a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15ca06710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15ca06b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15ca06ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15ca07460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15ca078d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15ca08570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15ca08830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15ca08af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15ca08f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15ca093d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15ca09840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15ca09cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15ca0a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15ca0a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15ca0aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15ca0ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15ca0b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15ca0b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15ca0bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15ca0c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15ca0c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15ca0c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15ca0cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15ca0d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15ca0d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15ca0dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15ca0df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15ca0e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15ca0e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15ca0ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15ca0f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15ca0f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15ca0f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15ca0fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15ca102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15ca10730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15ca10ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15ca11010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15ca11480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15ca118f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15ca11d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15ca121d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15ca12640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15ca12ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15ca12f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15ca13390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15ca13800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15ca13c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15ca140e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15ca14550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15ca149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15ca14e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15ca152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15ca15710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15ca15b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15ca15ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15ca16460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15ca168d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15ca16d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15ca171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15ca17620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15ca17a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15ca17f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15ca18370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15ca187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15ca18c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15ca190c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15ca19530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15ca199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15ca19e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15ca1a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15ca1a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15ca1ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15ca1afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15ca1b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15ca1b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15ca1bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15ca1c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15ca1c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15ca1ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15ca1cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15ca1d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15ca1d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15ca1dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15ca1e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15ca1e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15ca1ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15ca1eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15ca1f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15ca1f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15ca1fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15ca201d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15ca20d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15ca21000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15ca215c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15ca21b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15ca22140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15ca22700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15ca22cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15ca23280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15ca23840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15ca23e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15ca243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15ca24980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15ca24f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15ca25500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15ca25ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15ca26080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15ca26640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15ca26c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15ca271c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15ca27780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15ca27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15ca28300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15ca288c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15ca28e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15ca29440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15ca29a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15ca29fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15ca2a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15ca2ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15ca2b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15ca2b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15ca2bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15ca2c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15ca2c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15ca2cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15ca2d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15ca2d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15ca2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15ca2e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15ca2ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15ca2f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15ca2f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15ca2fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15ca30180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15ca30740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15ca30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15ca312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15ca31880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15ca31e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15ca32400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15ca329c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15ca32f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15ca33540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15ca33b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15ca340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15ca34680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15ca34c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15ca35200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15ca35700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15ca35c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15ca36100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15ca36600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15ca36b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15ca37000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15ca37500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15ca37a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15ca37f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15ca38400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15ca38900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15ca38e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15ca39300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15ca39800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15ca39d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15ca3a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15ca3ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15ca3b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15ca3bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15ca3bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15ca3c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15ca3c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15ca3cff0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.768.304 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.768.307 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15cc1fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15cc25350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15cc1f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15cc27570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15cc24da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15cc2b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15cc26fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15cc21a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15cc29c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15cc26a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15cc214c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15cc247f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15cc23130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15cc296c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15cc2b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15cc26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15cc20f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15cc22b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15cc2ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15cc23c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15cc2a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15cc236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15cc1aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15cc27b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15cc2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15cc2c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15cc2c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15cc2ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15cc2cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15cc2d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15cc2d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15cc2d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15cc25eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15cc2d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15cc2db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15cc2ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15cc2e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15cc2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15cc2e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15cc2e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15cc2eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15cc2ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15cc2f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15cc2f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15cc2f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15cc2f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15cc2fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15cc2fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15cc30190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15cc30450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15cc30710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15cc309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15cc30c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15cc30f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15cc31210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15cc314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15cc31790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15cc31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15cc31d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15cc31fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15cc32290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15cc32550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15cc32810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15cc32ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15cc32d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15cc33200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15cc33670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15cc33ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15cc33f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15cc343c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15cc34830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15cc34ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15cc35110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15cc35580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15cc359f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15cc35e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15cc362d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15cc36740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15cc36bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15cc37020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15cc37490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15cc37900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15cc37d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15cc381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15cc38650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15cc38ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15cc38f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15cc393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15cc39810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15cc39c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15cc3a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15cc3a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15cc3a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15cc3ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15cc3b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15cc3b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15cc3bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15cc3c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15cc3c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15cc3c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15cc3cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15cc3d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15cc3d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15cc3daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15cc3df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15cc3e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15cc3e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15cc3ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15cc3f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15cc3f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15cc3f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15cc3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15cc40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15cc40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15cc40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15cc40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15cc41450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15cc418c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15cc41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15cc421a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15cc42610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15cc42a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15cc42ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15cc43360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15cc437d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15cc43c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15cc440b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15cc44520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15cc44990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15cc44e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15cc45270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15cc456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15cc45b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15cc45fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15cc46430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15cc468a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15cc46d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15cc47180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15cc475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15cc47a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15cc47ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15cc48340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15cc487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15cc48c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15cc49090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15cc49500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15cc49970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15cc49de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15cc4a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15cc4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15cc4ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15cc4afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15cc4b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15cc4b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15cc4bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15cc4c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15cc4c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15cc4ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15cc4ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15cc4d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15cc4d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15cc4dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15cc4e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15cc4e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15cc4e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15cc4edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15cc4f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15cc4f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15cc4fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15cc4ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15cc503f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15cc50860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15cc50cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15cc51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15cc515b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15cc51a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15cc51e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15cc52300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15cc52770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15cc52be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15cc53050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15cc534c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15cc53930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15cc53da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15cc54210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15cc54680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15cc54af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15cc54f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15cc553d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15cc55840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15cc55cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15cc56120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15cc56590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15cc56a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15cc56e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15cc572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15cc57750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15cc57bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15cc58030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15cc584a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15cc58910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15cc58d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15cc59570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15cc59830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15cc59e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15cc5a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15cc5ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15cc5b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15cc5b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15cc5ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15cc5c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15cc5c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15cc5cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15cc5d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15cc5d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15cc5dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15cc5e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15cc5e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15cc5ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15cc5f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15cc5f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15cc5fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15cc60190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15cc606e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15cc60c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15cc61180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15cc616d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15cc61c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15cc62170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15cc626c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15cc62c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15cc63160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15cc636b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15cc63c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15cc64150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15cc646a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15cc64bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15cc65140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15cc65690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15cc65be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15cc66130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15cc66680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15cc66bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15cc67120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15cc67670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15cc67bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15cc68110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15cc68660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15cc68bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15cc69100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15cc69650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15cc69ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15cc6a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15cc6a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15cc6ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15cc6b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15cc6b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15cc6bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15cc6c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15cc6c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15cc6cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15cc6d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15cc6d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15cc6db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15cc6e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15cc6e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15cc6eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15cc6eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15cc6f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15cc6f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15cc6fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15cc70270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15cc70710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15cc70bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15cc71050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15cc714f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15cc71990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15cc71e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15cc722d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15cc72770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15cc72c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15cc730b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15cc73600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15cc73d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15cc74440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15cc74b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15cc75280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15cc75540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15cc75d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15cc75ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15cc76600 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1053044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105304950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x105304dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x105305230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1053056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x105305b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105305f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1053063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x105306860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x105306cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105307140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x105307860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x105308380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x105308b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x105309340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x105309a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10530a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10530a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10530afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10530b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10530be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10530c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10530cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10530d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10530da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10530dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10530e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10530e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10530e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10530ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10530f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10530f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10530fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10530fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1053102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105310710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105310b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x105310ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105311460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1053118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105311d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1053121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x105312620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x105312a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x105312f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105313370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1053137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x105313c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1053140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105314530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1053149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x105314e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105315280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1053156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x105315b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x105315fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x105316540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x105316a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x105316eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105317320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x105317790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x105317c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x105318070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1053184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x105318950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x105318dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x105319230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1053196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x105319b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x105319f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10531a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10531a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10531acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10531b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10531b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10531ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10531be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10531c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10531c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10531cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10531d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10531d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10531d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10531dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10531e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10531e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10531eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10531ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10531f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10531f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10531fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x105320120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x105320590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x105320a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x105320e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1053212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x105321750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x105321bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x105322030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1053224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x105322910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x105322d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1053231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x105323660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x105323fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x105324290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x105324700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15cb086a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15cb08b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15cb08f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15cb093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15cb09860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15cb09cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15cb0a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15cb0a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15cb0aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15cb0ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15cb0b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15cb0b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15cb0bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15cb0c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15cb0c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15cb0c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15cb0cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15cb0d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15cb0d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15cb0daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15cb0df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15cb0e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15cb0e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15cb0ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15cb0f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15cb0f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15cb0fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15cb0fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15cb102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15cb10750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15cb10bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15cb11030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15cb114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15cb11910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15cb11d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15cb121f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15cb12660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15cb12ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15cb12f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15cb133b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15cb13820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15cb13c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15cb14100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15cb14570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15cb149e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15cb14e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15cb152c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15cb15730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15cb15ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15cb16010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15cb16480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15cb168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15cb16d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15cb171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15cb17640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15cb17ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15cb17f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15cb18390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15cb18800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15cb18c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15cb190e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15cb19550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15cb199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15cb19e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15cb1a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15cb1a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15cb1ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15cb1aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15cb1b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15cb1b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15cb1bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15cb1c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15cb1c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15cb1ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15cb1cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15cb1d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15cb1d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15cb1dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15cb1e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15cb1e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15cb1e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15cb1ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15cb1f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15cb1f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15cb1fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15cb1ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15cb20440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15cb208b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15cb20d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15cb21190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15cb21600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15cb21a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15cb21ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15cb22350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15cb227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15cb22c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15cb230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15cb23510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15cb23980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15cb23df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15cb24380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15cb247f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15cb24c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15cb257b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15cb25a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15cb25d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15cb261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15cb26610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15cb26a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15cb26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15cb27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15cb277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15cb27c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15cb280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15cb28520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15cb28990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15cb28e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15cb29270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15cb296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15cb29b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15cb29fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15cb2a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15cb2a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15cb2ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15cb2b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15cb2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15cb2ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15cb2bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15cb2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15cb2c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15cb2cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15cb2d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15cb2d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15cb2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15cb2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15cb2e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15cb2e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15cb2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15cb2efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15cb2f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15cb2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15cb2fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15cb30160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15cb305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15cb30a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15cb30eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15cb31320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15cb31790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15cb31c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15cb32070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15cb324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15cb32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15cb32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15cb33230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15cb336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15cb33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15cb33f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15cb343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15cb34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15cb34cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15cb35140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15cb355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15cb35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15cb35e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15cb36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15cb36770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15cb36be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15cb37050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15cb374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15cb37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15cb37da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15cb38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15cb38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15cb38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15cb38f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15cb393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15cb39e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15cb3a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15cb3ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15cb3b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15cb3b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15cb3bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15cb3c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15cb3c6e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.841s
user	0m0.284s
sys	0m0.352s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4754 (de8b5a36)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14d008bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14d009310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14d0098c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14d009e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14d00a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14d00a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14d00af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14d00b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14d00bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14d00bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14d00c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14d00c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14d00d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14d00dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14d00e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14d00ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14d00f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14d00fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14d010140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14d010910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14d011030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14d011750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14d011e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14d012710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14d012e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14d0130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14d013700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14d014370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14d0148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14d014b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14d015010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14d0152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14d015b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14d0160a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14d016360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14d016800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14d016ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14d017140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14d0175e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14d017a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14d017f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14d0183c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14d018860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14d018d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14d018fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14d0195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14d019be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14d01a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14d01ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14d01b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14d01b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14d01bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14d01c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14d01c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14d01d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14d01d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14d01da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14d01dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14d01e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14d01eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14d01ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14d01f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14d01f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14d01fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14d020090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14d020530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14d0209d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14d020e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14d021310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14d0217b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14d021c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14d0220f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14d022590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14d022ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14d023030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14d023580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14d023ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14d024020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14d024570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14d024ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14d025010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14d025560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14d025ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14d026000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14d026550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14d026aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14d026ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14d027540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14d027a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14d027fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14d028530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14d028a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14d028fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14d029520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14d029a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14d029fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14d02a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14d01a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14d02a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14d02b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14d02b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14d02bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14d02c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14d02c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14d02cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14d02d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14d02d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14d02dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14d02e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14d02e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14d02eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14d02f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14d02f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14d02fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14d02ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14d030420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14d0308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14d030d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14d031200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14d0316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14d031b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14d031fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14d032480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14d032920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14d032dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14d033260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14d033700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14d033ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14d034040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14d0344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14d034980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14d034e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14d0352c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14d035760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14d035c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14d0360a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14d036540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14d0369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14d036e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14d037320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14d0377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14d037c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14d038100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14d0385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14d038a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14d038ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14d039380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14d039820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14d039cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14d03a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14d03a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14d03aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14d03af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14d03b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14d03b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14d03bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14d03c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14d03c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14d03cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14d03cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14d03d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14d03d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14d03dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14d03e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14d03e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14d03eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14d03f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14d03f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14d03f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14d03fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14d040280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14d040720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14d040bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14d041060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14d041500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14d0419a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14d041e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14d0422e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14d042780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14d042c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14d0430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14d043560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14d043a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14d043ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14d044340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14d0447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14d044c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14d045120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14d0455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14d045a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14d045f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14d0463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14d046840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14d046d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14d0472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14d047830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14d047d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14d048040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14d048650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14d048c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14d049270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14d049a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14d049f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14d04a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14d04a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14d04ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14d04b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14d04ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14d04bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14d04c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14d04cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14d04d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14d04d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14d04db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14d04e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14d04e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14d04eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14d04f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14d04f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14d04fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14d050080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14d0505d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14d050b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14d051070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14d0515c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14d051b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14d052060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14d0525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14d052b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14d053050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14d0535a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14d053af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14d054040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14d054590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14d054ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14d055030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14d055580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14d055ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14d056020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14d056570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14d056ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14d057010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14d057560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14d057ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14d058000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14d058550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14d058aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14d058ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14d059540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14d059a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14d059fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14d05a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14d05aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14d05afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14d05b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14d05ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14d05bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14d05c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14d05ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14d05cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14d05d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14d05da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14d05dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14d05e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14d05ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14d05ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14d05f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14d05f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14d05fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14d0602c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14d060760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14d060c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14d0610a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14d061540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14d0619e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14d061e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14d062320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14d0627c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14d062c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14d063100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14d0635a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14d063a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14d063f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14d0646b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14d064dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14d0654f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14d065c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14d065ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14d0666c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14d066980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14d066f90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.103.847 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.852 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14bf05640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14bf05900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14bf05bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14bf06030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14bf064a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14bf06910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14bf06d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14bf071f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14bf07660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14bf07b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14bf07ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14bf08670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14bf09190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14bf09940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14bf0a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14bf0a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14bf0af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14bf0b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14bf0bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14bf0c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14bf0ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14bf0d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14bf0db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14bf0e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14bf0e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14bf0ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14bf0eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14bf0f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14bf0f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14bf0fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14bf10080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14bf105b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14bf10a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14bf10ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14bf11150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14bf115c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14bf11a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14bf11ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14bf12310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14bf12780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14bf12bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14bf13060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14bf134d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14bf13940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14bf13db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14bf14220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14bf14690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14bf14b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14bf14f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14bf153e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14bf15850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14bf15cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14bf16130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14bf165a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14bf16a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14bf16e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14bf173f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14bf178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14bf17d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14bf181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14bf18640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14bf18ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14bf18f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14bf19390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14bf19800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14bf19c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14bf1a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14bf1a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14bf1a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14bf1ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14bf1b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14bf1b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14bf1bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14bf1bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14bf1c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14bf1c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14bf1cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14bf1d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14bf1d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14bf1da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14bf1df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14bf1e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14bf1e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14bf1ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14bf1f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14bf1f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14bf1f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14bf1fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14bf20280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14bf206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14bf20b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14bf20fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14bf21440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14bf218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14bf21d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14bf22190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14bf22600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14bf22a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14bf22ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14bf23350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14bf237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14bf23c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14bf240a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14bf24510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14bf24980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14bf24df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14bf25260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14bf256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14bf25b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14bf25fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14bf26420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14bf26890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14bf26d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14bf27170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14bf275e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14bf27a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14bf27ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14bf28330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14bf287a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14bf28c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14bf29080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14bf294f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14bf29960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14bf29dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14bf2a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14bf2a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14bf2ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14bf2af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14bf2b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14bf2b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14bf2bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14bf2c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14bf2c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14bf2ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14bf2cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14bf2d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14bf2d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14bf2dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14bf2e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14bf2e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14bf2e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14bf2edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14bf2f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14bf2f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14bf2fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14bf2ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14bf303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14bf30850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14bf30cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14bf31130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14bf315a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14bf31a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14bf31e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14bf322f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14bf32760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14bf32bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14bf33040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14bf334b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14bf33920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14bf33d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14bf34200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14bf34670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14bf34ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14bf34f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14bf353c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14bf35830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14bf36460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14bf36720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14bf369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14bf36e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14bf372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14bf37730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14bf37ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14bf38010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14bf38480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14bf388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14bf38d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14bf391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14bf39640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14bf39ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14bf39f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14bf3a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14bf3a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14bf3ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14bf3b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14bf3b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14bf3b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14bf3be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14bf3c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14bf3c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14bf3cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14bf3cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14bf3d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14bf3d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14bf3dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14bf3e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14bf3e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14bf3ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14bf3ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14bf3f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14bf3f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14bf3fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14bf401b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14bf406c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14bf40b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14bf40fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14bf41410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14bf41880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14bf41da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14bf422b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14bf42e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14bf430e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14bf436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14bf43c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14bf44220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14bf447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14bf44da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14bf45360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14bf45920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14bf45ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14bf464a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14bf46a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14bf47020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14bf475e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14bf47ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14bf48160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14bf48720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14bf48ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14bf492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14bf49860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14bf49e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14bf4a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14bf4a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14bf4af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14bf4b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14bf4bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14bf4c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14bf4c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14bf4cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14bf4d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14bf4d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14bf4dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14bf4e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14bf4e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14bf4eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14bf4f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14bf4fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14bf4ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14bf505a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14bf50b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14bf51120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14bf516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14bf51ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14bf52260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14bf52820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14bf52de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14bf533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14bf53960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14bf53f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14bf544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14bf54aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14bf55060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14bf55620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14bf55be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14bf561a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14bf56760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14bf56d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14bf572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14bf577e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14bf57ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14bf581e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14bf586e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14bf58be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14bf590e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14bf595e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14bf59ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14bf59fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14bf5a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14bf5a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14bf5aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14bf5b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14bf5b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14bf5bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14bf5c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14bf5cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14bf5d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14bf5dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14bf5e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14bf5e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14bf5eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14bf5f0d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14be0ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14be0b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14be0b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14be0ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14be0db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14be0dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14be0e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14be0e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14be0eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14be0f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14be0f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14be0fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14be10670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14be10e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14be11630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14be11d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14be12470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14be12b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14be132b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14be13a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14be141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14be148c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14be14fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14be15700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14be15e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14be160e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14be163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14be16810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14be16c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14be170f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14be17560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14be17a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14be17f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14be181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14be18630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14be18aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14be18f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14be19380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14be197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14be19c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14be1a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14be1a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14be1a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14be1ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14be1b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14be1b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14be1bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14be1bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14be1c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14be1c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14be1cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14be1d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14be1d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14be1da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14be1def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14be1e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14be1e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14be1edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14be1f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14be1f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14be1fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14be1ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14be20400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14be20870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14be20ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14be21150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14be215c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14be21a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14be21ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14be22310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14be22780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14be22bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14be23060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14be234d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14be23940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14be23db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14be24220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14be24690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14be24b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14be24f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14be253e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14be25850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14be25cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14be26130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14be265a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14be26a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14be26e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14be272f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14be27760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14be27bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14be28040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14be284b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14be28920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14be28d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14be29200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14be29670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14be29ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14be29f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14be2a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14be2a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14be2aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14be2b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14be2b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14be2b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14be2c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14be2c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14be2ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14be2cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14be2d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14be2d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14be2dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14be2e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14be2e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14be2e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14be2ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14be2f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14be2f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14be2fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14be2ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14be30440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14be308b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14be30d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14be31190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14be31600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14be31a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14be31ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14be32350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14be327c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14be32c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14be330a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14be33510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14be33980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14be33df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14be34260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14be346d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14be34b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14be34fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14be35420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14be35890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14be35d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14be36170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14be365e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14be36a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14be36ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14be37330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14be377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14be37c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14be38080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14be384f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14be38960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14be38dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14be39240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14be396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14be39b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14be39f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14be3a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14be3a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14be3ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14be3b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14be3b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14be3ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14be3bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14be3c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14be3c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14be3cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14be3d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14be3d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14be3d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14be3ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14be3e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14be3e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14be3eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14be3ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14be3f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14be3f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14be3fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14be40130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14be405a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14be40a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14be40e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14be412f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14be41760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14be41bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14be42040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14be424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14be42920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14be42d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14be43200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14be43670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14be43ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14be43f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14be443c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14be44830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14be44ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14be45110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14be45580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14be459f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14be45e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14be462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14be46740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14be46bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14be47020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14be47490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14be47900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14be47d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14be481e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14be48650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14be48be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14be49050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14be494c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14be4a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14be4a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14be4a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14be4aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14be4ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14be4b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14be4b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14be4bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14be4c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14be4c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14be4c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14be4cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14be4d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14be4d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14be4dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14be4df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14be4e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14be4e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14be4ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14be4f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14be4f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14be4f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14be4fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14be502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14be50730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14be50ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14be51010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14be51480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14be518f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14be51d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14be521d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14be52640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14be52ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14be52f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14be53390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14be53800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14be53c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14be540e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14be54550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14be549c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14be54e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14be552a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14be55710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14be55b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14be55ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14be56460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14be568d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14be56d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14be571b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14be57620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14be57a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14be57f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14be58370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14be587e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14be58c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14be590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14be59530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14be599a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14be59e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14be5a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14be5a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14be5ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14be5afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14be5b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14be5b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14be5bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14be5c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14be5c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14be5ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14be5cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14be5d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14be5d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14be5dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14be5e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14be5edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14be5f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14be5fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14be5fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14be60330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14be60930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14be60f40 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.976s
user	0m0.232s
sys	0m0.191s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
