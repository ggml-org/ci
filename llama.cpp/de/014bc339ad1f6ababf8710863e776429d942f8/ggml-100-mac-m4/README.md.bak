### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.40 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.77 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.45 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.99 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.24 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.03 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.29 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.23 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.45 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  179.00 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.91 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.12 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 222.23 sec*proc (28 tests)

Total Test time (real) = 222.24 sec

real	3m42.266s
user	7m40.071s
sys	0m6.231s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.34 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    1.11 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.27 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.15 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.36 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.19 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.48 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.05 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.22 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.76 sec*proc (28 tests)

Total Test time (real) =  51.77 sec

real	0m51.778s
user	1m11.619s
sys	0m5.532s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.109 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.776 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.900 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.021.908 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.910 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.021.911 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.912 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.021.913 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.021.913 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.021.915 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.021.915 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.021.920 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.021.920 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.021.921 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.021.924 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.021.925 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.021.926 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.021.926 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.021.927 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.021.928 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.021.928 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.026.925 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.028.108 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.110 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.028.110 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.028.111 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.028.111 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.028.112 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.028.112 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.028.113 I llama_model_loader: - type  f32:  124 tensors
0.00.028.113 I llama_model_loader: - type  f16:   73 tensors
0.00.032.641 I llm_load_vocab: special tokens cache size = 5
0.00.034.801 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.034.805 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.034.805 I llm_load_print_meta: arch             = bert
0.00.034.805 I llm_load_print_meta: vocab type       = WPM
0.00.034.806 I llm_load_print_meta: n_vocab          = 30522
0.00.034.806 I llm_load_print_meta: n_merges         = 0
0.00.034.806 I llm_load_print_meta: vocab_only       = 0
0.00.034.807 I llm_load_print_meta: n_ctx_train      = 512
0.00.034.807 I llm_load_print_meta: n_embd           = 384
0.00.034.807 I llm_load_print_meta: n_layer          = 12
0.00.034.810 I llm_load_print_meta: n_head           = 12
0.00.034.811 I llm_load_print_meta: n_head_kv        = 12
0.00.034.811 I llm_load_print_meta: n_rot            = 32
0.00.034.812 I llm_load_print_meta: n_swa            = 0
0.00.034.812 I llm_load_print_meta: n_embd_head_k    = 32
0.00.034.812 I llm_load_print_meta: n_embd_head_v    = 32
0.00.034.813 I llm_load_print_meta: n_gqa            = 1
0.00.034.814 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.034.814 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.034.815 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.034.816 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.034.818 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.034.818 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.034.818 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.034.819 I llm_load_print_meta: n_ff             = 1536
0.00.034.820 I llm_load_print_meta: n_expert         = 0
0.00.034.822 I llm_load_print_meta: n_expert_used    = 0
0.00.034.822 I llm_load_print_meta: causal attn      = 0
0.00.034.822 I llm_load_print_meta: pooling type     = 2
0.00.034.823 I llm_load_print_meta: rope type        = 2
0.00.034.823 I llm_load_print_meta: rope scaling     = linear
0.00.034.823 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.034.824 I llm_load_print_meta: freq_scale_train = 1
0.00.034.824 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.034.824 I llm_load_print_meta: rope_finetuned   = unknown
0.00.034.824 I llm_load_print_meta: ssm_d_conv       = 0
0.00.034.825 I llm_load_print_meta: ssm_d_inner      = 0
0.00.034.825 I llm_load_print_meta: ssm_d_state      = 0
0.00.034.825 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.034.825 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.034.846 I llm_load_print_meta: model type       = 33M
0.00.034.847 I llm_load_print_meta: model ftype      = F16
0.00.034.847 I llm_load_print_meta: model params     = 33.21 M
0.00.034.848 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.034.848 I llm_load_print_meta: general.name     = Bge Small
0.00.034.851 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.034.851 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.034.851 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.034.851 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.034.852 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.034.852 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.034.853 I llm_load_print_meta: max token length = 21
0.00.036.875 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.036.875 I llm_load_tensors: offloading output layer to GPU
0.00.036.878 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.036.905 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.036.907 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.037.508 I llama_new_context_with_model: n_seq_max     = 1
0.00.037.510 I llama_new_context_with_model: n_ctx         = 512
0.00.037.510 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.037.510 I llama_new_context_with_model: n_batch       = 2048
0.00.037.511 I llama_new_context_with_model: n_ubatch      = 2048
0.00.037.511 I llama_new_context_with_model: flash_attn    = 0
0.00.037.512 I llama_new_context_with_model: freq_base     = 10000.0
0.00.037.512 I llama_new_context_with_model: freq_scale    = 1
0.00.037.513 I ggml_metal_init: allocating
0.00.037.524 I ggml_metal_init: found device: Apple M4
0.00.037.531 I ggml_metal_init: picking default device: Apple M4
0.00.038.441 I ggml_metal_init: using embedded metal library
0.00.042.707 I ggml_metal_init: GPU name:   Apple M4
0.00.042.709 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.042.710 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.042.711 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.042.711 I ggml_metal_init: simdgroup reduction   = true
0.00.042.711 I ggml_metal_init: simdgroup matrix mul. = true
0.00.042.711 I ggml_metal_init: has bfloat            = true
0.00.042.712 I ggml_metal_init: use bfloat            = true
0.00.042.712 I ggml_metal_init: hasUnifiedMemory      = true
0.00.042.713 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.054.910 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.055.483 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.055.485 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.055.486 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.056.222 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.056.224 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.056.224 I llama_new_context_with_model: graph nodes  = 429
0.00.056.224 I llama_new_context_with_model: graph splits = 2
0.00.056.245 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.056.246 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.062.904 I 
0.00.062.933 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.063.602 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.068.788 I llama_perf_context_print:        load time =      45.12 ms
0.00.068.789 I llama_perf_context_print: prompt eval time =       5.04 ms /     9 tokens (    0.56 ms per token,  1786.07 tokens per second)
0.00.068.789 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.068.790 I llama_perf_context_print:       total time =       5.88 ms /    10 tokens
0.00.068.936 I ggml_metal_free: deallocating

real	0m0.245s
user	0m0.048s
sys	0m0.029s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.038 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.226 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.270 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.274 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.276 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.276 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.276 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.277 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.277 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.278 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.278 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.278 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.279 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.279 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.281 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.281 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.281 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.281 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.284 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.284 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.284 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.720 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.419 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.420 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.420 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.420 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.421 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.421 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.421 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.422 I llama_model_loader: - type  f32:  124 tensors
0.00.014.422 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.900 I llm_load_vocab: special tokens cache size = 5
0.00.018.198 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.201 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.201 I llm_load_print_meta: arch             = bert
0.00.018.202 I llm_load_print_meta: vocab type       = WPM
0.00.018.202 I llm_load_print_meta: n_vocab          = 30522
0.00.018.202 I llm_load_print_meta: n_merges         = 0
0.00.018.202 I llm_load_print_meta: vocab_only       = 0
0.00.018.203 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.203 I llm_load_print_meta: n_embd           = 384
0.00.018.203 I llm_load_print_meta: n_layer          = 12
0.00.018.205 I llm_load_print_meta: n_head           = 12
0.00.018.206 I llm_load_print_meta: n_head_kv        = 12
0.00.018.206 I llm_load_print_meta: n_rot            = 32
0.00.018.208 I llm_load_print_meta: n_swa            = 0
0.00.018.208 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.208 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.209 I llm_load_print_meta: n_gqa            = 1
0.00.018.209 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.210 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.210 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.211 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.211 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.211 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.211 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.212 I llm_load_print_meta: n_ff             = 1536
0.00.018.212 I llm_load_print_meta: n_expert         = 0
0.00.018.212 I llm_load_print_meta: n_expert_used    = 0
0.00.018.212 I llm_load_print_meta: causal attn      = 0
0.00.018.212 I llm_load_print_meta: pooling type     = 2
0.00.018.212 I llm_load_print_meta: rope type        = 2
0.00.018.213 I llm_load_print_meta: rope scaling     = linear
0.00.018.213 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.213 I llm_load_print_meta: freq_scale_train = 1
0.00.018.213 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.213 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.214 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.214 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.214 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.214 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.214 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.222 I llm_load_print_meta: model type       = 33M
0.00.018.224 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.224 I llm_load_print_meta: model params     = 33.21 M
0.00.018.225 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.225 I llm_load_print_meta: general.name     = Bge Small
0.00.018.225 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.225 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.225 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.226 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.226 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.226 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.226 I llm_load_print_meta: max token length = 21
0.00.019.523 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.523 I llm_load_tensors: offloading output layer to GPU
0.00.019.523 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.531 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.532 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.879 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.879 I llama_new_context_with_model: n_ctx         = 512
0.00.019.880 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.880 I llama_new_context_with_model: n_batch       = 2048
0.00.019.880 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.880 I llama_new_context_with_model: flash_attn    = 0
0.00.019.881 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.881 I llama_new_context_with_model: freq_scale    = 1
0.00.019.881 I ggml_metal_init: allocating
0.00.019.884 I ggml_metal_init: found device: Apple M4
0.00.019.886 I ggml_metal_init: picking default device: Apple M4
0.00.020.499 I ggml_metal_init: using embedded metal library
0.00.022.979 I ggml_metal_init: GPU name:   Apple M4
0.00.022.981 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.981 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.982 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.982 I ggml_metal_init: simdgroup reduction   = true
0.00.022.982 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.982 I ggml_metal_init: has bfloat            = true
0.00.022.983 I ggml_metal_init: use bfloat            = true
0.00.022.983 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.984 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.368 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.033.863 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.864 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.867 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.520 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.521 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.521 I llama_new_context_with_model: graph nodes  = 429
0.00.034.522 I llama_new_context_with_model: graph splits = 2
0.00.034.535 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.536 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.949 I 
0.00.038.971 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.496 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.912 I llama_perf_context_print:        load time =      29.72 ms
0.00.043.914 I llama_perf_context_print: prompt eval time =       4.28 ms /     9 tokens (    0.48 ms per token,  2100.84 tokens per second)
0.00.043.915 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.918 I llama_perf_context_print:       total time =       4.96 ms /    10 tokens
0.00.044.084 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.030s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.131 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.265 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.441 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.447 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.450 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.030.460 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.461 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.030.462 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.030.462 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.030.464 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.030.465 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.030.465 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.030.469 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.030.470 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.030.473 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.030.474 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.030.474 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.030.477 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.478 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.038.644 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.040.796 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.589 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.045.592 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.593 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.045.593 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.045.594 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.045.594 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.045.594 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.045.595 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.045.595 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.045.596 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.045.596 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.045.596 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.045.600 I llama_model_loader: - type  f32:   40 tensors
0.00.045.600 I llama_model_loader: - type  f16:   30 tensors
0.00.064.643 W llm_load_vocab: empty token at index 5
0.00.069.407 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.070.820 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.070.871 I llm_load_vocab: special tokens cache size = 5
0.00.339.409 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.339.414 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.339.414 I llm_load_print_meta: arch             = jina-bert-v2
0.00.339.414 I llm_load_print_meta: vocab type       = BPE
0.00.339.415 I llm_load_print_meta: n_vocab          = 61056
0.00.339.415 I llm_load_print_meta: n_merges         = 39382
0.00.339.415 I llm_load_print_meta: vocab_only       = 0
0.00.339.415 I llm_load_print_meta: n_ctx_train      = 8192
0.00.339.415 I llm_load_print_meta: n_embd           = 384
0.00.339.416 I llm_load_print_meta: n_layer          = 4
0.00.339.421 I llm_load_print_meta: n_head           = 12
0.00.339.422 I llm_load_print_meta: n_head_kv        = 12
0.00.339.422 I llm_load_print_meta: n_rot            = 32
0.00.339.422 I llm_load_print_meta: n_swa            = 0
0.00.339.425 I llm_load_print_meta: n_embd_head_k    = 32
0.00.339.425 I llm_load_print_meta: n_embd_head_v    = 32
0.00.339.426 I llm_load_print_meta: n_gqa            = 1
0.00.339.426 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.339.427 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.339.428 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.339.429 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.339.429 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.339.429 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.339.429 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.339.430 I llm_load_print_meta: n_ff             = 1536
0.00.339.430 I llm_load_print_meta: n_expert         = 0
0.00.339.430 I llm_load_print_meta: n_expert_used    = 0
0.00.339.430 I llm_load_print_meta: causal attn      = 0
0.00.339.431 I llm_load_print_meta: pooling type     = -1
0.00.339.431 I llm_load_print_meta: rope type        = -1
0.00.339.431 I llm_load_print_meta: rope scaling     = linear
0.00.339.432 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.339.432 I llm_load_print_meta: freq_scale_train = 1
0.00.339.432 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.339.432 I llm_load_print_meta: rope_finetuned   = unknown
0.00.339.433 I llm_load_print_meta: ssm_d_conv       = 0
0.00.339.433 I llm_load_print_meta: ssm_d_inner      = 0
0.00.339.433 I llm_load_print_meta: ssm_d_state      = 0
0.00.339.433 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.339.433 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.339.434 I llm_load_print_meta: model type       = 33M
0.00.339.436 I llm_load_print_meta: model ftype      = F16
0.00.339.436 I llm_load_print_meta: model params     = 32.90 M
0.00.339.436 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.339.437 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.339.437 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.339.437 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.339.437 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.339.437 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.339.438 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.339.438 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.339.438 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.339.438 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.339.438 I llm_load_print_meta: max token length = 45
0.00.340.389 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.340.389 I llm_load_tensors: offloading output layer to GPU
0.00.340.390 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.340.414 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.340.415 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.341.145 I llama_new_context_with_model: n_seq_max     = 1
0.00.341.146 I llama_new_context_with_model: n_ctx         = 8192
0.00.341.146 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.341.147 I llama_new_context_with_model: n_batch       = 2048
0.00.341.147 I llama_new_context_with_model: n_ubatch      = 2048
0.00.341.147 I llama_new_context_with_model: flash_attn    = 0
0.00.341.148 I llama_new_context_with_model: freq_base     = 10000.0
0.00.341.148 I llama_new_context_with_model: freq_scale    = 1
0.00.341.148 I ggml_metal_init: allocating
0.00.341.152 I ggml_metal_init: found device: Apple M4
0.00.341.154 I ggml_metal_init: picking default device: Apple M4
0.00.341.940 I ggml_metal_init: using embedded metal library
0.00.344.606 I ggml_metal_init: GPU name:   Apple M4
0.00.344.608 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.344.608 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.344.609 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.344.609 I ggml_metal_init: simdgroup reduction   = true
0.00.344.609 I ggml_metal_init: simdgroup matrix mul. = true
0.00.344.609 I ggml_metal_init: has bfloat            = true
0.00.344.609 I ggml_metal_init: use bfloat            = true
0.00.344.610 I ggml_metal_init: hasUnifiedMemory      = true
0.00.344.611 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.354.774 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4
0.00.357.139 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.357.140 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.357.142 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.357.635 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.357.636 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.357.637 I llama_new_context_with_model: graph nodes  = 154
0.00.357.637 I llama_new_context_with_model: graph splits = 2
0.00.357.654 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.357.655 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.367.132 I 
0.00.367.163 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.367.308 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.367.308 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.367.311 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.367.311 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.367.314 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.367.314 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.367.810 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.371.443 I llama_perf_context_print:        load time =     346.86 ms
0.00.371.444 I llama_perf_context_print: prompt eval time =       3.62 ms /    62 tokens (    0.06 ms per token, 17103.45 tokens per second)
0.00.371.447 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.371.447 I llama_perf_context_print:       total time =       4.31 ms /    63 tokens
0.00.371.667 I ggml_metal_free: deallocating

real	0m1.111s
user	0m0.349s
sys	0m0.038s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.125 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.237 I main: llama backend init
0.00.000.244 I main: load the model and apply lora adapter, if any
0.00.030.332 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.041.058 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.076 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.080 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.080 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.081 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.082 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.082 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.085 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.085 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.086 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.086 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.087 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.087 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.088 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.099 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.099 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.100 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.246 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.458 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.060.085 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.060.088 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.060.089 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.060.089 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.060.090 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.060.091 I llama_model_loader: - type  f32:  194 tensors
0.00.060.092 I llama_model_loader: - type  f16:   98 tensors
0.00.091.317 I llm_load_vocab: special tokens cache size = 25
0.00.098.205 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.098.207 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.098.208 I llm_load_print_meta: arch             = gptneox
0.00.098.208 I llm_load_print_meta: vocab type       = BPE
0.00.098.208 I llm_load_print_meta: n_vocab          = 50304
0.00.098.208 I llm_load_print_meta: n_merges         = 50009
0.00.098.208 I llm_load_print_meta: vocab_only       = 0
0.00.098.209 I llm_load_print_meta: n_ctx_train      = 2048
0.00.098.209 I llm_load_print_meta: n_embd           = 2048
0.00.098.209 I llm_load_print_meta: n_layer          = 24
0.00.098.211 I llm_load_print_meta: n_head           = 16
0.00.098.212 I llm_load_print_meta: n_head_kv        = 16
0.00.098.212 I llm_load_print_meta: n_rot            = 32
0.00.098.214 I llm_load_print_meta: n_swa            = 0
0.00.098.215 I llm_load_print_meta: n_embd_head_k    = 128
0.00.098.215 I llm_load_print_meta: n_embd_head_v    = 128
0.00.098.215 I llm_load_print_meta: n_gqa            = 1
0.00.098.216 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.098.217 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.098.217 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.098.218 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.098.218 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.098.218 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.098.218 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.098.219 I llm_load_print_meta: n_ff             = 8192
0.00.098.219 I llm_load_print_meta: n_expert         = 0
0.00.098.219 I llm_load_print_meta: n_expert_used    = 0
0.00.098.219 I llm_load_print_meta: causal attn      = 1
0.00.098.219 I llm_load_print_meta: pooling type     = 0
0.00.098.219 I llm_load_print_meta: rope type        = 2
0.00.098.220 I llm_load_print_meta: rope scaling     = linear
0.00.098.220 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.098.220 I llm_load_print_meta: freq_scale_train = 1
0.00.098.220 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.098.221 I llm_load_print_meta: rope_finetuned   = unknown
0.00.098.221 I llm_load_print_meta: ssm_d_conv       = 0
0.00.098.222 I llm_load_print_meta: ssm_d_inner      = 0
0.00.098.222 I llm_load_print_meta: ssm_d_state      = 0
0.00.098.222 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.098.222 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.098.222 I llm_load_print_meta: model type       = 1.4B
0.00.098.223 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.098.223 I llm_load_print_meta: model params     = 1.41 B
0.00.098.224 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.098.224 I llm_load_print_meta: general.name     = 1.4B
0.00.098.224 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.098.224 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.098.224 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.098.225 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.098.225 I llm_load_print_meta: LF token         = 128 ''
0.00.098.225 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.098.225 I llm_load_print_meta: max token length = 1024
0.00.100.723 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.100.723 I llm_load_tensors: offloading output layer to GPU
0.00.100.724 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.100.742 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.100.743 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.101.698 I llama_new_context_with_model: n_seq_max     = 1
0.00.101.699 I llama_new_context_with_model: n_ctx         = 2048
0.00.101.699 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.101.699 I llama_new_context_with_model: n_batch       = 2048
0.00.101.699 I llama_new_context_with_model: n_ubatch      = 512
0.00.101.699 I llama_new_context_with_model: flash_attn    = 0
0.00.101.700 I llama_new_context_with_model: freq_base     = 10000.0
0.00.101.700 I llama_new_context_with_model: freq_scale    = 1
0.00.101.701 I ggml_metal_init: allocating
0.00.101.709 I ggml_metal_init: found device: Apple M4
0.00.101.712 I ggml_metal_init: picking default device: Apple M4
0.00.102.402 I ggml_metal_init: using embedded metal library
0.00.112.663 I ggml_metal_init: GPU name:   Apple M4
0.00.112.665 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.112.665 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.112.665 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.112.666 I ggml_metal_init: simdgroup reduction   = true
0.00.112.666 I ggml_metal_init: simdgroup matrix mul. = true
0.00.112.666 I ggml_metal_init: has bfloat            = true
0.00.112.666 I ggml_metal_init: use bfloat            = true
0.00.112.666 I ggml_metal_init: hasUnifiedMemory      = true
0.00.112.667 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.136.332 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.156.681 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.156.688 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.156.708 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.157.658 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.157.660 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.157.661 I llama_new_context_with_model: graph nodes  = 967
0.00.157.661 I llama_new_context_with_model: graph splits = 2
0.00.157.686 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.157.826 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.157.827 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.237.669 I main: llama threadpool init, n_threads = 4
0.00.237.703 I 
0.00.237.738 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.237.740 I 
0.00.237.811 I sampler seed: 1234
0.00.237.815 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.237.840 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.237.841 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.237.841 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.082.790 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54364.47 tokens per second)
0.02.082.791 I llama_perf_context_print:        load time =     207.33 ms
0.02.082.792 I llama_perf_context_print: prompt eval time =      43.83 ms /     7 tokens (    6.26 ms per token,   159.70 tokens per second)
0.02.082.792 I llama_perf_context_print:        eval time =    1798.06 ms /    63 runs   (   28.54 ms per token,    35.04 tokens per second)
0.02.082.793 I llama_perf_context_print:       total time =    1845.12 ms /    70 tokens
0.02.083.030 I ggml_metal_free: deallocating

real	0m2.362s
user	0m0.143s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.661 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.662 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.044 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.073 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.079 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.080 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.080 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.081 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.081 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.084 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.085 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.085 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.086 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.087 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.087 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.088 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.094 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.094 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.095 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.969 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.702 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.781 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.783 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.784 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.784 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.785 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.786 I llama_model_loader: - type  f32:  194 tensors
0.00.055.786 I llama_model_loader: - type  f16:   98 tensors
0.00.088.195 I llm_load_vocab: special tokens cache size = 25
0.00.095.189 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.095.193 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.095.193 I llm_load_print_meta: arch             = gptneox
0.00.095.193 I llm_load_print_meta: vocab type       = BPE
0.00.095.194 I llm_load_print_meta: n_vocab          = 50304
0.00.095.194 I llm_load_print_meta: n_merges         = 50009
0.00.095.194 I llm_load_print_meta: vocab_only       = 0
0.00.095.194 I llm_load_print_meta: n_ctx_train      = 2048
0.00.095.194 I llm_load_print_meta: n_embd           = 2048
0.00.095.194 I llm_load_print_meta: n_layer          = 24
0.00.095.198 I llm_load_print_meta: n_head           = 16
0.00.095.198 I llm_load_print_meta: n_head_kv        = 16
0.00.095.199 I llm_load_print_meta: n_rot            = 32
0.00.095.199 I llm_load_print_meta: n_swa            = 0
0.00.095.199 I llm_load_print_meta: n_embd_head_k    = 128
0.00.095.199 I llm_load_print_meta: n_embd_head_v    = 128
0.00.095.201 I llm_load_print_meta: n_gqa            = 1
0.00.095.202 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.095.202 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.095.203 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.095.203 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.095.204 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.095.204 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.095.204 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.095.205 I llm_load_print_meta: n_ff             = 8192
0.00.095.205 I llm_load_print_meta: n_expert         = 0
0.00.095.205 I llm_load_print_meta: n_expert_used    = 0
0.00.095.205 I llm_load_print_meta: causal attn      = 1
0.00.095.205 I llm_load_print_meta: pooling type     = 0
0.00.095.205 I llm_load_print_meta: rope type        = 2
0.00.095.206 I llm_load_print_meta: rope scaling     = linear
0.00.095.206 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.095.206 I llm_load_print_meta: freq_scale_train = 1
0.00.095.206 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.095.207 I llm_load_print_meta: rope_finetuned   = unknown
0.00.095.207 I llm_load_print_meta: ssm_d_conv       = 0
0.00.095.207 I llm_load_print_meta: ssm_d_inner      = 0
0.00.095.207 I llm_load_print_meta: ssm_d_state      = 0
0.00.095.207 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.095.207 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.095.208 I llm_load_print_meta: model type       = 1.4B
0.00.095.208 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.095.209 I llm_load_print_meta: model params     = 1.41 B
0.00.095.211 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.095.211 I llm_load_print_meta: general.name     = 1.4B
0.00.095.211 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.095.211 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.095.212 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.095.212 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.095.212 I llm_load_print_meta: LF token         = 128 ''
0.00.095.212 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.095.213 I llm_load_print_meta: max token length = 1024
0.00.097.147 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.097.147 I llm_load_tensors: offloading output layer to GPU
0.00.097.148 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.097.158 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.097.159 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.098.108 I llama_new_context_with_model: n_seq_max     = 1
0.00.098.109 I llama_new_context_with_model: n_ctx         = 128
0.00.098.110 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.098.110 I llama_new_context_with_model: n_batch       = 128
0.00.098.110 I llama_new_context_with_model: n_ubatch      = 128
0.00.098.110 I llama_new_context_with_model: flash_attn    = 0
0.00.098.111 I llama_new_context_with_model: freq_base     = 10000.0
0.00.098.111 I llama_new_context_with_model: freq_scale    = 1
0.00.098.111 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.098.112 I ggml_metal_init: allocating
0.00.098.118 I ggml_metal_init: found device: Apple M4
0.00.098.120 I ggml_metal_init: picking default device: Apple M4
0.00.098.785 I ggml_metal_init: using embedded metal library
0.00.101.461 I ggml_metal_init: GPU name:   Apple M4
0.00.101.462 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.101.463 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.101.463 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.101.463 I ggml_metal_init: simdgroup reduction   = true
0.00.101.464 I ggml_metal_init: simdgroup matrix mul. = true
0.00.101.464 I ggml_metal_init: has bfloat            = true
0.00.101.464 I ggml_metal_init: use bfloat            = true
0.00.101.464 I ggml_metal_init: hasUnifiedMemory      = true
0.00.101.465 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.637 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.112.042 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.112.045 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.112.060 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.113.050 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.113.051 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.113.052 I llama_new_context_with_model: graph nodes  = 967
0.00.113.052 I llama_new_context_with_model: graph splits = 2
0.00.113.064 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.113.065 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.487.375 I 
0.01.487.419 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.487.469 I perplexity: tokenizing the input ..
0.01.501.416 I perplexity: tokenization took 13.944 ms
0.01.501.429 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.623.984 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.625.738 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.625.768 I llama_perf_context_print:        load time =    1461.69 ms
0.01.625.770 I llama_perf_context_print: prompt eval time =     121.53 ms /   128 tokens (    0.95 ms per token,  1053.20 tokens per second)
0.01.625.771 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.625.772 I llama_perf_context_print:       total time =     138.39 ms /   129 tokens
0.01.626.560 I ggml_metal_free: deallocating

real	0m1.820s
user	0m0.129s
sys	0m0.270s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.522 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.262 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.023.267 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.270 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.271 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.271 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.271 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.271 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.272 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.273 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.273 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.273 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.274 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.274 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.274 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.276 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.276 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.278 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.213 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.299 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.278 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.279 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.280 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.280 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.280 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.281 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.281 I llama_model_loader: - type  f32:  194 tensors
0.00.032.282 I llama_model_loader: - type q8_0:   98 tensors
0.00.054.031 I llm_load_vocab: special tokens cache size = 25
0.00.060.014 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.018 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.018 I llm_load_print_meta: arch             = gptneox
0.00.060.019 I llm_load_print_meta: vocab type       = BPE
0.00.060.019 I llm_load_print_meta: n_vocab          = 50304
0.00.060.019 I llm_load_print_meta: n_merges         = 50009
0.00.060.019 I llm_load_print_meta: vocab_only       = 0
0.00.060.020 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.021 I llm_load_print_meta: n_embd           = 2048
0.00.060.022 I llm_load_print_meta: n_layer          = 24
0.00.060.028 I llm_load_print_meta: n_head           = 16
0.00.060.029 I llm_load_print_meta: n_head_kv        = 16
0.00.060.029 I llm_load_print_meta: n_rot            = 32
0.00.060.029 I llm_load_print_meta: n_swa            = 0
0.00.060.029 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.029 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.030 I llm_load_print_meta: n_gqa            = 1
0.00.060.031 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.031 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.032 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.032 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.033 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.033 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.033 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.034 I llm_load_print_meta: n_ff             = 8192
0.00.060.034 I llm_load_print_meta: n_expert         = 0
0.00.060.034 I llm_load_print_meta: n_expert_used    = 0
0.00.060.035 I llm_load_print_meta: causal attn      = 1
0.00.060.035 I llm_load_print_meta: pooling type     = 0
0.00.060.035 I llm_load_print_meta: rope type        = 2
0.00.060.035 I llm_load_print_meta: rope scaling     = linear
0.00.060.036 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.036 I llm_load_print_meta: freq_scale_train = 1
0.00.060.036 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.036 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.036 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.037 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.037 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.037 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.037 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.038 I llm_load_print_meta: model type       = 1.4B
0.00.060.038 I llm_load_print_meta: model ftype      = Q8_0
0.00.060.039 I llm_load_print_meta: model params     = 1.41 B
0.00.060.039 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.060.039 I llm_load_print_meta: general.name     = 1.4B
0.00.060.039 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.039 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.040 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.040 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.040 I llm_load_print_meta: LF token         = 128 ''
0.00.060.040 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.040 I llm_load_print_meta: max token length = 1024
0.00.062.175 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.175 I llm_load_tensors: offloading output layer to GPU
0.00.062.175 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.181 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.182 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.063.153 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.154 I llama_new_context_with_model: n_ctx         = 2048
0.00.063.155 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.063.155 I llama_new_context_with_model: n_batch       = 2048
0.00.063.155 I llama_new_context_with_model: n_ubatch      = 512
0.00.063.155 I llama_new_context_with_model: flash_attn    = 0
0.00.063.155 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.156 I llama_new_context_with_model: freq_scale    = 1
0.00.063.156 I ggml_metal_init: allocating
0.00.063.160 I ggml_metal_init: found device: Apple M4
0.00.063.162 I ggml_metal_init: picking default device: Apple M4
0.00.063.891 I ggml_metal_init: using embedded metal library
0.00.066.505 I ggml_metal_init: GPU name:   Apple M4
0.00.066.506 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.507 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.507 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.507 I ggml_metal_init: simdgroup reduction   = true
0.00.066.508 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.508 I ggml_metal_init: has bfloat            = true
0.00.066.508 I ggml_metal_init: use bfloat            = true
0.00.066.508 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.509 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.069 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.102.630 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.637 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.660 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.802 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.103.805 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.103.805 I llama_new_context_with_model: graph nodes  = 967
0.00.103.805 I llama_new_context_with_model: graph splits = 2
0.00.103.824 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.103.968 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.968 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.251.087 I main: llama threadpool init, n_threads = 4
0.01.251.119 I 
0.01.251.153 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.251.153 I 
0.01.251.381 I sampler seed: 1234
0.01.251.386 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.251.397 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.251.397 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.251.397 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.336.270 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62720.85 tokens per second)
0.02.336.271 I llama_perf_context_print:        load time =    1241.56 ms
0.02.336.272 I llama_perf_context_print: prompt eval time =      39.82 ms /     7 tokens (    5.69 ms per token,   175.79 tokens per second)
0.02.336.272 I llama_perf_context_print:        eval time =    1042.20 ms /    63 runs   (   16.54 ms per token,    60.45 tokens per second)
0.02.336.273 I llama_perf_context_print:       total time =    1085.19 ms /    70 tokens
0.02.336.495 I ggml_metal_free: deallocating

real	0m2.352s
user	0m0.113s
sys	0m0.237s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.335 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.685 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.531 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.537 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.539 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.540 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.540 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.541 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.541 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.543 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.543 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.543 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.544 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.544 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.545 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.545 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.548 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.548 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.548 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.508 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.062 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.521 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.523 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.524 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.524 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.524 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.525 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.525 I llama_model_loader: - type  f32:  194 tensors
0.00.034.526 I llama_model_loader: - type q8_0:   98 tensors
0.00.060.336 I llm_load_vocab: special tokens cache size = 25
0.00.066.486 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.488 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.489 I llm_load_print_meta: arch             = gptneox
0.00.066.489 I llm_load_print_meta: vocab type       = BPE
0.00.066.489 I llm_load_print_meta: n_vocab          = 50304
0.00.066.489 I llm_load_print_meta: n_merges         = 50009
0.00.066.490 I llm_load_print_meta: vocab_only       = 0
0.00.066.490 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.490 I llm_load_print_meta: n_embd           = 2048
0.00.066.490 I llm_load_print_meta: n_layer          = 24
0.00.066.493 I llm_load_print_meta: n_head           = 16
0.00.066.494 I llm_load_print_meta: n_head_kv        = 16
0.00.066.494 I llm_load_print_meta: n_rot            = 32
0.00.066.494 I llm_load_print_meta: n_swa            = 0
0.00.066.494 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.494 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.495 I llm_load_print_meta: n_gqa            = 1
0.00.066.495 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.496 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.496 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.497 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.497 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.497 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.497 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.498 I llm_load_print_meta: n_ff             = 8192
0.00.066.498 I llm_load_print_meta: n_expert         = 0
0.00.066.498 I llm_load_print_meta: n_expert_used    = 0
0.00.066.498 I llm_load_print_meta: causal attn      = 1
0.00.066.498 I llm_load_print_meta: pooling type     = 0
0.00.066.499 I llm_load_print_meta: rope type        = 2
0.00.066.499 I llm_load_print_meta: rope scaling     = linear
0.00.066.499 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.499 I llm_load_print_meta: freq_scale_train = 1
0.00.066.500 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.500 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.500 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.500 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.500 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.501 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.502 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.502 I llm_load_print_meta: model type       = 1.4B
0.00.066.502 I llm_load_print_meta: model ftype      = Q8_0
0.00.066.503 I llm_load_print_meta: model params     = 1.41 B
0.00.066.503 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.066.503 I llm_load_print_meta: general.name     = 1.4B
0.00.066.504 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.504 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.504 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.505 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.505 I llm_load_print_meta: LF token         = 128 ''
0.00.066.505 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.505 I llm_load_print_meta: max token length = 1024
0.00.068.817 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.817 I llm_load_tensors: offloading output layer to GPU
0.00.068.818 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.828 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.829 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.069.798 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.799 I llama_new_context_with_model: n_ctx         = 128
0.00.069.799 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.069.799 I llama_new_context_with_model: n_batch       = 128
0.00.069.800 I llama_new_context_with_model: n_ubatch      = 128
0.00.069.800 I llama_new_context_with_model: flash_attn    = 0
0.00.069.800 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.801 I llama_new_context_with_model: freq_scale    = 1
0.00.069.801 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.801 I ggml_metal_init: allocating
0.00.069.804 I ggml_metal_init: found device: Apple M4
0.00.069.808 I ggml_metal_init: picking default device: Apple M4
0.00.070.485 I ggml_metal_init: using embedded metal library
0.00.073.159 I ggml_metal_init: GPU name:   Apple M4
0.00.073.160 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.161 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.161 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.161 I ggml_metal_init: simdgroup reduction   = true
0.00.073.162 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.162 I ggml_metal_init: has bfloat            = true
0.00.073.162 I ggml_metal_init: use bfloat            = true
0.00.073.162 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.163 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.997 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.084.380 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.084.385 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.084.402 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.335 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.085.336 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.085.336 I llama_new_context_with_model: graph nodes  = 967
0.00.085.337 I llama_new_context_with_model: graph splits = 2
0.00.085.345 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.085.347 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.977.530 I 
0.00.977.561 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.977.597 I perplexity: tokenizing the input ..
0.00.985.288 I perplexity: tokenization took 7.689 ms
0.00.985.291 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.109.368 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.110.526 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.110.541 I llama_perf_context_print:        load time =     964.84 ms
0.01.110.543 I llama_perf_context_print: prompt eval time =     123.85 ms /   128 tokens (    0.97 ms per token,  1033.50 tokens per second)
0.01.110.543 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.110.544 I llama_perf_context_print:       total time =     133.01 ms /   129 tokens
0.01.110.969 I ggml_metal_free: deallocating

real	0m1.132s
user	0m0.094s
sys	0m0.179s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.017.000 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.118 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.031.124 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.126 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.126 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.127 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.127 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.127 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.128 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.128 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.129 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.132 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.132 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.132 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.133 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.135 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.136 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.136 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.152 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.318 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.598 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.040.600 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.600 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.601 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.601 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.601 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.040.602 I llama_model_loader: - type  f32:  194 tensors
0.00.040.602 I llama_model_loader: - type q4_0:   97 tensors
0.00.040.602 I llama_model_loader: - type q6_K:    1 tensors
0.00.065.712 I llm_load_vocab: special tokens cache size = 25
0.00.073.723 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.073.726 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.073.727 I llm_load_print_meta: arch             = gptneox
0.00.073.727 I llm_load_print_meta: vocab type       = BPE
0.00.073.727 I llm_load_print_meta: n_vocab          = 50304
0.00.073.727 I llm_load_print_meta: n_merges         = 50009
0.00.073.728 I llm_load_print_meta: vocab_only       = 0
0.00.073.728 I llm_load_print_meta: n_ctx_train      = 2048
0.00.073.728 I llm_load_print_meta: n_embd           = 2048
0.00.073.728 I llm_load_print_meta: n_layer          = 24
0.00.073.733 I llm_load_print_meta: n_head           = 16
0.00.073.734 I llm_load_print_meta: n_head_kv        = 16
0.00.073.734 I llm_load_print_meta: n_rot            = 32
0.00.073.736 I llm_load_print_meta: n_swa            = 0
0.00.073.736 I llm_load_print_meta: n_embd_head_k    = 128
0.00.073.738 I llm_load_print_meta: n_embd_head_v    = 128
0.00.073.739 I llm_load_print_meta: n_gqa            = 1
0.00.073.740 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.073.741 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.073.741 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.073.742 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.073.742 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.073.742 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.073.742 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.073.743 I llm_load_print_meta: n_ff             = 8192
0.00.073.744 I llm_load_print_meta: n_expert         = 0
0.00.073.744 I llm_load_print_meta: n_expert_used    = 0
0.00.073.744 I llm_load_print_meta: causal attn      = 1
0.00.073.744 I llm_load_print_meta: pooling type     = 0
0.00.073.744 I llm_load_print_meta: rope type        = 2
0.00.073.744 I llm_load_print_meta: rope scaling     = linear
0.00.073.745 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.073.745 I llm_load_print_meta: freq_scale_train = 1
0.00.073.746 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.073.746 I llm_load_print_meta: rope_finetuned   = unknown
0.00.073.746 I llm_load_print_meta: ssm_d_conv       = 0
0.00.073.746 I llm_load_print_meta: ssm_d_inner      = 0
0.00.073.748 I llm_load_print_meta: ssm_d_state      = 0
0.00.073.748 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.073.748 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.073.749 I llm_load_print_meta: model type       = 1.4B
0.00.073.749 I llm_load_print_meta: model ftype      = Q4_0
0.00.073.750 I llm_load_print_meta: model params     = 1.41 B
0.00.073.751 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.073.751 I llm_load_print_meta: general.name     = 1.4B
0.00.073.751 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.073.751 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.073.753 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.073.753 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.073.754 I llm_load_print_meta: LF token         = 128 ''
0.00.073.754 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.073.755 I llm_load_print_meta: max token length = 1024
0.00.076.150 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.076.150 I llm_load_tensors: offloading output layer to GPU
0.00.076.151 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.076.157 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.076.158 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.077.334 I llama_new_context_with_model: n_seq_max     = 1
0.00.077.334 I llama_new_context_with_model: n_ctx         = 2048
0.00.077.335 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.077.335 I llama_new_context_with_model: n_batch       = 2048
0.00.077.335 I llama_new_context_with_model: n_ubatch      = 512
0.00.077.335 I llama_new_context_with_model: flash_attn    = 0
0.00.077.336 I llama_new_context_with_model: freq_base     = 10000.0
0.00.077.336 I llama_new_context_with_model: freq_scale    = 1
0.00.077.337 I ggml_metal_init: allocating
0.00.077.340 I ggml_metal_init: found device: Apple M4
0.00.077.343 I ggml_metal_init: picking default device: Apple M4
0.00.078.153 I ggml_metal_init: using embedded metal library
0.00.081.333 I ggml_metal_init: GPU name:   Apple M4
0.00.081.335 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.081.336 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.081.336 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.081.336 I ggml_metal_init: simdgroup reduction   = true
0.00.081.337 I ggml_metal_init: simdgroup matrix mul. = true
0.00.081.337 I ggml_metal_init: has bfloat            = true
0.00.081.337 I ggml_metal_init: use bfloat            = true
0.00.081.337 I ggml_metal_init: hasUnifiedMemory      = true
0.00.081.338 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.448 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.120.657 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.120.664 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.120.685 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.121.821 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.121.823 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.121.823 I llama_new_context_with_model: graph nodes  = 967
0.00.121.823 I llama_new_context_with_model: graph splits = 2
0.00.121.842 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.121.982 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.121.983 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.720.679 I main: llama threadpool init, n_threads = 4
0.00.720.717 I 
0.00.720.755 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.720.756 I 
0.00.720.981 I sampler seed: 1234
0.00.720.986 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.720.997 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.720.997 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.720.997 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.407.268 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52167.52 tokens per second)
0.01.407.269 I llama_perf_context_print:        load time =     703.68 ms
0.01.407.270 I llama_perf_context_print: prompt eval time =      44.75 ms /     7 tokens (    6.39 ms per token,   156.42 tokens per second)
0.01.407.271 I llama_perf_context_print:        eval time =     638.47 ms /    63 runs   (   10.13 ms per token,    98.67 tokens per second)
0.01.407.271 I llama_perf_context_print:       total time =     686.59 ms /    70 tokens
0.01.407.461 I ggml_metal_free: deallocating

real	0m1.425s
user	0m0.124s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.280 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.624 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.337 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.341 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.342 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.343 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.343 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.343 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.344 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.345 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.345 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.345 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.346 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.346 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.346 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.347 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.350 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.350 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.351 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.287 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.415 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.336 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.337 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.337 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.337 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.338 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.338 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.339 I llama_model_loader: - type  f32:  194 tensors
0.00.024.339 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.339 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.608 I llm_load_vocab: special tokens cache size = 25
0.00.051.561 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.563 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.563 I llm_load_print_meta: arch             = gptneox
0.00.051.563 I llm_load_print_meta: vocab type       = BPE
0.00.051.564 I llm_load_print_meta: n_vocab          = 50304
0.00.051.564 I llm_load_print_meta: n_merges         = 50009
0.00.051.564 I llm_load_print_meta: vocab_only       = 0
0.00.051.564 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.564 I llm_load_print_meta: n_embd           = 2048
0.00.051.564 I llm_load_print_meta: n_layer          = 24
0.00.051.567 I llm_load_print_meta: n_head           = 16
0.00.051.568 I llm_load_print_meta: n_head_kv        = 16
0.00.051.568 I llm_load_print_meta: n_rot            = 32
0.00.051.568 I llm_load_print_meta: n_swa            = 0
0.00.051.568 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.570 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.571 I llm_load_print_meta: n_gqa            = 1
0.00.051.572 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.573 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.573 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.574 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.574 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.574 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.574 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.575 I llm_load_print_meta: n_ff             = 8192
0.00.051.575 I llm_load_print_meta: n_expert         = 0
0.00.051.576 I llm_load_print_meta: n_expert_used    = 0
0.00.051.576 I llm_load_print_meta: causal attn      = 1
0.00.051.576 I llm_load_print_meta: pooling type     = 0
0.00.051.576 I llm_load_print_meta: rope type        = 2
0.00.051.577 I llm_load_print_meta: rope scaling     = linear
0.00.051.577 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.577 I llm_load_print_meta: freq_scale_train = 1
0.00.051.579 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.579 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.579 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.579 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.580 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.580 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.580 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.580 I llm_load_print_meta: model type       = 1.4B
0.00.051.581 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.581 I llm_load_print_meta: model params     = 1.41 B
0.00.051.582 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.582 I llm_load_print_meta: general.name     = 1.4B
0.00.051.582 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.582 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.583 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.584 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.584 I llm_load_print_meta: LF token         = 128 ''
0.00.051.584 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.584 I llm_load_print_meta: max token length = 1024
0.00.053.521 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.521 I llm_load_tensors: offloading output layer to GPU
0.00.053.522 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.532 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.534 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.446 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.447 I llama_new_context_with_model: n_ctx         = 128
0.00.054.447 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.448 I llama_new_context_with_model: n_batch       = 128
0.00.054.448 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.448 I llama_new_context_with_model: flash_attn    = 0
0.00.054.448 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.449 I llama_new_context_with_model: freq_scale    = 1
0.00.054.449 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.449 I ggml_metal_init: allocating
0.00.054.456 I ggml_metal_init: found device: Apple M4
0.00.054.459 I ggml_metal_init: picking default device: Apple M4
0.00.055.070 I ggml_metal_init: using embedded metal library
0.00.057.472 I ggml_metal_init: GPU name:   Apple M4
0.00.057.473 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.473 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.474 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.474 I ggml_metal_init: simdgroup reduction   = true
0.00.057.474 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.474 I ggml_metal_init: has bfloat            = true
0.00.057.475 I ggml_metal_init: use bfloat            = true
0.00.057.475 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.476 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.394 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.610 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.613 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.629 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.478 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.479 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.480 I llama_new_context_with_model: graph nodes  = 967
0.00.069.480 I llama_new_context_with_model: graph splits = 2
0.00.069.492 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.493 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.618.712 I 
0.00.618.757 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.618.769 I perplexity: tokenizing the input ..
0.00.626.673 I perplexity: tokenization took 7.903 ms
0.00.626.677 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.749.566 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.750.721 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.750.737 I llama_perf_context_print:        load time =     609.08 ms
0.00.750.738 I llama_perf_context_print: prompt eval time =     122.63 ms /   128 tokens (    0.96 ms per token,  1043.76 tokens per second)
0.00.750.739 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.750.739 I llama_perf_context_print:       total time =     132.03 ms /   129 tokens
0.00.751.145 I ggml_metal_free: deallocating

real	0m0.765s
user	0m0.079s
sys	0m0.105s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.659 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.980 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.019.983 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.985 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.985 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.985 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.986 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.986 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.987 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.987 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.987 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.988 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.988 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.988 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.988 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.991 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.992 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.992 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.771 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.833 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.542 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.543 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.544 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.544 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.544 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.545 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.028.545 I llama_model_loader: - type  f32:  194 tensors
0.00.028.545 I llama_model_loader: - type q4_1:   97 tensors
0.00.028.546 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.057 I llm_load_vocab: special tokens cache size = 25
0.00.054.940 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.942 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.943 I llm_load_print_meta: arch             = gptneox
0.00.054.943 I llm_load_print_meta: vocab type       = BPE
0.00.054.943 I llm_load_print_meta: n_vocab          = 50304
0.00.054.944 I llm_load_print_meta: n_merges         = 50009
0.00.054.944 I llm_load_print_meta: vocab_only       = 0
0.00.054.944 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.944 I llm_load_print_meta: n_embd           = 2048
0.00.054.944 I llm_load_print_meta: n_layer          = 24
0.00.054.947 I llm_load_print_meta: n_head           = 16
0.00.054.948 I llm_load_print_meta: n_head_kv        = 16
0.00.054.948 I llm_load_print_meta: n_rot            = 32
0.00.054.949 I llm_load_print_meta: n_swa            = 0
0.00.054.949 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.950 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.951 I llm_load_print_meta: n_gqa            = 1
0.00.054.951 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.952 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.953 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.953 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.953 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.955 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.955 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.956 I llm_load_print_meta: n_ff             = 8192
0.00.054.956 I llm_load_print_meta: n_expert         = 0
0.00.054.958 I llm_load_print_meta: n_expert_used    = 0
0.00.054.960 I llm_load_print_meta: causal attn      = 1
0.00.054.960 I llm_load_print_meta: pooling type     = 0
0.00.054.960 I llm_load_print_meta: rope type        = 2
0.00.054.960 I llm_load_print_meta: rope scaling     = linear
0.00.054.961 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.961 I llm_load_print_meta: freq_scale_train = 1
0.00.054.961 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.961 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.961 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.962 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.962 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.962 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.966 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.966 I llm_load_print_meta: model type       = 1.4B
0.00.054.966 I llm_load_print_meta: model ftype      = Q4_1
0.00.054.967 I llm_load_print_meta: model params     = 1.41 B
0.00.054.967 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.054.967 I llm_load_print_meta: general.name     = 1.4B
0.00.054.968 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.968 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.968 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.968 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.968 I llm_load_print_meta: LF token         = 128 ''
0.00.054.969 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.969 I llm_load_print_meta: max token length = 1024
0.00.056.957 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.958 I llm_load_tensors: offloading output layer to GPU
0.00.056.958 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.968 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.056.969 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.057.956 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.957 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.957 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.957 I llama_new_context_with_model: n_batch       = 2048
0.00.057.958 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.958 I llama_new_context_with_model: flash_attn    = 0
0.00.057.958 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.958 I llama_new_context_with_model: freq_scale    = 1
0.00.057.959 I ggml_metal_init: allocating
0.00.057.962 I ggml_metal_init: found device: Apple M4
0.00.057.964 I ggml_metal_init: picking default device: Apple M4
0.00.058.569 I ggml_metal_init: using embedded metal library
0.00.060.898 I ggml_metal_init: GPU name:   Apple M4
0.00.060.900 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.901 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.901 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.901 I ggml_metal_init: simdgroup reduction   = true
0.00.060.901 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.901 I ggml_metal_init: has bfloat            = true
0.00.060.903 I ggml_metal_init: use bfloat            = true
0.00.060.903 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.904 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.787 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.090.365 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.371 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.388 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.417 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.419 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.419 I llama_new_context_with_model: graph nodes  = 967
0.00.091.420 I llama_new_context_with_model: graph splits = 2
0.00.091.431 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.571 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.572 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.273.271 I main: llama threadpool init, n_threads = 4
0.01.273.317 I 
0.01.273.349 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.273.351 I 
0.01.273.578 I sampler seed: 1234
0.01.273.583 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.273.602 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.273.602 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.273.602 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.02.011.735 I llama_perf_sampler_print:    sampling time =       1.51 ms /    71 runs   (    0.02 ms per token, 46988.75 tokens per second)
0.02.011.736 I llama_perf_context_print:        load time =    1264.61 ms
0.02.011.737 I llama_perf_context_print: prompt eval time =      49.35 ms /     7 tokens (    7.05 ms per token,   141.86 tokens per second)
0.02.011.737 I llama_perf_context_print:        eval time =     685.97 ms /    63 runs   (   10.89 ms per token,    91.84 tokens per second)
0.02.011.738 I llama_perf_context_print:       total time =     738.47 ms /    70 tokens
0.02.011.958 I ggml_metal_free: deallocating

real	0m2.033s
user	0m0.112s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.694 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.549 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.553 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.555 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.556 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.556 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.556 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.557 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.559 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.560 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.560 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.560 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.561 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.561 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.561 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.563 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.563 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.563 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.409 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.419 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.198 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.199 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.200 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.200 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.200 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.201 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.201 I llama_model_loader: - type  f32:  194 tensors
0.00.023.202 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.202 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.215 I llm_load_vocab: special tokens cache size = 25
0.00.050.127 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.129 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.130 I llm_load_print_meta: arch             = gptneox
0.00.050.130 I llm_load_print_meta: vocab type       = BPE
0.00.050.130 I llm_load_print_meta: n_vocab          = 50304
0.00.050.131 I llm_load_print_meta: n_merges         = 50009
0.00.050.131 I llm_load_print_meta: vocab_only       = 0
0.00.050.131 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.131 I llm_load_print_meta: n_embd           = 2048
0.00.050.131 I llm_load_print_meta: n_layer          = 24
0.00.050.134 I llm_load_print_meta: n_head           = 16
0.00.050.135 I llm_load_print_meta: n_head_kv        = 16
0.00.050.135 I llm_load_print_meta: n_rot            = 32
0.00.050.135 I llm_load_print_meta: n_swa            = 0
0.00.050.135 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.135 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.136 I llm_load_print_meta: n_gqa            = 1
0.00.050.137 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.138 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.138 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.139 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.139 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.139 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.144 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.145 I llm_load_print_meta: n_ff             = 8192
0.00.050.145 I llm_load_print_meta: n_expert         = 0
0.00.050.145 I llm_load_print_meta: n_expert_used    = 0
0.00.050.145 I llm_load_print_meta: causal attn      = 1
0.00.050.145 I llm_load_print_meta: pooling type     = 0
0.00.050.145 I llm_load_print_meta: rope type        = 2
0.00.050.146 I llm_load_print_meta: rope scaling     = linear
0.00.050.146 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.147 I llm_load_print_meta: freq_scale_train = 1
0.00.050.147 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.147 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.147 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.147 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.148 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.148 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.148 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.148 I llm_load_print_meta: model type       = 1.4B
0.00.050.149 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.150 I llm_load_print_meta: model params     = 1.41 B
0.00.050.151 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.151 I llm_load_print_meta: general.name     = 1.4B
0.00.050.151 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.152 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.152 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.152 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.152 I llm_load_print_meta: LF token         = 128 ''
0.00.050.152 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.153 I llm_load_print_meta: max token length = 1024
0.00.052.113 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.113 I llm_load_tensors: offloading output layer to GPU
0.00.052.113 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.124 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.125 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.044 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.045 I llama_new_context_with_model: n_ctx         = 128
0.00.053.045 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.045 I llama_new_context_with_model: n_batch       = 128
0.00.053.045 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.045 I llama_new_context_with_model: flash_attn    = 0
0.00.053.046 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.046 I llama_new_context_with_model: freq_scale    = 1
0.00.053.046 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.047 I ggml_metal_init: allocating
0.00.053.050 I ggml_metal_init: found device: Apple M4
0.00.053.052 I ggml_metal_init: picking default device: Apple M4
0.00.053.638 I ggml_metal_init: using embedded metal library
0.00.055.957 I ggml_metal_init: GPU name:   Apple M4
0.00.055.959 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.959 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.959 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.960 I ggml_metal_init: simdgroup reduction   = true
0.00.055.960 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.960 I ggml_metal_init: has bfloat            = true
0.00.055.960 I ggml_metal_init: use bfloat            = true
0.00.055.960 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.961 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.765 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.283 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.288 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.303 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.154 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.154 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.155 I llama_new_context_with_model: graph nodes  = 967
0.00.068.155 I llama_new_context_with_model: graph splits = 2
0.00.068.167 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.168 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.031 I 
0.00.650.076 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.088 I perplexity: tokenizing the input ..
0.00.657.750 I perplexity: tokenization took 7.661 ms
0.00.657.754 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.780.498 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.781.726 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.781.743 I llama_perf_context_print:        load time =     641.33 ms
0.00.781.744 I llama_perf_context_print: prompt eval time =     122.52 ms /   128 tokens (    0.96 ms per token,  1044.75 tokens per second)
0.00.781.745 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.781.745 I llama_perf_context_print:       total time =     131.72 ms /   129 tokens
0.00.782.225 I ggml_metal_free: deallocating

real	0m0.796s
user	0m0.079s
sys	0m0.096s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.430 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.619 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.623 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.625 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.625 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.625 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.630 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.630 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.631 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.631 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.632 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.632 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.632 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.633 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.633 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.636 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.636 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.636 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.373 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.389 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.059 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.060 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.060 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.060 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.061 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.061 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.061 I llama_model_loader: - type  f32:  194 tensors
0.00.024.062 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.062 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.283 I llm_load_vocab: special tokens cache size = 25
0.00.050.151 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.154 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.154 I llm_load_print_meta: arch             = gptneox
0.00.050.154 I llm_load_print_meta: vocab type       = BPE
0.00.050.154 I llm_load_print_meta: n_vocab          = 50304
0.00.050.155 I llm_load_print_meta: n_merges         = 50009
0.00.050.155 I llm_load_print_meta: vocab_only       = 0
0.00.050.155 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.155 I llm_load_print_meta: n_embd           = 2048
0.00.050.155 I llm_load_print_meta: n_layer          = 24
0.00.050.158 I llm_load_print_meta: n_head           = 16
0.00.050.159 I llm_load_print_meta: n_head_kv        = 16
0.00.050.159 I llm_load_print_meta: n_rot            = 32
0.00.050.159 I llm_load_print_meta: n_swa            = 0
0.00.050.159 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.160 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.160 I llm_load_print_meta: n_gqa            = 1
0.00.050.161 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.162 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.162 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.163 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.163 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.163 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.165 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.166 I llm_load_print_meta: n_ff             = 8192
0.00.050.166 I llm_load_print_meta: n_expert         = 0
0.00.050.166 I llm_load_print_meta: n_expert_used    = 0
0.00.050.168 I llm_load_print_meta: causal attn      = 1
0.00.050.170 I llm_load_print_meta: pooling type     = 0
0.00.050.170 I llm_load_print_meta: rope type        = 2
0.00.050.170 I llm_load_print_meta: rope scaling     = linear
0.00.050.170 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.171 I llm_load_print_meta: freq_scale_train = 1
0.00.050.171 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.171 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.171 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.171 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.171 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.172 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.172 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.172 I llm_load_print_meta: model type       = 1.4B
0.00.050.172 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.173 I llm_load_print_meta: model params     = 1.41 B
0.00.050.177 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.177 I llm_load_print_meta: general.name     = 1.4B
0.00.050.178 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.178 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.178 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.178 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.178 I llm_load_print_meta: LF token         = 128 ''
0.00.050.179 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.179 I llm_load_print_meta: max token length = 1024
0.00.051.929 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.929 I llm_load_tensors: offloading output layer to GPU
0.00.051.929 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.934 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.935 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.799 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.799 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.800 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.800 I llama_new_context_with_model: n_batch       = 2048
0.00.052.800 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.800 I llama_new_context_with_model: flash_attn    = 0
0.00.052.800 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.801 I llama_new_context_with_model: freq_scale    = 1
0.00.052.801 I ggml_metal_init: allocating
0.00.052.804 I ggml_metal_init: found device: Apple M4
0.00.052.806 I ggml_metal_init: picking default device: Apple M4
0.00.053.451 I ggml_metal_init: using embedded metal library
0.00.055.768 I ggml_metal_init: GPU name:   Apple M4
0.00.055.769 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.769 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.770 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.770 I ggml_metal_init: simdgroup reduction   = true
0.00.055.770 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.770 I ggml_metal_init: has bfloat            = true
0.00.055.771 I ggml_metal_init: use bfloat            = true
0.00.055.771 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.773 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.482 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.688 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.697 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.730 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.858 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.860 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.860 I llama_new_context_with_model: graph nodes  = 967
0.00.086.861 I llama_new_context_with_model: graph splits = 2
0.00.086.876 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.037 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.038 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.639 I main: llama threadpool init, n_threads = 4
0.00.769.677 I 
0.00.769.707 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.708 I 
0.00.769.935 I sampler seed: 1234
0.00.769.939 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.963 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.964 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.964 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.562.485 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60839.76 tokens per second)
0.01.562.486 I llama_perf_context_print:        load time =     760.21 ms
0.01.562.487 I llama_perf_context_print: prompt eval time =      45.57 ms /     7 tokens (    6.51 ms per token,   153.62 tokens per second)
0.01.562.489 I llama_perf_context_print:        eval time =     744.01 ms /    63 runs   (   11.81 ms per token,    84.68 tokens per second)
0.01.562.490 I llama_perf_context_print:       total time =     792.85 ms /    70 tokens
0.01.562.690 I ggml_metal_free: deallocating

real	0m1.581s
user	0m0.110s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.367 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.888 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.014.893 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.895 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.895 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.901 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.901 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.902 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.904 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.904 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.905 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.905 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.905 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.906 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.906 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.908 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.908 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.908 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.729 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.752 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.504 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.505 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.505 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.506 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.506 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.506 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.507 I llama_model_loader: - type  f32:  194 tensors
0.00.023.507 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.507 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.892 I llm_load_vocab: special tokens cache size = 25
0.00.049.845 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.848 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.848 I llm_load_print_meta: arch             = gptneox
0.00.049.848 I llm_load_print_meta: vocab type       = BPE
0.00.049.848 I llm_load_print_meta: n_vocab          = 50304
0.00.049.849 I llm_load_print_meta: n_merges         = 50009
0.00.049.849 I llm_load_print_meta: vocab_only       = 0
0.00.049.849 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.849 I llm_load_print_meta: n_embd           = 2048
0.00.049.849 I llm_load_print_meta: n_layer          = 24
0.00.049.852 I llm_load_print_meta: n_head           = 16
0.00.049.853 I llm_load_print_meta: n_head_kv        = 16
0.00.049.853 I llm_load_print_meta: n_rot            = 32
0.00.049.854 I llm_load_print_meta: n_swa            = 0
0.00.049.854 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.854 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.855 I llm_load_print_meta: n_gqa            = 1
0.00.049.855 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.856 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.857 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.857 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.857 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.859 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.860 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.860 I llm_load_print_meta: n_ff             = 8192
0.00.049.861 I llm_load_print_meta: n_expert         = 0
0.00.049.861 I llm_load_print_meta: n_expert_used    = 0
0.00.049.861 I llm_load_print_meta: causal attn      = 1
0.00.049.862 I llm_load_print_meta: pooling type     = 0
0.00.049.862 I llm_load_print_meta: rope type        = 2
0.00.049.863 I llm_load_print_meta: rope scaling     = linear
0.00.049.863 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.863 I llm_load_print_meta: freq_scale_train = 1
0.00.049.864 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.864 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.864 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.864 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.864 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.864 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.864 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.866 I llm_load_print_meta: model type       = 1.4B
0.00.049.866 I llm_load_print_meta: model ftype      = Q5_0
0.00.049.867 I llm_load_print_meta: model params     = 1.41 B
0.00.049.867 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.049.867 I llm_load_print_meta: general.name     = 1.4B
0.00.049.872 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.873 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.873 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.873 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.873 I llm_load_print_meta: LF token         = 128 ''
0.00.049.875 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.875 I llm_load_print_meta: max token length = 1024
0.00.051.689 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.689 I llm_load_tensors: offloading output layer to GPU
0.00.051.689 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.695 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.696 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.685 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.686 I llama_new_context_with_model: n_ctx         = 128
0.00.052.687 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.687 I llama_new_context_with_model: n_batch       = 128
0.00.052.687 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.687 I llama_new_context_with_model: flash_attn    = 0
0.00.052.687 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.688 I llama_new_context_with_model: freq_scale    = 1
0.00.052.688 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.689 I ggml_metal_init: allocating
0.00.052.692 I ggml_metal_init: found device: Apple M4
0.00.052.694 I ggml_metal_init: picking default device: Apple M4
0.00.053.262 I ggml_metal_init: using embedded metal library
0.00.055.576 I ggml_metal_init: GPU name:   Apple M4
0.00.055.577 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.578 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.578 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.578 I ggml_metal_init: simdgroup reduction   = true
0.00.055.578 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.579 I ggml_metal_init: has bfloat            = true
0.00.055.579 I ggml_metal_init: use bfloat            = true
0.00.055.579 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.580 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.381 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.686 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.688 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.711 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.623 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.624 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.624 I llama_new_context_with_model: graph nodes  = 967
0.00.067.625 I llama_new_context_with_model: graph splits = 2
0.00.067.638 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.639 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.719.902 I 
0.00.719.942 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.719.954 I perplexity: tokenizing the input ..
0.00.727.634 I perplexity: tokenization took 7.679 ms
0.00.727.638 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.862.816 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.863.989 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.864.005 I llama_perf_context_print:        load time =     710.53 ms
0.00.864.006 I llama_perf_context_print: prompt eval time =     134.95 ms /   128 tokens (    1.05 ms per token,   948.48 tokens per second)
0.00.864.007 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.864.007 I llama_perf_context_print:       total time =     144.11 ms /   129 tokens
0.00.864.475 I ggml_metal_free: deallocating

real	0m0.879s
user	0m0.078s
sys	0m0.121s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.683 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.003 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.007 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.009 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.013 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.014 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.014 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.014 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.015 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.015 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.016 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.016 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.016 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.017 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.017 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.019 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.019 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.019 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.955 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.078 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.933 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.934 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.934 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.935 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.935 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.935 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.936 I llama_model_loader: - type  f32:  194 tensors
0.00.023.936 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.937 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.010 I llm_load_vocab: special tokens cache size = 25
0.00.050.947 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.949 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.950 I llm_load_print_meta: arch             = gptneox
0.00.050.950 I llm_load_print_meta: vocab type       = BPE
0.00.050.950 I llm_load_print_meta: n_vocab          = 50304
0.00.050.950 I llm_load_print_meta: n_merges         = 50009
0.00.050.951 I llm_load_print_meta: vocab_only       = 0
0.00.050.951 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.951 I llm_load_print_meta: n_embd           = 2048
0.00.050.951 I llm_load_print_meta: n_layer          = 24
0.00.050.954 I llm_load_print_meta: n_head           = 16
0.00.050.955 I llm_load_print_meta: n_head_kv        = 16
0.00.050.955 I llm_load_print_meta: n_rot            = 32
0.00.050.955 I llm_load_print_meta: n_swa            = 0
0.00.050.955 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.955 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.956 I llm_load_print_meta: n_gqa            = 1
0.00.050.957 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.960 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.960 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.961 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.962 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.962 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.962 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.963 I llm_load_print_meta: n_ff             = 8192
0.00.050.965 I llm_load_print_meta: n_expert         = 0
0.00.050.965 I llm_load_print_meta: n_expert_used    = 0
0.00.050.965 I llm_load_print_meta: causal attn      = 1
0.00.050.965 I llm_load_print_meta: pooling type     = 0
0.00.050.965 I llm_load_print_meta: rope type        = 2
0.00.050.965 I llm_load_print_meta: rope scaling     = linear
0.00.050.966 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.966 I llm_load_print_meta: freq_scale_train = 1
0.00.050.967 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.967 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.967 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.967 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.967 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.967 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.967 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.968 I llm_load_print_meta: model type       = 1.4B
0.00.050.968 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.969 I llm_load_print_meta: model params     = 1.41 B
0.00.050.969 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.969 I llm_load_print_meta: general.name     = 1.4B
0.00.050.970 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.970 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.970 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.971 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.971 I llm_load_print_meta: LF token         = 128 ''
0.00.050.972 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.972 I llm_load_print_meta: max token length = 1024
0.00.053.007 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.007 I llm_load_tensors: offloading output layer to GPU
0.00.053.007 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.018 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.019 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.922 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.922 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.923 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.923 I llama_new_context_with_model: n_batch       = 2048
0.00.053.923 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.923 I llama_new_context_with_model: flash_attn    = 0
0.00.053.924 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.924 I llama_new_context_with_model: freq_scale    = 1
0.00.053.925 I ggml_metal_init: allocating
0.00.053.931 I ggml_metal_init: found device: Apple M4
0.00.053.933 I ggml_metal_init: picking default device: Apple M4
0.00.054.517 I ggml_metal_init: using embedded metal library
0.00.056.858 I ggml_metal_init: GPU name:   Apple M4
0.00.056.859 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.860 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.860 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.860 I ggml_metal_init: simdgroup reduction   = true
0.00.056.860 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.861 I ggml_metal_init: has bfloat            = true
0.00.056.861 I ggml_metal_init: use bfloat            = true
0.00.056.861 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.862 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.615 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.822 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.829 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.847 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.780 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.781 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.781 I llama_new_context_with_model: graph nodes  = 967
0.00.087.781 I llama_new_context_with_model: graph splits = 2
0.00.087.798 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.927 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.927 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.726.496 I main: llama threadpool init, n_threads = 4
0.00.726.541 I 
0.00.726.590 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.726.592 I 
0.00.726.823 I sampler seed: 1234
0.00.726.829 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.726.840 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.726.840 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.726.841 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.566.006 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59764.31 tokens per second)
0.01.566.006 I llama_perf_context_print:        load time =     717.81 ms
0.01.566.007 I llama_perf_context_print: prompt eval time =      42.22 ms /     7 tokens (    6.03 ms per token,   165.79 tokens per second)
0.01.566.008 I llama_perf_context_print:        eval time =     794.02 ms /    63 runs   (   12.60 ms per token,    79.34 tokens per second)
0.01.566.008 I llama_perf_context_print:       total time =     839.51 ms /    70 tokens
0.01.566.194 I ggml_metal_free: deallocating

real	0m1.583s
user	0m0.110s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.708 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.349 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.353 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.355 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.361 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.362 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.362 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.362 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.363 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.363 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.365 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.366 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.366 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.366 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.367 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.368 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.368 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.369 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.160 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.166 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.944 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.945 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.946 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.946 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.946 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.947 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.022.947 I llama_model_loader: - type  f32:  194 tensors
0.00.022.947 I llama_model_loader: - type q5_1:   97 tensors
0.00.022.948 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.980 I llm_load_vocab: special tokens cache size = 25
0.00.049.768 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.771 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.771 I llm_load_print_meta: arch             = gptneox
0.00.049.771 I llm_load_print_meta: vocab type       = BPE
0.00.049.771 I llm_load_print_meta: n_vocab          = 50304
0.00.049.772 I llm_load_print_meta: n_merges         = 50009
0.00.049.772 I llm_load_print_meta: vocab_only       = 0
0.00.049.772 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.772 I llm_load_print_meta: n_embd           = 2048
0.00.049.772 I llm_load_print_meta: n_layer          = 24
0.00.049.775 I llm_load_print_meta: n_head           = 16
0.00.049.776 I llm_load_print_meta: n_head_kv        = 16
0.00.049.776 I llm_load_print_meta: n_rot            = 32
0.00.049.776 I llm_load_print_meta: n_swa            = 0
0.00.049.776 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.777 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.777 I llm_load_print_meta: n_gqa            = 1
0.00.049.778 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.779 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.779 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.782 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.782 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.782 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.782 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.783 I llm_load_print_meta: n_ff             = 8192
0.00.049.785 I llm_load_print_meta: n_expert         = 0
0.00.049.785 I llm_load_print_meta: n_expert_used    = 0
0.00.049.785 I llm_load_print_meta: causal attn      = 1
0.00.049.785 I llm_load_print_meta: pooling type     = 0
0.00.049.785 I llm_load_print_meta: rope type        = 2
0.00.049.785 I llm_load_print_meta: rope scaling     = linear
0.00.049.786 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.786 I llm_load_print_meta: freq_scale_train = 1
0.00.049.786 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.786 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.787 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.787 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.787 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.787 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.787 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.788 I llm_load_print_meta: model type       = 1.4B
0.00.049.788 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.789 I llm_load_print_meta: model params     = 1.41 B
0.00.049.791 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.791 I llm_load_print_meta: general.name     = 1.4B
0.00.049.791 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.792 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.792 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.793 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.793 I llm_load_print_meta: LF token         = 128 ''
0.00.049.794 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.794 I llm_load_print_meta: max token length = 1024
0.00.051.804 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.804 I llm_load_tensors: offloading output layer to GPU
0.00.051.804 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.815 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.816 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.713 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.714 I llama_new_context_with_model: n_ctx         = 128
0.00.052.714 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.714 I llama_new_context_with_model: n_batch       = 128
0.00.052.715 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.715 I llama_new_context_with_model: flash_attn    = 0
0.00.052.715 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.715 I llama_new_context_with_model: freq_scale    = 1
0.00.052.716 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.716 I ggml_metal_init: allocating
0.00.052.722 I ggml_metal_init: found device: Apple M4
0.00.052.725 I ggml_metal_init: picking default device: Apple M4
0.00.053.315 I ggml_metal_init: using embedded metal library
0.00.055.623 I ggml_metal_init: GPU name:   Apple M4
0.00.055.624 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.625 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.625 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.625 I ggml_metal_init: simdgroup reduction   = true
0.00.055.625 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.626 I ggml_metal_init: has bfloat            = true
0.00.055.626 I ggml_metal_init: use bfloat            = true
0.00.055.626 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.627 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.997 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.264 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.266 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.279 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.114 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.115 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.115 I llama_new_context_with_model: graph nodes  = 967
0.00.067.115 I llama_new_context_with_model: graph splits = 2
0.00.067.128 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.129 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.678.166 I 
0.00.678.195 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.678.206 I perplexity: tokenizing the input ..
0.00.685.885 I perplexity: tokenization took 7.678 ms
0.00.685.889 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.820.608 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.821.759 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.821.776 I llama_perf_context_print:        load time =     669.46 ms
0.00.821.777 I llama_perf_context_print: prompt eval time =     134.49 ms /   128 tokens (    1.05 ms per token,   951.72 tokens per second)
0.00.821.778 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.821.778 I llama_perf_context_print:       total time =     143.61 ms /   129 tokens
0.00.822.199 I ggml_metal_free: deallocating

real	0m0.836s
user	0m0.078s
sys	0m0.130s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.266 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.772 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.777 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.778 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.779 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.779 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.780 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.780 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.781 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.781 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.781 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.782 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.782 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.783 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.784 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.787 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.787 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.788 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.662 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.715 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.586 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.587 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.587 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.588 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.588 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.588 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.589 I llama_model_loader: - type  f32:  194 tensors
0.00.023.589 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.590 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.590 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.697 I llm_load_vocab: special tokens cache size = 25
0.00.050.673 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.675 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.675 I llm_load_print_meta: arch             = gptneox
0.00.050.676 I llm_load_print_meta: vocab type       = BPE
0.00.050.676 I llm_load_print_meta: n_vocab          = 50304
0.00.050.676 I llm_load_print_meta: n_merges         = 50009
0.00.050.676 I llm_load_print_meta: vocab_only       = 0
0.00.050.676 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.676 I llm_load_print_meta: n_embd           = 2048
0.00.050.677 I llm_load_print_meta: n_layer          = 24
0.00.050.680 I llm_load_print_meta: n_head           = 16
0.00.050.680 I llm_load_print_meta: n_head_kv        = 16
0.00.050.681 I llm_load_print_meta: n_rot            = 32
0.00.050.681 I llm_load_print_meta: n_swa            = 0
0.00.050.681 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.681 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.682 I llm_load_print_meta: n_gqa            = 1
0.00.050.683 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.684 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.684 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.684 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.685 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.685 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.685 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.686 I llm_load_print_meta: n_ff             = 8192
0.00.050.688 I llm_load_print_meta: n_expert         = 0
0.00.050.688 I llm_load_print_meta: n_expert_used    = 0
0.00.050.688 I llm_load_print_meta: causal attn      = 1
0.00.050.688 I llm_load_print_meta: pooling type     = 0
0.00.050.688 I llm_load_print_meta: rope type        = 2
0.00.050.689 I llm_load_print_meta: rope scaling     = linear
0.00.050.689 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.689 I llm_load_print_meta: freq_scale_train = 1
0.00.050.690 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.690 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.690 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.690 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.690 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.690 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.690 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.691 I llm_load_print_meta: model type       = 1.4B
0.00.050.691 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.692 I llm_load_print_meta: model params     = 1.41 B
0.00.050.692 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.693 I llm_load_print_meta: general.name     = 1.4B
0.00.050.693 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.693 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.693 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.693 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.694 I llm_load_print_meta: LF token         = 128 ''
0.00.050.694 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.694 I llm_load_print_meta: max token length = 1024
0.00.052.599 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.599 I llm_load_tensors: offloading output layer to GPU
0.00.052.599 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.610 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.611 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.508 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.509 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.509 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.509 I llama_new_context_with_model: n_batch       = 2048
0.00.053.510 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.510 I llama_new_context_with_model: flash_attn    = 0
0.00.053.510 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.510 I llama_new_context_with_model: freq_scale    = 1
0.00.053.511 I ggml_metal_init: allocating
0.00.053.514 I ggml_metal_init: found device: Apple M4
0.00.053.516 I ggml_metal_init: picking default device: Apple M4
0.00.054.119 I ggml_metal_init: using embedded metal library
0.00.056.490 I ggml_metal_init: GPU name:   Apple M4
0.00.056.492 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.492 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.492 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.493 I ggml_metal_init: simdgroup reduction   = true
0.00.056.493 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.493 I ggml_metal_init: has bfloat            = true
0.00.056.493 I ggml_metal_init: use bfloat            = true
0.00.056.494 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.494 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.473 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.744 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.749 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.767 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.833 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.834 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.835 I llama_new_context_with_model: graph nodes  = 967
0.00.086.835 I llama_new_context_with_model: graph splits = 2
0.00.086.851 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.983 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.984 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.441.573 I main: llama threadpool init, n_threads = 4
0.00.441.619 I 
0.00.441.652 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.441.653 I 
0.00.441.880 I sampler seed: 1234
0.00.441.885 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.441.922 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.441.926 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.441.926 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.121.166 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62117.24 tokens per second)
0.01.121.167 I llama_perf_context_print:        load time =     432.30 ms
0.01.121.168 I llama_perf_context_print: prompt eval time =      35.78 ms /     7 tokens (    5.11 ms per token,   195.64 tokens per second)
0.01.121.168 I llama_perf_context_print:        eval time =     640.50 ms /    63 runs   (   10.17 ms per token,    98.36 tokens per second)
0.01.121.169 I llama_perf_context_print:       total time =     679.60 ms /    70 tokens
0.01.121.421 I ggml_metal_free: deallocating

real	0m1.138s
user	0m0.111s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.347 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.883 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.887 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.889 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.890 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.890 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.890 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.890 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.891 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.892 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.892 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.892 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.893 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.893 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.893 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.897 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.897 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.898 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.774 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.784 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.664 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.665 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.665 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.666 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.666 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.666 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.667 I llama_model_loader: - type  f32:  194 tensors
0.00.023.667 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.667 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.668 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.711 I llm_load_vocab: special tokens cache size = 25
0.00.050.468 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.470 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.471 I llm_load_print_meta: arch             = gptneox
0.00.050.471 I llm_load_print_meta: vocab type       = BPE
0.00.050.471 I llm_load_print_meta: n_vocab          = 50304
0.00.050.472 I llm_load_print_meta: n_merges         = 50009
0.00.050.472 I llm_load_print_meta: vocab_only       = 0
0.00.050.472 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.472 I llm_load_print_meta: n_embd           = 2048
0.00.050.472 I llm_load_print_meta: n_layer          = 24
0.00.050.475 I llm_load_print_meta: n_head           = 16
0.00.050.476 I llm_load_print_meta: n_head_kv        = 16
0.00.050.476 I llm_load_print_meta: n_rot            = 32
0.00.050.476 I llm_load_print_meta: n_swa            = 0
0.00.050.476 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.477 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.479 I llm_load_print_meta: n_gqa            = 1
0.00.050.479 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.480 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.481 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.481 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.481 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.481 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.481 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.482 I llm_load_print_meta: n_ff             = 8192
0.00.050.484 I llm_load_print_meta: n_expert         = 0
0.00.050.484 I llm_load_print_meta: n_expert_used    = 0
0.00.050.484 I llm_load_print_meta: causal attn      = 1
0.00.050.484 I llm_load_print_meta: pooling type     = 0
0.00.050.484 I llm_load_print_meta: rope type        = 2
0.00.050.485 I llm_load_print_meta: rope scaling     = linear
0.00.050.485 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.485 I llm_load_print_meta: freq_scale_train = 1
0.00.050.485 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.486 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.486 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.486 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.486 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.486 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.486 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.487 I llm_load_print_meta: model type       = 1.4B
0.00.050.487 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.487 I llm_load_print_meta: model params     = 1.41 B
0.00.050.493 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.493 I llm_load_print_meta: general.name     = 1.4B
0.00.050.493 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.493 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.495 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.495 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.495 I llm_load_print_meta: LF token         = 128 ''
0.00.050.495 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.495 I llm_load_print_meta: max token length = 1024
0.00.052.391 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.391 I llm_load_tensors: offloading output layer to GPU
0.00.052.391 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.401 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.403 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.363 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.364 I llama_new_context_with_model: n_ctx         = 128
0.00.053.364 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.364 I llama_new_context_with_model: n_batch       = 128
0.00.053.364 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.364 I llama_new_context_with_model: flash_attn    = 0
0.00.053.365 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.365 I llama_new_context_with_model: freq_scale    = 1
0.00.053.365 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.366 I ggml_metal_init: allocating
0.00.053.369 I ggml_metal_init: found device: Apple M4
0.00.053.371 I ggml_metal_init: picking default device: Apple M4
0.00.053.945 I ggml_metal_init: using embedded metal library
0.00.056.261 I ggml_metal_init: GPU name:   Apple M4
0.00.056.262 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.262 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.263 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.263 I ggml_metal_init: simdgroup reduction   = true
0.00.056.263 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.263 I ggml_metal_init: has bfloat            = true
0.00.056.263 I ggml_metal_init: use bfloat            = true
0.00.056.264 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.264 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.113 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.403 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.405 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.419 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.372 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.373 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.374 I llama_new_context_with_model: graph nodes  = 967
0.00.068.374 I llama_new_context_with_model: graph splits = 2
0.00.068.386 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.387 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.382.305 I 
0.00.382.358 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.382.377 I perplexity: tokenizing the input ..
0.00.390.102 I perplexity: tokenization took 7.723 ms
0.00.390.106 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.522.135 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.523.337 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.523.353 I llama_perf_context_print:        load time =     372.95 ms
0.00.523.354 I llama_perf_context_print: prompt eval time =     131.80 ms /   128 tokens (    1.03 ms per token,   971.14 tokens per second)
0.00.523.355 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.523.356 I llama_perf_context_print:       total time =     141.05 ms /   129 tokens
0.00.523.844 I ggml_metal_free: deallocating

real	0m0.538s
user	0m0.079s
sys	0m0.066s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.181 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.492 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.496 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.498 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.498 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.499 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.499 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.499 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.500 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.500 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.501 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.501 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.501 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.504 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.504 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.508 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.508 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.508 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.341 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.367 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.173 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.174 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.174 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.175 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.175 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.175 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.176 I llama_model_loader: - type  f32:  194 tensors
0.00.024.176 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.176 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.176 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.176 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.588 I llm_load_vocab: special tokens cache size = 25
0.00.050.329 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.331 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.332 I llm_load_print_meta: arch             = gptneox
0.00.050.332 I llm_load_print_meta: vocab type       = BPE
0.00.050.332 I llm_load_print_meta: n_vocab          = 50304
0.00.050.332 I llm_load_print_meta: n_merges         = 50009
0.00.050.333 I llm_load_print_meta: vocab_only       = 0
0.00.050.333 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.333 I llm_load_print_meta: n_embd           = 2048
0.00.050.333 I llm_load_print_meta: n_layer          = 24
0.00.050.336 I llm_load_print_meta: n_head           = 16
0.00.050.337 I llm_load_print_meta: n_head_kv        = 16
0.00.050.337 I llm_load_print_meta: n_rot            = 32
0.00.050.337 I llm_load_print_meta: n_swa            = 0
0.00.050.337 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.340 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.341 I llm_load_print_meta: n_gqa            = 1
0.00.050.341 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.342 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.343 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.343 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.343 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.343 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.343 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.344 I llm_load_print_meta: n_ff             = 8192
0.00.050.346 I llm_load_print_meta: n_expert         = 0
0.00.050.347 I llm_load_print_meta: n_expert_used    = 0
0.00.050.347 I llm_load_print_meta: causal attn      = 1
0.00.050.347 I llm_load_print_meta: pooling type     = 0
0.00.050.347 I llm_load_print_meta: rope type        = 2
0.00.050.348 I llm_load_print_meta: rope scaling     = linear
0.00.050.348 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.348 I llm_load_print_meta: freq_scale_train = 1
0.00.050.348 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.349 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.349 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.349 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.349 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.349 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.349 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.350 I llm_load_print_meta: model type       = 1.4B
0.00.050.354 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.354 I llm_load_print_meta: model params     = 1.41 B
0.00.050.355 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.355 I llm_load_print_meta: general.name     = 1.4B
0.00.050.356 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.356 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.356 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.356 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.357 I llm_load_print_meta: LF token         = 128 ''
0.00.050.358 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.358 I llm_load_print_meta: max token length = 1024
0.00.052.301 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.301 I llm_load_tensors: offloading output layer to GPU
0.00.052.301 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.312 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.313 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.244 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.245 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.245 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.245 I llama_new_context_with_model: n_batch       = 2048
0.00.053.245 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.245 I llama_new_context_with_model: flash_attn    = 0
0.00.053.246 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.246 I llama_new_context_with_model: freq_scale    = 1
0.00.053.247 I ggml_metal_init: allocating
0.00.053.253 I ggml_metal_init: found device: Apple M4
0.00.053.255 I ggml_metal_init: picking default device: Apple M4
0.00.053.855 I ggml_metal_init: using embedded metal library
0.00.056.154 I ggml_metal_init: GPU name:   Apple M4
0.00.056.156 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.157 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.157 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.157 I ggml_metal_init: simdgroup reduction   = true
0.00.056.157 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.158 I ggml_metal_init: has bfloat            = true
0.00.056.158 I ggml_metal_init: use bfloat            = true
0.00.056.158 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.162 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.760 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.084.817 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.825 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.844 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.739 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.740 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.740 I llama_new_context_with_model: graph nodes  = 967
0.00.085.740 I llama_new_context_with_model: graph splits = 2
0.00.085.756 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.897 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.898 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.537.511 I main: llama threadpool init, n_threads = 4
0.00.537.549 I 
0.00.537.576 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.537.577 I 
0.00.537.809 I sampler seed: 1234
0.00.537.816 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.537.836 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.537.836 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.537.836 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.279.384 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55642.63 tokens per second)
0.01.279.385 I llama_perf_context_print:        load time =     528.33 ms
0.01.279.385 I llama_perf_context_print: prompt eval time =      40.54 ms /     7 tokens (    5.79 ms per token,   172.67 tokens per second)
0.01.279.386 I llama_perf_context_print:        eval time =     697.85 ms /    63 runs   (   11.08 ms per token,    90.28 tokens per second)
0.01.279.387 I llama_perf_context_print:       total time =     741.88 ms /    70 tokens
0.01.279.617 I ggml_metal_free: deallocating

real	0m1.294s
user	0m0.109s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.632 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.386 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.391 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.392 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.393 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.393 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.393 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.394 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.394 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.395 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.395 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.396 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.397 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.397 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.399 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.401 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.401 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.402 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.218 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.256 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.135 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.136 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.137 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.137 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.137 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.137 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.138 I llama_model_loader: - type  f32:  194 tensors
0.00.023.138 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.139 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.139 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.139 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.552 I llm_load_vocab: special tokens cache size = 25
0.00.049.501 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.504 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.504 I llm_load_print_meta: arch             = gptneox
0.00.049.505 I llm_load_print_meta: vocab type       = BPE
0.00.049.505 I llm_load_print_meta: n_vocab          = 50304
0.00.049.505 I llm_load_print_meta: n_merges         = 50009
0.00.049.505 I llm_load_print_meta: vocab_only       = 0
0.00.049.505 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.505 I llm_load_print_meta: n_embd           = 2048
0.00.049.506 I llm_load_print_meta: n_layer          = 24
0.00.049.508 I llm_load_print_meta: n_head           = 16
0.00.049.509 I llm_load_print_meta: n_head_kv        = 16
0.00.049.509 I llm_load_print_meta: n_rot            = 32
0.00.049.509 I llm_load_print_meta: n_swa            = 0
0.00.049.510 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.510 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.510 I llm_load_print_meta: n_gqa            = 1
0.00.049.511 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.512 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.513 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.513 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.513 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.513 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.513 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.514 I llm_load_print_meta: n_ff             = 8192
0.00.049.514 I llm_load_print_meta: n_expert         = 0
0.00.049.514 I llm_load_print_meta: n_expert_used    = 0
0.00.049.514 I llm_load_print_meta: causal attn      = 1
0.00.049.514 I llm_load_print_meta: pooling type     = 0
0.00.049.515 I llm_load_print_meta: rope type        = 2
0.00.049.515 I llm_load_print_meta: rope scaling     = linear
0.00.049.522 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.526 I llm_load_print_meta: freq_scale_train = 1
0.00.049.528 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.528 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.528 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.529 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.529 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.529 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.529 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.530 I llm_load_print_meta: model type       = 1.4B
0.00.049.530 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.531 I llm_load_print_meta: model params     = 1.41 B
0.00.049.531 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.531 I llm_load_print_meta: general.name     = 1.4B
0.00.049.532 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.532 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.532 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.532 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.532 I llm_load_print_meta: LF token         = 128 ''
0.00.049.533 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.533 I llm_load_print_meta: max token length = 1024
0.00.051.439 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.440 I llm_load_tensors: offloading output layer to GPU
0.00.051.440 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.451 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.452 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.326 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.327 I llama_new_context_with_model: n_ctx         = 128
0.00.052.327 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.327 I llama_new_context_with_model: n_batch       = 128
0.00.052.327 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.327 I llama_new_context_with_model: flash_attn    = 0
0.00.052.328 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.328 I llama_new_context_with_model: freq_scale    = 1
0.00.052.329 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.329 I ggml_metal_init: allocating
0.00.052.332 I ggml_metal_init: found device: Apple M4
0.00.052.334 I ggml_metal_init: picking default device: Apple M4
0.00.052.892 I ggml_metal_init: using embedded metal library
0.00.055.184 I ggml_metal_init: GPU name:   Apple M4
0.00.055.186 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.186 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.187 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.187 I ggml_metal_init: simdgroup reduction   = true
0.00.055.187 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.187 I ggml_metal_init: has bfloat            = true
0.00.055.187 I ggml_metal_init: use bfloat            = true
0.00.055.188 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.188 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.627 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.080 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.084 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.097 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.975 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.976 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.977 I llama_new_context_with_model: graph nodes  = 967
0.00.066.977 I llama_new_context_with_model: graph splits = 2
0.00.066.984 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.984 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.488.592 I 
0.00.488.661 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.488.682 I perplexity: tokenizing the input ..
0.00.496.081 I perplexity: tokenization took 7.398 ms
0.00.496.085 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.628.237 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.629.384 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.629.399 I llama_perf_context_print:        load time =     479.96 ms
0.00.629.400 I llama_perf_context_print: prompt eval time =     131.93 ms /   128 tokens (    1.03 ms per token,   970.24 tokens per second)
0.00.629.401 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.629.401 I llama_perf_context_print:       total time =     140.81 ms /   129 tokens
0.00.630.007 I ggml_metal_free: deallocating

real	0m0.643s
user	0m0.077s
sys	0m0.091s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.548 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.882 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.886 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.888 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.889 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.889 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.891 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.891 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.892 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.892 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.892 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.894 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.895 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.895 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.895 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.898 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.898 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.899 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.701 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.763 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.488 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.489 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.489 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.490 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.490 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.490 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.491 I llama_model_loader: - type  f32:  194 tensors
0.00.023.491 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.491 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.491 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.772 I llm_load_vocab: special tokens cache size = 25
0.00.049.646 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.648 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.649 I llm_load_print_meta: arch             = gptneox
0.00.049.649 I llm_load_print_meta: vocab type       = BPE
0.00.049.649 I llm_load_print_meta: n_vocab          = 50304
0.00.049.650 I llm_load_print_meta: n_merges         = 50009
0.00.049.650 I llm_load_print_meta: vocab_only       = 0
0.00.049.650 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.650 I llm_load_print_meta: n_embd           = 2048
0.00.049.650 I llm_load_print_meta: n_layer          = 24
0.00.049.653 I llm_load_print_meta: n_head           = 16
0.00.049.656 I llm_load_print_meta: n_head_kv        = 16
0.00.049.656 I llm_load_print_meta: n_rot            = 32
0.00.049.656 I llm_load_print_meta: n_swa            = 0
0.00.049.656 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.656 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.657 I llm_load_print_meta: n_gqa            = 1
0.00.049.658 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.658 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.659 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.659 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.659 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.659 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.660 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.661 I llm_load_print_meta: n_ff             = 8192
0.00.049.661 I llm_load_print_meta: n_expert         = 0
0.00.049.662 I llm_load_print_meta: n_expert_used    = 0
0.00.049.664 I llm_load_print_meta: causal attn      = 1
0.00.049.664 I llm_load_print_meta: pooling type     = 0
0.00.049.664 I llm_load_print_meta: rope type        = 2
0.00.049.664 I llm_load_print_meta: rope scaling     = linear
0.00.049.665 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.665 I llm_load_print_meta: freq_scale_train = 1
0.00.049.665 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.665 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.665 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.666 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.666 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.666 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.666 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.667 I llm_load_print_meta: model type       = 1.4B
0.00.049.667 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.671 I llm_load_print_meta: model params     = 1.41 B
0.00.049.672 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.672 I llm_load_print_meta: general.name     = 1.4B
0.00.049.672 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.673 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.673 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.673 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.673 I llm_load_print_meta: LF token         = 128 ''
0.00.049.673 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.674 I llm_load_print_meta: max token length = 1024
0.00.051.675 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.675 I llm_load_tensors: offloading output layer to GPU
0.00.051.675 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.686 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.687 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.690 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.691 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.691 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.691 I llama_new_context_with_model: n_batch       = 2048
0.00.052.691 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.691 I llama_new_context_with_model: flash_attn    = 0
0.00.052.692 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.692 I llama_new_context_with_model: freq_scale    = 1
0.00.052.693 I ggml_metal_init: allocating
0.00.052.699 I ggml_metal_init: found device: Apple M4
0.00.052.702 I ggml_metal_init: picking default device: Apple M4
0.00.053.299 I ggml_metal_init: using embedded metal library
0.00.055.623 I ggml_metal_init: GPU name:   Apple M4
0.00.055.624 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.625 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.625 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.627 I ggml_metal_init: simdgroup reduction   = true
0.00.055.627 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.627 I ggml_metal_init: has bfloat            = true
0.00.055.627 I ggml_metal_init: use bfloat            = true
0.00.055.628 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.628 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.401 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.084.835 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.841 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.858 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.854 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.856 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.856 I llama_new_context_with_model: graph nodes  = 967
0.00.085.856 I llama_new_context_with_model: graph splits = 2
0.00.085.872 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.013 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.014 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.622.051 I main: llama threadpool init, n_threads = 4
0.00.622.090 I 
0.00.622.145 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.622.146 I 
0.00.622.371 I sampler seed: 1234
0.00.622.375 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.622.395 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.622.395 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.622.395 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.389.297 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58244.46 tokens per second)
0.01.389.298 I llama_perf_context_print:        load time =     613.50 ms
0.01.389.299 I llama_perf_context_print: prompt eval time =      53.51 ms /     7 tokens (    7.64 ms per token,   130.82 tokens per second)
0.01.389.299 I llama_perf_context_print:        eval time =     710.41 ms /    63 runs   (   11.28 ms per token,    88.68 tokens per second)
0.01.389.299 I llama_perf_context_print:       total time =     767.25 ms /    70 tokens
0.01.389.532 I ggml_metal_free: deallocating

real	0m1.406s
user	0m0.109s
sys	0m0.142s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.717 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.488 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.492 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.494 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.494 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.495 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.495 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.495 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.496 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.496 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.496 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.499 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.499 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.499 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.500 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.502 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.503 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.503 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.352 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.404 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.237 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.238 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.239 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.239 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.239 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.239 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.240 I llama_model_loader: - type  f32:  194 tensors
0.00.023.240 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.241 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.241 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.366 I llm_load_vocab: special tokens cache size = 25
0.00.049.261 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.264 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.264 I llm_load_print_meta: arch             = gptneox
0.00.049.264 I llm_load_print_meta: vocab type       = BPE
0.00.049.265 I llm_load_print_meta: n_vocab          = 50304
0.00.049.265 I llm_load_print_meta: n_merges         = 50009
0.00.049.265 I llm_load_print_meta: vocab_only       = 0
0.00.049.265 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.265 I llm_load_print_meta: n_embd           = 2048
0.00.049.266 I llm_load_print_meta: n_layer          = 24
0.00.049.268 I llm_load_print_meta: n_head           = 16
0.00.049.269 I llm_load_print_meta: n_head_kv        = 16
0.00.049.269 I llm_load_print_meta: n_rot            = 32
0.00.049.270 I llm_load_print_meta: n_swa            = 0
0.00.049.270 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.270 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.271 I llm_load_print_meta: n_gqa            = 1
0.00.049.271 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.272 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.273 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.274 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.274 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.274 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.274 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.275 I llm_load_print_meta: n_ff             = 8192
0.00.049.275 I llm_load_print_meta: n_expert         = 0
0.00.049.275 I llm_load_print_meta: n_expert_used    = 0
0.00.049.276 I llm_load_print_meta: causal attn      = 1
0.00.049.276 I llm_load_print_meta: pooling type     = 0
0.00.049.276 I llm_load_print_meta: rope type        = 2
0.00.049.276 I llm_load_print_meta: rope scaling     = linear
0.00.049.277 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.277 I llm_load_print_meta: freq_scale_train = 1
0.00.049.277 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.277 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.277 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.278 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.278 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.278 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.278 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.279 I llm_load_print_meta: model type       = 1.4B
0.00.049.279 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.279 I llm_load_print_meta: model params     = 1.41 B
0.00.049.280 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.280 I llm_load_print_meta: general.name     = 1.4B
0.00.049.280 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.280 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.281 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.281 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.281 I llm_load_print_meta: LF token         = 128 ''
0.00.049.281 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.281 I llm_load_print_meta: max token length = 1024
0.00.051.208 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.208 I llm_load_tensors: offloading output layer to GPU
0.00.051.208 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.219 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.220 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.162 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.163 I llama_new_context_with_model: n_ctx         = 128
0.00.052.163 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.163 I llama_new_context_with_model: n_batch       = 128
0.00.052.163 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.163 I llama_new_context_with_model: flash_attn    = 0
0.00.052.164 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.164 I llama_new_context_with_model: freq_scale    = 1
0.00.052.164 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.165 I ggml_metal_init: allocating
0.00.052.168 I ggml_metal_init: found device: Apple M4
0.00.052.170 I ggml_metal_init: picking default device: Apple M4
0.00.052.746 I ggml_metal_init: using embedded metal library
0.00.055.056 I ggml_metal_init: GPU name:   Apple M4
0.00.055.057 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.058 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.058 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.058 I ggml_metal_init: simdgroup reduction   = true
0.00.055.059 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.059 I ggml_metal_init: has bfloat            = true
0.00.055.059 I ggml_metal_init: use bfloat            = true
0.00.055.059 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.060 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.709 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.937 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.939 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.952 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.891 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.892 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.892 I llama_new_context_with_model: graph nodes  = 967
0.00.066.892 I llama_new_context_with_model: graph splits = 2
0.00.066.905 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.906 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.558.460 I 
0.00.558.492 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.558.504 I perplexity: tokenizing the input ..
0.00.566.075 I perplexity: tokenization took 7.569 ms
0.00.566.078 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.700.587 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.701.838 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.701.853 I llama_perf_context_print:        load time =     549.74 ms
0.00.701.855 I llama_perf_context_print: prompt eval time =     134.28 ms /   128 tokens (    1.05 ms per token,   953.20 tokens per second)
0.00.701.856 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.701.858 I llama_perf_context_print:       total time =     143.39 ms /   129 tokens
0.00.702.276 I ggml_metal_free: deallocating

real	0m0.717s
user	0m0.077s
sys	0m0.098s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.803 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.265 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.269 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.271 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.271 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.272 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.272 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.272 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.273 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.274 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.274 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.276 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.277 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.277 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.277 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.280 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.280 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.281 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.305 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.387 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.286 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.287 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.287 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.287 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.288 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.288 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.289 I llama_model_loader: - type  f32:  194 tensors
0.00.025.289 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.289 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.527 I llm_load_vocab: special tokens cache size = 25
0.00.052.485 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.488 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.488 I llm_load_print_meta: arch             = gptneox
0.00.052.488 I llm_load_print_meta: vocab type       = BPE
0.00.052.489 I llm_load_print_meta: n_vocab          = 50304
0.00.052.489 I llm_load_print_meta: n_merges         = 50009
0.00.052.489 I llm_load_print_meta: vocab_only       = 0
0.00.052.489 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.489 I llm_load_print_meta: n_embd           = 2048
0.00.052.489 I llm_load_print_meta: n_layer          = 24
0.00.052.492 I llm_load_print_meta: n_head           = 16
0.00.052.493 I llm_load_print_meta: n_head_kv        = 16
0.00.052.494 I llm_load_print_meta: n_rot            = 32
0.00.052.495 I llm_load_print_meta: n_swa            = 0
0.00.052.495 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.495 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.496 I llm_load_print_meta: n_gqa            = 1
0.00.052.498 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.499 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.499 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.499 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.500 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.500 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.500 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.501 I llm_load_print_meta: n_ff             = 8192
0.00.052.501 I llm_load_print_meta: n_expert         = 0
0.00.052.501 I llm_load_print_meta: n_expert_used    = 0
0.00.052.503 I llm_load_print_meta: causal attn      = 1
0.00.052.504 I llm_load_print_meta: pooling type     = 0
0.00.052.505 I llm_load_print_meta: rope type        = 2
0.00.052.505 I llm_load_print_meta: rope scaling     = linear
0.00.052.505 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.505 I llm_load_print_meta: freq_scale_train = 1
0.00.052.506 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.506 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.506 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.506 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.506 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.506 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.506 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.511 I llm_load_print_meta: model type       = 1.4B
0.00.052.511 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.511 I llm_load_print_meta: model params     = 1.41 B
0.00.052.512 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.512 I llm_load_print_meta: general.name     = 1.4B
0.00.052.512 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.513 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.513 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.513 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.514 I llm_load_print_meta: LF token         = 128 ''
0.00.052.514 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.514 I llm_load_print_meta: max token length = 1024
0.00.054.544 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.544 I llm_load_tensors: offloading output layer to GPU
0.00.054.544 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.555 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.556 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.469 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.470 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.470 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.470 I llama_new_context_with_model: n_batch       = 2048
0.00.055.470 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.471 I llama_new_context_with_model: flash_attn    = 0
0.00.055.471 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.472 I llama_new_context_with_model: freq_scale    = 1
0.00.055.472 I ggml_metal_init: allocating
0.00.055.480 I ggml_metal_init: found device: Apple M4
0.00.055.482 I ggml_metal_init: picking default device: Apple M4
0.00.056.109 I ggml_metal_init: using embedded metal library
0.00.058.460 I ggml_metal_init: GPU name:   Apple M4
0.00.058.461 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.463 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.463 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.464 I ggml_metal_init: simdgroup reduction   = true
0.00.058.464 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.464 I ggml_metal_init: has bfloat            = true
0.00.058.464 I ggml_metal_init: use bfloat            = true
0.00.058.465 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.465 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.305 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.272 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.279 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.299 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.292 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.293 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.294 I llama_new_context_with_model: graph nodes  = 967
0.00.088.294 I llama_new_context_with_model: graph splits = 2
0.00.088.307 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.447 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.448 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.670 I main: llama threadpool init, n_threads = 4
0.00.689.702 I 
0.00.689.727 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.727 I 
0.00.689.875 I sampler seed: 1234
0.00.689.879 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.689.913 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.689.925 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.689.926 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.542.888 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52282.77 tokens per second)
0.01.542.889 I llama_perf_context_print:        load time =     679.86 ms
0.01.542.890 I llama_perf_context_print: prompt eval time =      51.54 ms /     7 tokens (    7.36 ms per token,   135.81 tokens per second)
0.01.542.890 I llama_perf_context_print:        eval time =     798.80 ms /    63 runs   (   12.68 ms per token,    78.87 tokens per second)
0.01.542.891 I llama_perf_context_print:       total time =     853.22 ms /    70 tokens
0.01.543.096 I ggml_metal_free: deallocating

real	0m1.560s
user	0m0.111s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.968 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.590 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.594 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.596 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.596 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.597 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.597 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.597 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.598 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.598 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.598 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.599 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.599 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.600 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.602 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.604 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.604 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.604 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.416 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.495 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.307 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.308 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.308 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.308 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.309 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.309 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.310 I llama_model_loader: - type  f32:  194 tensors
0.00.024.310 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.310 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.523 I llm_load_vocab: special tokens cache size = 25
0.00.050.412 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.415 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.415 I llm_load_print_meta: arch             = gptneox
0.00.050.416 I llm_load_print_meta: vocab type       = BPE
0.00.050.416 I llm_load_print_meta: n_vocab          = 50304
0.00.050.416 I llm_load_print_meta: n_merges         = 50009
0.00.050.416 I llm_load_print_meta: vocab_only       = 0
0.00.050.416 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.416 I llm_load_print_meta: n_embd           = 2048
0.00.050.417 I llm_load_print_meta: n_layer          = 24
0.00.050.419 I llm_load_print_meta: n_head           = 16
0.00.050.420 I llm_load_print_meta: n_head_kv        = 16
0.00.050.420 I llm_load_print_meta: n_rot            = 32
0.00.050.420 I llm_load_print_meta: n_swa            = 0
0.00.050.421 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.421 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.422 I llm_load_print_meta: n_gqa            = 1
0.00.050.422 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.424 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.425 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.425 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.425 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.427 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.427 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.428 I llm_load_print_meta: n_ff             = 8192
0.00.050.428 I llm_load_print_meta: n_expert         = 0
0.00.050.428 I llm_load_print_meta: n_expert_used    = 0
0.00.050.428 I llm_load_print_meta: causal attn      = 1
0.00.050.429 I llm_load_print_meta: pooling type     = 0
0.00.050.429 I llm_load_print_meta: rope type        = 2
0.00.050.429 I llm_load_print_meta: rope scaling     = linear
0.00.050.431 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.431 I llm_load_print_meta: freq_scale_train = 1
0.00.050.432 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.432 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.432 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.432 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.432 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.432 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.432 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.433 I llm_load_print_meta: model type       = 1.4B
0.00.050.433 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.434 I llm_load_print_meta: model params     = 1.41 B
0.00.050.438 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.438 I llm_load_print_meta: general.name     = 1.4B
0.00.050.439 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.439 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.439 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.439 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.440 I llm_load_print_meta: LF token         = 128 ''
0.00.050.440 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.440 I llm_load_print_meta: max token length = 1024
0.00.052.247 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.247 I llm_load_tensors: offloading output layer to GPU
0.00.052.247 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.253 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.253 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.150 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.150 I llama_new_context_with_model: n_ctx         = 128
0.00.053.150 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.151 I llama_new_context_with_model: n_batch       = 128
0.00.053.151 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.151 I llama_new_context_with_model: flash_attn    = 0
0.00.053.151 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.152 I llama_new_context_with_model: freq_scale    = 1
0.00.053.152 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.153 I ggml_metal_init: allocating
0.00.053.158 I ggml_metal_init: found device: Apple M4
0.00.053.161 I ggml_metal_init: picking default device: Apple M4
0.00.053.711 I ggml_metal_init: using embedded metal library
0.00.056.016 I ggml_metal_init: GPU name:   Apple M4
0.00.056.017 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.017 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.018 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.018 I ggml_metal_init: simdgroup reduction   = true
0.00.056.018 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.018 I ggml_metal_init: has bfloat            = true
0.00.056.018 I ggml_metal_init: use bfloat            = true
0.00.056.019 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.019 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.621 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.913 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.916 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.929 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.769 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.770 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.770 I llama_new_context_with_model: graph nodes  = 967
0.00.067.770 I llama_new_context_with_model: graph splits = 2
0.00.067.777 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.777 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.628.455 I 
0.00.628.491 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.628.503 I perplexity: tokenizing the input ..
0.00.636.046 I perplexity: tokenization took 7.541 ms
0.00.636.049 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.776.823 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.778.005 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.778.025 I llama_perf_context_print:        load time =     618.48 ms
0.00.778.026 I llama_perf_context_print: prompt eval time =     140.55 ms /   128 tokens (    1.10 ms per token,   910.71 tokens per second)
0.00.778.027 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.778.027 I llama_perf_context_print:       total time =     149.57 ms /   129 tokens
0.00.778.496 I ggml_metal_free: deallocating

real	0m0.793s
user	0m0.078s
sys	0m0.108s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.719 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.052 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.056 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.062 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.063 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.063 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.064 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.064 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.065 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.065 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.067 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.068 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.068 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.068 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.069 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.070 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.071 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.071 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.957 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.992 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.835 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.837 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.837 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.837 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.837 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.838 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.838 I llama_model_loader: - type  f32:  194 tensors
0.00.023.838 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.186 I llm_load_vocab: special tokens cache size = 25
0.00.049.975 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.977 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.978 I llm_load_print_meta: arch             = gptneox
0.00.049.978 I llm_load_print_meta: vocab type       = BPE
0.00.049.978 I llm_load_print_meta: n_vocab          = 50304
0.00.049.978 I llm_load_print_meta: n_merges         = 50009
0.00.049.978 I llm_load_print_meta: vocab_only       = 0
0.00.049.979 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.979 I llm_load_print_meta: n_embd           = 2048
0.00.049.979 I llm_load_print_meta: n_layer          = 24
0.00.049.982 I llm_load_print_meta: n_head           = 16
0.00.049.983 I llm_load_print_meta: n_head_kv        = 16
0.00.049.983 I llm_load_print_meta: n_rot            = 32
0.00.049.983 I llm_load_print_meta: n_swa            = 0
0.00.049.983 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.983 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.987 I llm_load_print_meta: n_gqa            = 1
0.00.049.988 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.990 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.990 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.991 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.991 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.991 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.991 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.992 I llm_load_print_meta: n_ff             = 8192
0.00.049.993 I llm_load_print_meta: n_expert         = 0
0.00.049.994 I llm_load_print_meta: n_expert_used    = 0
0.00.049.994 I llm_load_print_meta: causal attn      = 1
0.00.049.994 I llm_load_print_meta: pooling type     = 0
0.00.049.994 I llm_load_print_meta: rope type        = 2
0.00.049.994 I llm_load_print_meta: rope scaling     = linear
0.00.049.995 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.995 I llm_load_print_meta: freq_scale_train = 1
0.00.049.995 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.995 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.995 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.996 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.996 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.996 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.996 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.997 I llm_load_print_meta: model type       = 1.4B
0.00.049.997 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.997 I llm_load_print_meta: model params     = 1.41 B
0.00.049.998 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.998 I llm_load_print_meta: general.name     = 1.4B
0.00.049.998 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.998 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.998 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.999 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.999 I llm_load_print_meta: LF token         = 128 ''
0.00.050.000 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.000 I llm_load_print_meta: max token length = 1024
0.00.051.816 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.816 I llm_load_tensors: offloading output layer to GPU
0.00.051.816 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.822 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.822 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.743 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.744 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.744 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.744 I llama_new_context_with_model: n_batch       = 2048
0.00.052.744 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.744 I llama_new_context_with_model: flash_attn    = 0
0.00.052.745 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.745 I llama_new_context_with_model: freq_scale    = 1
0.00.052.746 I ggml_metal_init: allocating
0.00.052.749 I ggml_metal_init: found device: Apple M4
0.00.052.751 I ggml_metal_init: picking default device: Apple M4
0.00.053.351 I ggml_metal_init: using embedded metal library
0.00.055.671 I ggml_metal_init: GPU name:   Apple M4
0.00.055.672 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.673 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.673 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.673 I ggml_metal_init: simdgroup reduction   = true
0.00.055.673 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.674 I ggml_metal_init: has bfloat            = true
0.00.055.674 I ggml_metal_init: use bfloat            = true
0.00.055.674 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.675 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.384 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.084.242 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.249 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.271 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.318 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.319 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.320 I llama_new_context_with_model: graph nodes  = 967
0.00.085.320 I llama_new_context_with_model: graph splits = 2
0.00.085.337 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.469 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.470 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.621 I main: llama threadpool init, n_threads = 4
0.00.747.667 I 
0.00.747.712 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.714 I 
0.00.747.955 I sampler seed: 1234
0.00.747.960 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.747.995 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.748.012 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.748.012 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.660.143 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59865.09 tokens per second)
0.01.660.143 I llama_perf_context_print:        load time =     738.89 ms
0.01.660.144 I llama_perf_context_print: prompt eval time =      54.43 ms /     7 tokens (    7.78 ms per token,   128.61 tokens per second)
0.01.660.145 I llama_perf_context_print:        eval time =     854.80 ms /    63 runs   (   13.57 ms per token,    73.70 tokens per second)
0.01.660.145 I llama_perf_context_print:       total time =     912.53 ms /    70 tokens
0.01.660.345 I ggml_metal_free: deallocating

real	0m1.675s
user	0m0.108s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4394 (de014bc3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.676 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.299 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.304 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.309 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.310 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.310 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.311 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.311 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.314 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.314 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.314 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.315 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.317 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.317 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.317 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.321 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.321 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.322 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.084 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.083 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.817 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.818 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.819 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.819 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.819 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.820 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.022.820 I llama_model_loader: - type  f32:  194 tensors
0.00.022.821 I llama_model_loader: - type q6_K:   98 tensors
0.00.042.878 I llm_load_vocab: special tokens cache size = 25
0.00.048.866 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.869 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.869 I llm_load_print_meta: arch             = gptneox
0.00.048.869 I llm_load_print_meta: vocab type       = BPE
0.00.048.869 I llm_load_print_meta: n_vocab          = 50304
0.00.048.870 I llm_load_print_meta: n_merges         = 50009
0.00.048.870 I llm_load_print_meta: vocab_only       = 0
0.00.048.870 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.870 I llm_load_print_meta: n_embd           = 2048
0.00.048.870 I llm_load_print_meta: n_layer          = 24
0.00.048.873 I llm_load_print_meta: n_head           = 16
0.00.048.874 I llm_load_print_meta: n_head_kv        = 16
0.00.048.874 I llm_load_print_meta: n_rot            = 32
0.00.048.875 I llm_load_print_meta: n_swa            = 0
0.00.048.875 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.875 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.876 I llm_load_print_meta: n_gqa            = 1
0.00.048.876 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.877 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.878 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.880 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.880 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.880 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.881 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.881 I llm_load_print_meta: n_ff             = 8192
0.00.048.887 I llm_load_print_meta: n_expert         = 0
0.00.048.889 I llm_load_print_meta: n_expert_used    = 0
0.00.048.889 I llm_load_print_meta: causal attn      = 1
0.00.048.889 I llm_load_print_meta: pooling type     = 0
0.00.048.890 I llm_load_print_meta: rope type        = 2
0.00.048.890 I llm_load_print_meta: rope scaling     = linear
0.00.048.890 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.891 I llm_load_print_meta: freq_scale_train = 1
0.00.048.891 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.891 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.892 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.892 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.892 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.892 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.892 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.893 I llm_load_print_meta: model type       = 1.4B
0.00.048.893 I llm_load_print_meta: model ftype      = Q6_K
0.00.048.893 I llm_load_print_meta: model params     = 1.41 B
0.00.048.894 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.048.894 I llm_load_print_meta: general.name     = 1.4B
0.00.048.894 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.895 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.895 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.895 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.896 I llm_load_print_meta: LF token         = 128 ''
0.00.048.896 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.896 I llm_load_print_meta: max token length = 1024
0.00.050.494 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.495 I llm_load_tensors: offloading output layer to GPU
0.00.050.495 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.505 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.506 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.051.298 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.299 I llama_new_context_with_model: n_ctx         = 128
0.00.051.299 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.300 I llama_new_context_with_model: n_batch       = 128
0.00.051.300 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.300 I llama_new_context_with_model: flash_attn    = 0
0.00.051.300 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.301 I llama_new_context_with_model: freq_scale    = 1
0.00.051.301 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.301 I ggml_metal_init: allocating
0.00.051.305 I ggml_metal_init: found device: Apple M4
0.00.051.307 I ggml_metal_init: picking default device: Apple M4
0.00.051.903 I ggml_metal_init: using embedded metal library
0.00.054.246 I ggml_metal_init: GPU name:   Apple M4
0.00.054.248 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.248 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.248 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.249 I ggml_metal_init: simdgroup reduction   = true
0.00.054.249 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.249 I ggml_metal_init: has bfloat            = true
0.00.054.249 I ggml_metal_init: use bfloat            = true
0.00.054.250 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.250 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.818 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.107 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.109 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.122 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.937 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.938 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.938 I llama_new_context_with_model: graph nodes  = 967
0.00.065.938 I llama_new_context_with_model: graph splits = 2
0.00.065.950 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.951 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.638.841 I 
0.00.638.877 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.638.889 I perplexity: tokenizing the input ..
0.00.646.374 I perplexity: tokenization took 7.483 ms
0.00.646.377 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.785.501 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.786.901 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.786.912 I llama_perf_context_print:        load time =     630.16 ms
0.00.786.913 I llama_perf_context_print: prompt eval time =     138.88 ms /   128 tokens (    1.09 ms per token,   921.64 tokens per second)
0.00.786.914 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.786.914 I llama_perf_context_print:       total time =     148.07 ms /   129 tokens
0.00.787.237 I ggml_metal_free: deallocating

real	0m0.801s
user	0m0.077s
sys	0m0.117s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4394 (de014bc3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e607350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e607a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e608010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e6085c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e608b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e609120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e6096d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e609c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e60a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e60a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e60ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e60b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e60bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e60c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e60cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e60d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e60da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e60e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e60e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e60f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e60f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e60fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e6105c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e610e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e611580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e611840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e611e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e612ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e613000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e6132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e613760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e613a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e6142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e6147f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e614ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12e707280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12e707830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12e707d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12e708230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12e708730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12e708c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12e709130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12e709630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12e709b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12e70a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12e70a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12e70a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12e70ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12e70b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12e70b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12e70bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12e70c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12e70c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12e70cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12e70cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12e70d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12e70dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12e70ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12e70e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12e70ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12e70f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12e70f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12e70fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12e70ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12e7103f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12e710890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12e710d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12e7111d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12e711670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12e711b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12e711fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12e712450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12e7128f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12e712e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12e713390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12e7138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12e713e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12e714380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12e7148d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12e714e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12e715370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12e7158c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12e715e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12e716360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12e7168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12e716e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12e717350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12e7178a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12e717df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12e718340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12e718890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12e718de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12e719330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12e719880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12e719dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12e71a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12e71a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12e70b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12e71ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12e71b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12e71b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12e71bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12e71c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12e71c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12e71cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12e71d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12e71d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12e71df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12e71e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12e71e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12e71ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12e71f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12e71f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12e71fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12e7202e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12e720780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12e720c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12e7210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12e721560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12e721a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12e721ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12e722340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12e7227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12e722ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12e7231b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12e7236b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12e723bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12e7240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12e7245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12e724ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12e724fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12e7254b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12e7259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12e725eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12e7263b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12e7268b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12e726db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12e7272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12e7277b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12e727cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12e7281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12e7286b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12e728bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12e7290b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12e7295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12e729ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12e729fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12e72a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12e72a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12e72aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12e72b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12e72b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12e72bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12e72c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12e72c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12e72ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12e72d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12e72d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12e72dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12e72e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12e72e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12e72eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12e72efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12e72f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12e72f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12e72feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12e7303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12e7308b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12e730db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12e7312b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12e7317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12e731cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12e7321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12e7326b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12e732bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12e7330b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12e7335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12e733ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12e733fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12e7344b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12e7349b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12e734eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12e7353b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12e7358b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12e735db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12e7362b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12e7367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12e736cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12e7371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12e7376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12e737bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12e7380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12e7385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12e738ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12e739060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12e739610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12e739bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12e73a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12e73a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12e73ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12e73b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12e73bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12e73c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12e73c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12e73c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12e73cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12e73d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12e73dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12e73e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12e73e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12e73ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12e73f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12e73f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12e73fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12e7401d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12e740720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12e740c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12e7411c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12e741710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12e741c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12e7421b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12e742700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12e742c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12e7431a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12e7436f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12e743c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12e744190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12e7446e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12e744c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12e745180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12e7456d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12e745c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12e746170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12e7466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12e746c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12e747160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12e7476b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12e747c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12e748150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12e7486a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12e748bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12e749140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12e749690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12e749be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12e74a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12e74a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12e74abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12e74b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12e74b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12e74bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12e74c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12e74c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12e74cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12e74d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12e74d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12e74dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12e74e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12e74e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12e74eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12e74f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12e74f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12e74fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12e7500d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12e750620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12e750b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12e7510c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12e751610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12e751ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12e751f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12e7523f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12e752890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12e752d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12e7531d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12e753670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12e753b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12e753fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12e754450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12e7548f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12e754d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12e755230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12e7556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12e755b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12e7560c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12e7567e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12e756f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12e757620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12e757d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12e758000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12e7587f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12e758ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12e7590c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.143.369 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.143.373 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10e704dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10e705240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10e7056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10e705b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10e705f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10e706400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10e706870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10e706ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10e707150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10e7075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10e707a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10e708120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10e708c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10e7093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10e709c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10e70a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10e70aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10e70b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10e70b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10e70bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10e70c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10e70cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10e70d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10e70dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10e70e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10e70e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10e70e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10e70ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10e70f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10e70f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10e70fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10e70ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10e710430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10e7106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10e710b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10e710fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10e711440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10e7118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10e711d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10e712190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10e712600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10e712a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10e712ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10e713350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10e7137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10e713c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10e7140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10e714510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10e714980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10e714df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10e715260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10e7156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10e715b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10e715fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10e716420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10e716890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10e716e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10e717300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10e717770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10e717be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10e718050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10e7184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10e718930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10e718da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10e719210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10e719680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10e719af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10e719f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10e71a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10e71a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10e71acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10e71b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10e71b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10e71ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10e71be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10e71c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10e71c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10e71cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10e71d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10e71d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10e71d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10e71dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10e71e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10e71e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10e71ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10e71ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10e71f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10e71f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10e71fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10e720100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10e720570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10e7209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10e720e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10e7212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10e721730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10e721ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10e722010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10e722480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10e7228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10e722d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10e7231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10e723640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10e723ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10e723f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10e724390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10e724800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10e724c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10e7250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10e725550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10e7259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10e725e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10e7262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10e726710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10e726b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10e726ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10e727460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10e7278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10e727d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10e7281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10e728620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10e728a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10e728f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10e729370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10e7297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10e729c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10e72a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10e72a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10e72a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10e72ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10e72b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10e72b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10e72bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10e72bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10e72c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10e72c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10e72cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10e72d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10e72d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10e72da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10e72dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10e72e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10e72e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10e72ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10e72f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10e72f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10e72f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10e72fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10e730260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10e7306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10e730b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10e730fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10e731420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10e731890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10e731d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10e732170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10e7325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10e732a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10e732ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10e733330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10e7337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10e733c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10e734080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10e7344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10e734960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10e734dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10e735240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10e735e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10e736130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10e7363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10e736860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10e736cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10e737140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10e7375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10e737a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10e737e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10e738300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10e738770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10e738be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10e739050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10e7394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10e739930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10e739da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10e73a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10e73a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10e73aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10e73af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10e73b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10e73b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10e73bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10e73c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10e73c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10e73ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10e73ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10e73d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10e73d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10e73dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10e73e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10e73e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10e73e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10e73ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10e73f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10e73f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10e73fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10e7400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10e740540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10e7409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10e740e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10e741290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10e7417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10e741cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10e742830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10e742af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10e7430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10e743670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10e743c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10e7441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10e7447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10e744d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10e745330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10e7458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10e745eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10e746470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10e746a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10e746ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10e7475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10e747b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10e748130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10e7486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10e748cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10e749270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10e749830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10e749df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10e74a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10e74a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10e74af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10e74b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10e74bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10e74c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10e74c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10e74cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10e74d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10e74d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10e74dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10e74e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10e74e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10e74ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10e74f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10e74f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10e74ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10e750570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10e750b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10e7510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10e7516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10e751c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10e752230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10e7527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10e752db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10e753370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10e753930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10e753ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10e7544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10e754a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10e755030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10e7555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10e755bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10e756170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10e756730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10e756cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10e7571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10e7576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10e757bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10e7580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10e7585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10e758af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10e758ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10e7594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10e7599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10e759ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10e75a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10e75a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10e75adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10e75b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10e75b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10e75c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10e75c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10e75d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10e75d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10e75da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10e75e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10e75e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10e75eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e609f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e6093e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e609990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e608880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e607d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e6082d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e611b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e610b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e60b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e606980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e613ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e613fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e612420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e614d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e6153a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e615660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e615b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e616220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e616940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e6172f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e617a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e618130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e618850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e618f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e619690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e619950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e619f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e61a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e61ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e61b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e61b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e61bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e61c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e61c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e61cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e61d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e61d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e61d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e61dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e61e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e61e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e61ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e61f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e61f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e61f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e61fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e6203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e6209f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e621000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e621610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e621c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e622230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e622840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e622e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e623640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e623ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e623f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e624240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e624850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e625040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e6254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e625980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e625e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e6262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e626760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e626c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e6270a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e627540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e6279e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e627e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e628320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e6287c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e628c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e6291b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e629700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e629c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e62a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e62a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e62ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e62b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e62b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e62bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e62c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e62c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e62cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e62d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e62d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e62dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e62e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e62e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e62ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e62f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e62f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e62fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e630140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e630690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e630be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e631130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e631680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e631bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e632120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e632670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e632bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e633110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e633660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e633bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e634100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e634650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e634ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e6350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e635640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e635b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e6360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e636580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e636a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e636ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e637360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e637800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e637ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e638140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e6385e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e638a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e638f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e6393c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e639860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e639d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e63a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e63a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e63aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e63af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e63b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e63b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e63bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e63c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e63c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e63cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e63cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e63d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e63d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e63ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e63e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e63e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e63eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e63f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e63f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e63f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e63fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e6402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e640760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e640c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e6410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e641540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e6419e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e641e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e642320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e6427c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e642c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e643100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e6435a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e643a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e643ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e644380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e644820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e644cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e645160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e645600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e645aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e645f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e6463e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e646880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e646d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e6471c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e647660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e647b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e647fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e648440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e6488e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e648d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e649220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e6496c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e649b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e64a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e64a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e64a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e64ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e64b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e64b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e64bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e64c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e64c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e64c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e64ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e64d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e64d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e64dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e64e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e64e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e64eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e64f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e64f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e64fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e650500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e6509a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e650c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e651270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e651880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e652070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e652510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e6529b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e652e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e653600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e653b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e6540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e6545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e654b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e655090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e6555e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e655b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e656080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e6565d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e656b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e657070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e6575c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e657b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e658060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e6585b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e658b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e659050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e6595a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e659af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e65a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e65a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e65aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e65b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e65b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e65bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e65c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e65c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e65cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e65d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e65d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e65dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e65e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e65e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e65eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e65eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e65f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e65fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e65ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e660530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e660a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e660fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e661520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e661a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e661fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e662510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e662a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e662fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e663500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e663a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e663fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e6644f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e664a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e664f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e6654e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e665a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e665f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e666420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e6668c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e666d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e667200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e6676a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e667b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e667fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e668480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e668920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e668dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e669260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e669700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e669ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e66a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e66a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e66aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e66b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e66b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e66bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e66c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e66c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e66d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e66d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e66da30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.810s
user	0m0.295s
sys	0m0.318s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4394 (de014bc3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x144f0b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x144f0b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x144f0bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x144f0c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x144f0caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x144f0d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x144f0d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x144f0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x144f0e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x144f0e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x144f0eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x144f0f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x144f0fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x144f10330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x144f10b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x144f11260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x144f11980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x144f120a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x144f127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x144f12f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x144f136b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x144f13dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x144f144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x144f14d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x144f154b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x144f15770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x144f15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x144f169f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x144f16f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x144f171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x144f17690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x144f17950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x144f181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x144f18720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x144f189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x144f18e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x144f19320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x144f197c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x144f19c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x144f1a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x144f1a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x144f1aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x144f1aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x144f1b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x144f1b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x144f1bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x144f1c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x144f1cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x144f1d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x144f1d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x144f1ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x144f1e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x144f1e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x144f1efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x144f1f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x144f1fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x144f20110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x144f203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x144f209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x144f211d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x144f21490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x144f21930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x144f21dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x144f22270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x144f22710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x144f22bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x144f23050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x144f234f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x144f23990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x144f23e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x144f242d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x144f24770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x144f24c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x144f25160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x144f256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x144f25c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x144f26150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x144f266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x144f26bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x144f27140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x144f27690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x144f27be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x144f28130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x144f28680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x144f28bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x144f29120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x144f29670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x144f29bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x144f2a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x144f2a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x144f2abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x144f2b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x144f2b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x144f2bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x144f2c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x144f2c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x144f2cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x144f1c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x144f2d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x144f2d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x144f2dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x144f2e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x144f2e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x144f2ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x144f2f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x144f2f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x144f2fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x144f30230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x144f30780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x144f30cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x144f31220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x144f31770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x144f31cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x144f32160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x144f32600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x144f32aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x144f32f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x144f333e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x144f33880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x144f33d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x144f341c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x144f34660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x144f34b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x144f34fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x144f35440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x144f358e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x144f35d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x144f36220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x144f366c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x144f36b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x144f37000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x144f374a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x144f37940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x144f37de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x144f38280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x144f38720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x144f38bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x144f39060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x144f39500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x144f399a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x144f39e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x144f3a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x144f3a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x144f3ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x144f3b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x144f3b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x144f3ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x144f3bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x144f3c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x144f3c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x144f3cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x144f3d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x144f3d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x144f3da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x144f3df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x144f3e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x144f3e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x144f3ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x144f3f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x144f3f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x144f3fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x144f3ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x144f40400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x144f408a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x144f40d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x144f411e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x144f41680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x144f41b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x144f41fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x144f42460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x144f42900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x144f42da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x144f43240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x144f436e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x144f43b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x144f44020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x144f444c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x144f44960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x144f44e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x144f452a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x144f45740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x144f45be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x144f46080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x144f46520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x144f469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x144f46e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x144f47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x144f477a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x144f47c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x144f480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x144f48580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x144f48a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x144f48ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x144f49410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x144f49960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x144f49eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x144f4a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x144f4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x144f4acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x144f4b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x144f4b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x144f4c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x144f4c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x144f4c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x144f4ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x144f4d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x144f4dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x144f4e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x144f4e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x144f4ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x144f4f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x144f4f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x144f4fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x144f501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x144f50720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x144f50c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x144f511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x144f51710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x144f51c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x144f521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x144f52700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x144f52c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x144f531a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x144f536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x144f53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x144f54190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x144f546e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x144f54c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x144f55180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x144f556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x144f55c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x144f56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x144f566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x144f56c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x144f57160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x144f576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x144f57c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x144f58150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x144f586a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x144f58bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x144f59140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x144f59690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x144f59be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x144f5a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x144f5a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x144f5abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x144f5b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x144f5b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x144f5bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x144f5c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x144f5c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x144f5cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x144f5d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x144f5d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x144f5dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x144f5e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x144f5e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x144f5eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x144f5f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x144f5f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x144f5fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x144f600d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x144f60620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x144f60b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x144f610c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x144f61610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x144f61b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x144f62000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x144f624a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x144f62940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x144f62de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x144f63280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x144f63720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x144f63bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x144f64060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x144f64500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x144f649a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x144f64e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x144f652e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x144f65780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x144f65c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x144f660c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x144f66610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x144f66d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x144f67450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x144f67b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x144f68290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x144f68550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x144f68d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x144f69000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x144f69610 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.089.477 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.480 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x143e04d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x143e051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x143e05630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x143e05aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x143e05f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x143e06380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x143e067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x143e06c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x143e070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x143e07540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x143e079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x143e080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x143e08bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x143e09370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x143e09b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x143e0a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x143e0a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x143e0b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x143e0b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x143e0bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x143e0c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x143e0cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x143e0d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x143e0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x143e0e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x143e0e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x143e0e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x143e0ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x143e0f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x143e0f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x143e0fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x143e0ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x143e103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x143e10670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x143e10ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x143e10f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x143e113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x143e11830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x143e11ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x143e12110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x143e12580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x143e129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x143e12e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x143e132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x143e13740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x143e13bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x143e14020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x143e14490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x143e14900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x143e14d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x143e151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x143e15650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x143e15ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x143e15f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x143e163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x143e16810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x143e16d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x143e17280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x143e176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x143e17b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x143e17fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x143e18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x143e188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x143e18d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x143e19190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x143e19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x143e19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x143e19ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x143e1a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x143e1a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x143e1ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x143e1b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x143e1b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x143e1b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x143e1bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x143e1c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x143e1c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x143e1cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x143e1cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x143e1d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x143e1d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x143e1dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x143e1e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x143e1e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x143e1ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x143e1eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x143e1f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x143e1f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x143e1fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x143e20080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x143e204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x143e20960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x143e20dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x143e21240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x143e216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x143e21b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x143e21f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x143e22400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x143e22870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x143e22ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x143e23150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x143e235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x143e23a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x143e23ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x143e24310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x143e24780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x143e24bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x143e25060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x143e254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x143e25940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x143e25db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x143e26220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x143e26690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x143e26b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x143e26f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x143e273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x143e27850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x143e27cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x143e28130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x143e285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x143e28a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x143e28e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x143e292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x143e29760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x143e29bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x143e2a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x143e2a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x143e2a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x143e2ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x143e2b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x143e2b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x143e2bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x143e2bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x143e2c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x143e2c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x143e2cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x143e2d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x143e2d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x143e2d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x143e2de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x143e2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x143e2e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x143e2ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x143e2f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x143e2f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x143e2f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x143e2fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x143e301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x143e30650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x143e30ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x143e30f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x143e313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x143e31810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x143e31c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x143e320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x143e32560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x143e329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x143e32e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x143e332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x143e33720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x143e33b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x143e34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x143e34470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x143e348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x143e34d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x143e351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x143e35df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x143e360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x143e36370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x143e367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x143e36c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x143e370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x143e37530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x143e379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x143e37e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x143e38280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x143e386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x143e38b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x143e38fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x143e39440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x143e398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x143e39d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x143e3a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x143e3a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x143e3aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x143e3aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x143e3b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x143e3b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x143e3bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x143e3c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x143e3c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x143e3c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x143e3cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x143e3d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x143e3d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x143e3db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x143e3dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x143e3e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x143e3e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x143e3ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x143e3f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x143e3f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x143e3fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x143e40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x143e404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x143e40930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x143e40da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x143e41210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x143e41730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x143e41c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x143e427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x143e42a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x143e43030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x143e435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x143e43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x143e44170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x143e44730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x143e44cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x143e452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x143e45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x143e45e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x143e463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x143e469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x143e46f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x143e47530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x143e47af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x143e480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x143e48670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x143e48c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x143e491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x143e497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x143e49d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x143e4a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x143e4a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x143e4aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x143e4b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x143e4ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x143e4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x143e4c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x143e4cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x143e4d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x143e4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x143e4dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x143e4e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x143e4e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x143e4edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x143e4f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x143e4f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x143e4ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x143e504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x143e50ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x143e51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x143e51630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x143e51bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x143e521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x143e52770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x143e52d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x143e532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x143e538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x143e53e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x143e54430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x143e549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x143e54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x143e55570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x143e55b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x143e560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x143e566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x143e56c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x143e57170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x143e57670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x143e57b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x143e58070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x143e58570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x143e58a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x143e58f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x143e59470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x143e59970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x143e59e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x143e5a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x143e5a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x143e5ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x143e5b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x143e5b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x143e5c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x143e5c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x143e5cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x143e5d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x143e5d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x143e5e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x143e5e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x143e5ea60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x144f692c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x144f4cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x144f4a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x144f4b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x144f1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x144f1e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x144f20690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x144f4d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x144f15a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x144f1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x144f1ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x144f1d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x144f1b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x144f1da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x144f14a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x144f20ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x144f2d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x144f68810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x144f17c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x144f17ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x144f4d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x144f4bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x144f16040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x144f16300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x144f165c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x144f69a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x144f69d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x144f69ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x144f6a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x144f6a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x144f6a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x144f6aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x144f6adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x144f6b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x144f6b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x144f6b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x144f6b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x144f6bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x144f6be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x144f6c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x144f6c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x144f6c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x144f6c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x144f6cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x144f6ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x144f6d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x144f6d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x144f6d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x144f6d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x144f6dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x144f6df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x144f6e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x144f6e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x144f6e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x144f6ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x144f6ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x144f6efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x144f6f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x144f6f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x144f6f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x144f6fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x144f6fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x144f70030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x144f702f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x144f705b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x144f70870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x144f70b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x144f70df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x144f710b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x144f71370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x144f71630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x144f718f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x144f71bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x144f71e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x144f72130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x144f723f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x144f726b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x144f72970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x144f72c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x144f72ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x144f731b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x144f73470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x144f73730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x144f739f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x144f73cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x144f73f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x144f74230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x144f744f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x144f747b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x144f74a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x144f74d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x144f74ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x144f752b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x144f75570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x144f75830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x144f75af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x144f75db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x144f76070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x144f76330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x144f765f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x144f768b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x144f76b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x144f76e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x144f770f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x144f773b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x144f77670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x144f77930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x144f77bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x144f77eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x144f78170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x144f78430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x144f786f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x144f789b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x144f78c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x144f78f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x144f791f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x144f794b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x144f79770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x144f79a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x144f79cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x144f79fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x144f7a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x144f7a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x144f7a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x144f7aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x144f7ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x144f7b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x144f7b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x144f7b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x144f7b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x144f7bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x144f7bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x144f7c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x144f7c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x144f7c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x144f7c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x144f7cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x144f7ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x144f7d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x144f7d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x144f7d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x144f7d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x144f7dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x144f7def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x144f7e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x144f7e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x144f7e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x144f7e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x144f7ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x144f7ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x144f7f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x144f7f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x144f7f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x144f7fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x144f7fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x144f7fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x144f802b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x144f80570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x144f80830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x144f80af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x144f80db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x144f81070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x144f81330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x144f815f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x144f818b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x144f81b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x144f81e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x144f820f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x144f823b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x144f82670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x144f82930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x144f82bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x144f82eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x144f83170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x144f83430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x144f836f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x144f839b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x144f83c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x144f83f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x144f841f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x144f844b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x144f84770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x144f84a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x144f84cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x144f84fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x144f85270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x144f85530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x144f857f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x144f85ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x144f85d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x144f86030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x144f862f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x144f865b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x144f86870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x144f86b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x144f86df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x144f870b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x144f87370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x144f87630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x144f878f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x144f87bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x144f87e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x144f88130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x144f883f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x144f886b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x144f88970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x144f88c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x144f88ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x144f891b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x144f89470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x144f89a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x144f89d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x144f8a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x144f8a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x144f8acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x144f8b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x144f8b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x144f8bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x144f8c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x144f8c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x144f8ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x144f8d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x144f8d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x144f8dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x144f8e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x144f8e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x144f8ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x144f8f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x144f8f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x144f8fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x144f901f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x144f90740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x144f90c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x144f911e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x144f91730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x144f91c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x144f921d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x144f92720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x144f92c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x144f931c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x144f93710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x144f93c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x144f941b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x144f94700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x144f94c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x144f951a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x144f956f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x144f95c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x144f96190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x144f966e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x144f96c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x144f97180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x144f976d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x144f97c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x144f98170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x144f986c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x144f98c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x144f99160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x144f996b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x144f99c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x144f9a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x144f9a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x144f9abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x144f9b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x144f9b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x144f9bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x144f9c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x144f9c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x144f9c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x144f9c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x144f9cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x144f9d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x144f9d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x144f9db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x144f9dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x144f9e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x144f9e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x144f9ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x144f9f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x144f9f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x144f9fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x144f9feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x144fa0320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x144fa0790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x144fa1480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x144fa1ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x144fa22c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x144fa2580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x144fa29f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x144fa2ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x144fa3600 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.919s
user	0m0.243s
sys	0m0.134s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.54 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.61 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.15 sec*proc (2 tests)

Total Test time (real) =   1.16 sec
        1.18 real         0.74 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.24 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.50 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.14 user         0.04 sys
```
