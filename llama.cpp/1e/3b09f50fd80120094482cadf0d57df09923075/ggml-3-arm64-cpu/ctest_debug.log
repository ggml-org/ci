+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: aarch64
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m0.763s
user	0m0.436s
sys	0m0.331s
+ make -j
[  0%] Generating build details from Git
[  1%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  4%] Built target ggml
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  6%] Linking C static library libggml_static.a
[  7%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[  8%] Linking CXX executable ../../bin/gguf
[  8%] Built target build_info
[  8%] Built target ggml_static
[  8%] Built target gguf
/home/ggml/work/llama.cpp/llama.cpp: In function ‘int llama_decode_internal(llama_context&, llama_batch)’:
/home/ggml/work/llama.cpp/llama.cpp:8100:36: error: format ‘%d’ expects argument of type ‘int’, but argument 2 has type ‘long int’ [-Werror=format=]
 8100 |                 printf("reading: [%d, %d], embeddings_out.size() = %d, n_tokens = %d, n_embd = %d\n", (n_embd*i), n_embd, embeddings_out.size(), n_tokens, n_embd);
      |                                   ~^                                                                  ~~~~~~~~~~
      |                                    |                                                                         |
      |                                    int                                                                       long int
      |                                   %ld
/home/ggml/work/llama.cpp/llama.cpp:8100:40: error: format ‘%d’ expects argument of type ‘int’, but argument 3 has type ‘long int’ [-Werror=format=]
 8100 |                 printf("reading: [%d, %d], embeddings_out.size() = %d, n_tokens = %d, n_embd = %d\n", (n_embd*i), n_embd, embeddings_out.size(), n_tokens, n_embd);
      |                                       ~^                                                                          ~~~~~~
      |                                        |                                                                          |
      |                                        int                                                                        long int
      |                                       %ld
/home/ggml/work/llama.cpp/llama.cpp:8100:69: error: format ‘%d’ expects argument of type ‘int’, but argument 4 has type ‘std::vector<float>::size_type’ {aka ‘long unsigned int’} [-Werror=format=]
 8100 |                 printf("reading: [%d, %d], embeddings_out.size() = %d, n_tokens = %d, n_embd = %d\n", (n_embd*i), n_embd, embeddings_out.size(), n_tokens, n_embd);
      |                                                                    ~^                                                     ~~~~~~~~~~~~~~~~~~~~~
      |                                                                     |                                                                        |
      |                                                                     int                                                                      std::vector<float>::size_type {aka long unsigned int}
      |                                                                    %ld
/home/ggml/work/llama.cpp/llama.cpp:8100:97: error: format ‘%d’ expects argument of type ‘int’, but argument 6 has type ‘long int’ [-Werror=format=]
 8100 |                 printf("reading: [%d, %d], embeddings_out.size() = %d, n_tokens = %d, n_embd = %d\n", (n_embd*i), n_embd, embeddings_out.size(), n_tokens, n_embd);
      |                                                                                                ~^                                                          ~~~~~~
      |                                                                                                 |                                                          |
      |                                                                                                 int                                                        long int
      |                                                                                                %ld
cc1plus: all warnings being treated as errors
make[2]: *** [CMakeFiles/llama.dir/build.make:76: CMakeFiles/llama.dir/llama.cpp.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:792: CMakeFiles/llama.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	0m38.181s
user	0m37.724s
sys	0m0.908s
