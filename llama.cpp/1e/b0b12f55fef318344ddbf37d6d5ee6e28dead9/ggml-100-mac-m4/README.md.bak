### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.24 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.63 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.21 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.63 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.39 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.31 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.27 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.31 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.90 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.31 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.31 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.18 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.21 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.87 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  175.26 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.88 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   25.72 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 217.67 sec*proc (28 tests)

Total Test time (real) = 217.68 sec

real	3m37.711s
user	7m25.350s
sys	0m6.187s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.43 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.29 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.21 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.92 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.39 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.20 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.38 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.02 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.46 sec*proc (28 tests)

Total Test time (real) =  51.48 sec

real	0m51.488s
user	1m11.126s
sys	0m5.679s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.138 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.668 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.342 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.024.350 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.353 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.024.354 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.355 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.024.355 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.024.356 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.024.357 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.024.358 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.024.359 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.024.360 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.024.360 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.024.364 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.024.364 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.024.365 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.024.366 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.024.366 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.024.367 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.024.368 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.029.556 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.030.837 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.839 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.030.839 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.030.840 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.030.840 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.030.841 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.030.841 I llama_model_loader: - type  f32:  124 tensors
0.00.030.842 I llama_model_loader: - type  f16:   73 tensors
0.00.030.842 I print_info: file format = GGUF V3 (latest)
0.00.030.843 I print_info: file type   = F16
0.00.030.845 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.035.285 I load: special tokens cache size = 5
0.00.037.411 I load: token to piece cache size = 0.2032 MB
0.00.037.415 I print_info: arch             = bert
0.00.037.415 I print_info: vocab_only       = 0
0.00.037.416 I print_info: n_ctx_train      = 512
0.00.037.416 I print_info: n_embd           = 384
0.00.037.416 I print_info: n_layer          = 12
0.00.037.419 I print_info: n_head           = 12
0.00.037.420 I print_info: n_head_kv        = 12
0.00.037.421 I print_info: n_rot            = 32
0.00.037.421 I print_info: n_swa            = 0
0.00.037.421 I print_info: n_embd_head_k    = 32
0.00.037.421 I print_info: n_embd_head_v    = 32
0.00.037.422 I print_info: n_gqa            = 1
0.00.037.423 I print_info: n_embd_k_gqa     = 384
0.00.037.424 I print_info: n_embd_v_gqa     = 384
0.00.037.428 I print_info: f_norm_eps       = 1.0e-12
0.00.037.428 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.429 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.429 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.430 I print_info: f_logit_scale    = 0.0e+00
0.00.037.433 I print_info: n_ff             = 1536
0.00.037.433 I print_info: n_expert         = 0
0.00.037.433 I print_info: n_expert_used    = 0
0.00.037.433 I print_info: causal attn      = 0
0.00.037.434 I print_info: pooling type     = 2
0.00.037.435 I print_info: rope type        = 2
0.00.037.435 I print_info: rope scaling     = linear
0.00.037.435 I print_info: freq_base_train  = 10000.0
0.00.037.436 I print_info: freq_scale_train = 1
0.00.037.436 I print_info: n_ctx_orig_yarn  = 512
0.00.037.436 I print_info: rope_finetuned   = unknown
0.00.037.437 I print_info: ssm_d_conv       = 0
0.00.037.437 I print_info: ssm_d_inner      = 0
0.00.037.437 I print_info: ssm_d_state      = 0
0.00.037.437 I print_info: ssm_dt_rank      = 0
0.00.037.439 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.439 I print_info: model type       = 33M
0.00.037.440 I print_info: model params     = 33.21 M
0.00.037.440 I print_info: general.name     = Bge Small
0.00.037.446 I print_info: vocab type       = WPM
0.00.037.448 I print_info: n_vocab          = 30522
0.00.037.448 I print_info: n_merges         = 0
0.00.037.449 I print_info: BOS token        = 101 '[CLS]'
0.00.037.449 I print_info: UNK token        = 100 '[UNK]'
0.00.037.449 I print_info: SEP token        = 102 '[SEP]'
0.00.037.449 I print_info: PAD token        = 0 '[PAD]'
0.00.037.450 I print_info: MASK token       = 103 '[MASK]'
0.00.037.450 I print_info: LF token         = 0 '[PAD]'
0.00.037.452 I print_info: max token length = 21
0.00.039.499 I load_tensors: offloading 12 repeating layers to GPU
0.00.039.500 I load_tensors: offloading output layer to GPU
0.00.039.500 I load_tensors: offloaded 13/13 layers to GPU
0.00.039.527 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.039.529 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.039.800 I llama_init_from_model: n_seq_max     = 1
0.00.039.802 I llama_init_from_model: n_ctx         = 512
0.00.039.802 I llama_init_from_model: n_ctx_per_seq = 512
0.00.039.802 I llama_init_from_model: n_batch       = 2048
0.00.039.803 I llama_init_from_model: n_ubatch      = 2048
0.00.039.803 I llama_init_from_model: flash_attn    = 0
0.00.039.803 I llama_init_from_model: freq_base     = 10000.0
0.00.039.804 I llama_init_from_model: freq_scale    = 1
0.00.039.804 I ggml_metal_init: allocating
0.00.039.809 I ggml_metal_init: found device: Apple M4
0.00.039.812 I ggml_metal_init: picking default device: Apple M4
0.00.040.694 I ggml_metal_init: using embedded metal library
0.00.045.109 I ggml_metal_init: GPU name:   Apple M4
0.00.045.112 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.045.112 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.045.113 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.045.113 I ggml_metal_init: simdgroup reduction   = true
0.00.045.113 I ggml_metal_init: simdgroup matrix mul. = true
0.00.045.114 I ggml_metal_init: has bfloat            = true
0.00.045.114 I ggml_metal_init: use bfloat            = true
0.00.045.114 I ggml_metal_init: hasUnifiedMemory      = true
0.00.045.115 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.057.776 I init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.058.487 I init:      Metal KV buffer size =     9.00 MiB
0.00.058.490 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.058.491 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.059.316 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.059.318 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.059.318 I llama_init_from_model: graph nodes  = 429
0.00.059.318 I llama_init_from_model: graph splits = 2
0.00.059.319 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.059.320 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.066.596 I 
0.00.066.626 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.067.278 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.072.408 I llama_perf_context_print:        load time =      47.92 ms
0.00.072.409 I llama_perf_context_print: prompt eval time =       4.98 ms /     9 tokens (    0.55 ms per token,  1808.68 tokens per second)
0.00.072.409 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.072.410 I llama_perf_context_print:       total time =       5.81 ms /    10 tokens
0.00.072.627 I ggml_metal_free: deallocating

real	0m0.263s
user	0m0.051s
sys	0m0.034s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.040 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.276 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.984 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.988 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.990 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.990 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.991 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.991 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.991 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.992 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.993 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.993 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.993 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.994 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.996 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.996 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.997 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.997 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.997 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.997 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.476 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.112 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.113 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.114 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.114 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.114 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.114 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.115 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.115 I llama_model_loader: - type  f32:  124 tensors
0.00.015.115 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.116 I print_info: file format = GGUF V3 (latest)
0.00.015.117 I print_info: file type   = Q8_0
0.00.015.117 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.509 I load: special tokens cache size = 5
0.00.018.817 I load: token to piece cache size = 0.2032 MB
0.00.018.820 I print_info: arch             = bert
0.00.018.820 I print_info: vocab_only       = 0
0.00.018.820 I print_info: n_ctx_train      = 512
0.00.018.820 I print_info: n_embd           = 384
0.00.018.820 I print_info: n_layer          = 12
0.00.018.823 I print_info: n_head           = 12
0.00.018.823 I print_info: n_head_kv        = 12
0.00.018.823 I print_info: n_rot            = 32
0.00.018.824 I print_info: n_swa            = 0
0.00.018.824 I print_info: n_embd_head_k    = 32
0.00.018.824 I print_info: n_embd_head_v    = 32
0.00.018.824 I print_info: n_gqa            = 1
0.00.018.825 I print_info: n_embd_k_gqa     = 384
0.00.018.825 I print_info: n_embd_v_gqa     = 384
0.00.018.826 I print_info: f_norm_eps       = 1.0e-12
0.00.018.826 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.826 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.827 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.827 I print_info: f_logit_scale    = 0.0e+00
0.00.018.827 I print_info: n_ff             = 1536
0.00.018.827 I print_info: n_expert         = 0
0.00.018.828 I print_info: n_expert_used    = 0
0.00.018.828 I print_info: causal attn      = 0
0.00.018.828 I print_info: pooling type     = 2
0.00.018.828 I print_info: rope type        = 2
0.00.018.828 I print_info: rope scaling     = linear
0.00.018.829 I print_info: freq_base_train  = 10000.0
0.00.018.829 I print_info: freq_scale_train = 1
0.00.018.829 I print_info: n_ctx_orig_yarn  = 512
0.00.018.829 I print_info: rope_finetuned   = unknown
0.00.018.829 I print_info: ssm_d_conv       = 0
0.00.018.830 I print_info: ssm_d_inner      = 0
0.00.018.830 I print_info: ssm_d_state      = 0
0.00.018.830 I print_info: ssm_dt_rank      = 0
0.00.018.830 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.830 I print_info: model type       = 33M
0.00.018.830 I print_info: model params     = 33.21 M
0.00.018.831 I print_info: general.name     = Bge Small
0.00.018.831 I print_info: vocab type       = WPM
0.00.018.831 I print_info: n_vocab          = 30522
0.00.018.831 I print_info: n_merges         = 0
0.00.018.832 I print_info: BOS token        = 101 '[CLS]'
0.00.018.832 I print_info: UNK token        = 100 '[UNK]'
0.00.018.832 I print_info: SEP token        = 102 '[SEP]'
0.00.018.832 I print_info: PAD token        = 0 '[PAD]'
0.00.018.833 I print_info: MASK token       = 103 '[MASK]'
0.00.018.833 I print_info: LF token         = 0 '[PAD]'
0.00.018.833 I print_info: max token length = 21
0.00.020.144 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.144 I load_tensors: offloading output layer to GPU
0.00.020.146 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.154 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.154 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.020.300 I llama_init_from_model: n_seq_max     = 1
0.00.020.301 I llama_init_from_model: n_ctx         = 512
0.00.020.301 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.301 I llama_init_from_model: n_batch       = 2048
0.00.020.301 I llama_init_from_model: n_ubatch      = 2048
0.00.020.301 I llama_init_from_model: flash_attn    = 0
0.00.020.302 I llama_init_from_model: freq_base     = 10000.0
0.00.020.302 I llama_init_from_model: freq_scale    = 1
0.00.020.302 I ggml_metal_init: allocating
0.00.020.305 I ggml_metal_init: found device: Apple M4
0.00.020.307 I ggml_metal_init: picking default device: Apple M4
0.00.020.925 I ggml_metal_init: using embedded metal library
0.00.023.456 I ggml_metal_init: GPU name:   Apple M4
0.00.023.458 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.459 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.459 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.459 I ggml_metal_init: simdgroup reduction   = true
0.00.023.460 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.460 I ggml_metal_init: has bfloat            = true
0.00.023.460 I ggml_metal_init: use bfloat            = true
0.00.023.460 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.461 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.899 I init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.393 I init:      Metal KV buffer size =     9.00 MiB
0.00.034.396 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.400 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.035.010 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.035.011 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.035.012 I llama_init_from_model: graph nodes  = 429
0.00.035.012 I llama_init_from_model: graph splits = 2
0.00.035.013 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.013 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.247 I 
0.00.040.279 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.789 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.224 I llama_perf_context_print:        load time =      30.96 ms
0.00.045.225 I llama_perf_context_print: prompt eval time =       4.32 ms /     9 tokens (    0.48 ms per token,  2082.85 tokens per second)
0.00.045.226 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.226 I llama_perf_context_print:       total time =       4.98 ms /    10 tokens
0.00.045.542 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.189 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.345 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.637 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.642 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.644 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.652 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.653 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.654 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.654 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.655 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.656 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.659 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.660 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.661 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.664 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.664 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.665 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.666 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.666 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.043.307 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.504 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.848 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.850 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.851 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.851 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.852 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.852 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.852 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.853 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.853 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.853 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.854 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.049.854 I llama_model_loader: - type  f32:   40 tensors
0.00.049.854 I llama_model_loader: - type  f16:   30 tensors
0.00.049.855 I print_info: file format = GGUF V3 (latest)
0.00.049.856 I print_info: file type   = F16
0.00.049.858 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.066.316 W load: empty token at index 5
0.00.070.674 W load: model vocab missing newline token, using special_pad_id instead
0.00.071.936 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.071.968 I load: special tokens cache size = 5
0.00.334.102 I load: token to piece cache size = 1.5060 MB
0.00.334.111 I print_info: arch             = jina-bert-v2
0.00.334.112 I print_info: vocab_only       = 0
0.00.334.112 I print_info: n_ctx_train      = 8192
0.00.334.112 I print_info: n_embd           = 384
0.00.334.113 I print_info: n_layer          = 4
0.00.334.119 I print_info: n_head           = 12
0.00.334.120 I print_info: n_head_kv        = 12
0.00.334.120 I print_info: n_rot            = 32
0.00.334.123 I print_info: n_swa            = 0
0.00.334.123 I print_info: n_embd_head_k    = 32
0.00.334.123 I print_info: n_embd_head_v    = 32
0.00.334.124 I print_info: n_gqa            = 1
0.00.334.125 I print_info: n_embd_k_gqa     = 384
0.00.334.125 I print_info: n_embd_v_gqa     = 384
0.00.334.126 I print_info: f_norm_eps       = 1.0e-12
0.00.334.127 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.334.127 I print_info: f_clamp_kqv      = 0.0e+00
0.00.334.128 I print_info: f_max_alibi_bias = 8.0e+00
0.00.334.128 I print_info: f_logit_scale    = 0.0e+00
0.00.334.128 I print_info: n_ff             = 1536
0.00.334.129 I print_info: n_expert         = 0
0.00.334.129 I print_info: n_expert_used    = 0
0.00.334.129 I print_info: causal attn      = 0
0.00.334.129 I print_info: pooling type     = -1
0.00.334.129 I print_info: rope type        = -1
0.00.334.130 I print_info: rope scaling     = linear
0.00.334.130 I print_info: freq_base_train  = 10000.0
0.00.334.130 I print_info: freq_scale_train = 1
0.00.334.131 I print_info: n_ctx_orig_yarn  = 8192
0.00.334.131 I print_info: rope_finetuned   = unknown
0.00.334.131 I print_info: ssm_d_conv       = 0
0.00.334.131 I print_info: ssm_d_inner      = 0
0.00.334.131 I print_info: ssm_d_state      = 0
0.00.334.131 I print_info: ssm_dt_rank      = 0
0.00.334.131 I print_info: ssm_dt_b_c_rms   = 0
0.00.334.132 I print_info: model type       = 33M
0.00.334.132 I print_info: model params     = 32.90 M
0.00.334.134 I print_info: general.name     = Jina Bert Implementation
0.00.334.134 I print_info: vocab type       = BPE
0.00.334.135 I print_info: n_vocab          = 61056
0.00.334.135 I print_info: n_merges         = 39382
0.00.334.135 I print_info: BOS token        = 0 '<s>'
0.00.334.135 I print_info: EOS token        = 2 '</s>'
0.00.334.135 I print_info: UNK token        = 3 '<unk>'
0.00.334.135 I print_info: SEP token        = 2 '</s>'
0.00.334.136 I print_info: PAD token        = 1 '<pad>'
0.00.334.136 I print_info: MASK token       = 4 '<mask>'
0.00.334.136 I print_info: EOG token        = 2 '</s>'
0.00.334.136 I print_info: max token length = 45
0.00.335.250 I load_tensors: offloading 4 repeating layers to GPU
0.00.335.250 I load_tensors: offloading output layer to GPU
0.00.335.250 I load_tensors: offloaded 5/5 layers to GPU
0.00.335.275 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.335.277 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.335.739 I llama_init_from_model: n_seq_max     = 1
0.00.335.740 I llama_init_from_model: n_ctx         = 8192
0.00.335.741 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.335.741 I llama_init_from_model: n_batch       = 2048
0.00.335.741 I llama_init_from_model: n_ubatch      = 2048
0.00.335.741 I llama_init_from_model: flash_attn    = 0
0.00.335.741 I llama_init_from_model: freq_base     = 10000.0
0.00.335.742 I llama_init_from_model: freq_scale    = 1
0.00.335.742 I ggml_metal_init: allocating
0.00.335.746 I ggml_metal_init: found device: Apple M4
0.00.335.749 I ggml_metal_init: picking default device: Apple M4
0.00.336.667 I ggml_metal_init: using embedded metal library
0.00.339.329 I ggml_metal_init: GPU name:   Apple M4
0.00.339.330 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.339.331 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.339.331 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.339.331 I ggml_metal_init: simdgroup reduction   = true
0.00.339.331 I ggml_metal_init: simdgroup matrix mul. = true
0.00.339.332 I ggml_metal_init: has bfloat            = true
0.00.339.332 I ggml_metal_init: use bfloat            = true
0.00.339.332 I ggml_metal_init: hasUnifiedMemory      = true
0.00.339.333 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.348.854 I init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.351.283 I init:      Metal KV buffer size =    48.00 MiB
0.00.351.285 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.351.288 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.351.957 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.351.958 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.351.958 I llama_init_from_model: graph nodes  = 154
0.00.351.958 I llama_init_from_model: graph splits = 2
0.00.351.959 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.351.959 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.364.956 I 
0.00.364.990 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.365.223 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.365.223 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.365.236 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.365.236 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.365.241 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.365.241 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.365.745 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.369.525 I llama_perf_context_print:        load time =     342.61 ms
0.00.369.526 I llama_perf_context_print: prompt eval time =       3.77 ms /    62 tokens (    0.06 ms per token, 16449.99 tokens per second)
0.00.369.527 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.369.529 I llama_perf_context_print:       total time =       4.57 ms /    63 tokens
0.00.370.274 I ggml_metal_free: deallocating

real	0m1.086s
user	0m0.341s
sys	0m0.046s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.203 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.345 I main: llama backend init
0.00.000.352 I main: load the model and apply lora adapter, if any
0.00.045.241 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.057.878 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.057.890 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.057.895 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.057.895 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.057.896 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.057.901 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.057.902 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.057.903 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.057.904 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.057.905 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.057.905 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.057.906 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.057.906 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.057.907 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.057.912 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.057.912 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.057.913 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.064.943 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.067.078 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.074.663 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.074.672 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.074.672 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.074.673 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.074.674 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.074.675 I llama_model_loader: - type  f32:  194 tensors
0.00.074.675 I llama_model_loader: - type  f16:   98 tensors
0.00.074.680 I print_info: file format = GGUF V3 (latest)
0.00.074.682 I print_info: file type   = all F32 (guessed)
0.00.074.684 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.110.173 I load: special tokens cache size = 25
0.00.117.675 I load: token to piece cache size = 0.2984 MB
0.00.117.678 I print_info: arch             = gptneox
0.00.117.679 I print_info: vocab_only       = 0
0.00.117.679 I print_info: n_ctx_train      = 2048
0.00.117.679 I print_info: n_embd           = 2048
0.00.117.679 I print_info: n_layer          = 24
0.00.117.683 I print_info: n_head           = 16
0.00.117.683 I print_info: n_head_kv        = 16
0.00.117.684 I print_info: n_rot            = 32
0.00.117.684 I print_info: n_swa            = 0
0.00.117.684 I print_info: n_embd_head_k    = 128
0.00.117.684 I print_info: n_embd_head_v    = 128
0.00.117.685 I print_info: n_gqa            = 1
0.00.117.686 I print_info: n_embd_k_gqa     = 2048
0.00.117.686 I print_info: n_embd_v_gqa     = 2048
0.00.117.687 I print_info: f_norm_eps       = 1.0e-05
0.00.117.687 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.117.687 I print_info: f_clamp_kqv      = 0.0e+00
0.00.117.688 I print_info: f_max_alibi_bias = 0.0e+00
0.00.117.688 I print_info: f_logit_scale    = 0.0e+00
0.00.117.688 I print_info: n_ff             = 8192
0.00.117.689 I print_info: n_expert         = 0
0.00.117.689 I print_info: n_expert_used    = 0
0.00.117.689 I print_info: causal attn      = 1
0.00.117.690 I print_info: pooling type     = 0
0.00.117.690 I print_info: rope type        = 2
0.00.117.690 I print_info: rope scaling     = linear
0.00.117.691 I print_info: freq_base_train  = 10000.0
0.00.117.691 I print_info: freq_scale_train = 1
0.00.117.691 I print_info: n_ctx_orig_yarn  = 2048
0.00.117.692 I print_info: rope_finetuned   = unknown
0.00.117.698 I print_info: ssm_d_conv       = 0
0.00.117.700 I print_info: ssm_d_inner      = 0
0.00.117.700 I print_info: ssm_d_state      = 0
0.00.117.700 I print_info: ssm_dt_rank      = 0
0.00.117.700 I print_info: ssm_dt_b_c_rms   = 0
0.00.117.701 I print_info: model type       = 1.4B
0.00.117.701 I print_info: model params     = 1.41 B
0.00.117.701 I print_info: general.name     = 1.4B
0.00.117.702 I print_info: vocab type       = BPE
0.00.117.702 I print_info: n_vocab          = 50304
0.00.117.702 I print_info: n_merges         = 50009
0.00.117.703 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.117.703 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.117.703 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.117.703 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.117.704 I print_info: LF token         = 128 'Ä'
0.00.117.704 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.117.704 I print_info: max token length = 1024
0.00.120.403 I load_tensors: offloading 24 repeating layers to GPU
0.00.120.403 I load_tensors: offloading output layer to GPU
0.00.120.403 I load_tensors: offloaded 25/25 layers to GPU
0.00.120.422 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.120.423 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.120.764 I llama_init_from_model: n_seq_max     = 1
0.00.120.765 I llama_init_from_model: n_ctx         = 2048
0.00.120.765 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.120.765 I llama_init_from_model: n_batch       = 2048
0.00.120.765 I llama_init_from_model: n_ubatch      = 512
0.00.120.766 I llama_init_from_model: flash_attn    = 0
0.00.120.766 I llama_init_from_model: freq_base     = 10000.0
0.00.120.766 I llama_init_from_model: freq_scale    = 1
0.00.120.767 I ggml_metal_init: allocating
0.00.120.770 I ggml_metal_init: found device: Apple M4
0.00.120.772 I ggml_metal_init: picking default device: Apple M4
0.00.121.483 I ggml_metal_init: using embedded metal library
0.00.132.996 I ggml_metal_init: GPU name:   Apple M4
0.00.132.997 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.132.998 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.132.998 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.132.998 I ggml_metal_init: simdgroup reduction   = true
0.00.132.999 I ggml_metal_init: simdgroup matrix mul. = true
0.00.132.999 I ggml_metal_init: has bfloat            = true
0.00.132.999 I ggml_metal_init: use bfloat            = true
0.00.132.999 I ggml_metal_init: hasUnifiedMemory      = true
0.00.133.000 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.157.357 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.178.994 I init:      Metal KV buffer size =   384.00 MiB
0.00.179.003 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.179.025 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.180.002 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.180.004 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.180.004 I llama_init_from_model: graph nodes  = 967
0.00.180.004 I llama_init_from_model: graph splits = 2
0.00.180.008 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.180.137 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.180.137 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.262.512 I main: llama threadpool init, n_threads = 4
0.00.262.559 I 
0.00.262.597 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.262.599 I 
0.00.262.665 I sampler seed: 1234
0.00.262.669 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.262.695 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.262.696 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.262.696 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.102.762 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55382.22 tokens per second)
0.02.102.763 I llama_perf_context_print:        load time =     217.26 ms
0.02.102.763 I llama_perf_context_print: prompt eval time =      43.55 ms /     7 tokens (    6.22 ms per token,   160.75 tokens per second)
0.02.102.764 I llama_perf_context_print:        eval time =    1793.48 ms /    63 runs   (   28.47 ms per token,    35.13 tokens per second)
0.02.102.764 I llama_perf_context_print:       total time =    1840.25 ms /    70 tokens
0.02.106.590 I ggml_metal_free: deallocating

real	0m2.393s
user	0m0.148s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.647 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.027.147 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.624 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.634 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.637 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.638 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.639 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.640 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.640 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.643 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.643 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.644 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.645 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.646 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.647 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.648 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.652 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.653 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.654 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.859 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.335 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.714 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.059.716 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.717 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.717 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.718 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.719 I llama_model_loader: - type  f32:  194 tensors
0.00.059.719 I llama_model_loader: - type  f16:   98 tensors
0.00.059.720 I print_info: file format = GGUF V3 (latest)
0.00.059.721 I print_info: file type   = all F32 (guessed)
0.00.059.725 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.087.962 I load: special tokens cache size = 25
0.00.094.734 I load: token to piece cache size = 0.2984 MB
0.00.094.737 I print_info: arch             = gptneox
0.00.094.737 I print_info: vocab_only       = 0
0.00.094.737 I print_info: n_ctx_train      = 2048
0.00.094.737 I print_info: n_embd           = 2048
0.00.094.737 I print_info: n_layer          = 24
0.00.094.741 I print_info: n_head           = 16
0.00.094.742 I print_info: n_head_kv        = 16
0.00.094.742 I print_info: n_rot            = 32
0.00.094.742 I print_info: n_swa            = 0
0.00.094.742 I print_info: n_embd_head_k    = 128
0.00.094.742 I print_info: n_embd_head_v    = 128
0.00.094.744 I print_info: n_gqa            = 1
0.00.094.745 I print_info: n_embd_k_gqa     = 2048
0.00.094.745 I print_info: n_embd_v_gqa     = 2048
0.00.094.746 I print_info: f_norm_eps       = 1.0e-05
0.00.094.746 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.094.748 I print_info: f_clamp_kqv      = 0.0e+00
0.00.094.748 I print_info: f_max_alibi_bias = 0.0e+00
0.00.094.748 I print_info: f_logit_scale    = 0.0e+00
0.00.094.749 I print_info: n_ff             = 8192
0.00.094.749 I print_info: n_expert         = 0
0.00.094.749 I print_info: n_expert_used    = 0
0.00.094.749 I print_info: causal attn      = 1
0.00.094.749 I print_info: pooling type     = 0
0.00.094.750 I print_info: rope type        = 2
0.00.094.751 I print_info: rope scaling     = linear
0.00.094.751 I print_info: freq_base_train  = 10000.0
0.00.094.751 I print_info: freq_scale_train = 1
0.00.094.751 I print_info: n_ctx_orig_yarn  = 2048
0.00.094.752 I print_info: rope_finetuned   = unknown
0.00.094.752 I print_info: ssm_d_conv       = 0
0.00.094.752 I print_info: ssm_d_inner      = 0
0.00.094.752 I print_info: ssm_d_state      = 0
0.00.094.752 I print_info: ssm_dt_rank      = 0
0.00.094.752 I print_info: ssm_dt_b_c_rms   = 0
0.00.094.752 I print_info: model type       = 1.4B
0.00.094.753 I print_info: model params     = 1.41 B
0.00.094.753 I print_info: general.name     = 1.4B
0.00.094.757 I print_info: vocab type       = BPE
0.00.094.757 I print_info: n_vocab          = 50304
0.00.094.757 I print_info: n_merges         = 50009
0.00.094.757 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.094.758 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.094.758 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.094.758 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.094.758 I print_info: LF token         = 128 'Ä'
0.00.094.758 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.094.759 I print_info: max token length = 1024
0.00.097.352 I load_tensors: offloading 24 repeating layers to GPU
0.00.097.352 I load_tensors: offloading output layer to GPU
0.00.097.353 I load_tensors: offloaded 25/25 layers to GPU
0.00.097.363 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.097.364 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.097.661 I llama_init_from_model: n_seq_max     = 1
0.00.097.662 I llama_init_from_model: n_ctx         = 128
0.00.097.662 I llama_init_from_model: n_ctx_per_seq = 128
0.00.097.663 I llama_init_from_model: n_batch       = 128
0.00.097.663 I llama_init_from_model: n_ubatch      = 128
0.00.097.663 I llama_init_from_model: flash_attn    = 0
0.00.097.663 I llama_init_from_model: freq_base     = 10000.0
0.00.097.664 I llama_init_from_model: freq_scale    = 1
0.00.097.664 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.097.664 I ggml_metal_init: allocating
0.00.097.667 I ggml_metal_init: found device: Apple M4
0.00.097.669 I ggml_metal_init: picking default device: Apple M4
0.00.098.291 I ggml_metal_init: using embedded metal library
0.00.100.900 I ggml_metal_init: GPU name:   Apple M4
0.00.100.902 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.100.902 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.100.903 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.100.903 I ggml_metal_init: simdgroup reduction   = true
0.00.100.903 I ggml_metal_init: simdgroup matrix mul. = true
0.00.100.903 I ggml_metal_init: has bfloat            = true
0.00.100.903 I ggml_metal_init: use bfloat            = true
0.00.100.904 I ggml_metal_init: hasUnifiedMemory      = true
0.00.100.904 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.483 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.111.810 I init:      Metal KV buffer size =    24.00 MiB
0.00.111.814 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.111.830 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.112.697 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.112.698 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.112.698 I llama_init_from_model: graph nodes  = 967
0.00.112.699 I llama_init_from_model: graph splits = 2
0.00.112.700 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.112.700 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.958.106 I 
0.00.958.165 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.958.172 I perplexity: tokenizing the input ..
0.00.971.847 I perplexity: tokenization took 13.672 ms
0.00.971.852 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.091.903 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.093.749 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.093.806 I llama_perf_context_print:        load time =     930.94 ms
0.01.093.808 I llama_perf_context_print: prompt eval time =     119.51 ms /   128 tokens (    0.93 ms per token,  1071.02 tokens per second)
0.01.093.809 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.093.810 I llama_perf_context_print:       total time =     135.70 ms /   129 tokens
0.01.095.047 I ggml_metal_free: deallocating

real	0m1.289s
user	0m0.127s
sys	0m0.191s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.776 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.957 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.964 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.966 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.966 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.967 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.969 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.970 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.971 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.971 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.971 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.972 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.972 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.972 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.973 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.975 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.975 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.975 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.061 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.104 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.051 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.052 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.053 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.053 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.054 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.054 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.055 I llama_model_loader: - type  f32:  194 tensors
0.00.039.055 I llama_model_loader: - type q8_0:   98 tensors
0.00.039.056 I print_info: file format = GGUF V3 (latest)
0.00.039.057 I print_info: file type   = Q8_0
0.00.039.058 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.060.928 I load: special tokens cache size = 25
0.00.066.980 I load: token to piece cache size = 0.2984 MB
0.00.066.983 I print_info: arch             = gptneox
0.00.066.983 I print_info: vocab_only       = 0
0.00.066.984 I print_info: n_ctx_train      = 2048
0.00.066.984 I print_info: n_embd           = 2048
0.00.066.984 I print_info: n_layer          = 24
0.00.066.989 I print_info: n_head           = 16
0.00.066.990 I print_info: n_head_kv        = 16
0.00.066.990 I print_info: n_rot            = 32
0.00.066.990 I print_info: n_swa            = 0
0.00.066.990 I print_info: n_embd_head_k    = 128
0.00.066.991 I print_info: n_embd_head_v    = 128
0.00.066.991 I print_info: n_gqa            = 1
0.00.066.992 I print_info: n_embd_k_gqa     = 2048
0.00.066.993 I print_info: n_embd_v_gqa     = 2048
0.00.066.993 I print_info: f_norm_eps       = 1.0e-05
0.00.066.994 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.066.994 I print_info: f_clamp_kqv      = 0.0e+00
0.00.066.994 I print_info: f_max_alibi_bias = 0.0e+00
0.00.066.994 I print_info: f_logit_scale    = 0.0e+00
0.00.066.995 I print_info: n_ff             = 8192
0.00.066.995 I print_info: n_expert         = 0
0.00.066.996 I print_info: n_expert_used    = 0
0.00.066.996 I print_info: causal attn      = 1
0.00.066.996 I print_info: pooling type     = 0
0.00.066.996 I print_info: rope type        = 2
0.00.066.996 I print_info: rope scaling     = linear
0.00.066.997 I print_info: freq_base_train  = 10000.0
0.00.066.997 I print_info: freq_scale_train = 1
0.00.066.997 I print_info: n_ctx_orig_yarn  = 2048
0.00.066.997 I print_info: rope_finetuned   = unknown
0.00.066.997 I print_info: ssm_d_conv       = 0
0.00.066.998 I print_info: ssm_d_inner      = 0
0.00.066.998 I print_info: ssm_d_state      = 0
0.00.067.002 I print_info: ssm_dt_rank      = 0
0.00.067.002 I print_info: ssm_dt_b_c_rms   = 0
0.00.067.003 I print_info: model type       = 1.4B
0.00.067.003 I print_info: model params     = 1.41 B
0.00.067.003 I print_info: general.name     = 1.4B
0.00.067.004 I print_info: vocab type       = BPE
0.00.067.004 I print_info: n_vocab          = 50304
0.00.067.004 I print_info: n_merges         = 50009
0.00.067.004 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.067.005 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.067.005 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.067.005 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.067.005 I print_info: LF token         = 128 'Ä'
0.00.067.006 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.067.006 I print_info: max token length = 1024
0.00.069.422 I load_tensors: offloading 24 repeating layers to GPU
0.00.069.422 I load_tensors: offloading output layer to GPU
0.00.069.422 I load_tensors: offloaded 25/25 layers to GPU
0.00.069.434 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.069.435 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.069.789 I llama_init_from_model: n_seq_max     = 1
0.00.069.790 I llama_init_from_model: n_ctx         = 2048
0.00.069.790 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.069.790 I llama_init_from_model: n_batch       = 2048
0.00.069.790 I llama_init_from_model: n_ubatch      = 512
0.00.069.791 I llama_init_from_model: flash_attn    = 0
0.00.069.791 I llama_init_from_model: freq_base     = 10000.0
0.00.069.791 I llama_init_from_model: freq_scale    = 1
0.00.069.792 I ggml_metal_init: allocating
0.00.069.795 I ggml_metal_init: found device: Apple M4
0.00.069.797 I ggml_metal_init: picking default device: Apple M4
0.00.070.567 I ggml_metal_init: using embedded metal library
0.00.073.340 I ggml_metal_init: GPU name:   Apple M4
0.00.073.342 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.342 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.343 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.343 I ggml_metal_init: simdgroup reduction   = true
0.00.073.343 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.343 I ggml_metal_init: has bfloat            = true
0.00.073.344 I ggml_metal_init: use bfloat            = true
0.00.073.344 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.345 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.444 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.109.434 I init:      Metal KV buffer size =   384.00 MiB
0.00.109.443 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.109.467 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.110.738 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.110.741 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.110.741 I llama_init_from_model: graph nodes  = 967
0.00.110.741 I llama_init_from_model: graph splits = 2
0.00.110.746 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.110.874 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.110.875 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.480.927 I main: llama threadpool init, n_threads = 4
0.01.481.002 I 
0.01.481.071 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.481.073 I 
0.01.481.589 I sampler seed: 1234
0.01.481.594 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.481.663 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.481.667 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.481.667 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.570.867 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53343.35 tokens per second)
0.02.570.867 I llama_perf_context_print:        load time =    1471.14 ms
0.02.570.868 I llama_perf_context_print: prompt eval time =      50.47 ms /     7 tokens (    7.21 ms per token,   138.69 tokens per second)
0.02.570.869 I llama_perf_context_print:        eval time =    1035.70 ms /    63 runs   (   16.44 ms per token,    60.83 tokens per second)
0.02.570.869 I llama_perf_context_print:       total time =    1089.95 ms /    70 tokens
0.02.573.340 I ggml_metal_free: deallocating

real	0m2.591s
user	0m0.123s
sys	0m0.239s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.125 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.850 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.674 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.680 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.683 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.683 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.684 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.684 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.685 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.686 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.686 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.687 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.687 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.687 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.688 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.688 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.691 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.691 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.691 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.554 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.958 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.371 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.373 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.374 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.374 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.374 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.375 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.375 I llama_model_loader: - type  f32:  194 tensors
0.00.035.376 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.377 I print_info: file format = GGUF V3 (latest)
0.00.035.379 I print_info: file type   = Q8_0
0.00.035.380 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.059.816 I load: special tokens cache size = 25
0.00.066.259 I load: token to piece cache size = 0.2984 MB
0.00.066.262 I print_info: arch             = gptneox
0.00.066.263 I print_info: vocab_only       = 0
0.00.066.263 I print_info: n_ctx_train      = 2048
0.00.066.263 I print_info: n_embd           = 2048
0.00.066.263 I print_info: n_layer          = 24
0.00.066.266 I print_info: n_head           = 16
0.00.066.267 I print_info: n_head_kv        = 16
0.00.066.267 I print_info: n_rot            = 32
0.00.066.268 I print_info: n_swa            = 0
0.00.066.268 I print_info: n_embd_head_k    = 128
0.00.066.269 I print_info: n_embd_head_v    = 128
0.00.066.269 I print_info: n_gqa            = 1
0.00.066.270 I print_info: n_embd_k_gqa     = 2048
0.00.066.272 I print_info: n_embd_v_gqa     = 2048
0.00.066.272 I print_info: f_norm_eps       = 1.0e-05
0.00.066.273 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.066.273 I print_info: f_clamp_kqv      = 0.0e+00
0.00.066.273 I print_info: f_max_alibi_bias = 0.0e+00
0.00.066.273 I print_info: f_logit_scale    = 0.0e+00
0.00.066.274 I print_info: n_ff             = 8192
0.00.066.274 I print_info: n_expert         = 0
0.00.066.274 I print_info: n_expert_used    = 0
0.00.066.274 I print_info: causal attn      = 1
0.00.066.275 I print_info: pooling type     = 0
0.00.066.275 I print_info: rope type        = 2
0.00.066.275 I print_info: rope scaling     = linear
0.00.066.275 I print_info: freq_base_train  = 10000.0
0.00.066.275 I print_info: freq_scale_train = 1
0.00.066.276 I print_info: n_ctx_orig_yarn  = 2048
0.00.066.276 I print_info: rope_finetuned   = unknown
0.00.066.276 I print_info: ssm_d_conv       = 0
0.00.066.277 I print_info: ssm_d_inner      = 0
0.00.066.277 I print_info: ssm_d_state      = 0
0.00.066.277 I print_info: ssm_dt_rank      = 0
0.00.066.278 I print_info: ssm_dt_b_c_rms   = 0
0.00.066.278 I print_info: model type       = 1.4B
0.00.066.278 I print_info: model params     = 1.41 B
0.00.066.278 I print_info: general.name     = 1.4B
0.00.066.279 I print_info: vocab type       = BPE
0.00.066.279 I print_info: n_vocab          = 50304
0.00.066.279 I print_info: n_merges         = 50009
0.00.066.280 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.066.280 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.066.280 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.066.280 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.066.280 I print_info: LF token         = 128 'Ä'
0.00.066.281 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.066.281 I print_info: max token length = 1024
0.00.068.648 I load_tensors: offloading 24 repeating layers to GPU
0.00.068.648 I load_tensors: offloading output layer to GPU
0.00.068.648 I load_tensors: offloaded 25/25 layers to GPU
0.00.068.659 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.660 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.068.969 I llama_init_from_model: n_seq_max     = 1
0.00.068.970 I llama_init_from_model: n_ctx         = 128
0.00.068.970 I llama_init_from_model: n_ctx_per_seq = 128
0.00.068.971 I llama_init_from_model: n_batch       = 128
0.00.068.971 I llama_init_from_model: n_ubatch      = 128
0.00.068.971 I llama_init_from_model: flash_attn    = 0
0.00.068.971 I llama_init_from_model: freq_base     = 10000.0
0.00.068.972 I llama_init_from_model: freq_scale    = 1
0.00.068.972 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.068.972 I ggml_metal_init: allocating
0.00.068.976 I ggml_metal_init: found device: Apple M4
0.00.068.978 I ggml_metal_init: picking default device: Apple M4
0.00.069.637 I ggml_metal_init: using embedded metal library
0.00.072.196 I ggml_metal_init: GPU name:   Apple M4
0.00.072.198 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.198 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.198 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.199 I ggml_metal_init: simdgroup reduction   = true
0.00.072.199 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.199 I ggml_metal_init: has bfloat            = true
0.00.072.199 I ggml_metal_init: use bfloat            = true
0.00.072.200 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.200 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.003 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.313 I init:      Metal KV buffer size =    24.00 MiB
0.00.083.318 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.083.336 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.177 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.084.179 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.084.179 I llama_init_from_model: graph nodes  = 967
0.00.084.179 I llama_init_from_model: graph splits = 2
0.00.084.181 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.084.181 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.941.225 I 
0.00.941.255 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.941.258 I perplexity: tokenizing the input ..
0.00.949.288 I perplexity: tokenization took 8.029 ms
0.00.949.291 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.073.406 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.074.564 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.074.589 I llama_perf_context_print:        load time =     929.37 ms
0.01.074.590 I llama_perf_context_print: prompt eval time =     123.89 ms /   128 tokens (    0.97 ms per token,  1033.19 tokens per second)
0.01.074.591 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.074.592 I llama_perf_context_print:       total time =     133.37 ms /   129 tokens
0.01.075.317 I ggml_metal_free: deallocating

real	0m1.094s
user	0m0.095s
sys	0m0.152s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.011.666 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.276 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.282 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.284 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.284 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.285 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.285 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.285 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.287 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.287 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.287 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.288 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.288 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.288 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.289 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.292 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.293 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.293 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.150 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.174 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.022 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.023 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.024 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.024 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.024 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.025 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.025 I llama_model_loader: - type  f32:  194 tensors
0.00.028.025 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.026 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.027 I print_info: file format = GGUF V3 (latest)
0.00.028.029 I print_info: file type   = Q4_0
0.00.028.030 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.047.184 I load: special tokens cache size = 25
0.00.053.231 I load: token to piece cache size = 0.2984 MB
0.00.053.235 I print_info: arch             = gptneox
0.00.053.236 I print_info: vocab_only       = 0
0.00.053.236 I print_info: n_ctx_train      = 2048
0.00.053.236 I print_info: n_embd           = 2048
0.00.053.236 I print_info: n_layer          = 24
0.00.053.241 I print_info: n_head           = 16
0.00.053.242 I print_info: n_head_kv        = 16
0.00.053.242 I print_info: n_rot            = 32
0.00.053.242 I print_info: n_swa            = 0
0.00.053.242 I print_info: n_embd_head_k    = 128
0.00.053.242 I print_info: n_embd_head_v    = 128
0.00.053.244 I print_info: n_gqa            = 1
0.00.053.245 I print_info: n_embd_k_gqa     = 2048
0.00.053.246 I print_info: n_embd_v_gqa     = 2048
0.00.053.246 I print_info: f_norm_eps       = 1.0e-05
0.00.053.247 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.248 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.248 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.248 I print_info: f_logit_scale    = 0.0e+00
0.00.053.249 I print_info: n_ff             = 8192
0.00.053.249 I print_info: n_expert         = 0
0.00.053.249 I print_info: n_expert_used    = 0
0.00.053.249 I print_info: causal attn      = 1
0.00.053.249 I print_info: pooling type     = 0
0.00.053.249 I print_info: rope type        = 2
0.00.053.252 I print_info: rope scaling     = linear
0.00.053.252 I print_info: freq_base_train  = 10000.0
0.00.053.252 I print_info: freq_scale_train = 1
0.00.053.252 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.253 I print_info: rope_finetuned   = unknown
0.00.053.253 I print_info: ssm_d_conv       = 0
0.00.053.253 I print_info: ssm_d_inner      = 0
0.00.053.253 I print_info: ssm_d_state      = 0
0.00.053.253 I print_info: ssm_dt_rank      = 0
0.00.053.253 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.253 I print_info: model type       = 1.4B
0.00.053.254 I print_info: model params     = 1.41 B
0.00.053.254 I print_info: general.name     = 1.4B
0.00.053.259 I print_info: vocab type       = BPE
0.00.053.259 I print_info: n_vocab          = 50304
0.00.053.259 I print_info: n_merges         = 50009
0.00.053.260 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.260 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.260 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.260 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.260 I print_info: LF token         = 128 'Ä'
0.00.053.261 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.261 I print_info: max token length = 1024
0.00.055.730 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.730 I load_tensors: offloading output layer to GPU
0.00.055.731 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.738 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.739 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.056.204 I llama_init_from_model: n_seq_max     = 1
0.00.056.205 I llama_init_from_model: n_ctx         = 2048
0.00.056.206 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.206 I llama_init_from_model: n_batch       = 2048
0.00.056.206 I llama_init_from_model: n_ubatch      = 512
0.00.056.206 I llama_init_from_model: flash_attn    = 0
0.00.056.206 I llama_init_from_model: freq_base     = 10000.0
0.00.056.207 I llama_init_from_model: freq_scale    = 1
0.00.056.207 I ggml_metal_init: allocating
0.00.056.211 I ggml_metal_init: found device: Apple M4
0.00.056.213 I ggml_metal_init: picking default device: Apple M4
0.00.056.961 I ggml_metal_init: using embedded metal library
0.00.059.516 I ggml_metal_init: GPU name:   Apple M4
0.00.059.518 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.518 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.519 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.519 I ggml_metal_init: simdgroup reduction   = true
0.00.059.519 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.519 I ggml_metal_init: has bfloat            = true
0.00.059.519 I ggml_metal_init: use bfloat            = true
0.00.059.520 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.520 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.641 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.093.848 I init:      Metal KV buffer size =   384.00 MiB
0.00.093.862 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.892 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.095.117 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.095.119 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.095.119 I llama_init_from_model: graph nodes  = 967
0.00.095.119 I llama_init_from_model: graph splits = 2
0.00.095.127 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.095.257 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.258 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.402 I main: llama threadpool init, n_threads = 4
0.00.670.451 I 
0.00.670.481 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.484 I 
0.00.670.707 I sampler seed: 1234
0.00.670.714 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.670.753 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.670.754 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.670.754 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.345.460 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57676.69 tokens per second)
0.01.345.461 I llama_perf_context_print:        load time =     658.73 ms
0.01.345.462 I llama_perf_context_print: prompt eval time =      43.01 ms /     7 tokens (    6.14 ms per token,   162.77 tokens per second)
0.01.345.462 I llama_perf_context_print:        eval time =     628.74 ms /    63 runs   (    9.98 ms per token,   100.20 tokens per second)
0.01.345.463 I llama_perf_context_print:       total time =     675.06 ms /    70 tokens
0.01.347.866 I ggml_metal_free: deallocating

real	0m1.365s
user	0m0.110s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.885 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.065 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.069 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.071 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.071 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.072 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.072 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.072 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.073 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.074 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.074 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.074 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.075 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.075 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.076 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.077 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.077 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.077 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.820 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.854 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.579 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.580 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.580 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.581 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.581 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.581 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.582 I llama_model_loader: - type  f32:  194 tensors
0.00.026.582 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.582 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.583 I print_info: file format = GGUF V3 (latest)
0.00.026.583 I print_info: file type   = Q4_0
0.00.026.584 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.089 I load: special tokens cache size = 25
0.00.050.935 I load: token to piece cache size = 0.2984 MB
0.00.050.937 I print_info: arch             = gptneox
0.00.050.938 I print_info: vocab_only       = 0
0.00.050.938 I print_info: n_ctx_train      = 2048
0.00.050.938 I print_info: n_embd           = 2048
0.00.050.938 I print_info: n_layer          = 24
0.00.050.941 I print_info: n_head           = 16
0.00.050.942 I print_info: n_head_kv        = 16
0.00.050.942 I print_info: n_rot            = 32
0.00.050.943 I print_info: n_swa            = 0
0.00.050.943 I print_info: n_embd_head_k    = 128
0.00.050.943 I print_info: n_embd_head_v    = 128
0.00.050.944 I print_info: n_gqa            = 1
0.00.050.944 I print_info: n_embd_k_gqa     = 2048
0.00.050.945 I print_info: n_embd_v_gqa     = 2048
0.00.050.946 I print_info: f_norm_eps       = 1.0e-05
0.00.050.946 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.946 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.946 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.947 I print_info: f_logit_scale    = 0.0e+00
0.00.050.947 I print_info: n_ff             = 8192
0.00.050.948 I print_info: n_expert         = 0
0.00.050.950 I print_info: n_expert_used    = 0
0.00.050.951 I print_info: causal attn      = 1
0.00.050.951 I print_info: pooling type     = 0
0.00.050.951 I print_info: rope type        = 2
0.00.050.951 I print_info: rope scaling     = linear
0.00.050.952 I print_info: freq_base_train  = 10000.0
0.00.050.952 I print_info: freq_scale_train = 1
0.00.050.952 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.952 I print_info: rope_finetuned   = unknown
0.00.050.952 I print_info: ssm_d_conv       = 0
0.00.050.953 I print_info: ssm_d_inner      = 0
0.00.050.953 I print_info: ssm_d_state      = 0
0.00.050.953 I print_info: ssm_dt_rank      = 0
0.00.050.953 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.953 I print_info: model type       = 1.4B
0.00.050.954 I print_info: model params     = 1.41 B
0.00.050.954 I print_info: general.name     = 1.4B
0.00.050.954 I print_info: vocab type       = BPE
0.00.050.955 I print_info: n_vocab          = 50304
0.00.050.955 I print_info: n_merges         = 50009
0.00.050.956 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.957 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.957 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.957 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.957 I print_info: LF token         = 128 'Ä'
0.00.050.957 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.958 I print_info: max token length = 1024
0.00.052.845 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.845 I load_tensors: offloading output layer to GPU
0.00.052.845 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.856 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.857 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.142 I llama_init_from_model: n_seq_max     = 1
0.00.053.143 I llama_init_from_model: n_ctx         = 128
0.00.053.143 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.144 I llama_init_from_model: n_batch       = 128
0.00.053.144 I llama_init_from_model: n_ubatch      = 128
0.00.053.144 I llama_init_from_model: flash_attn    = 0
0.00.053.144 I llama_init_from_model: freq_base     = 10000.0
0.00.053.145 I llama_init_from_model: freq_scale    = 1
0.00.053.145 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.145 I ggml_metal_init: allocating
0.00.053.148 I ggml_metal_init: found device: Apple M4
0.00.053.150 I ggml_metal_init: picking default device: Apple M4
0.00.053.701 I ggml_metal_init: using embedded metal library
0.00.056.030 I ggml_metal_init: GPU name:   Apple M4
0.00.056.031 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.031 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.032 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.032 I ggml_metal_init: simdgroup reduction   = true
0.00.056.032 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.032 I ggml_metal_init: has bfloat            = true
0.00.056.032 I ggml_metal_init: use bfloat            = true
0.00.056.033 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.033 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.910 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.171 I init:      Metal KV buffer size =    24.00 MiB
0.00.067.173 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.187 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.045 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.046 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.047 I llama_init_from_model: graph nodes  = 967
0.00.068.047 I llama_init_from_model: graph splits = 2
0.00.068.048 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.048 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.596.805 I 
0.00.596.867 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.596.872 I perplexity: tokenizing the input ..
0.00.604.637 I perplexity: tokenization took 7.764 ms
0.00.604.641 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.727.371 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.728.515 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.728.541 I llama_perf_context_print:        load time =     585.91 ms
0.00.728.542 I llama_perf_context_print: prompt eval time =     122.50 ms /   128 tokens (    0.96 ms per token,  1044.88 tokens per second)
0.00.728.543 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.728.543 I llama_perf_context_print:       total time =     131.74 ms /   129 tokens
0.00.729.080 I ggml_metal_free: deallocating

real	0m0.744s
user	0m0.076s
sys	0m0.087s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.483 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.076 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.080 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.082 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.083 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.083 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.083 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.085 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.087 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.088 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.088 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.090 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.091 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.091 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.092 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.094 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.094 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.095 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.938 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.977 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.751 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.752 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.752 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.752 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.753 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.753 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.754 I llama_model_loader: - type  f32:  194 tensors
0.00.026.754 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.754 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.754 I print_info: file format = GGUF V3 (latest)
0.00.026.755 I print_info: file type   = Q4_1
0.00.026.756 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.387 I load: special tokens cache size = 25
0.00.051.431 I load: token to piece cache size = 0.2984 MB
0.00.051.433 I print_info: arch             = gptneox
0.00.051.434 I print_info: vocab_only       = 0
0.00.051.434 I print_info: n_ctx_train      = 2048
0.00.051.434 I print_info: n_embd           = 2048
0.00.051.434 I print_info: n_layer          = 24
0.00.051.437 I print_info: n_head           = 16
0.00.051.438 I print_info: n_head_kv        = 16
0.00.051.438 I print_info: n_rot            = 32
0.00.051.438 I print_info: n_swa            = 0
0.00.051.438 I print_info: n_embd_head_k    = 128
0.00.051.438 I print_info: n_embd_head_v    = 128
0.00.051.439 I print_info: n_gqa            = 1
0.00.051.440 I print_info: n_embd_k_gqa     = 2048
0.00.051.441 I print_info: n_embd_v_gqa     = 2048
0.00.051.441 I print_info: f_norm_eps       = 1.0e-05
0.00.051.442 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.442 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.442 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.442 I print_info: f_logit_scale    = 0.0e+00
0.00.051.443 I print_info: n_ff             = 8192
0.00.051.443 I print_info: n_expert         = 0
0.00.051.443 I print_info: n_expert_used    = 0
0.00.051.443 I print_info: causal attn      = 1
0.00.051.443 I print_info: pooling type     = 0
0.00.051.445 I print_info: rope type        = 2
0.00.051.447 I print_info: rope scaling     = linear
0.00.051.448 I print_info: freq_base_train  = 10000.0
0.00.051.448 I print_info: freq_scale_train = 1
0.00.051.448 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.448 I print_info: rope_finetuned   = unknown
0.00.051.448 I print_info: ssm_d_conv       = 0
0.00.051.449 I print_info: ssm_d_inner      = 0
0.00.051.449 I print_info: ssm_d_state      = 0
0.00.051.449 I print_info: ssm_dt_rank      = 0
0.00.051.449 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.449 I print_info: model type       = 1.4B
0.00.051.450 I print_info: model params     = 1.41 B
0.00.051.450 I print_info: general.name     = 1.4B
0.00.051.450 I print_info: vocab type       = BPE
0.00.051.451 I print_info: n_vocab          = 50304
0.00.051.451 I print_info: n_merges         = 50009
0.00.051.451 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.452 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.452 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.452 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.453 I print_info: LF token         = 128 'Ä'
0.00.051.453 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.453 I print_info: max token length = 1024
0.00.053.184 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.185 I load_tensors: offloading output layer to GPU
0.00.053.185 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.190 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.191 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.053.459 I llama_init_from_model: n_seq_max     = 1
0.00.053.460 I llama_init_from_model: n_ctx         = 2048
0.00.053.460 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.460 I llama_init_from_model: n_batch       = 2048
0.00.053.460 I llama_init_from_model: n_ubatch      = 512
0.00.053.461 I llama_init_from_model: flash_attn    = 0
0.00.053.461 I llama_init_from_model: freq_base     = 10000.0
0.00.053.461 I llama_init_from_model: freq_scale    = 1
0.00.053.462 I ggml_metal_init: allocating
0.00.053.465 I ggml_metal_init: found device: Apple M4
0.00.053.467 I ggml_metal_init: picking default device: Apple M4
0.00.054.054 I ggml_metal_init: using embedded metal library
0.00.056.378 I ggml_metal_init: GPU name:   Apple M4
0.00.056.379 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.380 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.380 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.380 I ggml_metal_init: simdgroup reduction   = true
0.00.056.381 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.381 I ggml_metal_init: has bfloat            = true
0.00.056.381 I ggml_metal_init: use bfloat            = true
0.00.056.381 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.382 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.008 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.546 I init:      Metal KV buffer size =   384.00 MiB
0.00.085.556 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.579 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.517 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.518 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.519 I llama_init_from_model: graph nodes  = 967
0.00.086.519 I llama_init_from_model: graph splits = 2
0.00.086.521 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.652 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.652 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.733.914 I main: llama threadpool init, n_threads = 4
0.00.733.960 I 
0.00.733.996 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.733.997 I 
0.00.734.551 I sampler seed: 1234
0.00.734.559 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.734.590 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.734.609 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.734.609 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.454.278 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53143.71 tokens per second)
0.01.454.280 I llama_perf_context_print:        load time =     723.43 ms
0.01.454.280 I llama_perf_context_print: prompt eval time =      44.55 ms /     7 tokens (    6.36 ms per token,   157.14 tokens per second)
0.01.454.281 I llama_perf_context_print:        eval time =     672.79 ms /    63 runs   (   10.68 ms per token,    93.64 tokens per second)
0.01.454.282 I llama_perf_context_print:       total time =     720.37 ms /    70 tokens
0.01.457.044 I ggml_metal_free: deallocating

real	0m1.475s
user	0m0.108s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.615 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.709 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.714 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.716 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.718 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.718 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.718 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.719 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.720 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.720 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.723 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.724 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.724 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.724 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.725 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.727 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.728 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.728 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.477 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.526 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.321 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.322 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.323 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.323 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.323 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.324 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.324 I llama_model_loader: - type  f32:  194 tensors
0.00.024.325 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.325 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.325 I print_info: file format = GGUF V3 (latest)
0.00.024.326 I print_info: file type   = Q4_1
0.00.024.332 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.603 I load: special tokens cache size = 25
0.00.049.569 I load: token to piece cache size = 0.2984 MB
0.00.049.572 I print_info: arch             = gptneox
0.00.049.573 I print_info: vocab_only       = 0
0.00.049.573 I print_info: n_ctx_train      = 2048
0.00.049.573 I print_info: n_embd           = 2048
0.00.049.573 I print_info: n_layer          = 24
0.00.049.576 I print_info: n_head           = 16
0.00.049.577 I print_info: n_head_kv        = 16
0.00.049.579 I print_info: n_rot            = 32
0.00.049.579 I print_info: n_swa            = 0
0.00.049.579 I print_info: n_embd_head_k    = 128
0.00.049.579 I print_info: n_embd_head_v    = 128
0.00.049.580 I print_info: n_gqa            = 1
0.00.049.581 I print_info: n_embd_k_gqa     = 2048
0.00.049.582 I print_info: n_embd_v_gqa     = 2048
0.00.049.582 I print_info: f_norm_eps       = 1.0e-05
0.00.049.583 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.583 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.583 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.583 I print_info: f_logit_scale    = 0.0e+00
0.00.049.584 I print_info: n_ff             = 8192
0.00.049.584 I print_info: n_expert         = 0
0.00.049.584 I print_info: n_expert_used    = 0
0.00.049.584 I print_info: causal attn      = 1
0.00.049.586 I print_info: pooling type     = 0
0.00.049.586 I print_info: rope type        = 2
0.00.049.586 I print_info: rope scaling     = linear
0.00.049.588 I print_info: freq_base_train  = 10000.0
0.00.049.588 I print_info: freq_scale_train = 1
0.00.049.592 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.592 I print_info: rope_finetuned   = unknown
0.00.049.593 I print_info: ssm_d_conv       = 0
0.00.049.594 I print_info: ssm_d_inner      = 0
0.00.049.594 I print_info: ssm_d_state      = 0
0.00.049.594 I print_info: ssm_dt_rank      = 0
0.00.049.594 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.594 I print_info: model type       = 1.4B
0.00.049.594 I print_info: model params     = 1.41 B
0.00.049.594 I print_info: general.name     = 1.4B
0.00.049.595 I print_info: vocab type       = BPE
0.00.049.595 I print_info: n_vocab          = 50304
0.00.049.595 I print_info: n_merges         = 50009
0.00.049.596 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.596 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.596 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.600 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.601 I print_info: LF token         = 128 'Ä'
0.00.049.603 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.603 I print_info: max token length = 1024
0.00.051.594 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.594 I load_tensors: offloading output layer to GPU
0.00.051.594 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.605 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.606 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.923 I llama_init_from_model: n_seq_max     = 1
0.00.051.924 I llama_init_from_model: n_ctx         = 128
0.00.051.924 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.924 I llama_init_from_model: n_batch       = 128
0.00.051.924 I llama_init_from_model: n_ubatch      = 128
0.00.051.925 I llama_init_from_model: flash_attn    = 0
0.00.051.925 I llama_init_from_model: freq_base     = 10000.0
0.00.051.925 I llama_init_from_model: freq_scale    = 1
0.00.051.926 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.926 I ggml_metal_init: allocating
0.00.051.929 I ggml_metal_init: found device: Apple M4
0.00.051.931 I ggml_metal_init: picking default device: Apple M4
0.00.052.507 I ggml_metal_init: using embedded metal library
0.00.054.828 I ggml_metal_init: GPU name:   Apple M4
0.00.054.829 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.830 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.830 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.830 I ggml_metal_init: simdgroup reduction   = true
0.00.054.831 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.831 I ggml_metal_init: has bfloat            = true
0.00.054.831 I ggml_metal_init: use bfloat            = true
0.00.054.831 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.832 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.124 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.513 I init:      Metal KV buffer size =    24.00 MiB
0.00.065.518 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.533 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.345 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.346 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.346 I llama_init_from_model: graph nodes  = 967
0.00.066.346 I llama_init_from_model: graph splits = 2
0.00.066.348 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.348 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.392 I 
0.00.679.436 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.679.443 I perplexity: tokenizing the input ..
0.00.687.546 I perplexity: tokenization took 8.101 ms
0.00.687.550 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.810.414 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.811.594 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.811.621 I llama_perf_context_print:        load time =     670.77 ms
0.00.811.622 I llama_perf_context_print: prompt eval time =     122.64 ms /   128 tokens (    0.96 ms per token,  1043.74 tokens per second)
0.00.811.622 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.811.623 I llama_perf_context_print:       total time =     132.23 ms /   129 tokens
0.00.812.375 I ggml_metal_free: deallocating

real	0m0.826s
user	0m0.077s
sys	0m0.115s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.010.654 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.327 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.333 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.340 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.340 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.341 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.341 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.341 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.343 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.344 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.344 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.345 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.346 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.346 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.347 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.348 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.349 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.349 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.169 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.229 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.058 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.059 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.060 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.060 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.060 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.061 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.061 I llama_model_loader: - type  f32:  194 tensors
0.00.027.062 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.062 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.063 I print_info: file format = GGUF V3 (latest)
0.00.027.063 I print_info: file type   = Q5_0
0.00.027.064 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.047.615 I load: special tokens cache size = 25
0.00.053.711 I load: token to piece cache size = 0.2984 MB
0.00.053.718 I print_info: arch             = gptneox
0.00.053.718 I print_info: vocab_only       = 0
0.00.053.718 I print_info: n_ctx_train      = 2048
0.00.053.721 I print_info: n_embd           = 2048
0.00.053.721 I print_info: n_layer          = 24
0.00.053.725 I print_info: n_head           = 16
0.00.053.726 I print_info: n_head_kv        = 16
0.00.053.726 I print_info: n_rot            = 32
0.00.053.726 I print_info: n_swa            = 0
0.00.053.726 I print_info: n_embd_head_k    = 128
0.00.053.726 I print_info: n_embd_head_v    = 128
0.00.053.727 I print_info: n_gqa            = 1
0.00.053.728 I print_info: n_embd_k_gqa     = 2048
0.00.053.728 I print_info: n_embd_v_gqa     = 2048
0.00.053.729 I print_info: f_norm_eps       = 1.0e-05
0.00.053.729 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.729 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.729 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.729 I print_info: f_logit_scale    = 0.0e+00
0.00.053.730 I print_info: n_ff             = 8192
0.00.053.730 I print_info: n_expert         = 0
0.00.053.730 I print_info: n_expert_used    = 0
0.00.053.730 I print_info: causal attn      = 1
0.00.053.731 I print_info: pooling type     = 0
0.00.053.731 I print_info: rope type        = 2
0.00.053.731 I print_info: rope scaling     = linear
0.00.053.731 I print_info: freq_base_train  = 10000.0
0.00.053.732 I print_info: freq_scale_train = 1
0.00.053.732 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.732 I print_info: rope_finetuned   = unknown
0.00.053.732 I print_info: ssm_d_conv       = 0
0.00.053.732 I print_info: ssm_d_inner      = 0
0.00.053.732 I print_info: ssm_d_state      = 0
0.00.053.732 I print_info: ssm_dt_rank      = 0
0.00.053.734 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.735 I print_info: model type       = 1.4B
0.00.053.735 I print_info: model params     = 1.41 B
0.00.053.735 I print_info: general.name     = 1.4B
0.00.053.736 I print_info: vocab type       = BPE
0.00.053.737 I print_info: n_vocab          = 50304
0.00.053.737 I print_info: n_merges         = 50009
0.00.053.737 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.737 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.737 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.737 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.738 I print_info: LF token         = 128 'Ä'
0.00.053.738 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.738 I print_info: max token length = 1024
0.00.055.834 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.834 I load_tensors: offloading output layer to GPU
0.00.055.834 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.845 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.055.847 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.056.151 I llama_init_from_model: n_seq_max     = 1
0.00.056.152 I llama_init_from_model: n_ctx         = 2048
0.00.056.152 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.152 I llama_init_from_model: n_batch       = 2048
0.00.056.152 I llama_init_from_model: n_ubatch      = 512
0.00.056.152 I llama_init_from_model: flash_attn    = 0
0.00.056.153 I llama_init_from_model: freq_base     = 10000.0
0.00.056.153 I llama_init_from_model: freq_scale    = 1
0.00.056.154 I ggml_metal_init: allocating
0.00.056.157 I ggml_metal_init: found device: Apple M4
0.00.056.160 I ggml_metal_init: picking default device: Apple M4
0.00.056.833 I ggml_metal_init: using embedded metal library
0.00.059.276 I ggml_metal_init: GPU name:   Apple M4
0.00.059.278 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.279 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.279 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.279 I ggml_metal_init: simdgroup reduction   = true
0.00.059.279 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.280 I ggml_metal_init: has bfloat            = true
0.00.059.280 I ggml_metal_init: use bfloat            = true
0.00.059.280 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.281 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.684 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.841 I init:      Metal KV buffer size =   384.00 MiB
0.00.088.848 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.868 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.811 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.813 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.814 I llama_init_from_model: graph nodes  = 967
0.00.089.814 I llama_init_from_model: graph splits = 2
0.00.089.819 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.950 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.951 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.785.173 I main: llama threadpool init, n_threads = 4
0.00.785.216 I 
0.00.785.257 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.785.259 I 
0.00.785.485 I sampler seed: 1234
0.00.785.490 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.785.502 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.785.502 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.785.502 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.577.778 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59068.22 tokens per second)
0.01.577.779 I llama_perf_context_print:        load time =     774.51 ms
0.01.577.780 I llama_perf_context_print: prompt eval time =      43.13 ms /     7 tokens (    6.16 ms per token,   162.29 tokens per second)
0.01.577.780 I llama_perf_context_print:        eval time =     746.22 ms /    63 runs   (   11.84 ms per token,    84.43 tokens per second)
0.01.577.781 I llama_perf_context_print:       total time =     792.61 ms /    70 tokens
0.01.580.662 I ggml_metal_free: deallocating

real	0m1.601s
user	0m0.111s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.750 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.628 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.632 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.635 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.636 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.636 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.637 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.637 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.638 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.638 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.639 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.639 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.639 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.640 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.642 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.643 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.644 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.644 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.503 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.490 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.279 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.280 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.280 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.280 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.281 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.281 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.282 I llama_model_loader: - type  f32:  194 tensors
0.00.026.282 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.282 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.282 I print_info: file format = GGUF V3 (latest)
0.00.026.283 I print_info: file type   = Q5_0
0.00.026.284 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.444 I load: special tokens cache size = 25
0.00.051.499 I load: token to piece cache size = 0.2984 MB
0.00.051.502 I print_info: arch             = gptneox
0.00.051.502 I print_info: vocab_only       = 0
0.00.051.502 I print_info: n_ctx_train      = 2048
0.00.051.502 I print_info: n_embd           = 2048
0.00.051.502 I print_info: n_layer          = 24
0.00.051.506 I print_info: n_head           = 16
0.00.051.506 I print_info: n_head_kv        = 16
0.00.051.506 I print_info: n_rot            = 32
0.00.051.507 I print_info: n_swa            = 0
0.00.051.507 I print_info: n_embd_head_k    = 128
0.00.051.507 I print_info: n_embd_head_v    = 128
0.00.051.508 I print_info: n_gqa            = 1
0.00.051.508 I print_info: n_embd_k_gqa     = 2048
0.00.051.511 I print_info: n_embd_v_gqa     = 2048
0.00.051.512 I print_info: f_norm_eps       = 1.0e-05
0.00.051.512 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.512 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.512 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.513 I print_info: f_logit_scale    = 0.0e+00
0.00.051.513 I print_info: n_ff             = 8192
0.00.051.513 I print_info: n_expert         = 0
0.00.051.514 I print_info: n_expert_used    = 0
0.00.051.514 I print_info: causal attn      = 1
0.00.051.514 I print_info: pooling type     = 0
0.00.051.514 I print_info: rope type        = 2
0.00.051.514 I print_info: rope scaling     = linear
0.00.051.515 I print_info: freq_base_train  = 10000.0
0.00.051.515 I print_info: freq_scale_train = 1
0.00.051.515 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.516 I print_info: rope_finetuned   = unknown
0.00.051.516 I print_info: ssm_d_conv       = 0
0.00.051.516 I print_info: ssm_d_inner      = 0
0.00.051.516 I print_info: ssm_d_state      = 0
0.00.051.518 I print_info: ssm_dt_rank      = 0
0.00.051.518 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.518 I print_info: model type       = 1.4B
0.00.051.518 I print_info: model params     = 1.41 B
0.00.051.519 I print_info: general.name     = 1.4B
0.00.051.519 I print_info: vocab type       = BPE
0.00.051.519 I print_info: n_vocab          = 50304
0.00.051.519 I print_info: n_merges         = 50009
0.00.051.520 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.520 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.520 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.520 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.520 I print_info: LF token         = 128 'Ä'
0.00.051.524 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.524 I print_info: max token length = 1024
0.00.053.494 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.495 I load_tensors: offloading output layer to GPU
0.00.053.495 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.505 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.506 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.792 I llama_init_from_model: n_seq_max     = 1
0.00.053.792 I llama_init_from_model: n_ctx         = 128
0.00.053.792 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.793 I llama_init_from_model: n_batch       = 128
0.00.053.793 I llama_init_from_model: n_ubatch      = 128
0.00.053.793 I llama_init_from_model: flash_attn    = 0
0.00.053.793 I llama_init_from_model: freq_base     = 10000.0
0.00.053.793 I llama_init_from_model: freq_scale    = 1
0.00.053.794 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.794 I ggml_metal_init: allocating
0.00.053.797 I ggml_metal_init: found device: Apple M4
0.00.053.799 I ggml_metal_init: picking default device: Apple M4
0.00.054.351 I ggml_metal_init: using embedded metal library
0.00.056.693 I ggml_metal_init: GPU name:   Apple M4
0.00.056.694 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.694 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.695 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.695 I ggml_metal_init: simdgroup reduction   = true
0.00.056.695 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.695 I ggml_metal_init: has bfloat            = true
0.00.056.695 I ggml_metal_init: use bfloat            = true
0.00.056.696 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.697 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.325 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.577 I init:      Metal KV buffer size =    24.00 MiB
0.00.067.582 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.598 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.413 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.414 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.415 I llama_init_from_model: graph nodes  = 967
0.00.068.415 I llama_init_from_model: graph splits = 2
0.00.068.416 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.416 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.708.624 I 
0.00.708.666 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.708.670 I perplexity: tokenizing the input ..
0.00.716.573 I perplexity: tokenization took 7.901 ms
0.00.716.577 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.851.652 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.852.919 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.852.947 I llama_perf_context_print:        load time =     697.87 ms
0.00.852.948 I llama_perf_context_print: prompt eval time =     134.85 ms /   128 tokens (    1.05 ms per token,   949.22 tokens per second)
0.00.852.949 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.852.949 I llama_perf_context_print:       total time =     144.32 ms /   129 tokens
0.00.853.681 I ggml_metal_free: deallocating

real	0m0.869s
user	0m0.077s
sys	0m0.109s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.808 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.181 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.185 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.187 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.188 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.188 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.188 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.189 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.193 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.193 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.194 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.194 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.194 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.199 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.199 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.203 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.204 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.204 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.901 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.939 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.717 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.718 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.718 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.718 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.719 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.719 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.720 I llama_model_loader: - type  f32:  194 tensors
0.00.025.720 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.720 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.721 I print_info: file format = GGUF V3 (latest)
0.00.025.721 I print_info: file type   = Q5_1
0.00.025.722 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.269 I load: special tokens cache size = 25
0.00.050.299 I load: token to piece cache size = 0.2984 MB
0.00.050.301 I print_info: arch             = gptneox
0.00.050.302 I print_info: vocab_only       = 0
0.00.050.302 I print_info: n_ctx_train      = 2048
0.00.050.302 I print_info: n_embd           = 2048
0.00.050.302 I print_info: n_layer          = 24
0.00.050.305 I print_info: n_head           = 16
0.00.050.306 I print_info: n_head_kv        = 16
0.00.050.306 I print_info: n_rot            = 32
0.00.050.308 I print_info: n_swa            = 0
0.00.050.309 I print_info: n_embd_head_k    = 128
0.00.050.309 I print_info: n_embd_head_v    = 128
0.00.050.310 I print_info: n_gqa            = 1
0.00.050.310 I print_info: n_embd_k_gqa     = 2048
0.00.050.311 I print_info: n_embd_v_gqa     = 2048
0.00.050.312 I print_info: f_norm_eps       = 1.0e-05
0.00.050.312 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.312 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.312 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.313 I print_info: f_logit_scale    = 0.0e+00
0.00.050.313 I print_info: n_ff             = 8192
0.00.050.314 I print_info: n_expert         = 0
0.00.050.314 I print_info: n_expert_used    = 0
0.00.050.314 I print_info: causal attn      = 1
0.00.050.314 I print_info: pooling type     = 0
0.00.050.316 I print_info: rope type        = 2
0.00.050.317 I print_info: rope scaling     = linear
0.00.050.317 I print_info: freq_base_train  = 10000.0
0.00.050.318 I print_info: freq_scale_train = 1
0.00.050.318 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.319 I print_info: rope_finetuned   = unknown
0.00.050.319 I print_info: ssm_d_conv       = 0
0.00.050.320 I print_info: ssm_d_inner      = 0
0.00.050.320 I print_info: ssm_d_state      = 0
0.00.050.320 I print_info: ssm_dt_rank      = 0
0.00.050.320 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.320 I print_info: model type       = 1.4B
0.00.050.322 I print_info: model params     = 1.41 B
0.00.050.322 I print_info: general.name     = 1.4B
0.00.050.323 I print_info: vocab type       = BPE
0.00.050.323 I print_info: n_vocab          = 50304
0.00.050.323 I print_info: n_merges         = 50009
0.00.050.323 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.324 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.324 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.324 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.324 I print_info: LF token         = 128 'Ä'
0.00.050.324 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.324 I print_info: max token length = 1024
0.00.052.255 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.255 I load_tensors: offloading output layer to GPU
0.00.052.255 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.266 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.267 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.550 I llama_init_from_model: n_seq_max     = 1
0.00.052.550 I llama_init_from_model: n_ctx         = 2048
0.00.052.551 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.551 I llama_init_from_model: n_batch       = 2048
0.00.052.551 I llama_init_from_model: n_ubatch      = 512
0.00.052.551 I llama_init_from_model: flash_attn    = 0
0.00.052.552 I llama_init_from_model: freq_base     = 10000.0
0.00.052.552 I llama_init_from_model: freq_scale    = 1
0.00.052.552 I ggml_metal_init: allocating
0.00.052.555 I ggml_metal_init: found device: Apple M4
0.00.052.557 I ggml_metal_init: picking default device: Apple M4
0.00.053.147 I ggml_metal_init: using embedded metal library
0.00.055.464 I ggml_metal_init: GPU name:   Apple M4
0.00.055.465 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.465 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.466 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.466 I ggml_metal_init: simdgroup reduction   = true
0.00.055.466 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.466 I ggml_metal_init: has bfloat            = true
0.00.055.466 I ggml_metal_init: use bfloat            = true
0.00.055.467 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.468 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.008 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.673 I init:      Metal KV buffer size =   384.00 MiB
0.00.083.682 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.702 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.800 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.801 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.801 I llama_init_from_model: graph nodes  = 967
0.00.084.802 I llama_init_from_model: graph splits = 2
0.00.084.805 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.951 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.952 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.178 I main: llama threadpool init, n_threads = 4
0.00.703.216 I 
0.00.703.247 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.249 I 
0.00.703.484 I sampler seed: 1234
0.00.703.489 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.703.533 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.703.538 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.703.538 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.545.753 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59613.77 tokens per second)
0.01.545.753 I llama_perf_context_print:        load time =     693.36 ms
0.01.545.754 I llama_perf_context_print: prompt eval time =      42.23 ms /     7 tokens (    6.03 ms per token,   165.76 tokens per second)
0.01.545.755 I llama_perf_context_print:        eval time =     796.99 ms /    63 runs   (   12.65 ms per token,    79.05 tokens per second)
0.01.545.756 I llama_perf_context_print:       total time =     842.58 ms /    70 tokens
0.01.548.940 I ggml_metal_free: deallocating

real	0m1.566s
user	0m0.107s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.898 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.549 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.554 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.560 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.561 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.561 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.561 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.563 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.564 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.564 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.565 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.565 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.565 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.566 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.566 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.568 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.568 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.568 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.294 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.276 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.991 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.993 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.993 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.993 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.994 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.994 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.994 I llama_model_loader: - type  f32:  194 tensors
0.00.023.995 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.995 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.995 I print_info: file format = GGUF V3 (latest)
0.00.023.996 I print_info: file type   = Q5_1
0.00.023.997 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.042.362 I load: special tokens cache size = 25
0.00.048.260 I load: token to piece cache size = 0.2984 MB
0.00.048.263 I print_info: arch             = gptneox
0.00.048.263 I print_info: vocab_only       = 0
0.00.048.264 I print_info: n_ctx_train      = 2048
0.00.048.264 I print_info: n_embd           = 2048
0.00.048.264 I print_info: n_layer          = 24
0.00.048.267 I print_info: n_head           = 16
0.00.048.268 I print_info: n_head_kv        = 16
0.00.048.268 I print_info: n_rot            = 32
0.00.048.268 I print_info: n_swa            = 0
0.00.048.268 I print_info: n_embd_head_k    = 128
0.00.048.269 I print_info: n_embd_head_v    = 128
0.00.048.270 I print_info: n_gqa            = 1
0.00.048.271 I print_info: n_embd_k_gqa     = 2048
0.00.048.272 I print_info: n_embd_v_gqa     = 2048
0.00.048.272 I print_info: f_norm_eps       = 1.0e-05
0.00.048.273 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.273 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.273 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.273 I print_info: f_logit_scale    = 0.0e+00
0.00.048.274 I print_info: n_ff             = 8192
0.00.048.274 I print_info: n_expert         = 0
0.00.048.274 I print_info: n_expert_used    = 0
0.00.048.274 I print_info: causal attn      = 1
0.00.048.274 I print_info: pooling type     = 0
0.00.048.274 I print_info: rope type        = 2
0.00.048.282 I print_info: rope scaling     = linear
0.00.048.283 I print_info: freq_base_train  = 10000.0
0.00.048.284 I print_info: freq_scale_train = 1
0.00.048.284 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.286 I print_info: rope_finetuned   = unknown
0.00.048.286 I print_info: ssm_d_conv       = 0
0.00.048.286 I print_info: ssm_d_inner      = 0
0.00.048.286 I print_info: ssm_d_state      = 0
0.00.048.287 I print_info: ssm_dt_rank      = 0
0.00.048.287 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.287 I print_info: model type       = 1.4B
0.00.048.288 I print_info: model params     = 1.41 B
0.00.048.288 I print_info: general.name     = 1.4B
0.00.048.288 I print_info: vocab type       = BPE
0.00.048.289 I print_info: n_vocab          = 50304
0.00.048.289 I print_info: n_merges         = 50009
0.00.048.289 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.289 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.290 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.290 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.291 I print_info: LF token         = 128 'Ä'
0.00.048.291 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.291 I print_info: max token length = 1024
0.00.050.269 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.270 I load_tensors: offloading output layer to GPU
0.00.050.270 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.280 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.281 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.050.660 I llama_init_from_model: n_seq_max     = 1
0.00.050.661 I llama_init_from_model: n_ctx         = 128
0.00.050.661 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.662 I llama_init_from_model: n_batch       = 128
0.00.050.662 I llama_init_from_model: n_ubatch      = 128
0.00.050.662 I llama_init_from_model: flash_attn    = 0
0.00.050.662 I llama_init_from_model: freq_base     = 10000.0
0.00.050.662 I llama_init_from_model: freq_scale    = 1
0.00.050.663 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.663 I ggml_metal_init: allocating
0.00.050.670 I ggml_metal_init: found device: Apple M4
0.00.050.673 I ggml_metal_init: picking default device: Apple M4
0.00.051.224 I ggml_metal_init: using embedded metal library
0.00.053.617 I ggml_metal_init: GPU name:   Apple M4
0.00.053.618 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.619 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.619 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.619 I ggml_metal_init: simdgroup reduction   = true
0.00.053.619 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.620 I ggml_metal_init: has bfloat            = true
0.00.053.620 I ggml_metal_init: use bfloat            = true
0.00.053.620 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.621 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.091 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.381 I init:      Metal KV buffer size =    24.00 MiB
0.00.064.386 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.403 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.194 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.195 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.195 I llama_init_from_model: graph nodes  = 967
0.00.065.195 I llama_init_from_model: graph splits = 2
0.00.065.196 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.197 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.647.846 I 
0.00.647.885 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.647.888 I perplexity: tokenizing the input ..
0.00.655.783 I perplexity: tokenization took 7.893 ms
0.00.655.786 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.790.761 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.791.917 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.791.941 I llama_perf_context_print:        load time =     638.94 ms
0.00.791.947 I llama_perf_context_print: prompt eval time =     134.75 ms /   128 tokens (    1.05 ms per token,   949.91 tokens per second)
0.00.791.950 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.791.951 I llama_perf_context_print:       total time =     144.10 ms /   129 tokens
0.00.792.654 I ggml_metal_free: deallocating

real	0m0.807s
user	0m0.076s
sys	0m0.115s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.679 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.121 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.126 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.128 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.129 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.129 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.130 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.130 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.131 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.131 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.132 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.132 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.132 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.133 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.133 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.135 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.135 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.135 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.800 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.827 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.557 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.558 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.558 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.559 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.559 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.559 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.560 I llama_model_loader: - type  f32:  194 tensors
0.00.024.560 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.560 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.560 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.561 I print_info: file format = GGUF V3 (latest)
0.00.024.561 I print_info: file type   = Q2_K - Medium
0.00.024.564 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.157 I load: special tokens cache size = 25
0.00.049.196 I load: token to piece cache size = 0.2984 MB
0.00.049.199 I print_info: arch             = gptneox
0.00.049.199 I print_info: vocab_only       = 0
0.00.049.199 I print_info: n_ctx_train      = 2048
0.00.049.199 I print_info: n_embd           = 2048
0.00.049.200 I print_info: n_layer          = 24
0.00.049.202 I print_info: n_head           = 16
0.00.049.203 I print_info: n_head_kv        = 16
0.00.049.203 I print_info: n_rot            = 32
0.00.049.204 I print_info: n_swa            = 0
0.00.049.204 I print_info: n_embd_head_k    = 128
0.00.049.204 I print_info: n_embd_head_v    = 128
0.00.049.205 I print_info: n_gqa            = 1
0.00.049.205 I print_info: n_embd_k_gqa     = 2048
0.00.049.206 I print_info: n_embd_v_gqa     = 2048
0.00.049.207 I print_info: f_norm_eps       = 1.0e-05
0.00.049.207 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.207 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.207 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.207 I print_info: f_logit_scale    = 0.0e+00
0.00.049.208 I print_info: n_ff             = 8192
0.00.049.208 I print_info: n_expert         = 0
0.00.049.209 I print_info: n_expert_used    = 0
0.00.049.209 I print_info: causal attn      = 1
0.00.049.209 I print_info: pooling type     = 0
0.00.049.209 I print_info: rope type        = 2
0.00.049.209 I print_info: rope scaling     = linear
0.00.049.212 I print_info: freq_base_train  = 10000.0
0.00.049.212 I print_info: freq_scale_train = 1
0.00.049.212 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.212 I print_info: rope_finetuned   = unknown
0.00.049.212 I print_info: ssm_d_conv       = 0
0.00.049.213 I print_info: ssm_d_inner      = 0
0.00.049.213 I print_info: ssm_d_state      = 0
0.00.049.213 I print_info: ssm_dt_rank      = 0
0.00.049.213 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.213 I print_info: model type       = 1.4B
0.00.049.214 I print_info: model params     = 1.41 B
0.00.049.214 I print_info: general.name     = 1.4B
0.00.049.215 I print_info: vocab type       = BPE
0.00.049.215 I print_info: n_vocab          = 50304
0.00.049.215 I print_info: n_merges         = 50009
0.00.049.215 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.217 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.217 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.217 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.218 I print_info: LF token         = 128 'Ä'
0.00.049.218 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.218 I print_info: max token length = 1024
0.00.051.053 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.053 I load_tensors: offloading output layer to GPU
0.00.051.053 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.064 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.065 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.342 I llama_init_from_model: n_seq_max     = 1
0.00.051.342 I llama_init_from_model: n_ctx         = 2048
0.00.051.343 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.343 I llama_init_from_model: n_batch       = 2048
0.00.051.343 I llama_init_from_model: n_ubatch      = 512
0.00.051.343 I llama_init_from_model: flash_attn    = 0
0.00.051.344 I llama_init_from_model: freq_base     = 10000.0
0.00.051.344 I llama_init_from_model: freq_scale    = 1
0.00.051.344 I ggml_metal_init: allocating
0.00.051.348 I ggml_metal_init: found device: Apple M4
0.00.051.350 I ggml_metal_init: picking default device: Apple M4
0.00.051.950 I ggml_metal_init: using embedded metal library
0.00.054.257 I ggml_metal_init: GPU name:   Apple M4
0.00.054.259 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.259 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.260 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.260 I ggml_metal_init: simdgroup reduction   = true
0.00.054.260 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.260 I ggml_metal_init: has bfloat            = true
0.00.054.260 I ggml_metal_init: use bfloat            = true
0.00.054.261 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.261 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.002 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.770 I init:      Metal KV buffer size =   384.00 MiB
0.00.082.779 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.802 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.083.858 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.083.860 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.083.860 I llama_init_from_model: graph nodes  = 967
0.00.083.860 I llama_init_from_model: graph splits = 2
0.00.083.863 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.083.994 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.083.994 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.433.863 I main: llama threadpool init, n_threads = 4
0.00.433.904 I 
0.00.433.941 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.433.943 I 
0.00.434.168 I sampler seed: 1234
0.00.434.174 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.434.206 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.434.217 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.434.226 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.117.426 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 60996.56 tokens per second)
0.01.117.427 I llama_perf_context_print:        load time =     424.18 ms
0.01.117.427 I llama_perf_context_print: prompt eval time =      39.69 ms /     7 tokens (    5.67 ms per token,   176.36 tokens per second)
0.01.117.428 I llama_perf_context_print:        eval time =     640.54 ms /    63 runs   (   10.17 ms per token,    98.35 tokens per second)
0.01.117.428 I llama_perf_context_print:       total time =     683.57 ms /    70 tokens
0.01.120.382 I ggml_metal_free: deallocating

real	0m1.137s
user	0m0.107s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.591 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.440 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.445 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.447 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.447 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.448 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.448 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.448 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.450 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.450 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.450 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.451 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.451 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.452 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.452 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.455 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.455 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.455 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.198 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.214 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.982 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.983 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.983 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.984 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.984 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.984 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.985 I llama_model_loader: - type  f32:  194 tensors
0.00.024.985 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.985 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.986 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.986 I print_info: file format = GGUF V3 (latest)
0.00.024.987 I print_info: file type   = Q2_K - Medium
0.00.024.988 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.160 I load: special tokens cache size = 25
0.00.050.142 I load: token to piece cache size = 0.2984 MB
0.00.050.145 I print_info: arch             = gptneox
0.00.050.145 I print_info: vocab_only       = 0
0.00.050.146 I print_info: n_ctx_train      = 2048
0.00.050.146 I print_info: n_embd           = 2048
0.00.050.146 I print_info: n_layer          = 24
0.00.050.149 I print_info: n_head           = 16
0.00.050.150 I print_info: n_head_kv        = 16
0.00.050.150 I print_info: n_rot            = 32
0.00.050.151 I print_info: n_swa            = 0
0.00.050.152 I print_info: n_embd_head_k    = 128
0.00.050.152 I print_info: n_embd_head_v    = 128
0.00.050.153 I print_info: n_gqa            = 1
0.00.050.154 I print_info: n_embd_k_gqa     = 2048
0.00.050.154 I print_info: n_embd_v_gqa     = 2048
0.00.050.155 I print_info: f_norm_eps       = 1.0e-05
0.00.050.155 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.155 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.155 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.156 I print_info: f_logit_scale    = 0.0e+00
0.00.050.158 I print_info: n_ff             = 8192
0.00.050.158 I print_info: n_expert         = 0
0.00.050.158 I print_info: n_expert_used    = 0
0.00.050.158 I print_info: causal attn      = 1
0.00.050.159 I print_info: pooling type     = 0
0.00.050.159 I print_info: rope type        = 2
0.00.050.159 I print_info: rope scaling     = linear
0.00.050.169 I print_info: freq_base_train  = 10000.0
0.00.050.170 I print_info: freq_scale_train = 1
0.00.050.170 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.170 I print_info: rope_finetuned   = unknown
0.00.050.170 I print_info: ssm_d_conv       = 0
0.00.050.170 I print_info: ssm_d_inner      = 0
0.00.050.171 I print_info: ssm_d_state      = 0
0.00.050.172 I print_info: ssm_dt_rank      = 0
0.00.050.172 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.172 I print_info: model type       = 1.4B
0.00.050.173 I print_info: model params     = 1.41 B
0.00.050.173 I print_info: general.name     = 1.4B
0.00.050.173 I print_info: vocab type       = BPE
0.00.050.174 I print_info: n_vocab          = 50304
0.00.050.174 I print_info: n_merges         = 50009
0.00.050.176 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.176 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.176 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.176 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.176 I print_info: LF token         = 128 'Ä'
0.00.050.177 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.177 I print_info: max token length = 1024
0.00.051.999 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.000 I load_tensors: offloading output layer to GPU
0.00.052.000 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.010 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.011 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.365 I llama_init_from_model: n_seq_max     = 1
0.00.052.366 I llama_init_from_model: n_ctx         = 128
0.00.052.367 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.367 I llama_init_from_model: n_batch       = 128
0.00.052.367 I llama_init_from_model: n_ubatch      = 128
0.00.052.367 I llama_init_from_model: flash_attn    = 0
0.00.052.367 I llama_init_from_model: freq_base     = 10000.0
0.00.052.368 I llama_init_from_model: freq_scale    = 1
0.00.052.368 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.368 I ggml_metal_init: allocating
0.00.052.371 I ggml_metal_init: found device: Apple M4
0.00.052.373 I ggml_metal_init: picking default device: Apple M4
0.00.052.947 I ggml_metal_init: using embedded metal library
0.00.055.293 I ggml_metal_init: GPU name:   Apple M4
0.00.055.295 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.295 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.295 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.296 I ggml_metal_init: simdgroup reduction   = true
0.00.055.296 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.296 I ggml_metal_init: has bfloat            = true
0.00.055.296 I ggml_metal_init: use bfloat            = true
0.00.055.296 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.297 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.648 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.976 I init:      Metal KV buffer size =    24.00 MiB
0.00.065.979 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.001 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.859 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.860 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.860 I llama_init_from_model: graph nodes  = 967
0.00.066.860 I llama_init_from_model: graph splits = 2
0.00.066.861 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.862 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.373.644 I 
0.00.373.683 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.373.688 I perplexity: tokenizing the input ..
0.00.381.184 I perplexity: tokenization took 7.495 ms
0.00.381.188 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.512.930 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.514.376 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.514.401 I llama_perf_context_print:        load time =     364.05 ms
0.00.514.402 I llama_perf_context_print: prompt eval time =     131.51 ms /   128 tokens (    1.03 ms per token,   973.29 tokens per second)
0.00.514.402 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.514.403 I llama_perf_context_print:       total time =     140.76 ms /   129 tokens
0.00.514.951 I ggml_metal_free: deallocating

real	0m0.530s
user	0m0.077s
sys	0m0.053s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.734 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.115 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.120 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.122 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.123 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.123 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.123 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.124 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.128 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.128 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.128 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.129 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.129 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.136 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.138 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.141 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.141 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.141 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.922 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.899 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.665 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.666 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.666 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.667 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.667 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.667 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.668 I llama_model_loader: - type  f32:  194 tensors
0.00.025.668 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.668 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.669 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.669 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.669 I print_info: file format = GGUF V3 (latest)
0.00.025.670 I print_info: file type   = Q3_K - Medium
0.00.025.671 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.187 I load: special tokens cache size = 25
0.00.051.403 I load: token to piece cache size = 0.2984 MB
0.00.051.406 I print_info: arch             = gptneox
0.00.051.406 I print_info: vocab_only       = 0
0.00.051.407 I print_info: n_ctx_train      = 2048
0.00.051.407 I print_info: n_embd           = 2048
0.00.051.407 I print_info: n_layer          = 24
0.00.051.410 I print_info: n_head           = 16
0.00.051.411 I print_info: n_head_kv        = 16
0.00.051.411 I print_info: n_rot            = 32
0.00.051.411 I print_info: n_swa            = 0
0.00.051.412 I print_info: n_embd_head_k    = 128
0.00.051.412 I print_info: n_embd_head_v    = 128
0.00.051.412 I print_info: n_gqa            = 1
0.00.051.413 I print_info: n_embd_k_gqa     = 2048
0.00.051.416 I print_info: n_embd_v_gqa     = 2048
0.00.051.416 I print_info: f_norm_eps       = 1.0e-05
0.00.051.416 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.417 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.417 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.417 I print_info: f_logit_scale    = 0.0e+00
0.00.051.418 I print_info: n_ff             = 8192
0.00.051.418 I print_info: n_expert         = 0
0.00.051.418 I print_info: n_expert_used    = 0
0.00.051.421 I print_info: causal attn      = 1
0.00.051.423 I print_info: pooling type     = 0
0.00.051.423 I print_info: rope type        = 2
0.00.051.423 I print_info: rope scaling     = linear
0.00.051.423 I print_info: freq_base_train  = 10000.0
0.00.051.424 I print_info: freq_scale_train = 1
0.00.051.424 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.424 I print_info: rope_finetuned   = unknown
0.00.051.424 I print_info: ssm_d_conv       = 0
0.00.051.424 I print_info: ssm_d_inner      = 0
0.00.051.424 I print_info: ssm_d_state      = 0
0.00.051.425 I print_info: ssm_dt_rank      = 0
0.00.051.425 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.425 I print_info: model type       = 1.4B
0.00.051.425 I print_info: model params     = 1.41 B
0.00.051.426 I print_info: general.name     = 1.4B
0.00.051.426 I print_info: vocab type       = BPE
0.00.051.426 I print_info: n_vocab          = 50304
0.00.051.427 I print_info: n_merges         = 50009
0.00.051.427 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.428 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.428 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.429 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.429 I print_info: LF token         = 128 'Ä'
0.00.051.429 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.429 I print_info: max token length = 1024
0.00.053.357 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.357 I load_tensors: offloading output layer to GPU
0.00.053.357 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.368 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.369 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.053.672 I llama_init_from_model: n_seq_max     = 1
0.00.053.673 I llama_init_from_model: n_ctx         = 2048
0.00.053.673 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.673 I llama_init_from_model: n_batch       = 2048
0.00.053.673 I llama_init_from_model: n_ubatch      = 512
0.00.053.674 I llama_init_from_model: flash_attn    = 0
0.00.053.674 I llama_init_from_model: freq_base     = 10000.0
0.00.053.674 I llama_init_from_model: freq_scale    = 1
0.00.053.675 I ggml_metal_init: allocating
0.00.053.678 I ggml_metal_init: found device: Apple M4
0.00.053.679 I ggml_metal_init: picking default device: Apple M4
0.00.054.275 I ggml_metal_init: using embedded metal library
0.00.056.801 I ggml_metal_init: GPU name:   Apple M4
0.00.056.803 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.803 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.803 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.804 I ggml_metal_init: simdgroup reduction   = true
0.00.056.804 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.804 I ggml_metal_init: has bfloat            = true
0.00.056.804 I ggml_metal_init: use bfloat            = true
0.00.056.805 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.805 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.591 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.672 I init:      Metal KV buffer size =   384.00 MiB
0.00.086.680 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.700 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.645 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.647 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.647 I llama_init_from_model: graph nodes  = 967
0.00.087.647 I llama_init_from_model: graph splits = 2
0.00.087.650 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.769 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.770 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.557.769 I main: llama threadpool init, n_threads = 4
0.00.557.844 I 
0.00.557.872 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.557.873 I 
0.00.558.113 I sampler seed: 1234
0.00.558.117 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.558.153 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.558.165 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.558.165 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.307.141 I llama_perf_sampler_print:    sampling time =       1.12 ms /    71 runs   (    0.02 ms per token, 63279.86 tokens per second)
0.01.307.141 I llama_perf_context_print:        load time =     548.03 ms
0.01.307.146 I llama_perf_context_print: prompt eval time =      43.73 ms /     7 tokens (    6.25 ms per token,   160.09 tokens per second)
0.01.307.147 I llama_perf_context_print:        eval time =     702.40 ms /    63 runs   (   11.15 ms per token,    89.69 tokens per second)
0.01.307.147 I llama_perf_context_print:       total time =     749.38 ms /    70 tokens
0.01.310.095 I ggml_metal_free: deallocating

real	0m1.326s
user	0m0.108s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.766 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.977 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.982 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.984 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.984 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.985 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.985 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.985 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.986 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.986 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.986 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.987 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.987 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.987 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.988 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.989 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.989 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.990 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.753 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.785 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.689 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.691 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.691 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.692 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.692 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.693 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.693 I llama_model_loader: - type  f32:  194 tensors
0.00.024.693 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.694 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.694 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.694 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.695 I print_info: file format = GGUF V3 (latest)
0.00.024.695 I print_info: file type   = Q3_K - Medium
0.00.024.696 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.188 I load: special tokens cache size = 25
0.00.051.089 I load: token to piece cache size = 0.2984 MB
0.00.051.094 I print_info: arch             = gptneox
0.00.051.094 I print_info: vocab_only       = 0
0.00.051.094 I print_info: n_ctx_train      = 2048
0.00.051.094 I print_info: n_embd           = 2048
0.00.051.094 I print_info: n_layer          = 24
0.00.051.099 I print_info: n_head           = 16
0.00.051.100 I print_info: n_head_kv        = 16
0.00.051.100 I print_info: n_rot            = 32
0.00.051.100 I print_info: n_swa            = 0
0.00.051.100 I print_info: n_embd_head_k    = 128
0.00.051.101 I print_info: n_embd_head_v    = 128
0.00.051.103 I print_info: n_gqa            = 1
0.00.051.104 I print_info: n_embd_k_gqa     = 2048
0.00.051.104 I print_info: n_embd_v_gqa     = 2048
0.00.051.105 I print_info: f_norm_eps       = 1.0e-05
0.00.051.105 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.105 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.105 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.107 I print_info: f_logit_scale    = 0.0e+00
0.00.051.108 I print_info: n_ff             = 8192
0.00.051.108 I print_info: n_expert         = 0
0.00.051.108 I print_info: n_expert_used    = 0
0.00.051.108 I print_info: causal attn      = 1
0.00.051.109 I print_info: pooling type     = 0
0.00.051.109 I print_info: rope type        = 2
0.00.051.109 I print_info: rope scaling     = linear
0.00.051.111 I print_info: freq_base_train  = 10000.0
0.00.051.111 I print_info: freq_scale_train = 1
0.00.051.112 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.112 I print_info: rope_finetuned   = unknown
0.00.051.112 I print_info: ssm_d_conv       = 0
0.00.051.112 I print_info: ssm_d_inner      = 0
0.00.051.112 I print_info: ssm_d_state      = 0
0.00.051.112 I print_info: ssm_dt_rank      = 0
0.00.051.112 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.113 I print_info: model type       = 1.4B
0.00.051.113 I print_info: model params     = 1.41 B
0.00.051.113 I print_info: general.name     = 1.4B
0.00.051.114 I print_info: vocab type       = BPE
0.00.051.114 I print_info: n_vocab          = 50304
0.00.051.114 I print_info: n_merges         = 50009
0.00.051.114 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.115 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.115 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.115 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.115 I print_info: LF token         = 128 'Ä'
0.00.051.119 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.120 I print_info: max token length = 1024
0.00.053.011 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.011 I load_tensors: offloading output layer to GPU
0.00.053.011 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.022 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.024 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.053.327 I llama_init_from_model: n_seq_max     = 1
0.00.053.328 I llama_init_from_model: n_ctx         = 128
0.00.053.328 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.328 I llama_init_from_model: n_batch       = 128
0.00.053.329 I llama_init_from_model: n_ubatch      = 128
0.00.053.329 I llama_init_from_model: flash_attn    = 0
0.00.053.329 I llama_init_from_model: freq_base     = 10000.0
0.00.053.329 I llama_init_from_model: freq_scale    = 1
0.00.053.330 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.330 I ggml_metal_init: allocating
0.00.053.334 I ggml_metal_init: found device: Apple M4
0.00.053.336 I ggml_metal_init: picking default device: Apple M4
0.00.053.976 I ggml_metal_init: using embedded metal library
0.00.056.343 I ggml_metal_init: GPU name:   Apple M4
0.00.056.344 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.345 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.345 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.345 I ggml_metal_init: simdgroup reduction   = true
0.00.056.345 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.346 I ggml_metal_init: has bfloat            = true
0.00.056.346 I ggml_metal_init: use bfloat            = true
0.00.056.346 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.347 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.551 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.828 I init:      Metal KV buffer size =    24.00 MiB
0.00.066.834 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.850 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.688 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.689 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.689 I llama_init_from_model: graph nodes  = 967
0.00.067.689 I llama_init_from_model: graph splits = 2
0.00.067.691 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.691 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.472.216 I 
0.00.472.303 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.472.314 I perplexity: tokenizing the input ..
0.00.479.969 I perplexity: tokenization took 7.653 ms
0.00.479.973 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.611.165 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.612.630 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.612.687 I llama_perf_context_print:        load time =     463.44 ms
0.00.612.688 I llama_perf_context_print: prompt eval time =     130.95 ms /   128 tokens (    1.02 ms per token,   977.44 tokens per second)
0.00.612.689 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.612.689 I llama_perf_context_print:       total time =     140.48 ms /   129 tokens
0.00.613.222 I ggml_metal_free: deallocating

real	0m0.627s
user	0m0.078s
sys	0m0.081s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.787 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.297 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.303 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.305 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.307 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.307 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.308 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.312 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.315 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.316 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.316 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.317 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.320 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.321 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.321 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.325 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.325 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.325 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.134 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.150 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.901 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.902 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.903 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.903 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.903 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.904 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.904 I llama_model_loader: - type  f32:  194 tensors
0.00.025.905 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.905 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.905 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.906 I print_info: file format = GGUF V3 (latest)
0.00.025.906 I print_info: file type   = Q4_K - Medium
0.00.025.907 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.297 I load: special tokens cache size = 25
0.00.051.307 I load: token to piece cache size = 0.2984 MB
0.00.051.310 I print_info: arch             = gptneox
0.00.051.311 I print_info: vocab_only       = 0
0.00.051.311 I print_info: n_ctx_train      = 2048
0.00.051.311 I print_info: n_embd           = 2048
0.00.051.311 I print_info: n_layer          = 24
0.00.051.314 I print_info: n_head           = 16
0.00.051.315 I print_info: n_head_kv        = 16
0.00.051.315 I print_info: n_rot            = 32
0.00.051.315 I print_info: n_swa            = 0
0.00.051.316 I print_info: n_embd_head_k    = 128
0.00.051.316 I print_info: n_embd_head_v    = 128
0.00.051.316 I print_info: n_gqa            = 1
0.00.051.317 I print_info: n_embd_k_gqa     = 2048
0.00.051.318 I print_info: n_embd_v_gqa     = 2048
0.00.051.318 I print_info: f_norm_eps       = 1.0e-05
0.00.051.319 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.323 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.323 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.323 I print_info: f_logit_scale    = 0.0e+00
0.00.051.324 I print_info: n_ff             = 8192
0.00.051.324 I print_info: n_expert         = 0
0.00.051.326 I print_info: n_expert_used    = 0
0.00.051.326 I print_info: causal attn      = 1
0.00.051.327 I print_info: pooling type     = 0
0.00.051.328 I print_info: rope type        = 2
0.00.051.328 I print_info: rope scaling     = linear
0.00.051.328 I print_info: freq_base_train  = 10000.0
0.00.051.329 I print_info: freq_scale_train = 1
0.00.051.329 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.332 I print_info: rope_finetuned   = unknown
0.00.051.334 I print_info: ssm_d_conv       = 0
0.00.051.334 I print_info: ssm_d_inner      = 0
0.00.051.334 I print_info: ssm_d_state      = 0
0.00.051.334 I print_info: ssm_dt_rank      = 0
0.00.051.334 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.334 I print_info: model type       = 1.4B
0.00.051.334 I print_info: model params     = 1.41 B
0.00.051.335 I print_info: general.name     = 1.4B
0.00.051.335 I print_info: vocab type       = BPE
0.00.051.335 I print_info: n_vocab          = 50304
0.00.051.335 I print_info: n_merges         = 50009
0.00.051.336 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.336 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.336 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.336 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.336 I print_info: LF token         = 128 'Ä'
0.00.051.337 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.337 I print_info: max token length = 1024
0.00.053.314 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.315 I load_tensors: offloading output layer to GPU
0.00.053.315 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.325 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.326 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.613 I llama_init_from_model: n_seq_max     = 1
0.00.053.614 I llama_init_from_model: n_ctx         = 2048
0.00.053.614 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.614 I llama_init_from_model: n_batch       = 2048
0.00.053.614 I llama_init_from_model: n_ubatch      = 512
0.00.053.614 I llama_init_from_model: flash_attn    = 0
0.00.053.615 I llama_init_from_model: freq_base     = 10000.0
0.00.053.615 I llama_init_from_model: freq_scale    = 1
0.00.053.615 I ggml_metal_init: allocating
0.00.053.618 I ggml_metal_init: found device: Apple M4
0.00.053.620 I ggml_metal_init: picking default device: Apple M4
0.00.054.227 I ggml_metal_init: using embedded metal library
0.00.056.557 I ggml_metal_init: GPU name:   Apple M4
0.00.056.558 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.559 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.559 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.559 I ggml_metal_init: simdgroup reduction   = true
0.00.056.559 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.559 I ggml_metal_init: has bfloat            = true
0.00.056.560 I ggml_metal_init: use bfloat            = true
0.00.056.560 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.561 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.339 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.922 I init:      Metal KV buffer size =   384.00 MiB
0.00.085.929 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.950 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.933 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.934 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.934 I llama_init_from_model: graph nodes  = 967
0.00.086.935 I llama_init_from_model: graph splits = 2
0.00.086.937 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.072 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.072 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.934 I main: llama threadpool init, n_threads = 4
0.00.657.973 I 
0.00.658.003 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.004 I 
0.00.658.162 I sampler seed: 1234
0.00.658.167 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.658.176 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.658.177 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.658.177 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.420.650 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52709.73 tokens per second)
0.01.420.651 I llama_perf_context_print:        load time =     648.14 ms
0.01.420.652 I llama_perf_context_print: prompt eval time =      46.98 ms /     7 tokens (    6.71 ms per token,   148.99 tokens per second)
0.01.420.652 I llama_perf_context_print:        eval time =     712.69 ms /    63 runs   (   11.31 ms per token,    88.40 tokens per second)
0.01.420.653 I llama_perf_context_print:       total time =     762.72 ms /    70 tokens
0.01.423.904 I ggml_metal_free: deallocating

real	0m1.442s
user	0m0.110s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.810 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.132 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.019.138 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.139 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.140 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.146 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.149 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.150 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.151 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.151 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.152 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.152 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.152 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.153 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.158 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.159 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.160 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.160 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.993 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.105 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.989 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.992 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.993 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.993 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.993 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.994 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.994 I llama_model_loader: - type  f32:  194 tensors
0.00.027.995 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.996 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.996 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.997 I print_info: file format = GGUF V3 (latest)
0.00.027.998 I print_info: file type   = Q4_K - Medium
0.00.027.998 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.047.846 I load: special tokens cache size = 25
0.00.053.862 I load: token to piece cache size = 0.2984 MB
0.00.053.868 I print_info: arch             = gptneox
0.00.053.868 I print_info: vocab_only       = 0
0.00.053.869 I print_info: n_ctx_train      = 2048
0.00.053.869 I print_info: n_embd           = 2048
0.00.053.869 I print_info: n_layer          = 24
0.00.053.874 I print_info: n_head           = 16
0.00.053.875 I print_info: n_head_kv        = 16
0.00.053.875 I print_info: n_rot            = 32
0.00.053.875 I print_info: n_swa            = 0
0.00.053.875 I print_info: n_embd_head_k    = 128
0.00.053.876 I print_info: n_embd_head_v    = 128
0.00.053.876 I print_info: n_gqa            = 1
0.00.053.877 I print_info: n_embd_k_gqa     = 2048
0.00.053.878 I print_info: n_embd_v_gqa     = 2048
0.00.053.878 I print_info: f_norm_eps       = 1.0e-05
0.00.053.878 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.879 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.879 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.879 I print_info: f_logit_scale    = 0.0e+00
0.00.053.880 I print_info: n_ff             = 8192
0.00.053.881 I print_info: n_expert         = 0
0.00.053.881 I print_info: n_expert_used    = 0
0.00.053.881 I print_info: causal attn      = 1
0.00.053.881 I print_info: pooling type     = 0
0.00.053.881 I print_info: rope type        = 2
0.00.053.881 I print_info: rope scaling     = linear
0.00.053.882 I print_info: freq_base_train  = 10000.0
0.00.053.882 I print_info: freq_scale_train = 1
0.00.053.882 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.882 I print_info: rope_finetuned   = unknown
0.00.053.882 I print_info: ssm_d_conv       = 0
0.00.053.884 I print_info: ssm_d_inner      = 0
0.00.053.884 I print_info: ssm_d_state      = 0
0.00.053.884 I print_info: ssm_dt_rank      = 0
0.00.053.885 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.885 I print_info: model type       = 1.4B
0.00.053.885 I print_info: model params     = 1.41 B
0.00.053.885 I print_info: general.name     = 1.4B
0.00.053.886 I print_info: vocab type       = BPE
0.00.053.886 I print_info: n_vocab          = 50304
0.00.053.886 I print_info: n_merges         = 50009
0.00.053.886 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.886 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.887 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.887 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.887 I print_info: LF token         = 128 'Ä'
0.00.053.887 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.887 I print_info: max token length = 1024
0.00.055.873 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.873 I load_tensors: offloading output layer to GPU
0.00.055.874 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.885 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.055.886 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.056.172 I llama_init_from_model: n_seq_max     = 1
0.00.056.173 I llama_init_from_model: n_ctx         = 128
0.00.056.173 I llama_init_from_model: n_ctx_per_seq = 128
0.00.056.173 I llama_init_from_model: n_batch       = 128
0.00.056.173 I llama_init_from_model: n_ubatch      = 128
0.00.056.173 I llama_init_from_model: flash_attn    = 0
0.00.056.174 I llama_init_from_model: freq_base     = 10000.0
0.00.056.174 I llama_init_from_model: freq_scale    = 1
0.00.056.175 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.175 I ggml_metal_init: allocating
0.00.056.178 I ggml_metal_init: found device: Apple M4
0.00.056.180 I ggml_metal_init: picking default device: Apple M4
0.00.056.867 I ggml_metal_init: using embedded metal library
0.00.059.293 I ggml_metal_init: GPU name:   Apple M4
0.00.059.295 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.295 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.295 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.296 I ggml_metal_init: simdgroup reduction   = true
0.00.059.296 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.296 I ggml_metal_init: has bfloat            = true
0.00.059.296 I ggml_metal_init: use bfloat            = true
0.00.059.297 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.298 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.484 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.070.794 I init:      Metal KV buffer size =    24.00 MiB
0.00.070.796 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.814 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.071.716 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.071.717 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.071.718 I llama_init_from_model: graph nodes  = 967
0.00.071.718 I llama_init_from_model: graph splits = 2
0.00.071.719 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.719 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.588.710 I 
0.00.588.746 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.588.750 I perplexity: tokenizing the input ..
0.00.596.407 I perplexity: tokenization took 7.655 ms
0.00.596.411 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.729.930 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.731.130 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.731.150 I llama_perf_context_print:        load time =     576.89 ms
0.00.731.151 I llama_perf_context_print: prompt eval time =     133.27 ms /   128 tokens (    1.04 ms per token,   960.44 tokens per second)
0.00.731.152 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.731.152 I llama_perf_context_print:       total time =     142.44 ms /   129 tokens
0.00.731.702 I ggml_metal_free: deallocating

real	0m0.748s
user	0m0.079s
sys	0m0.097s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.306 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.788 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.795 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.796 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.797 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.799 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.800 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.800 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.801 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.801 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.802 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.802 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.802 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.803 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.803 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.806 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.806 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.807 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.415 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.484 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.363 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.364 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.365 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.365 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.365 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.366 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.367 I llama_model_loader: - type  f32:  194 tensors
0.00.025.367 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.367 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.368 I print_info: file format = GGUF V3 (latest)
0.00.025.368 I print_info: file type   = Q5_K - Medium
0.00.025.370 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.987 I load: special tokens cache size = 25
0.00.051.030 I load: token to piece cache size = 0.2984 MB
0.00.051.034 I print_info: arch             = gptneox
0.00.051.034 I print_info: vocab_only       = 0
0.00.051.035 I print_info: n_ctx_train      = 2048
0.00.051.035 I print_info: n_embd           = 2048
0.00.051.035 I print_info: n_layer          = 24
0.00.051.039 I print_info: n_head           = 16
0.00.051.040 I print_info: n_head_kv        = 16
0.00.051.040 I print_info: n_rot            = 32
0.00.051.040 I print_info: n_swa            = 0
0.00.051.040 I print_info: n_embd_head_k    = 128
0.00.051.041 I print_info: n_embd_head_v    = 128
0.00.051.041 I print_info: n_gqa            = 1
0.00.051.042 I print_info: n_embd_k_gqa     = 2048
0.00.051.043 I print_info: n_embd_v_gqa     = 2048
0.00.051.043 I print_info: f_norm_eps       = 1.0e-05
0.00.051.045 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.045 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.047 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.047 I print_info: f_logit_scale    = 0.0e+00
0.00.051.050 I print_info: n_ff             = 8192
0.00.051.050 I print_info: n_expert         = 0
0.00.051.050 I print_info: n_expert_used    = 0
0.00.051.050 I print_info: causal attn      = 1
0.00.051.050 I print_info: pooling type     = 0
0.00.051.050 I print_info: rope type        = 2
0.00.051.051 I print_info: rope scaling     = linear
0.00.051.051 I print_info: freq_base_train  = 10000.0
0.00.051.051 I print_info: freq_scale_train = 1
0.00.051.051 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.052 I print_info: rope_finetuned   = unknown
0.00.051.052 I print_info: ssm_d_conv       = 0
0.00.051.052 I print_info: ssm_d_inner      = 0
0.00.051.053 I print_info: ssm_d_state      = 0
0.00.051.053 I print_info: ssm_dt_rank      = 0
0.00.051.053 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.054 I print_info: model type       = 1.4B
0.00.051.054 I print_info: model params     = 1.41 B
0.00.051.055 I print_info: general.name     = 1.4B
0.00.051.055 I print_info: vocab type       = BPE
0.00.051.056 I print_info: n_vocab          = 50304
0.00.051.056 I print_info: n_merges         = 50009
0.00.051.056 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.056 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.056 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.057 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.057 I print_info: LF token         = 128 'Ä'
0.00.051.057 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.057 I print_info: max token length = 1024
0.00.053.037 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.038 I load_tensors: offloading output layer to GPU
0.00.053.038 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.049 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.050 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.344 I llama_init_from_model: n_seq_max     = 1
0.00.053.344 I llama_init_from_model: n_ctx         = 2048
0.00.053.345 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.345 I llama_init_from_model: n_batch       = 2048
0.00.053.345 I llama_init_from_model: n_ubatch      = 512
0.00.053.345 I llama_init_from_model: flash_attn    = 0
0.00.053.346 I llama_init_from_model: freq_base     = 10000.0
0.00.053.346 I llama_init_from_model: freq_scale    = 1
0.00.053.346 I ggml_metal_init: allocating
0.00.053.350 I ggml_metal_init: found device: Apple M4
0.00.053.352 I ggml_metal_init: picking default device: Apple M4
0.00.054.007 I ggml_metal_init: using embedded metal library
0.00.056.394 I ggml_metal_init: GPU name:   Apple M4
0.00.056.396 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.396 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.397 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.397 I ggml_metal_init: simdgroup reduction   = true
0.00.056.397 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.398 I ggml_metal_init: has bfloat            = true
0.00.056.398 I ggml_metal_init: use bfloat            = true
0.00.056.398 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.399 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.954 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.360 I init:      Metal KV buffer size =   384.00 MiB
0.00.086.367 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.387 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.334 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.336 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.336 I llama_init_from_model: graph nodes  = 967
0.00.087.337 I llama_init_from_model: graph splits = 2
0.00.087.339 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.464 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.464 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.998 I main: llama threadpool init, n_threads = 4
0.00.693.035 I 
0.00.693.067 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.693.068 I 
0.00.693.331 I sampler seed: 1234
0.00.693.335 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.693.346 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.693.347 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.693.347 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.541.341 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57911.91 tokens per second)
0.01.541.341 I llama_perf_context_print:        load time =     683.69 ms
0.01.541.342 I llama_perf_context_print: prompt eval time =      55.29 ms /     7 tokens (    7.90 ms per token,   126.60 tokens per second)
0.01.541.343 I llama_perf_context_print:        eval time =     789.59 ms /    63 runs   (   12.53 ms per token,    79.79 tokens per second)
0.01.541.343 I llama_perf_context_print:       total time =     848.35 ms /    70 tokens
0.01.544.274 I ggml_metal_free: deallocating

real	0m1.563s
user	0m0.109s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.753 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.848 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.853 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.858 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.859 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.861 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.861 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.862 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.863 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.863 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.863 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.867 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.867 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.867 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.868 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.869 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.871 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.871 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.625 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.635 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.391 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.392 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.392 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.393 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.393 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.393 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.394 I llama_model_loader: - type  f32:  194 tensors
0.00.024.394 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.394 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.395 I print_info: file format = GGUF V3 (latest)
0.00.024.395 I print_info: file type   = Q5_K - Medium
0.00.024.396 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.042.882 I load: special tokens cache size = 25
0.00.048.734 I load: token to piece cache size = 0.2984 MB
0.00.048.737 I print_info: arch             = gptneox
0.00.048.738 I print_info: vocab_only       = 0
0.00.048.738 I print_info: n_ctx_train      = 2048
0.00.048.738 I print_info: n_embd           = 2048
0.00.048.738 I print_info: n_layer          = 24
0.00.048.741 I print_info: n_head           = 16
0.00.048.742 I print_info: n_head_kv        = 16
0.00.048.742 I print_info: n_rot            = 32
0.00.048.742 I print_info: n_swa            = 0
0.00.048.742 I print_info: n_embd_head_k    = 128
0.00.048.743 I print_info: n_embd_head_v    = 128
0.00.048.743 I print_info: n_gqa            = 1
0.00.048.744 I print_info: n_embd_k_gqa     = 2048
0.00.048.745 I print_info: n_embd_v_gqa     = 2048
0.00.048.745 I print_info: f_norm_eps       = 1.0e-05
0.00.048.746 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.748 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.748 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.748 I print_info: f_logit_scale    = 0.0e+00
0.00.048.749 I print_info: n_ff             = 8192
0.00.048.749 I print_info: n_expert         = 0
0.00.048.749 I print_info: n_expert_used    = 0
0.00.048.749 I print_info: causal attn      = 1
0.00.048.749 I print_info: pooling type     = 0
0.00.048.750 I print_info: rope type        = 2
0.00.048.750 I print_info: rope scaling     = linear
0.00.048.750 I print_info: freq_base_train  = 10000.0
0.00.048.751 I print_info: freq_scale_train = 1
0.00.048.751 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.751 I print_info: rope_finetuned   = unknown
0.00.048.751 I print_info: ssm_d_conv       = 0
0.00.048.751 I print_info: ssm_d_inner      = 0
0.00.048.751 I print_info: ssm_d_state      = 0
0.00.048.752 I print_info: ssm_dt_rank      = 0
0.00.048.752 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.752 I print_info: model type       = 1.4B
0.00.048.752 I print_info: model params     = 1.41 B
0.00.048.753 I print_info: general.name     = 1.4B
0.00.048.753 I print_info: vocab type       = BPE
0.00.048.753 I print_info: n_vocab          = 50304
0.00.048.753 I print_info: n_merges         = 50009
0.00.048.754 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.755 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.755 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.756 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.756 I print_info: LF token         = 128 'Ä'
0.00.048.756 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.756 I print_info: max token length = 1024
0.00.050.723 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.723 I load_tensors: offloading output layer to GPU
0.00.050.724 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.734 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.050.735 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.036 I llama_init_from_model: n_seq_max     = 1
0.00.051.036 I llama_init_from_model: n_ctx         = 128
0.00.051.037 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.037 I llama_init_from_model: n_batch       = 128
0.00.051.037 I llama_init_from_model: n_ubatch      = 128
0.00.051.037 I llama_init_from_model: flash_attn    = 0
0.00.051.037 I llama_init_from_model: freq_base     = 10000.0
0.00.051.038 I llama_init_from_model: freq_scale    = 1
0.00.051.038 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.039 I ggml_metal_init: allocating
0.00.051.042 I ggml_metal_init: found device: Apple M4
0.00.051.044 I ggml_metal_init: picking default device: Apple M4
0.00.051.611 I ggml_metal_init: using embedded metal library
0.00.054.007 I ggml_metal_init: GPU name:   Apple M4
0.00.054.009 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.009 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.009 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.010 I ggml_metal_init: simdgroup reduction   = true
0.00.054.010 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.010 I ggml_metal_init: has bfloat            = true
0.00.054.010 I ggml_metal_init: use bfloat            = true
0.00.054.011 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.013 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.211 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.467 I init:      Metal KV buffer size =    24.00 MiB
0.00.065.472 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.487 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.401 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.402 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.403 I llama_init_from_model: graph nodes  = 967
0.00.066.403 I llama_init_from_model: graph splits = 2
0.00.066.404 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.405 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.196 I 
0.00.615.235 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.615.239 I perplexity: tokenizing the input ..
0.00.623.083 I perplexity: tokenization took 7.842 ms
0.00.623.091 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.763.689 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.764.862 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.764.897 I llama_perf_context_print:        load time =     606.44 ms
0.00.764.898 I llama_perf_context_print: prompt eval time =     140.37 ms /   128 tokens (    1.10 ms per token,   911.88 tokens per second)
0.00.764.898 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.764.899 I llama_perf_context_print:       total time =     149.70 ms /   129 tokens
0.00.765.600 I ggml_metal_free: deallocating

real	0m0.780s
user	0m0.077s
sys	0m0.102s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.649 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.864 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.868 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.872 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.873 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.873 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.873 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.874 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.875 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.875 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.876 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.876 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.876 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.877 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.877 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.878 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.879 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.879 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.662 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.707 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.454 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.455 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.456 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.456 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.456 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.456 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.457 I llama_model_loader: - type  f32:  194 tensors
0.00.025.457 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.458 I print_info: file format = GGUF V3 (latest)
0.00.025.458 I print_info: file type   = Q6_K
0.00.025.459 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.152 I load: special tokens cache size = 25
0.00.050.241 I load: token to piece cache size = 0.2984 MB
0.00.050.244 I print_info: arch             = gptneox
0.00.050.244 I print_info: vocab_only       = 0
0.00.050.244 I print_info: n_ctx_train      = 2048
0.00.050.244 I print_info: n_embd           = 2048
0.00.050.245 I print_info: n_layer          = 24
0.00.050.247 I print_info: n_head           = 16
0.00.050.248 I print_info: n_head_kv        = 16
0.00.050.248 I print_info: n_rot            = 32
0.00.050.248 I print_info: n_swa            = 0
0.00.050.248 I print_info: n_embd_head_k    = 128
0.00.050.248 I print_info: n_embd_head_v    = 128
0.00.050.249 I print_info: n_gqa            = 1
0.00.050.250 I print_info: n_embd_k_gqa     = 2048
0.00.050.250 I print_info: n_embd_v_gqa     = 2048
0.00.050.251 I print_info: f_norm_eps       = 1.0e-05
0.00.050.251 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.251 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.252 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.252 I print_info: f_logit_scale    = 0.0e+00
0.00.050.252 I print_info: n_ff             = 8192
0.00.050.253 I print_info: n_expert         = 0
0.00.050.253 I print_info: n_expert_used    = 0
0.00.050.253 I print_info: causal attn      = 1
0.00.050.253 I print_info: pooling type     = 0
0.00.050.253 I print_info: rope type        = 2
0.00.050.253 I print_info: rope scaling     = linear
0.00.050.254 I print_info: freq_base_train  = 10000.0
0.00.050.254 I print_info: freq_scale_train = 1
0.00.050.254 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.254 I print_info: rope_finetuned   = unknown
0.00.050.255 I print_info: ssm_d_conv       = 0
0.00.050.256 I print_info: ssm_d_inner      = 0
0.00.050.257 I print_info: ssm_d_state      = 0
0.00.050.257 I print_info: ssm_dt_rank      = 0
0.00.050.257 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.257 I print_info: model type       = 1.4B
0.00.050.257 I print_info: model params     = 1.41 B
0.00.050.258 I print_info: general.name     = 1.4B
0.00.050.258 I print_info: vocab type       = BPE
0.00.050.258 I print_info: n_vocab          = 50304
0.00.050.259 I print_info: n_merges         = 50009
0.00.050.259 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.259 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.259 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.259 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.260 I print_info: LF token         = 128 'Ä'
0.00.050.260 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.260 I print_info: max token length = 1024
0.00.052.232 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.232 I load_tensors: offloading output layer to GPU
0.00.052.233 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.243 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.244 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.520 I llama_init_from_model: n_seq_max     = 1
0.00.052.521 I llama_init_from_model: n_ctx         = 2048
0.00.052.521 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.521 I llama_init_from_model: n_batch       = 2048
0.00.052.521 I llama_init_from_model: n_ubatch      = 512
0.00.052.522 I llama_init_from_model: flash_attn    = 0
0.00.052.522 I llama_init_from_model: freq_base     = 10000.0
0.00.052.522 I llama_init_from_model: freq_scale    = 1
0.00.052.523 I ggml_metal_init: allocating
0.00.052.525 I ggml_metal_init: found device: Apple M4
0.00.052.527 I ggml_metal_init: picking default device: Apple M4
0.00.053.117 I ggml_metal_init: using embedded metal library
0.00.055.470 I ggml_metal_init: GPU name:   Apple M4
0.00.055.472 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.472 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.472 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.472 I ggml_metal_init: simdgroup reduction   = true
0.00.055.473 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.473 I ggml_metal_init: has bfloat            = true
0.00.055.473 I ggml_metal_init: use bfloat            = true
0.00.055.473 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.474 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.169 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.650 I init:      Metal KV buffer size =   384.00 MiB
0.00.084.656 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.675 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.770 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.771 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.772 I llama_init_from_model: graph nodes  = 967
0.00.085.772 I llama_init_from_model: graph splits = 2
0.00.085.774 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.903 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.904 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.425 I main: llama threadpool init, n_threads = 4
0.00.754.463 I 
0.00.754.510 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.754.511 I 
0.00.754.728 I sampler seed: 1234
0.00.754.734 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.754.745 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.754.745 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.754.745 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.626.265 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61049.01 tokens per second)
0.01.626.266 I llama_perf_context_print:        load time =     744.77 ms
0.01.626.267 I llama_perf_context_print: prompt eval time =      54.46 ms /     7 tokens (    7.78 ms per token,   128.54 tokens per second)
0.01.626.267 I llama_perf_context_print:        eval time =     814.18 ms /    63 runs   (   12.92 ms per token,    77.38 tokens per second)
0.01.626.268 I llama_perf_context_print:       total time =     871.84 ms /    70 tokens
0.01.629.243 I ggml_metal_free: deallocating

real	0m1.649s
user	0m0.108s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4507 (1eb0b12f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.956 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.649 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.654 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.659 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.660 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.660 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.660 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.661 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.661 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.662 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.662 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.662 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.663 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.663 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.664 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.665 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.666 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.666 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.437 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.465 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.199 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.200 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.201 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.201 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.201 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.202 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.202 I llama_model_loader: - type  f32:  194 tensors
0.00.025.202 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.203 I print_info: file format = GGUF V3 (latest)
0.00.025.203 I print_info: file type   = Q6_K
0.00.025.204 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.716 I load: special tokens cache size = 25
0.00.049.420 I load: token to piece cache size = 0.2984 MB
0.00.049.423 I print_info: arch             = gptneox
0.00.049.423 I print_info: vocab_only       = 0
0.00.049.423 I print_info: n_ctx_train      = 2048
0.00.049.424 I print_info: n_embd           = 2048
0.00.049.424 I print_info: n_layer          = 24
0.00.049.426 I print_info: n_head           = 16
0.00.049.427 I print_info: n_head_kv        = 16
0.00.049.427 I print_info: n_rot            = 32
0.00.049.427 I print_info: n_swa            = 0
0.00.049.428 I print_info: n_embd_head_k    = 128
0.00.049.428 I print_info: n_embd_head_v    = 128
0.00.049.428 I print_info: n_gqa            = 1
0.00.049.429 I print_info: n_embd_k_gqa     = 2048
0.00.049.430 I print_info: n_embd_v_gqa     = 2048
0.00.049.430 I print_info: f_norm_eps       = 1.0e-05
0.00.049.431 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.431 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.431 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.431 I print_info: f_logit_scale    = 0.0e+00
0.00.049.432 I print_info: n_ff             = 8192
0.00.049.432 I print_info: n_expert         = 0
0.00.049.432 I print_info: n_expert_used    = 0
0.00.049.433 I print_info: causal attn      = 1
0.00.049.433 I print_info: pooling type     = 0
0.00.049.433 I print_info: rope type        = 2
0.00.049.433 I print_info: rope scaling     = linear
0.00.049.433 I print_info: freq_base_train  = 10000.0
0.00.049.434 I print_info: freq_scale_train = 1
0.00.049.434 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.434 I print_info: rope_finetuned   = unknown
0.00.049.434 I print_info: ssm_d_conv       = 0
0.00.049.434 I print_info: ssm_d_inner      = 0
0.00.049.434 I print_info: ssm_d_state      = 0
0.00.049.435 I print_info: ssm_dt_rank      = 0
0.00.049.435 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.435 I print_info: model type       = 1.4B
0.00.049.435 I print_info: model params     = 1.41 B
0.00.049.436 I print_info: general.name     = 1.4B
0.00.049.436 I print_info: vocab type       = BPE
0.00.049.437 I print_info: n_vocab          = 50304
0.00.049.437 I print_info: n_merges         = 50009
0.00.049.438 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.438 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.438 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.438 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.438 I print_info: LF token         = 128 'Ä'
0.00.049.439 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.439 I print_info: max token length = 1024
0.00.051.416 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.416 I load_tensors: offloading output layer to GPU
0.00.051.416 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.427 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.428 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.051.708 I llama_init_from_model: n_seq_max     = 1
0.00.051.709 I llama_init_from_model: n_ctx         = 128
0.00.051.709 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.709 I llama_init_from_model: n_batch       = 128
0.00.051.709 I llama_init_from_model: n_ubatch      = 128
0.00.051.709 I llama_init_from_model: flash_attn    = 0
0.00.051.710 I llama_init_from_model: freq_base     = 10000.0
0.00.051.710 I llama_init_from_model: freq_scale    = 1
0.00.051.710 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.710 I ggml_metal_init: allocating
0.00.051.713 I ggml_metal_init: found device: Apple M4
0.00.051.715 I ggml_metal_init: picking default device: Apple M4
0.00.052.278 I ggml_metal_init: using embedded metal library
0.00.054.608 I ggml_metal_init: GPU name:   Apple M4
0.00.054.609 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.610 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.610 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.610 I ggml_metal_init: simdgroup reduction   = true
0.00.054.610 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.611 I ggml_metal_init: has bfloat            = true
0.00.054.611 I ggml_metal_init: use bfloat            = true
0.00.054.611 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.612 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.100 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.345 I init:      Metal KV buffer size =    24.00 MiB
0.00.065.347 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.362 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.248 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.249 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.249 I llama_init_from_model: graph nodes  = 967
0.00.066.250 I llama_init_from_model: graph splits = 2
0.00.066.251 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.251 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.205.228 I 
0.00.205.277 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.205.296 I perplexity: tokenizing the input ..
0.00.213.100 I perplexity: tokenization took 7.801 ms
0.00.213.105 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.352.588 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.353.863 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.353.894 I llama_perf_context_print:        load time =     195.27 ms
0.00.353.895 I llama_perf_context_print: prompt eval time =     139.25 ms /   128 tokens (    1.09 ms per token,   919.24 tokens per second)
0.00.353.896 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.353.896 I llama_perf_context_print:       total time =     148.67 ms /   129 tokens
0.00.354.623 I ggml_metal_free: deallocating

real	0m0.370s
user	0m0.076s
sys	0m0.045s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4507 (1eb0b12f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137609f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13760a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13760ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13760b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13760b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13760bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13760c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13760c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13760ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13760d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13760d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13760dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13760e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13760f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13760f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13760ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137610660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137610d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1376114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137611c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137612390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137612ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1376131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137613a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137614190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137614450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137614a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1376156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137615c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137615ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137616370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137616630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137616ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137617400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1376176c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137617b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137618000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1376184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137618940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137618de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137619280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137619720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137619bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13761a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13761a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13761a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13761af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13761b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13761be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13761c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13761ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13761d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13761d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13761dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13761e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13761e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13761edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13761f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13761f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13761feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137620170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137620610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137620ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137620f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1376213f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137621890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137621d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1376221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137622670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137622b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137622fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137623450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1376238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x137623e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x137624390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1376248e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x137624e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x137625380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1376258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x137625e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x137626370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1376268c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x137626e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x137627360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1376278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x137627e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x137628350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1376288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x137628df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x137629340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x137629890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x137629de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13762a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13762a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13762add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13762b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13762b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13761b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13762bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13762c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13762c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13762cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13762d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13762d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13762df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13762e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13762e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13762ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13762f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13762f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13762ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x137630450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1376309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137630e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1376312e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137631780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137631c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1376320c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137632560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137632a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137632ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137633340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1376337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137633c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137634120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1376345c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137634a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137634f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1376353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137635840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137635ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x137636180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137636620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137636ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137636f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137637400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1376378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137637d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1376381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137638680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x137638b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137638fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x137639460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137639900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x137639da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13763a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13763a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13763ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13763b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13763b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13763b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13763be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13763c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13763c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13763cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13763d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13763d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13763d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13763de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13763e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13763e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13763ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13763f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13763f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13763fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13763fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137640360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137640800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137640ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137641140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1376415e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137641a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137641f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1376423c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137642860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137642d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1376431a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137643640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137643ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137643f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137644420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1376448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137644d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137645200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1376456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137645b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137645fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137646480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137646920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137646dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137647260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137647700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137647ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1376480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137648640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137648b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1376490e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1376493a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1376499b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x137649fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13764a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13764adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13764b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13764b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13764bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13764c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13764c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13764cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13764d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13764d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13764dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13764e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13764e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13764eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13764f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13764f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13764fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1376503f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137650940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137650e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1376513e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137651930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137651e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1376523d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137652920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137652e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1376533c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x137653910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137653e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1376543b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x137654900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137654e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1376553a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1376558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x137655e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137656390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1376568e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137656e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137657380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1376578d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137657e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x137658370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1376588c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x137658e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x137659360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1376598b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x137659e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13765a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13765a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13765adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13765b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13765b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13765bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13765c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13765c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13765cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13765d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13765d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13765ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13765e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13765e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13765edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13765f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13765f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13765fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1376602f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137660840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x137660ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x137661180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137661620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137661ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137661f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137662400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1376628a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137662d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1376631e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137663680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137663b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137663fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137664460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137664900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137664da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1376652f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137665a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x137666130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137666850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137666f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137667230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x137667a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137667ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1376682f0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.183.259 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.183.263 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137667fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137649c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137649660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13764a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13761d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13761cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13761f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13764bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137614710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13761b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13761bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13761c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13761a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13761c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137613710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137609590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13761df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13761f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13762bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1376674f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1376168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137616bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13764c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13764a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137614d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137614fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1376152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137668750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137668a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137668cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137668f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137669250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137669510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1376697d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137669a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137669d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13766a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13766a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13766a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13766a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13766ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13766add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13766b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13766b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13766b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13766b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13766bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13766be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13766c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13766c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13766c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13766c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13766cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13766ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13766d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13766d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13766d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13766d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13766dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13766df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13766e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13766e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13766e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13766ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13766ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13766efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13766f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13766f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13766f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13766fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13766fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137670050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137670310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1376705d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x137670890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x137670b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x137670e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1376710d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x137671390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x137671650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x137671910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x137671bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x137671e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x137672150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x137672410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1376726d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x137672990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x137672c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x137672f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1376731d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x137673490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x137673750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x137673a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x137673cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x137673f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x137674250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x137674510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1376747d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x137674a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x137674d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x137675010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1376752d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x137675590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x137675850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x137675b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x137675dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x137676090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x137676350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x137676610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1376768d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x137676b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x137676e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x137677110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1376773d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137677690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137677950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137677c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137677ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137678190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137678450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137678710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1376789d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137678c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137678f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137679210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1376794d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137679790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137679a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137679d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137679fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13767a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13767a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13767a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13767aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13767ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13767b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13767b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13767b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13767b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13767bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13767be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13767c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13767c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13767c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13767c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13767cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13767ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13767d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13767d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13767d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13767d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13767dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13767df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13767e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13767e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13767e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13767ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13767ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13767ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13767f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13767f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13767f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13767fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13767fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137680010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1376802d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137680590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137680850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137680b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137680dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137681090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137681350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137681610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1376818d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137681b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137681e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137682110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1376823d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137682690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137682950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137682c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137682ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137683190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137683450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137683710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1376839d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137683c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137683f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137684210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1376844d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137684790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137684a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137684d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137684fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137685290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137685550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137685810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x137685ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137685d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x137686050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x137686310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1376865d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x137686890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x137686b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x137686e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1376870d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x137687390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x137687650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137687910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137687bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1376881a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137688460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137688720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1376889e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137688ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137688f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137689220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1376894e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1376897a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137689a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137689d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137689fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13768a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13768a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13768a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13768aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13768ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13768b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13768b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13768b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13768b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13768bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13768be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13768c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13768c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13768c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13768c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13768cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13768cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13768d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13768d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13768d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13768dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13768e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13768e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13768eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13768f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13768f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13768fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1376903f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137690940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x137690e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1376913e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x137691930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x137691e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1376923d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x137692920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x137692e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1376933c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137693910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137693e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1376943b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137694900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137694e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1376953a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1376958f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137695e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x137696100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1376963c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1376968c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137696dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1376972c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1376977c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137697cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1376981c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1376986c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137698bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1376990c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1376995c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137699ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137699fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13769a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13769a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13769b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13769baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13769c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13769c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13769cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13769d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13769d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13769dcb0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13769d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13764b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13769ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13769e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13769e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13769e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13769e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13769ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13769eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13769f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13769f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13769f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13769fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1376a02b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1376a08e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1376a0ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1376a0e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1376a1120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1376a13e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1376a16a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1376a1960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1376a1c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1376a1ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1376a21a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1376a2460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1376a2720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1376a29e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1376a2ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1376a2f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1376a3220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1376a34e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1376a37a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1376a3a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1376a3d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1376a3fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1376a42a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1376a4560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1376a4820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1376a4ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1376a4da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1376a5060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1376a5320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1376a55e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1376a58a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1376a5b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1376a5e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1376a60e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1376a63a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1376a6660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1376a6920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1376a6be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1376a6ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1376a7160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1376a7420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1376a76e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1376a79a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1376a7c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1376a7f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1376a81e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1376a84a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1376a8760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1376a8a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1376a8ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1376a8fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1376a9260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1376a9520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1376a97e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1376a9aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1376a9d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1376aa020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1376aa2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1376aa5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1376aa860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1376aab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1376aade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1376ab0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1376ab360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1376ab620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1376ab8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1376abba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1376abe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1376ac120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1376ac3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1376ac6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1376ac960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1376acc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1376acee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1376ad1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1376ad460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1376ad720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1376ad9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1376adca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1376adf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1376ae220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1376ae4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1376ae7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1376aea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1376aed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1376aefe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1376af2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1376af560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1376af820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1376afae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1376afda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1376b0060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1376b0320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1376b05e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1376b08a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1376b0b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1376b0e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1376b10e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1376b13a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1376b1660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1376b1920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1376b1be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1376b1ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1376b2160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1376b2420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1376b26e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1376b29a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1376b2c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1376b2f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1376b31e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1376b34a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1376b3760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1376b3a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1376b3ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1376b3fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1376b4260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1376b4520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1376b47e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1376b4aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1376b4d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1376b5020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1376b52e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1376b55a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1376b5860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1376b5b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1376b5de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1376b60a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1376b6360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1376b6620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1376b68e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1376b6ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1376b6e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1376b7120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1376b73e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1376b76a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1376b7960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1376b7c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1376b7ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1376b81a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1376b8460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1376b8720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1376b89e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1376b8ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1376b8f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1376b9220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1376b94e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1376b97a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1376b9a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1376b9d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1376b9fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1376ba2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1376ba560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1376ba820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1376baae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1376bada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1376bb060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1376bb320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1376bb5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1376bb8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1376bbb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1376bbe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1376bc0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1376bc3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1376bc660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1376bc920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1376bcbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1376bcea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1376bd160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1376bd420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1376bd6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1376bd9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1376bdc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1376bdf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1376be1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1376be4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1376be760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1376bea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1376bece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1376befa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1376bf260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1376bf520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1376bf7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1376bfaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1376bfd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1376c0020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1376c02e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1376c05a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1376c0860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1376c0b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1376c0de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1376c10a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1376c1360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1376c1620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1376c18e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1376c1ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1376c1e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1376c2120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1376c26f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1376c29b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1376c2c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1376c2f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1376c31f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1376c34b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1376c3770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1376c3a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1376c3cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1376c3fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1376c4270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1376c4530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1376c47f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1376c4ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1376c4d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1376c5030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1376c52f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1376c55b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1376c5870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1376c5b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1376c5df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1376c60b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1376c6370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1376c6630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1376c68f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1376c6bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1376c6e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1376c7130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1376c73f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1376c76b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1376c7970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1376c7c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1376c7ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1376c81b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1376c8470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1376c8730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1376c89f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1376c8cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1376c8f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1376c9230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1376c94f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1376c97b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1376c9a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1376c9d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1376c9ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1376ca2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1376ca570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1376ca830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1376caaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1376cadb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1376cb070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1376cb330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1376cb5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1376cb8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1376cbb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1376cbe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1376cc0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1376cc3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1376cc670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1376cc930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1376ccbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1376cceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1376cd170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1376cd570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1376cd830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1376cdaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1376cdf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1376ce3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1376ce840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1376cecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1376cf120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1376cf590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1376cfa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1376d0520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1376d0c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1376d1360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1376d1a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1376d1d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1376d2000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1376d2530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1376d29a0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.939s
user	0m0.304s
sys	0m0.323s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4507 (1eb0b12f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12cf0d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12cf0dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12cf0e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12cf0e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12cf0ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12cf0f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12cf0f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12cf0ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12cf10550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12cf10a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12cf10f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12cf11450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12cf11f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12cf12720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12cf12f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12cf13650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12cf13d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12cf14490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12cf14bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12cf15380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12cf15aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12cf161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12cf168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12cf17180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12cf178a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12cf17b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12cf18170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12cf18de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12cf19320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12cf195e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12cf19a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12cf19d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12cf1a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12cf1ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12cf1add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12cf1b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12cf1b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12cf1bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12cf1c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12cf1c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12cf1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12cf1ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12cf1d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12cf1d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12cf1da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12cf1e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12cf1e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12cf1ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12cf1f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12cf1fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12cf201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12cf207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12cf20dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12cf213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12cf21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12cf22060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12cf22500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12cf227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12cf22dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12cf235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12cf23880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12cf23d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12cf241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12cf24660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12cf24b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12cf24fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12cf25440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12cf258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12cf25d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12cf26220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12cf266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12cf26b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12cf27000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12cf27550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12cf27aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12cf27ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12cf28540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12cf28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12cf28fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12cf29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12cf29a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12cf29fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12cf2a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12cf2aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12cf2afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12cf2b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12cf2ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12cf2bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12cf2c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12cf2ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12cf2cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12cf2d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12cf2da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12cf2df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12cf2e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12cf2ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12cf2ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12cf1ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12cf2f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12cf2fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12cf300f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12cf30640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12cf30b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12cf310e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12cf31630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12cf31b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12cf320d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12cf32620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12cf32b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12cf330c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12cf33610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12cf33b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12cf340b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12cf34550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12cf349f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12cf34e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12cf35330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12cf357d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12cf35c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12cf36110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12cf365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12cf36a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12cf36ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12cf37390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12cf37830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12cf37cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12cf38170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12cf38610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12cf38ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12cf38f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12cf393f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12cf39890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12cf39d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12cf3a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12cf3a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12cf3ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12cf3afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12cf3b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12cf3b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12cf3bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12cf3c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12cf3c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12cf3cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12cf3d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12cf3d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12cf3d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12cf3ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12cf3e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12cf3e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12cf3ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12cf3f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12cf3f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12cf3f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12cf3fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12cf402f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12cf40790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12cf40c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12cf410d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12cf41570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12cf41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12cf41eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12cf42350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12cf427f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12cf42c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12cf43130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12cf435d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12cf43a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12cf43f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12cf443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12cf44850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12cf44cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12cf45190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12cf45630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12cf45ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12cf45f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12cf46410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12cf468b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12cf46d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12cf471f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12cf47690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12cf47b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12cf47fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12cf48470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12cf48910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12cf48db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12cf49250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12cf496f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12cf49b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12cf4a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12cf4a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12cf4a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12cf4ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12cf4b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12cf4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12cf4bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12cf4c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12cf4c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12cf4cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12cf4d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12cf4d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12cf4dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12cf4e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12cf4e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12cf4ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12cf4f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12cf4f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12cf50040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12cf504e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12cf50980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12cf50e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12cf515d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12cf51b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12cf52070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12cf525c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12cf52b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12cf53060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12cf535b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12cf53b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12cf54050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12cf545a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12cf54af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12cf55040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12cf55590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12cf55ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12cf56030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12cf56580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12cf56ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12cf57020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12cf57570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12cf57ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12cf58010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12cf58560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12cf58ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12cf59000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12cf59550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12cf59aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12cf59ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12cf5a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12cf5aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12cf5afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12cf5b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12cf5ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12cf5bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12cf5c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12cf5ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12cf5cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12cf5d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12cf5da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12cf5dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12cf5e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12cf5ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12cf5efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12cf5f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12cf5fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12cf5ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12cf604e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12cf60a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12cf60f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12cf614d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12cf61a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12cf61f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12cf624c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12cf62a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12cf62f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12cf634b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12cf63a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12cf63f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12cf643f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12cf64890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12cf64d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12cf651d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12cf65670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12cf65b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12cf65fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12cf66450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12cf668f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12cf66d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12cf67230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12cf676d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12cf67b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12cf68010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12cf684b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12cf68a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12cf69120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12cf69840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12cf69f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12cf6a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12cf6a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12cf6b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12cf6b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12cf6ba00 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.107.125 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.107.129 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12e806250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12e8066c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12e806b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12e806fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12e807410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12e807880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12e807cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12e808160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12e8085d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12e808a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12e808eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12e809520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12e80a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12e80a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12e80b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12e80b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12e80be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12e80c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12e80cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12e80d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12e80db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12e80e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12e80e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12e80f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12e80f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12e80fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12e80fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12e8101e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12e810650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12e810ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12e810f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12e811460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12e8118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12e811b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12e812000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12e812470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12e8128e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12e812d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12e8131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12e813630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12e813aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12e813f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12e814380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12e8147f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12e814c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12e8150d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12e815540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12e8159b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12e815e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12e816290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12e816700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12e816b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12e816fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12e817450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12e8178c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12e817d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12e8182a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12e8187a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12e818c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12e819080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12e8194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12e819960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12e819dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12e81a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12e81a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12e81ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12e81af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12e81b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12e81b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12e81bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12e81c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12e81c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12e81ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12e81cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12e81d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12e81d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12e81dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12e81e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12e81e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12e81e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12e81edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12e81f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12e81f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12e81fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12e81ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12e8203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12e820850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12e820cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12e821130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12e8215a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12e821a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12e821e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12e8222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12e822760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12e822bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12e823040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12e8234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12e823920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12e823d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12e824200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12e824670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12e824ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12e824f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12e8253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12e825830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12e825ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12e826110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12e826580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12e8269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12e826e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12e8272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12e827740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12e827bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12e828020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12e828490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12e828900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12e828d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12e8291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12e829650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12e829ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12e829f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12e82a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12e82a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12e82ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12e82b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12e82b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12e82b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12e82be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12e82c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12e82c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12e82cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12e82d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12e82d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12e82d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12e82dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12e82e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12e82e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12e82eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12e82ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12e82f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12e82f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12e82fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12e8300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12e830540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12e8309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12e830e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12e831290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12e831700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12e831b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12e831fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12e832450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12e8328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12e832d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12e8331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12e833610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12e833a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12e833ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12e834360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12e8347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12e834c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12e8350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12e835520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12e835990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12e835e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12e836270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12e8366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12e837310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12e8375d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12e837890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12e837d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12e838170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12e8385e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12e838a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12e838ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12e839330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12e8397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12e839c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12e83a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12e83a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12e83a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12e83add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12e83b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12e83b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12e83bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12e83bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12e83c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12e83c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12e83cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12e83d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12e83d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12e83da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12e83dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12e83e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12e83e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12e83ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12e83f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12e83f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12e83f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12e83fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12e840220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12e840690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12e840b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12e841060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12e841570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12e8419e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12e841e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12e8422c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12e842730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12e842c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12e843160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12e843cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12e843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12e844550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12e844b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12e8450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12e845690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12e845c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12e846210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12e8467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12e846d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12e847350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12e847910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12e847ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12e848490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12e848a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12e849010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12e8495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12e849b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12e84a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12e84a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12e84acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12e84b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12e84b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12e84be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12e84c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12e84c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12e84cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12e84d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12e84dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12e84e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12e84e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12e84ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12e84f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12e84f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12e84fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12e850310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12e8508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12e850e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12e851450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12e851a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12e851fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12e852590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12e852b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12e853110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12e8536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12e853c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12e854250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12e854810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12e854dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12e855390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12e855950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12e855f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12e8564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12e856a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12e857050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12e857610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12e857bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12e858190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12e858690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12e858b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12e859090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12e859590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12e859a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12e859f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12e85a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12e85a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12e85ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12e85b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12e85b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12e85bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12e85c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12e85c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12e85cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12e85d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12e85ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12e85e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12e85ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12e85eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12e85f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12e85f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12e85ff80 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12bc044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12bc04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12bc04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12bc05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12bc056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12bc05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12bc05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12bc063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12bc06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12bc06db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12bc07220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12bc078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12bc083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12bc08b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12bc09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12bc09aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12bc0a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12bc0a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12bc0b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12bc0b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12bc0bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12bc0c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12bc0cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12bc0d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12bc0db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12bc0de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12bc0e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12bc0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12bc0e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12bc0ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12bc0f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12bc0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12bc0fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12bc0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12bc10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12bc107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12bc10c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12bc110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12bc11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12bc119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12bc11e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12bc12290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12bc12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12bc12b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12bc12fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12bc13450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12bc138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12bc13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12bc141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12bc14610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12bc14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12bc14ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12bc15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12bc157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12bc15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12bc160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12bc16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12bc16b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12bc16f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12bc17400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12bc17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12bc17ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12bc18150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12bc185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12bc18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12bc18ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12bc19310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12bc19780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12bc19bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12bc1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12bc1a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12bc1a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12bc1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12bc1b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12bc1b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12bc1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12bc1bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12bc1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12bc1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12bc1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12bc1d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12bc1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12bc1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12bc1de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12bc1e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12bc1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12bc1ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12bc1f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12bc1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12bc1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12bc1fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12bc20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12bc20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12bc20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12bc20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12bc213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12bc21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12bc21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12bc22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12bc22580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12bc229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12bc22e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12bc232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12bc23b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12bc23e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12bc24290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12bc24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12bc24b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12bc24fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12bc25450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12bc258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12bc25d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12bc261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12bc26610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12bc26a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12bc26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12bc27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12bc277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12bc27c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12bc280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12bc28520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12bc28990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12bc28e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12bc29270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12bc296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12bc29b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12bc29fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12bc2a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12bc2a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12bc2ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12bc2b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12bc2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12bc2ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12bc2bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12bc2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12bc2c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12bc2cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12bc2d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12bc2d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12bc2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12bc2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12bc2e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12bc2e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12bc2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12bc2efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12bc2f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12bc2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12bc2fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12bc30160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12bc305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12bc30a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12bc30eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12bc31320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12bc31790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12bc31c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12bc32070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12bc324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12bc32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12bc32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12bc33230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12bc336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12bc33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12bc33f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12bc343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12bc34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12bc34cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12bc35140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12bc355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12bc35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12bc35e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12bc36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12bc36770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12bc36be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12bc37050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12bc374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12bc37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12bc37da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12bc38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12bc38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12bc38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12bc38f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12bc393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12bc39840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12bc39cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12bc3a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12bc3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12bc3aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12bc3ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12bc3b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12bc3b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12bc3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12bc3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12bc3c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12bc3c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12bc3cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12bc3d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12bc3d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12bc3dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12bc3df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12bc3e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12bc3e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12bc3ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12bc3f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12bc3f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12bc3f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12bc3fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12bc402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12bc40730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12bc40ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12bc41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12bc41b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12bc41e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12bc42110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12bc42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12bc429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12bc42e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12bc432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12bc43740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12bc43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12bc44020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12bc44490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12bc44900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12bc44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12bc451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12bc45650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12bc45ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12bc45f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12bc463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12bc46810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12bc46c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12bc470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12bc47560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12bc479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12bc47e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12bc482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12bc48720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12bc48b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12bc49000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12bc49470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12bc498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12bc49d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12bc4a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12bc4a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12bc4aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12bc4af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12bc4b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12bc4b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12bc4bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12bc4c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12bc4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12bc4c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12bc4ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12bc4d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12bc4d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12bc4db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12bc4dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12bc4e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12bc4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12bc4ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12bc4f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12bc4f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12bc4fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12bc4fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12bc50360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12bc507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12bc50c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12bc510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12bc51520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12bc51990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12bc51e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12bc52270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12bc526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12bc52b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12bc52fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12bc53430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12bc538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12bc53d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12bc54180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12bc545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12bc54a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12bc54ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12bc55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12bc557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12bc56220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12bc56940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12bc57060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12bc57780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12bc57a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12bc57eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12bc584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12bc58ac0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.956s
user	0m0.255s
sys	0m0.136s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.59 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.57 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.16 sec*proc (2 tests)

Total Test time (real) =   1.17 sec
        1.28 real         0.71 user         0.06 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.24 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.15 user         0.04 sys
```
