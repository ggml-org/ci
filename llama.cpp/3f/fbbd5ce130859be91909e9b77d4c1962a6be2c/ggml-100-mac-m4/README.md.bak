### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.42 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.10 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.27 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.61 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.21 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.17 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.27 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.23 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.80 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.30 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.08 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.24 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.35 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    3.00 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.90 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  104.64 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.85 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.92 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.34 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 166.21 sec*proc (29 tests)

Total Test time (real) = 166.22 sec

real	2m46.239s
user	4m39.933s
sys	0m5.685s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.23 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.24 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.09 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.15 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.87 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.20 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.79 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.20 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.33 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.24 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.48 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.43 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.33 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.28 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.04 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.23 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.33 sec*proc (29 tests)

Total Test time (real) =  48.34 sec

real	0m48.350s
user	0m54.303s
sys	0m5.166s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.217 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.288 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.983 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.026.990 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.992 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.026.993 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.994 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.026.994 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.026.995 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.026.997 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.026.997 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.026.998 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.026.999 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.027.000 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.027.003 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.027.003 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.027.006 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.027.006 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.027.007 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.027.007 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.027.009 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.031.802 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.033.030 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.032 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.033.033 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.033.033 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.033.034 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.033.034 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.033.035 I llama_model_loader: - type  f32:  124 tensors
0.00.033.035 I llama_model_loader: - type  f16:   73 tensors
0.00.033.036 I print_info: file format = GGUF V3 (latest)
0.00.033.037 I print_info: file type   = F16
0.00.033.038 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.037.170 I load: special tokens cache size = 5
0.00.039.178 I load: token to piece cache size = 0.2032 MB
0.00.039.205 I print_info: arch             = bert
0.00.039.206 I print_info: vocab_only       = 0
0.00.039.207 I print_info: n_ctx_train      = 512
0.00.039.207 I print_info: n_embd           = 384
0.00.039.207 I print_info: n_layer          = 12
0.00.039.210 I print_info: n_head           = 12
0.00.039.211 I print_info: n_head_kv        = 12
0.00.039.211 I print_info: n_rot            = 32
0.00.039.212 I print_info: n_swa            = 0
0.00.039.212 I print_info: n_embd_head_k    = 32
0.00.039.213 I print_info: n_embd_head_v    = 32
0.00.039.214 I print_info: n_gqa            = 1
0.00.039.215 I print_info: n_embd_k_gqa     = 384
0.00.039.216 I print_info: n_embd_v_gqa     = 384
0.00.039.217 I print_info: f_norm_eps       = 1.0e-12
0.00.039.218 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.218 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.218 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.219 I print_info: f_logit_scale    = 0.0e+00
0.00.039.220 I print_info: n_ff             = 1536
0.00.039.220 I print_info: n_expert         = 0
0.00.039.220 I print_info: n_expert_used    = 0
0.00.039.220 I print_info: causal attn      = 0
0.00.039.221 I print_info: pooling type     = 2
0.00.039.221 I print_info: rope type        = 2
0.00.039.221 I print_info: rope scaling     = linear
0.00.039.222 I print_info: freq_base_train  = 10000.0
0.00.039.222 I print_info: freq_scale_train = 1
0.00.039.222 I print_info: n_ctx_orig_yarn  = 512
0.00.039.223 I print_info: rope_finetuned   = unknown
0.00.039.223 I print_info: ssm_d_conv       = 0
0.00.039.223 I print_info: ssm_d_inner      = 0
0.00.039.223 I print_info: ssm_d_state      = 0
0.00.039.225 I print_info: ssm_dt_rank      = 0
0.00.039.225 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.226 I print_info: model type       = 33M
0.00.039.226 I print_info: model params     = 33.21 M
0.00.039.227 I print_info: general.name     = Bge Small
0.00.039.227 I print_info: vocab type       = WPM
0.00.039.227 I print_info: n_vocab          = 30522
0.00.039.227 I print_info: n_merges         = 0
0.00.039.228 I print_info: BOS token        = 101 '[CLS]'
0.00.039.228 I print_info: UNK token        = 100 '[UNK]'
0.00.039.228 I print_info: SEP token        = 102 '[SEP]'
0.00.039.229 I print_info: PAD token        = 0 '[PAD]'
0.00.039.229 I print_info: MASK token       = 103 '[MASK]'
0.00.039.229 I print_info: LF token         = 0 '[PAD]'
0.00.039.230 I print_info: max token length = 21
0.00.039.230 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.042.377 I load_tensors: offloading 12 repeating layers to GPU
0.00.042.379 I load_tensors: offloading output layer to GPU
0.00.042.379 I load_tensors: offloaded 13/13 layers to GPU
0.00.042.404 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.042.406 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.042.722 I llama_init_from_model: n_seq_max     = 1
0.00.042.723 I llama_init_from_model: n_ctx         = 512
0.00.042.724 I llama_init_from_model: n_ctx_per_seq = 512
0.00.042.724 I llama_init_from_model: n_batch       = 2048
0.00.042.724 I llama_init_from_model: n_ubatch      = 2048
0.00.042.724 I llama_init_from_model: flash_attn    = 0
0.00.042.725 I llama_init_from_model: freq_base     = 10000.0
0.00.042.725 I llama_init_from_model: freq_scale    = 1
0.00.042.726 I ggml_metal_init: allocating
0.00.042.732 I ggml_metal_init: found device: Apple M4
0.00.042.739 I ggml_metal_init: picking default device: Apple M4
0.00.043.416 I ggml_metal_init: using embedded metal library
0.00.047.524 I ggml_metal_init: GPU name:   Apple M4
0.00.047.527 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.047.528 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.047.528 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.047.529 I ggml_metal_init: simdgroup reduction   = true
0.00.047.529 I ggml_metal_init: simdgroup matrix mul. = true
0.00.047.529 I ggml_metal_init: has residency sets    = true
0.00.047.529 I ggml_metal_init: has bfloat            = true
0.00.047.529 I ggml_metal_init: use bfloat            = true
0.00.047.530 I ggml_metal_init: hasUnifiedMemory      = true
0.00.047.531 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.059.898 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.060.598 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.060.600 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.060.602 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.061.873 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.061.874 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.061.875 I llama_init_from_model: graph nodes  = 429
0.00.061.875 I llama_init_from_model: graph splits = 2
0.00.061.876 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.061.877 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.067.704 I 
0.00.067.731 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.068.368 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.073.396 I llama_perf_context_print:        load time =      46.41 ms
0.00.073.397 I llama_perf_context_print: prompt eval time =       4.88 ms /     9 tokens (    0.54 ms per token,  1843.88 tokens per second)
0.00.073.398 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.073.398 I llama_perf_context_print:       total time =       5.69 ms /    10 tokens
0.00.073.563 I ggml_metal_free: deallocating

real	0m0.284s
user	0m0.050s
sys	0m0.038s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.047 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.432 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.270 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.274 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.275 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.276 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.276 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.276 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.277 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.278 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.278 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.278 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.279 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.279 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.281 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.281 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.282 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.282 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.282 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.283 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.681 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.324 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.325 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.326 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.326 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.326 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.326 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.327 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.327 I llama_model_loader: - type  f32:  124 tensors
0.00.015.328 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.328 I print_info: file format = GGUF V3 (latest)
0.00.015.329 I print_info: file type   = Q8_0
0.00.015.330 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.808 I load: special tokens cache size = 5
0.00.019.159 I load: token to piece cache size = 0.2032 MB
0.00.019.168 I print_info: arch             = bert
0.00.019.170 I print_info: vocab_only       = 0
0.00.019.170 I print_info: n_ctx_train      = 512
0.00.019.170 I print_info: n_embd           = 384
0.00.019.170 I print_info: n_layer          = 12
0.00.019.173 I print_info: n_head           = 12
0.00.019.174 I print_info: n_head_kv        = 12
0.00.019.174 I print_info: n_rot            = 32
0.00.019.174 I print_info: n_swa            = 0
0.00.019.174 I print_info: n_embd_head_k    = 32
0.00.019.174 I print_info: n_embd_head_v    = 32
0.00.019.175 I print_info: n_gqa            = 1
0.00.019.175 I print_info: n_embd_k_gqa     = 384
0.00.019.176 I print_info: n_embd_v_gqa     = 384
0.00.019.176 I print_info: f_norm_eps       = 1.0e-12
0.00.019.177 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.177 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.177 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.177 I print_info: f_logit_scale    = 0.0e+00
0.00.019.178 I print_info: n_ff             = 1536
0.00.019.178 I print_info: n_expert         = 0
0.00.019.178 I print_info: n_expert_used    = 0
0.00.019.178 I print_info: causal attn      = 0
0.00.019.178 I print_info: pooling type     = 2
0.00.019.179 I print_info: rope type        = 2
0.00.019.179 I print_info: rope scaling     = linear
0.00.019.179 I print_info: freq_base_train  = 10000.0
0.00.019.179 I print_info: freq_scale_train = 1
0.00.019.179 I print_info: n_ctx_orig_yarn  = 512
0.00.019.180 I print_info: rope_finetuned   = unknown
0.00.019.183 I print_info: ssm_d_conv       = 0
0.00.019.183 I print_info: ssm_d_inner      = 0
0.00.019.183 I print_info: ssm_d_state      = 0
0.00.019.183 I print_info: ssm_dt_rank      = 0
0.00.019.183 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.183 I print_info: model type       = 33M
0.00.019.184 I print_info: model params     = 33.21 M
0.00.019.184 I print_info: general.name     = Bge Small
0.00.019.184 I print_info: vocab type       = WPM
0.00.019.185 I print_info: n_vocab          = 30522
0.00.019.185 I print_info: n_merges         = 0
0.00.019.185 I print_info: BOS token        = 101 '[CLS]'
0.00.019.185 I print_info: UNK token        = 100 '[UNK]'
0.00.019.185 I print_info: SEP token        = 102 '[SEP]'
0.00.019.185 I print_info: PAD token        = 0 '[PAD]'
0.00.019.186 I print_info: MASK token       = 103 '[MASK]'
0.00.019.186 I print_info: LF token         = 0 '[PAD]'
0.00.019.186 I print_info: max token length = 21
0.00.019.186 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.925 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.926 I load_tensors: offloading output layer to GPU
0.00.020.926 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.932 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.933 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.102 I llama_init_from_model: n_seq_max     = 1
0.00.021.102 I llama_init_from_model: n_ctx         = 512
0.00.021.103 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.103 I llama_init_from_model: n_batch       = 2048
0.00.021.103 I llama_init_from_model: n_ubatch      = 2048
0.00.021.103 I llama_init_from_model: flash_attn    = 0
0.00.021.104 I llama_init_from_model: freq_base     = 10000.0
0.00.021.104 I llama_init_from_model: freq_scale    = 1
0.00.021.104 I ggml_metal_init: allocating
0.00.021.108 I ggml_metal_init: found device: Apple M4
0.00.021.112 I ggml_metal_init: picking default device: Apple M4
0.00.021.563 I ggml_metal_init: using embedded metal library
0.00.024.079 I ggml_metal_init: GPU name:   Apple M4
0.00.024.081 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.081 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.081 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.082 I ggml_metal_init: simdgroup reduction   = true
0.00.024.082 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.082 I ggml_metal_init: has residency sets    = true
0.00.024.082 I ggml_metal_init: has bfloat            = true
0.00.024.082 I ggml_metal_init: use bfloat            = true
0.00.024.083 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.084 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.482 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.076 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.078 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.080 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.036.039 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.036.040 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.036.041 I llama_init_from_model: graph nodes  = 429
0.00.036.041 I llama_init_from_model: graph splits = 2
0.00.036.042 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.042 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.257 I 
0.00.040.278 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.782 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.217 I llama_perf_context_print:        load time =      30.82 ms
0.00.045.218 I llama_perf_context_print: prompt eval time =       4.31 ms /     9 tokens (    0.48 ms per token,  2090.11 tokens per second)
0.00.045.219 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.219 I llama_perf_context_print:       total time =       4.96 ms /    10 tokens
0.00.045.444 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.282 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.586 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.012 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.017 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.020 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.020 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.025 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.026 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.029 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.030 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.031 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.031 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.032 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.032 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.036 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.036 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.037 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.038 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.038 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.164 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.047 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.217 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.219 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.220 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.220 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.221 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.221 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.221 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.222 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.222 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.222 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.223 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.048.223 I llama_model_loader: - type  f32:   40 tensors
0.00.048.224 I llama_model_loader: - type  f16:   30 tensors
0.00.048.224 I print_info: file format = GGUF V3 (latest)
0.00.048.225 I print_info: file type   = F16
0.00.048.227 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.052.624 W load: empty token at index 5
0.00.057.695 W load: model vocab missing newline token, using special_pad_id instead
0.00.059.195 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.059.230 I load: special tokens cache size = 5
0.00.322.203 I load: token to piece cache size = 1.5060 MB
0.00.322.233 I print_info: arch             = jina-bert-v2
0.00.322.234 I print_info: vocab_only       = 0
0.00.322.234 I print_info: n_ctx_train      = 8192
0.00.322.234 I print_info: n_embd           = 384
0.00.322.234 I print_info: n_layer          = 4
0.00.322.238 I print_info: n_head           = 12
0.00.322.238 I print_info: n_head_kv        = 12
0.00.322.239 I print_info: n_rot            = 32
0.00.322.239 I print_info: n_swa            = 0
0.00.322.239 I print_info: n_embd_head_k    = 32
0.00.322.239 I print_info: n_embd_head_v    = 32
0.00.322.240 I print_info: n_gqa            = 1
0.00.322.240 I print_info: n_embd_k_gqa     = 384
0.00.322.245 I print_info: n_embd_v_gqa     = 384
0.00.322.245 I print_info: f_norm_eps       = 1.0e-12
0.00.322.246 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.322.246 I print_info: f_clamp_kqv      = 0.0e+00
0.00.322.247 I print_info: f_max_alibi_bias = 8.0e+00
0.00.322.248 I print_info: f_logit_scale    = 0.0e+00
0.00.322.249 I print_info: n_ff             = 1536
0.00.322.249 I print_info: n_expert         = 0
0.00.322.249 I print_info: n_expert_used    = 0
0.00.322.249 I print_info: causal attn      = 0
0.00.322.249 I print_info: pooling type     = -1
0.00.322.250 I print_info: rope type        = -1
0.00.322.251 I print_info: rope scaling     = linear
0.00.322.252 I print_info: freq_base_train  = 10000.0
0.00.322.252 I print_info: freq_scale_train = 1
0.00.322.252 I print_info: n_ctx_orig_yarn  = 8192
0.00.322.252 I print_info: rope_finetuned   = unknown
0.00.322.253 I print_info: ssm_d_conv       = 0
0.00.322.253 I print_info: ssm_d_inner      = 0
0.00.322.253 I print_info: ssm_d_state      = 0
0.00.322.253 I print_info: ssm_dt_rank      = 0
0.00.322.253 I print_info: ssm_dt_b_c_rms   = 0
0.00.322.253 I print_info: model type       = 33M
0.00.322.254 I print_info: model params     = 32.90 M
0.00.322.254 I print_info: general.name     = Jina Bert Implementation
0.00.322.255 I print_info: vocab type       = BPE
0.00.322.255 I print_info: n_vocab          = 61056
0.00.322.256 I print_info: n_merges         = 39382
0.00.322.256 I print_info: BOS token        = 0 '<s>'
0.00.322.256 I print_info: EOS token        = 2 '</s>'
0.00.322.256 I print_info: UNK token        = 3 '<unk>'
0.00.322.256 I print_info: SEP token        = 2 '</s>'
0.00.322.257 I print_info: PAD token        = 1 '<pad>'
0.00.322.257 I print_info: MASK token       = 4 '<mask>'
0.00.322.257 I print_info: EOG token        = 2 '</s>'
0.00.322.257 I print_info: max token length = 45
0.00.322.258 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.324.372 I load_tensors: offloading 4 repeating layers to GPU
0.00.324.373 I load_tensors: offloading output layer to GPU
0.00.324.374 I load_tensors: offloaded 5/5 layers to GPU
0.00.324.397 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.324.398 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.324.691 I llama_init_from_model: n_seq_max     = 1
0.00.324.693 I llama_init_from_model: n_ctx         = 8192
0.00.324.693 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.324.693 I llama_init_from_model: n_batch       = 2048
0.00.324.693 I llama_init_from_model: n_ubatch      = 2048
0.00.324.693 I llama_init_from_model: flash_attn    = 0
0.00.324.694 I llama_init_from_model: freq_base     = 10000.0
0.00.324.694 I llama_init_from_model: freq_scale    = 1
0.00.324.694 I ggml_metal_init: allocating
0.00.324.698 I ggml_metal_init: found device: Apple M4
0.00.324.701 I ggml_metal_init: picking default device: Apple M4
0.00.325.399 I ggml_metal_init: using embedded metal library
0.00.327.910 I ggml_metal_init: GPU name:   Apple M4
0.00.327.912 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.327.912 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.327.912 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.327.913 I ggml_metal_init: simdgroup reduction   = true
0.00.327.913 I ggml_metal_init: simdgroup matrix mul. = true
0.00.327.913 I ggml_metal_init: has residency sets    = true
0.00.327.913 I ggml_metal_init: has bfloat            = true
0.00.327.913 I ggml_metal_init: use bfloat            = true
0.00.327.914 I ggml_metal_init: hasUnifiedMemory      = true
0.00.327.914 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.337.643 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.340.645 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.340.647 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.340.649 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.347.320 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.347.322 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.347.322 I llama_init_from_model: graph nodes  = 154
0.00.347.323 I llama_init_from_model: graph splits = 2
0.00.347.324 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.347.324 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.354.721 I 
0.00.354.753 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.354.848 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.354.848 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.354.852 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.354.852 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.354.856 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.354.856 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.355.351 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.358.108 I llama_perf_context_print:        load time =     332.13 ms
0.00.358.109 I llama_perf_context_print: prompt eval time =       2.75 ms /    62 tokens (    0.04 ms per token, 22553.66 tokens per second)
0.00.358.110 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.358.110 I llama_perf_context_print:       total time =       3.39 ms /    63 tokens
0.00.358.373 I ggml_metal_free: deallocating

real	0m1.143s
user	0m0.329s
sys	0m0.051s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.160 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.319 I main: llama backend init
0.00.000.326 I main: load the model and apply lora adapter, if any
0.00.047.486 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.060.347 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.060.363 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.060.372 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.060.373 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.060.374 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.060.374 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.060.375 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.060.377 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.060.377 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.060.378 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.060.379 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.060.379 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.060.380 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.060.380 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.060.386 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.060.386 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.060.387 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.067.484 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.069.684 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.078.111 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.078.120 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.078.121 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.078.122 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.078.122 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.078.123 I llama_model_loader: - type  f32:  194 tensors
0.00.078.124 I llama_model_loader: - type  f16:   98 tensors
0.00.078.125 I print_info: file format = GGUF V3 (latest)
0.00.078.127 I print_info: file type   = all F32 (guessed)
0.00.078.128 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.093.896 I load: special tokens cache size = 25
0.00.104.090 I load: token to piece cache size = 0.2984 MB
0.00.104.117 I print_info: arch             = gptneox
0.00.104.119 I print_info: vocab_only       = 0
0.00.104.119 I print_info: n_ctx_train      = 2048
0.00.104.120 I print_info: n_embd           = 2048
0.00.104.120 I print_info: n_layer          = 24
0.00.104.125 I print_info: n_head           = 16
0.00.104.126 I print_info: n_head_kv        = 16
0.00.104.127 I print_info: n_rot            = 32
0.00.104.127 I print_info: n_swa            = 0
0.00.104.127 I print_info: n_embd_head_k    = 128
0.00.104.128 I print_info: n_embd_head_v    = 128
0.00.104.129 I print_info: n_gqa            = 1
0.00.104.130 I print_info: n_embd_k_gqa     = 2048
0.00.104.132 I print_info: n_embd_v_gqa     = 2048
0.00.104.133 I print_info: f_norm_eps       = 1.0e-05
0.00.104.134 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.104.134 I print_info: f_clamp_kqv      = 0.0e+00
0.00.104.134 I print_info: f_max_alibi_bias = 0.0e+00
0.00.104.135 I print_info: f_logit_scale    = 0.0e+00
0.00.104.136 I print_info: n_ff             = 8192
0.00.104.136 I print_info: n_expert         = 0
0.00.104.136 I print_info: n_expert_used    = 0
0.00.104.136 I print_info: causal attn      = 1
0.00.104.136 I print_info: pooling type     = 0
0.00.104.137 I print_info: rope type        = 2
0.00.104.137 I print_info: rope scaling     = linear
0.00.104.140 I print_info: freq_base_train  = 10000.0
0.00.104.140 I print_info: freq_scale_train = 1
0.00.104.140 I print_info: n_ctx_orig_yarn  = 2048
0.00.104.141 I print_info: rope_finetuned   = unknown
0.00.104.141 I print_info: ssm_d_conv       = 0
0.00.104.141 I print_info: ssm_d_inner      = 0
0.00.104.141 I print_info: ssm_d_state      = 0
0.00.104.141 I print_info: ssm_dt_rank      = 0
0.00.104.141 I print_info: ssm_dt_b_c_rms   = 0
0.00.104.142 I print_info: model type       = 1.4B
0.00.104.143 I print_info: model params     = 1.41 B
0.00.104.143 I print_info: general.name     = 1.4B
0.00.104.143 I print_info: vocab type       = BPE
0.00.104.144 I print_info: n_vocab          = 50304
0.00.104.144 I print_info: n_merges         = 50009
0.00.104.149 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.104.149 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.104.149 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.104.149 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.104.150 I print_info: LF token         = 187 ''
0.00.104.150 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.104.150 I print_info: max token length = 1024
0.00.104.152 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.157.309 I load_tensors: offloading 24 repeating layers to GPU
0.00.157.312 I load_tensors: offloading output layer to GPU
0.00.157.312 I load_tensors: offloaded 25/25 layers to GPU
0.00.157.338 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.157.339 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.157.959 I llama_init_from_model: n_seq_max     = 1
0.00.157.960 I llama_init_from_model: n_ctx         = 2048
0.00.157.960 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.157.960 I llama_init_from_model: n_batch       = 2048
0.00.157.960 I llama_init_from_model: n_ubatch      = 512
0.00.157.960 I llama_init_from_model: flash_attn    = 0
0.00.157.961 I llama_init_from_model: freq_base     = 10000.0
0.00.157.961 I llama_init_from_model: freq_scale    = 1
0.00.157.963 I ggml_metal_init: allocating
0.00.158.002 I ggml_metal_init: found device: Apple M4
0.00.158.009 I ggml_metal_init: picking default device: Apple M4
0.00.158.573 I ggml_metal_init: using embedded metal library
0.00.172.830 I ggml_metal_init: GPU name:   Apple M4
0.00.172.832 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.172.832 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.172.833 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.172.833 I ggml_metal_init: simdgroup reduction   = true
0.00.172.833 I ggml_metal_init: simdgroup matrix mul. = true
0.00.172.833 I ggml_metal_init: has residency sets    = true
0.00.172.833 I ggml_metal_init: has bfloat            = true
0.00.172.833 I ggml_metal_init: use bfloat            = true
0.00.172.834 I ggml_metal_init: hasUnifiedMemory      = true
0.00.172.834 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.199.581 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.230.030 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.230.036 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.230.062 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.233.550 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.233.552 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.233.552 I llama_init_from_model: graph nodes  = 967
0.00.233.552 I llama_init_from_model: graph splits = 2
0.00.233.557 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.233.687 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.233.688 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.297.281 I main: llama threadpool init, n_threads = 4
0.00.297.339 I 
0.00.297.368 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.297.369 I 
0.00.297.557 I sampler seed: 1234
0.00.297.562 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.297.596 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.297.597 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.297.597 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.132.547 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58726.22 tokens per second)
0.02.132.548 I llama_perf_context_print:        load time =     248.89 ms
0.02.132.549 I llama_perf_context_print: prompt eval time =      43.62 ms /     7 tokens (    6.23 ms per token,   160.48 tokens per second)
0.02.132.549 I llama_perf_context_print:        eval time =    1788.49 ms /    63 runs   (   28.39 ms per token,    35.23 tokens per second)
0.02.132.550 I llama_perf_context_print:       total time =    1836.16 ms /    70 tokens
0.02.132.782 I ggml_metal_free: deallocating

real	0m2.495s
user	0m0.134s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.625 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.304 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.688 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.695 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.698 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.699 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.700 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.700 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.701 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.703 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.703 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.704 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.704 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.705 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.706 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.707 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.709 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.710 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.711 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.233 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.158 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.765 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.053.767 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.768 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.768 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.768 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.769 I llama_model_loader: - type  f32:  194 tensors
0.00.053.769 I llama_model_loader: - type  f16:   98 tensors
0.00.053.770 I print_info: file format = GGUF V3 (latest)
0.00.053.771 I print_info: file type   = all F32 (guessed)
0.00.053.771 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.065.566 I load: special tokens cache size = 25
0.00.073.773 I load: token to piece cache size = 0.2984 MB
0.00.073.788 I print_info: arch             = gptneox
0.00.073.789 I print_info: vocab_only       = 0
0.00.073.790 I print_info: n_ctx_train      = 2048
0.00.073.790 I print_info: n_embd           = 2048
0.00.073.790 I print_info: n_layer          = 24
0.00.073.794 I print_info: n_head           = 16
0.00.073.794 I print_info: n_head_kv        = 16
0.00.073.794 I print_info: n_rot            = 32
0.00.073.795 I print_info: n_swa            = 0
0.00.073.795 I print_info: n_embd_head_k    = 128
0.00.073.796 I print_info: n_embd_head_v    = 128
0.00.073.797 I print_info: n_gqa            = 1
0.00.073.798 I print_info: n_embd_k_gqa     = 2048
0.00.073.798 I print_info: n_embd_v_gqa     = 2048
0.00.073.799 I print_info: f_norm_eps       = 1.0e-05
0.00.073.799 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.073.800 I print_info: f_clamp_kqv      = 0.0e+00
0.00.073.800 I print_info: f_max_alibi_bias = 0.0e+00
0.00.073.800 I print_info: f_logit_scale    = 0.0e+00
0.00.073.801 I print_info: n_ff             = 8192
0.00.073.801 I print_info: n_expert         = 0
0.00.073.801 I print_info: n_expert_used    = 0
0.00.073.801 I print_info: causal attn      = 1
0.00.073.801 I print_info: pooling type     = 0
0.00.073.801 I print_info: rope type        = 2
0.00.073.802 I print_info: rope scaling     = linear
0.00.073.802 I print_info: freq_base_train  = 10000.0
0.00.073.803 I print_info: freq_scale_train = 1
0.00.073.803 I print_info: n_ctx_orig_yarn  = 2048
0.00.073.803 I print_info: rope_finetuned   = unknown
0.00.073.803 I print_info: ssm_d_conv       = 0
0.00.073.803 I print_info: ssm_d_inner      = 0
0.00.073.804 I print_info: ssm_d_state      = 0
0.00.073.805 I print_info: ssm_dt_rank      = 0
0.00.073.806 I print_info: ssm_dt_b_c_rms   = 0
0.00.073.806 I print_info: model type       = 1.4B
0.00.073.806 I print_info: model params     = 1.41 B
0.00.073.806 I print_info: general.name     = 1.4B
0.00.073.807 I print_info: vocab type       = BPE
0.00.073.808 I print_info: n_vocab          = 50304
0.00.073.809 I print_info: n_merges         = 50009
0.00.073.809 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.073.809 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.073.809 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.073.809 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.073.810 I print_info: LF token         = 187 ''
0.00.073.811 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.073.811 I print_info: max token length = 1024
0.00.073.812 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.407.420 I load_tensors: offloading 24 repeating layers to GPU
0.01.407.427 I load_tensors: offloading output layer to GPU
0.01.407.428 I load_tensors: offloaded 25/25 layers to GPU
0.01.407.454 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.407.455 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.408.622 I llama_init_from_model: n_seq_max     = 1
0.01.408.623 I llama_init_from_model: n_ctx         = 128
0.01.408.624 I llama_init_from_model: n_ctx_per_seq = 128
0.01.408.624 I llama_init_from_model: n_batch       = 128
0.01.408.624 I llama_init_from_model: n_ubatch      = 128
0.01.408.624 I llama_init_from_model: flash_attn    = 0
0.01.408.625 I llama_init_from_model: freq_base     = 10000.0
0.01.408.625 I llama_init_from_model: freq_scale    = 1
0.01.408.625 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.408.626 I ggml_metal_init: allocating
0.01.408.651 I ggml_metal_init: found device: Apple M4
0.01.408.656 I ggml_metal_init: picking default device: Apple M4
0.01.409.526 I ggml_metal_init: using embedded metal library
0.01.413.573 I ggml_metal_init: GPU name:   Apple M4
0.01.413.575 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.413.576 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.413.576 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.413.577 I ggml_metal_init: simdgroup reduction   = true
0.01.413.577 I ggml_metal_init: simdgroup matrix mul. = true
0.01.413.577 I ggml_metal_init: has residency sets    = true
0.01.413.577 I ggml_metal_init: has bfloat            = true
0.01.413.577 I ggml_metal_init: use bfloat            = true
0.01.413.578 I ggml_metal_init: hasUnifiedMemory      = true
0.01.413.578 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.425.656 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.427.364 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.427.367 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.427.396 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.428.949 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.428.950 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.428.951 I llama_init_from_model: graph nodes  = 967
0.01.428.951 I llama_init_from_model: graph splits = 2
0.01.428.952 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.428.953 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.462.331 I 
0.01.462.372 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.462.403 I perplexity: tokenizing the input ..
0.01.467.309 I perplexity: tokenization took 4.904 ms
0.01.467.313 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.586.340 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.587.779 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.587.797 I llama_perf_context_print:        load time =    1441.02 ms
0.01.587.798 I llama_perf_context_print: prompt eval time =     118.76 ms /   128 tokens (    0.93 ms per token,  1077.78 tokens per second)
0.01.587.799 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.587.799 I llama_perf_context_print:       total time =     125.47 ms /   129 tokens
0.01.588.230 I ggml_metal_free: deallocating

real	0m1.824s
user	0m0.097s
sys	0m0.256s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.056 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.145 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.151 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.154 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.154 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.154 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.155 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.155 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.156 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.157 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.157 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.157 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.158 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.159 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.160 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.161 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.162 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.162 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.933 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.961 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.679 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.681 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.681 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.682 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.682 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.683 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.683 I llama_model_loader: - type  f32:  194 tensors
0.00.034.684 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.684 I print_info: file format = GGUF V3 (latest)
0.00.034.685 I print_info: file type   = Q8_0
0.00.034.687 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.043.917 I load: special tokens cache size = 25
0.00.050.852 I load: token to piece cache size = 0.2984 MB
0.00.050.869 I print_info: arch             = gptneox
0.00.050.870 I print_info: vocab_only       = 0
0.00.050.871 I print_info: n_ctx_train      = 2048
0.00.050.871 I print_info: n_embd           = 2048
0.00.050.871 I print_info: n_layer          = 24
0.00.050.878 I print_info: n_head           = 16
0.00.050.879 I print_info: n_head_kv        = 16
0.00.050.879 I print_info: n_rot            = 32
0.00.050.879 I print_info: n_swa            = 0
0.00.050.879 I print_info: n_embd_head_k    = 128
0.00.050.879 I print_info: n_embd_head_v    = 128
0.00.050.880 I print_info: n_gqa            = 1
0.00.050.881 I print_info: n_embd_k_gqa     = 2048
0.00.050.882 I print_info: n_embd_v_gqa     = 2048
0.00.050.882 I print_info: f_norm_eps       = 1.0e-05
0.00.050.886 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.886 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.886 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.886 I print_info: f_logit_scale    = 0.0e+00
0.00.050.887 I print_info: n_ff             = 8192
0.00.050.887 I print_info: n_expert         = 0
0.00.050.888 I print_info: n_expert_used    = 0
0.00.050.889 I print_info: causal attn      = 1
0.00.050.889 I print_info: pooling type     = 0
0.00.050.889 I print_info: rope type        = 2
0.00.050.889 I print_info: rope scaling     = linear
0.00.050.890 I print_info: freq_base_train  = 10000.0
0.00.050.890 I print_info: freq_scale_train = 1
0.00.050.891 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.891 I print_info: rope_finetuned   = unknown
0.00.050.891 I print_info: ssm_d_conv       = 0
0.00.050.891 I print_info: ssm_d_inner      = 0
0.00.050.892 I print_info: ssm_d_state      = 0
0.00.050.892 I print_info: ssm_dt_rank      = 0
0.00.050.892 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.892 I print_info: model type       = 1.4B
0.00.050.893 I print_info: model params     = 1.41 B
0.00.050.893 I print_info: general.name     = 1.4B
0.00.050.894 I print_info: vocab type       = BPE
0.00.050.894 I print_info: n_vocab          = 50304
0.00.050.895 I print_info: n_merges         = 50009
0.00.050.895 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.896 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.896 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.896 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.897 I print_info: LF token         = 187 ''
0.00.050.897 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.897 I print_info: max token length = 1024
0.00.050.898 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.031.930 I load_tensors: offloading 24 repeating layers to GPU
0.01.031.935 I load_tensors: offloading output layer to GPU
0.01.031.936 I load_tensors: offloaded 25/25 layers to GPU
0.01.031.958 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.031.960 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.032.838 I llama_init_from_model: n_seq_max     = 1
0.01.032.840 I llama_init_from_model: n_ctx         = 2048
0.01.032.840 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.032.841 I llama_init_from_model: n_batch       = 2048
0.01.032.841 I llama_init_from_model: n_ubatch      = 512
0.01.032.841 I llama_init_from_model: flash_attn    = 0
0.01.032.842 I llama_init_from_model: freq_base     = 10000.0
0.01.032.843 I llama_init_from_model: freq_scale    = 1
0.01.032.844 I ggml_metal_init: allocating
0.01.032.856 I ggml_metal_init: found device: Apple M4
0.01.032.863 I ggml_metal_init: picking default device: Apple M4
0.01.033.978 I ggml_metal_init: using embedded metal library
0.01.039.316 I ggml_metal_init: GPU name:   Apple M4
0.01.039.319 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.039.320 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.039.321 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.039.321 I ggml_metal_init: simdgroup reduction   = true
0.01.039.321 I ggml_metal_init: simdgroup matrix mul. = true
0.01.039.322 I ggml_metal_init: has residency sets    = true
0.01.039.322 I ggml_metal_init: has bfloat            = true
0.01.039.322 I ggml_metal_init: use bfloat            = true
0.01.039.323 I ggml_metal_init: hasUnifiedMemory      = true
0.01.039.324 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.055.518 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.110.878 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.110.884 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.110.907 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.115.651 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.115.653 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.115.653 I llama_init_from_model: graph nodes  = 967
0.01.115.653 I llama_init_from_model: graph splits = 2
0.01.115.662 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.115.800 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.115.800 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.170.997 I main: llama threadpool init, n_threads = 4
0.01.171.040 I 
0.01.171.059 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.171.060 I 
0.01.171.218 I sampler seed: 1234
0.01.171.223 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.171.238 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.171.238 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.171.238 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.265.359 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54911.06 tokens per second)
0.02.265.360 I llama_perf_context_print:        load time =    1160.24 ms
0.02.265.361 I llama_perf_context_print: prompt eval time =      46.97 ms /     7 tokens (    6.71 ms per token,   149.05 tokens per second)
0.02.265.363 I llama_perf_context_print:        eval time =    1044.35 ms /    63 runs   (   16.58 ms per token,    60.32 tokens per second)
0.02.265.363 I llama_perf_context_print:       total time =    1095.06 ms /    70 tokens
0.02.265.580 I ggml_metal_free: deallocating

real	0m2.284s
user	0m0.110s
sys	0m0.269s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.418 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.728 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.734 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.736 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.739 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.739 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.739 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.740 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.741 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.741 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.741 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.742 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.742 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.743 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.743 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.745 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.746 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.746 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.478 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.490 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.288 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.290 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.290 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.290 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.290 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.291 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.291 I llama_model_loader: - type  f32:  194 tensors
0.00.025.292 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.293 I print_info: file format = GGUF V3 (latest)
0.00.025.293 I print_info: file type   = Q8_0
0.00.025.298 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.430 I load: special tokens cache size = 25
0.00.039.554 I load: token to piece cache size = 0.2984 MB
0.00.039.571 I print_info: arch             = gptneox
0.00.039.572 I print_info: vocab_only       = 0
0.00.039.572 I print_info: n_ctx_train      = 2048
0.00.039.572 I print_info: n_embd           = 2048
0.00.039.572 I print_info: n_layer          = 24
0.00.039.576 I print_info: n_head           = 16
0.00.039.577 I print_info: n_head_kv        = 16
0.00.039.578 I print_info: n_rot            = 32
0.00.039.578 I print_info: n_swa            = 0
0.00.039.578 I print_info: n_embd_head_k    = 128
0.00.039.578 I print_info: n_embd_head_v    = 128
0.00.039.578 I print_info: n_gqa            = 1
0.00.039.579 I print_info: n_embd_k_gqa     = 2048
0.00.039.580 I print_info: n_embd_v_gqa     = 2048
0.00.039.580 I print_info: f_norm_eps       = 1.0e-05
0.00.039.581 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.581 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.581 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.581 I print_info: f_logit_scale    = 0.0e+00
0.00.039.582 I print_info: n_ff             = 8192
0.00.039.582 I print_info: n_expert         = 0
0.00.039.582 I print_info: n_expert_used    = 0
0.00.039.582 I print_info: causal attn      = 1
0.00.039.582 I print_info: pooling type     = 0
0.00.039.582 I print_info: rope type        = 2
0.00.039.583 I print_info: rope scaling     = linear
0.00.039.583 I print_info: freq_base_train  = 10000.0
0.00.039.583 I print_info: freq_scale_train = 1
0.00.039.583 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.584 I print_info: rope_finetuned   = unknown
0.00.039.584 I print_info: ssm_d_conv       = 0
0.00.039.584 I print_info: ssm_d_inner      = 0
0.00.039.584 I print_info: ssm_d_state      = 0
0.00.039.584 I print_info: ssm_dt_rank      = 0
0.00.039.584 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.584 I print_info: model type       = 1.4B
0.00.039.585 I print_info: model params     = 1.41 B
0.00.039.585 I print_info: general.name     = 1.4B
0.00.039.585 I print_info: vocab type       = BPE
0.00.039.585 I print_info: n_vocab          = 50304
0.00.039.586 I print_info: n_merges         = 50009
0.00.039.586 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.586 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.586 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.586 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.587 I print_info: LF token         = 187 ''
0.00.039.587 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.587 I print_info: max token length = 1024
0.00.039.587 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.864.102 I load_tensors: offloading 24 repeating layers to GPU
0.00.864.109 I load_tensors: offloading output layer to GPU
0.00.864.110 I load_tensors: offloaded 25/25 layers to GPU
0.00.864.136 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.864.139 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.865.517 I llama_init_from_model: n_seq_max     = 1
0.00.865.519 I llama_init_from_model: n_ctx         = 128
0.00.865.519 I llama_init_from_model: n_ctx_per_seq = 128
0.00.865.520 I llama_init_from_model: n_batch       = 128
0.00.865.520 I llama_init_from_model: n_ubatch      = 128
0.00.865.520 I llama_init_from_model: flash_attn    = 0
0.00.865.521 I llama_init_from_model: freq_base     = 10000.0
0.00.865.522 I llama_init_from_model: freq_scale    = 1
0.00.865.522 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.865.524 I ggml_metal_init: allocating
0.00.865.586 I ggml_metal_init: found device: Apple M4
0.00.865.597 I ggml_metal_init: picking default device: Apple M4
0.00.866.839 I ggml_metal_init: using embedded metal library
0.00.872.322 I ggml_metal_init: GPU name:   Apple M4
0.00.872.326 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.872.326 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.872.327 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.872.327 I ggml_metal_init: simdgroup reduction   = true
0.00.872.327 I ggml_metal_init: simdgroup matrix mul. = true
0.00.872.327 I ggml_metal_init: has residency sets    = true
0.00.872.328 I ggml_metal_init: has bfloat            = true
0.00.872.328 I ggml_metal_init: use bfloat            = true
0.00.872.329 I ggml_metal_init: hasUnifiedMemory      = true
0.00.872.337 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.887.573 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.890.128 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.890.131 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.890.151 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.892.559 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.892.560 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.892.561 I llama_init_from_model: graph nodes  = 967
0.00.892.561 I llama_init_from_model: graph splits = 2
0.00.892.563 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.892.564 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.917.657 I 
0.00.917.711 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.917.731 I perplexity: tokenizing the input ..
0.00.923.798 I perplexity: tokenization took 6.065 ms
0.00.923.807 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.060.803 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.062.149 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.062.163 I llama_perf_context_print:        load time =     908.23 ms
0.01.062.164 I llama_perf_context_print: prompt eval time =     136.77 ms /   128 tokens (    1.07 ms per token,   935.91 tokens per second)
0.01.062.165 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.062.165 I llama_perf_context_print:       total time =     144.51 ms /   129 tokens
0.01.062.516 I ggml_metal_free: deallocating

real	0m1.076s
user	0m0.075s
sys	0m0.165s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.028.239 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.691 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.038.698 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.700 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.700 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.701 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.701 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.702 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.703 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.703 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.704 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.704 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.705 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.705 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.706 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.708 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.708 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.709 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.941 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.377 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.527 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.050.529 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.529 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.529 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.530 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.530 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.050.531 I llama_model_loader: - type  f32:  194 tensors
0.00.050.531 I llama_model_loader: - type q4_0:   97 tensors
0.00.050.532 I llama_model_loader: - type q6_K:    1 tensors
0.00.050.532 I print_info: file format = GGUF V3 (latest)
0.00.050.535 I print_info: file type   = Q4_0
0.00.050.536 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.063.090 I load: special tokens cache size = 25
0.00.076.378 I load: token to piece cache size = 0.2984 MB
0.00.076.396 I print_info: arch             = gptneox
0.00.076.398 I print_info: vocab_only       = 0
0.00.076.399 I print_info: n_ctx_train      = 2048
0.00.076.399 I print_info: n_embd           = 2048
0.00.076.399 I print_info: n_layer          = 24
0.00.076.404 I print_info: n_head           = 16
0.00.076.406 I print_info: n_head_kv        = 16
0.00.076.406 I print_info: n_rot            = 32
0.00.076.406 I print_info: n_swa            = 0
0.00.076.407 I print_info: n_embd_head_k    = 128
0.00.076.407 I print_info: n_embd_head_v    = 128
0.00.076.410 I print_info: n_gqa            = 1
0.00.076.411 I print_info: n_embd_k_gqa     = 2048
0.00.076.413 I print_info: n_embd_v_gqa     = 2048
0.00.076.414 I print_info: f_norm_eps       = 1.0e-05
0.00.076.414 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.415 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.415 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.415 I print_info: f_logit_scale    = 0.0e+00
0.00.076.416 I print_info: n_ff             = 8192
0.00.076.424 I print_info: n_expert         = 0
0.00.076.427 I print_info: n_expert_used    = 0
0.00.076.427 I print_info: causal attn      = 1
0.00.076.428 I print_info: pooling type     = 0
0.00.076.428 I print_info: rope type        = 2
0.00.076.428 I print_info: rope scaling     = linear
0.00.076.429 I print_info: freq_base_train  = 10000.0
0.00.076.429 I print_info: freq_scale_train = 1
0.00.076.432 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.432 I print_info: rope_finetuned   = unknown
0.00.076.433 I print_info: ssm_d_conv       = 0
0.00.076.433 I print_info: ssm_d_inner      = 0
0.00.076.433 I print_info: ssm_d_state      = 0
0.00.076.433 I print_info: ssm_dt_rank      = 0
0.00.076.436 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.436 I print_info: model type       = 1.4B
0.00.076.437 I print_info: model params     = 1.41 B
0.00.076.437 I print_info: general.name     = 1.4B
0.00.076.438 I print_info: vocab type       = BPE
0.00.076.438 I print_info: n_vocab          = 50304
0.00.076.439 I print_info: n_merges         = 50009
0.00.076.439 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.439 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.440 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.440 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.440 I print_info: LF token         = 187 ''
0.00.076.441 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.441 I print_info: max token length = 1024
0.00.076.447 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.645.456 I load_tensors: offloading 24 repeating layers to GPU
0.00.645.472 I load_tensors: offloading output layer to GPU
0.00.645.473 I load_tensors: offloaded 25/25 layers to GPU
0.00.645.506 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.645.507 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.647.169 I llama_init_from_model: n_seq_max     = 1
0.00.647.172 I llama_init_from_model: n_ctx         = 2048
0.00.647.173 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.647.173 I llama_init_from_model: n_batch       = 2048
0.00.647.174 I llama_init_from_model: n_ubatch      = 512
0.00.647.174 I llama_init_from_model: flash_attn    = 0
0.00.647.176 I llama_init_from_model: freq_base     = 10000.0
0.00.647.177 I llama_init_from_model: freq_scale    = 1
0.00.647.179 I ggml_metal_init: allocating
0.00.647.255 I ggml_metal_init: found device: Apple M4
0.00.647.269 I ggml_metal_init: picking default device: Apple M4
0.00.648.820 I ggml_metal_init: using embedded metal library
0.00.655.121 I ggml_metal_init: GPU name:   Apple M4
0.00.655.126 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.655.126 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.655.127 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.655.128 I ggml_metal_init: simdgroup reduction   = true
0.00.655.128 I ggml_metal_init: simdgroup matrix mul. = true
0.00.655.129 I ggml_metal_init: has residency sets    = true
0.00.655.129 I ggml_metal_init: has bfloat            = true
0.00.655.129 I ggml_metal_init: use bfloat            = true
0.00.655.130 I ggml_metal_init: hasUnifiedMemory      = true
0.00.655.135 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.674.147 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.731.836 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.731.842 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.731.867 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.737.234 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.737.236 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.737.236 I llama_init_from_model: graph nodes  = 967
0.00.737.236 I llama_init_from_model: graph splits = 2
0.00.737.241 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.737.354 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.737.355 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.795.272 I main: llama threadpool init, n_threads = 4
0.00.795.325 I 
0.00.795.346 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.795.347 I 
0.00.795.498 I sampler seed: 1234
0.00.795.503 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.795.518 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.795.519 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.795.519 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.491.028 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 49134.95 tokens per second)
0.01.491.029 I llama_perf_context_print:        load time =     766.24 ms
0.01.491.030 I llama_perf_context_print: prompt eval time =      49.26 ms /     7 tokens (    7.04 ms per token,   142.09 tokens per second)
0.01.491.030 I llama_perf_context_print:        eval time =     643.36 ms /    63 runs   (   10.21 ms per token,    97.92 tokens per second)
0.01.491.031 I llama_perf_context_print:       total time =     696.54 ms /    70 tokens
0.01.491.281 I ggml_metal_free: deallocating

real	0m1.525s
user	0m0.128s
sys	0m0.209s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.306 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.311 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.316 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.320 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.321 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.321 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.321 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.321 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.322 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.323 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.323 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.324 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.324 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.326 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.326 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.328 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.329 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.329 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.064 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.109 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.829 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.831 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.832 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.832 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.832 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.833 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.833 I llama_model_loader: - type  f32:  194 tensors
0.00.025.834 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.834 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.835 I print_info: file format = GGUF V3 (latest)
0.00.025.835 I print_info: file type   = Q4_0
0.00.025.836 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.157 I load: special tokens cache size = 25
0.00.040.839 I load: token to piece cache size = 0.2984 MB
0.00.040.857 I print_info: arch             = gptneox
0.00.040.858 I print_info: vocab_only       = 0
0.00.040.858 I print_info: n_ctx_train      = 2048
0.00.040.858 I print_info: n_embd           = 2048
0.00.040.858 I print_info: n_layer          = 24
0.00.040.862 I print_info: n_head           = 16
0.00.040.862 I print_info: n_head_kv        = 16
0.00.040.863 I print_info: n_rot            = 32
0.00.040.863 I print_info: n_swa            = 0
0.00.040.863 I print_info: n_embd_head_k    = 128
0.00.040.863 I print_info: n_embd_head_v    = 128
0.00.040.864 I print_info: n_gqa            = 1
0.00.040.864 I print_info: n_embd_k_gqa     = 2048
0.00.040.865 I print_info: n_embd_v_gqa     = 2048
0.00.040.865 I print_info: f_norm_eps       = 1.0e-05
0.00.040.866 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.866 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.866 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.866 I print_info: f_logit_scale    = 0.0e+00
0.00.040.867 I print_info: n_ff             = 8192
0.00.040.867 I print_info: n_expert         = 0
0.00.040.867 I print_info: n_expert_used    = 0
0.00.040.867 I print_info: causal attn      = 1
0.00.040.867 I print_info: pooling type     = 0
0.00.040.867 I print_info: rope type        = 2
0.00.040.868 I print_info: rope scaling     = linear
0.00.040.868 I print_info: freq_base_train  = 10000.0
0.00.040.868 I print_info: freq_scale_train = 1
0.00.040.870 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.870 I print_info: rope_finetuned   = unknown
0.00.040.871 I print_info: ssm_d_conv       = 0
0.00.040.871 I print_info: ssm_d_inner      = 0
0.00.040.871 I print_info: ssm_d_state      = 0
0.00.040.871 I print_info: ssm_dt_rank      = 0
0.00.040.871 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.871 I print_info: model type       = 1.4B
0.00.040.872 I print_info: model params     = 1.41 B
0.00.040.872 I print_info: general.name     = 1.4B
0.00.040.872 I print_info: vocab type       = BPE
0.00.040.873 I print_info: n_vocab          = 50304
0.00.040.873 I print_info: n_merges         = 50009
0.00.040.873 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.873 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.873 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.873 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.874 I print_info: LF token         = 187 ''
0.00.040.874 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.874 I print_info: max token length = 1024
0.00.040.874 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.599.380 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.395 I load_tensors: offloading output layer to GPU
0.00.599.396 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.427 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.599.429 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.600.738 I llama_init_from_model: n_seq_max     = 1
0.00.600.741 I llama_init_from_model: n_ctx         = 128
0.00.600.742 I llama_init_from_model: n_ctx_per_seq = 128
0.00.600.742 I llama_init_from_model: n_batch       = 128
0.00.600.742 I llama_init_from_model: n_ubatch      = 128
0.00.600.743 I llama_init_from_model: flash_attn    = 0
0.00.600.745 I llama_init_from_model: freq_base     = 10000.0
0.00.600.745 I llama_init_from_model: freq_scale    = 1
0.00.600.746 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.600.748 I ggml_metal_init: allocating
0.00.600.829 I ggml_metal_init: found device: Apple M4
0.00.600.843 I ggml_metal_init: picking default device: Apple M4
0.00.602.403 I ggml_metal_init: using embedded metal library
0.00.609.282 I ggml_metal_init: GPU name:   Apple M4
0.00.609.290 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.609.291 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.609.292 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.609.293 I ggml_metal_init: simdgroup reduction   = true
0.00.609.293 I ggml_metal_init: simdgroup matrix mul. = true
0.00.609.293 I ggml_metal_init: has residency sets    = true
0.00.609.293 I ggml_metal_init: has bfloat            = true
0.00.609.294 I ggml_metal_init: use bfloat            = true
0.00.609.295 I ggml_metal_init: hasUnifiedMemory      = true
0.00.609.299 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.740 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.632.390 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.632.395 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.632.422 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.635.497 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.635.499 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.635.500 I llama_init_from_model: graph nodes  = 967
0.00.635.500 I llama_init_from_model: graph splits = 2
0.00.635.503 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.635.503 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.664.749 I 
0.00.664.850 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.664.877 I perplexity: tokenizing the input ..
0.00.671.741 I perplexity: tokenization took 6.859 ms
0.00.671.749 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.236 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.802.570 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.802.586 I llama_perf_context_print:        load time =     654.43 ms
0.00.802.587 I llama_perf_context_print: prompt eval time =     128.56 ms /   128 tokens (    1.00 ms per token,   995.68 tokens per second)
0.00.802.588 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.589 I llama_perf_context_print:       total time =     137.85 ms /   129 tokens
0.00.802.967 I ggml_metal_free: deallocating

real	0m0.819s
user	0m0.081s
sys	0m0.126s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.873 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.633 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.637 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.639 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.639 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.641 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.642 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.643 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.647 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.648 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.648 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.649 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.649 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.651 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.651 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.654 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.654 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.654 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.434 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.510 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.263 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.264 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.265 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.265 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.265 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.265 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.027.266 I llama_model_loader: - type  f32:  194 tensors
0.00.027.266 I llama_model_loader: - type q4_1:   97 tensors
0.00.027.267 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.267 I print_info: file format = GGUF V3 (latest)
0.00.027.268 I print_info: file type   = Q4_1
0.00.027.269 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.035.499 I load: special tokens cache size = 25
0.00.041.797 I load: token to piece cache size = 0.2984 MB
0.00.041.812 I print_info: arch             = gptneox
0.00.041.813 I print_info: vocab_only       = 0
0.00.041.813 I print_info: n_ctx_train      = 2048
0.00.041.813 I print_info: n_embd           = 2048
0.00.041.813 I print_info: n_layer          = 24
0.00.041.816 I print_info: n_head           = 16
0.00.041.817 I print_info: n_head_kv        = 16
0.00.041.817 I print_info: n_rot            = 32
0.00.041.817 I print_info: n_swa            = 0
0.00.041.817 I print_info: n_embd_head_k    = 128
0.00.041.819 I print_info: n_embd_head_v    = 128
0.00.041.820 I print_info: n_gqa            = 1
0.00.041.820 I print_info: n_embd_k_gqa     = 2048
0.00.041.821 I print_info: n_embd_v_gqa     = 2048
0.00.041.822 I print_info: f_norm_eps       = 1.0e-05
0.00.041.823 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.823 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.823 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.824 I print_info: f_logit_scale    = 0.0e+00
0.00.041.824 I print_info: n_ff             = 8192
0.00.041.824 I print_info: n_expert         = 0
0.00.041.824 I print_info: n_expert_used    = 0
0.00.041.824 I print_info: causal attn      = 1
0.00.041.824 I print_info: pooling type     = 0
0.00.041.826 I print_info: rope type        = 2
0.00.041.826 I print_info: rope scaling     = linear
0.00.041.830 I print_info: freq_base_train  = 10000.0
0.00.041.830 I print_info: freq_scale_train = 1
0.00.041.830 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.830 I print_info: rope_finetuned   = unknown
0.00.041.831 I print_info: ssm_d_conv       = 0
0.00.041.831 I print_info: ssm_d_inner      = 0
0.00.041.831 I print_info: ssm_d_state      = 0
0.00.041.831 I print_info: ssm_dt_rank      = 0
0.00.041.831 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.831 I print_info: model type       = 1.4B
0.00.041.832 I print_info: model params     = 1.41 B
0.00.041.832 I print_info: general.name     = 1.4B
0.00.041.832 I print_info: vocab type       = BPE
0.00.041.833 I print_info: n_vocab          = 50304
0.00.041.833 I print_info: n_merges         = 50009
0.00.041.833 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.833 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.833 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.833 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.833 I print_info: LF token         = 187 ''
0.00.041.834 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.834 I print_info: max token length = 1024
0.00.041.834 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.626.319 I load_tensors: offloading 24 repeating layers to GPU
0.00.626.337 I load_tensors: offloading output layer to GPU
0.00.626.337 I load_tensors: offloaded 25/25 layers to GPU
0.00.626.369 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.626.370 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.628.106 I llama_init_from_model: n_seq_max     = 1
0.00.628.109 I llama_init_from_model: n_ctx         = 2048
0.00.628.109 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.628.110 I llama_init_from_model: n_batch       = 2048
0.00.628.111 I llama_init_from_model: n_ubatch      = 512
0.00.628.111 I llama_init_from_model: flash_attn    = 0
0.00.628.113 I llama_init_from_model: freq_base     = 10000.0
0.00.628.113 I llama_init_from_model: freq_scale    = 1
0.00.628.117 I ggml_metal_init: allocating
0.00.628.192 I ggml_metal_init: found device: Apple M4
0.00.628.205 I ggml_metal_init: picking default device: Apple M4
0.00.629.951 I ggml_metal_init: using embedded metal library
0.00.636.493 I ggml_metal_init: GPU name:   Apple M4
0.00.636.497 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.636.497 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.636.498 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.636.499 I ggml_metal_init: simdgroup reduction   = true
0.00.636.499 I ggml_metal_init: simdgroup matrix mul. = true
0.00.636.499 I ggml_metal_init: has residency sets    = true
0.00.636.500 I ggml_metal_init: has bfloat            = true
0.00.636.500 I ggml_metal_init: use bfloat            = true
0.00.636.501 I ggml_metal_init: hasUnifiedMemory      = true
0.00.636.502 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.654.521 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.709.898 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.709.904 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.709.926 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.715.214 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.715.216 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.715.217 I llama_init_from_model: graph nodes  = 967
0.00.715.217 I llama_init_from_model: graph splits = 2
0.00.715.224 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.715.356 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.715.357 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.772.124 I main: llama threadpool init, n_threads = 4
0.00.772.166 I 
0.00.772.185 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.772.188 I 
0.00.772.350 I sampler seed: 1234
0.00.772.354 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.772.370 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.772.370 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.772.370 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.499.663 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55339.05 tokens per second)
0.01.499.664 I llama_perf_context_print:        load time =     762.50 ms
0.01.499.666 I llama_perf_context_print: prompt eval time =      48.97 ms /     7 tokens (    7.00 ms per token,   142.96 tokens per second)
0.01.499.666 I llama_perf_context_print:        eval time =     675.62 ms /    63 runs   (   10.72 ms per token,    93.25 tokens per second)
0.01.499.667 I llama_perf_context_print:       total time =     728.29 ms /    70 tokens
0.01.499.894 I ggml_metal_free: deallocating

real	0m1.514s
user	0m0.110s
sys	0m0.196s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.924 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.249 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.256 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.262 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.262 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.263 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.263 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.263 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.266 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.267 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.267 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.267 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.268 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.268 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.268 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.270 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.271 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.271 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.896 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.900 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.606 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.608 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.608 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.608 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.609 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.609 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.610 I llama_model_loader: - type  f32:  194 tensors
0.00.024.610 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.610 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.611 I print_info: file format = GGUF V3 (latest)
0.00.024.612 I print_info: file type   = Q4_1
0.00.024.613 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.008 I load: special tokens cache size = 25
0.00.039.580 I load: token to piece cache size = 0.2984 MB
0.00.039.599 I print_info: arch             = gptneox
0.00.039.599 I print_info: vocab_only       = 0
0.00.039.600 I print_info: n_ctx_train      = 2048
0.00.039.600 I print_info: n_embd           = 2048
0.00.039.600 I print_info: n_layer          = 24
0.00.039.604 I print_info: n_head           = 16
0.00.039.605 I print_info: n_head_kv        = 16
0.00.039.605 I print_info: n_rot            = 32
0.00.039.605 I print_info: n_swa            = 0
0.00.039.605 I print_info: n_embd_head_k    = 128
0.00.039.605 I print_info: n_embd_head_v    = 128
0.00.039.606 I print_info: n_gqa            = 1
0.00.039.607 I print_info: n_embd_k_gqa     = 2048
0.00.039.607 I print_info: n_embd_v_gqa     = 2048
0.00.039.608 I print_info: f_norm_eps       = 1.0e-05
0.00.039.608 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.608 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.608 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.609 I print_info: f_logit_scale    = 0.0e+00
0.00.039.609 I print_info: n_ff             = 8192
0.00.039.609 I print_info: n_expert         = 0
0.00.039.612 I print_info: n_expert_used    = 0
0.00.039.612 I print_info: causal attn      = 1
0.00.039.612 I print_info: pooling type     = 0
0.00.039.612 I print_info: rope type        = 2
0.00.039.613 I print_info: rope scaling     = linear
0.00.039.613 I print_info: freq_base_train  = 10000.0
0.00.039.613 I print_info: freq_scale_train = 1
0.00.039.613 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.614 I print_info: rope_finetuned   = unknown
0.00.039.614 I print_info: ssm_d_conv       = 0
0.00.039.615 I print_info: ssm_d_inner      = 0
0.00.039.615 I print_info: ssm_d_state      = 0
0.00.039.615 I print_info: ssm_dt_rank      = 0
0.00.039.616 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.616 I print_info: model type       = 1.4B
0.00.039.616 I print_info: model params     = 1.41 B
0.00.039.616 I print_info: general.name     = 1.4B
0.00.039.617 I print_info: vocab type       = BPE
0.00.039.617 I print_info: n_vocab          = 50304
0.00.039.617 I print_info: n_merges         = 50009
0.00.039.617 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.617 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.617 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.618 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.618 I print_info: LF token         = 187 ''
0.00.039.618 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.618 I print_info: max token length = 1024
0.00.039.619 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.633.145 I load_tensors: offloading 24 repeating layers to GPU
0.00.633.158 I load_tensors: offloading output layer to GPU
0.00.633.158 I load_tensors: offloaded 25/25 layers to GPU
0.00.633.191 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.633.193 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.634.545 I llama_init_from_model: n_seq_max     = 1
0.00.634.551 I llama_init_from_model: n_ctx         = 128
0.00.634.551 I llama_init_from_model: n_ctx_per_seq = 128
0.00.634.552 I llama_init_from_model: n_batch       = 128
0.00.634.552 I llama_init_from_model: n_ubatch      = 128
0.00.634.553 I llama_init_from_model: flash_attn    = 0
0.00.634.554 I llama_init_from_model: freq_base     = 10000.0
0.00.634.554 I llama_init_from_model: freq_scale    = 1
0.00.634.555 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.634.557 I ggml_metal_init: allocating
0.00.634.637 I ggml_metal_init: found device: Apple M4
0.00.634.652 I ggml_metal_init: picking default device: Apple M4
0.00.636.094 I ggml_metal_init: using embedded metal library
0.00.642.504 I ggml_metal_init: GPU name:   Apple M4
0.00.642.514 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.642.515 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.642.516 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.642.517 I ggml_metal_init: simdgroup reduction   = true
0.00.642.517 I ggml_metal_init: simdgroup matrix mul. = true
0.00.642.517 I ggml_metal_init: has residency sets    = true
0.00.642.517 I ggml_metal_init: has bfloat            = true
0.00.642.518 I ggml_metal_init: use bfloat            = true
0.00.642.519 I ggml_metal_init: hasUnifiedMemory      = true
0.00.642.532 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.662.038 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.665.683 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.665.687 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.665.718 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.669.153 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.669.155 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.669.155 I llama_init_from_model: graph nodes  = 967
0.00.669.156 I llama_init_from_model: graph splits = 2
0.00.669.159 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.669.159 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.698.367 I 
0.00.698.450 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.698.478 I perplexity: tokenizing the input ..
0.00.705.244 I perplexity: tokenization took 6.765 ms
0.00.705.248 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.840.768 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.842.082 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.842.094 I llama_perf_context_print:        load time =     689.43 ms
0.00.842.095 I llama_perf_context_print: prompt eval time =     135.29 ms /   128 tokens (    1.06 ms per token,   946.12 tokens per second)
0.00.842.096 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.842.096 I llama_perf_context_print:       total time =     143.73 ms /   129 tokens
0.00.842.480 I ggml_metal_free: deallocating

real	0m0.856s
user	0m0.079s
sys	0m0.127s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.897 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.632 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.636 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.638 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.639 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.639 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.639 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.640 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.641 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.641 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.642 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.642 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.642 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.643 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.643 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.645 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.646 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.646 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.426 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.477 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.262 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.263 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.264 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.264 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.264 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.265 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.265 I llama_model_loader: - type  f32:  194 tensors
0.00.025.265 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.266 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.266 I print_info: file format = GGUF V3 (latest)
0.00.025.267 I print_info: file type   = Q5_0
0.00.025.267 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.370 I load: special tokens cache size = 25
0.00.039.881 I load: token to piece cache size = 0.2984 MB
0.00.039.895 I print_info: arch             = gptneox
0.00.039.896 I print_info: vocab_only       = 0
0.00.039.896 I print_info: n_ctx_train      = 2048
0.00.039.897 I print_info: n_embd           = 2048
0.00.039.897 I print_info: n_layer          = 24
0.00.039.899 I print_info: n_head           = 16
0.00.039.900 I print_info: n_head_kv        = 16
0.00.039.900 I print_info: n_rot            = 32
0.00.039.900 I print_info: n_swa            = 0
0.00.039.901 I print_info: n_embd_head_k    = 128
0.00.039.901 I print_info: n_embd_head_v    = 128
0.00.039.901 I print_info: n_gqa            = 1
0.00.039.902 I print_info: n_embd_k_gqa     = 2048
0.00.039.903 I print_info: n_embd_v_gqa     = 2048
0.00.039.903 I print_info: f_norm_eps       = 1.0e-05
0.00.039.904 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.904 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.904 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.906 I print_info: f_logit_scale    = 0.0e+00
0.00.039.906 I print_info: n_ff             = 8192
0.00.039.907 I print_info: n_expert         = 0
0.00.039.907 I print_info: n_expert_used    = 0
0.00.039.907 I print_info: causal attn      = 1
0.00.039.907 I print_info: pooling type     = 0
0.00.039.907 I print_info: rope type        = 2
0.00.039.907 I print_info: rope scaling     = linear
0.00.039.908 I print_info: freq_base_train  = 10000.0
0.00.039.908 I print_info: freq_scale_train = 1
0.00.039.908 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.908 I print_info: rope_finetuned   = unknown
0.00.039.908 I print_info: ssm_d_conv       = 0
0.00.039.908 I print_info: ssm_d_inner      = 0
0.00.039.908 I print_info: ssm_d_state      = 0
0.00.039.910 I print_info: ssm_dt_rank      = 0
0.00.039.910 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.911 I print_info: model type       = 1.4B
0.00.039.911 I print_info: model params     = 1.41 B
0.00.039.911 I print_info: general.name     = 1.4B
0.00.039.911 I print_info: vocab type       = BPE
0.00.039.912 I print_info: n_vocab          = 50304
0.00.039.912 I print_info: n_merges         = 50009
0.00.039.912 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.912 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.912 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.912 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.913 I print_info: LF token         = 187 ''
0.00.039.913 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.913 I print_info: max token length = 1024
0.00.039.913 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.701.636 I load_tensors: offloading 24 repeating layers to GPU
0.00.701.654 I load_tensors: offloading output layer to GPU
0.00.701.655 I load_tensors: offloaded 25/25 layers to GPU
0.00.701.686 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.701.687 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.703.280 I llama_init_from_model: n_seq_max     = 1
0.00.703.282 I llama_init_from_model: n_ctx         = 2048
0.00.703.283 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.703.284 I llama_init_from_model: n_batch       = 2048
0.00.703.284 I llama_init_from_model: n_ubatch      = 512
0.00.703.284 I llama_init_from_model: flash_attn    = 0
0.00.703.285 I llama_init_from_model: freq_base     = 10000.0
0.00.703.286 I llama_init_from_model: freq_scale    = 1
0.00.703.287 I ggml_metal_init: allocating
0.00.703.314 I ggml_metal_init: found device: Apple M4
0.00.703.323 I ggml_metal_init: picking default device: Apple M4
0.00.704.727 I ggml_metal_init: using embedded metal library
0.00.710.932 I ggml_metal_init: GPU name:   Apple M4
0.00.710.935 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.710.936 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.710.937 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.710.938 I ggml_metal_init: simdgroup reduction   = true
0.00.710.938 I ggml_metal_init: simdgroup matrix mul. = true
0.00.710.938 I ggml_metal_init: has residency sets    = true
0.00.710.938 I ggml_metal_init: has bfloat            = true
0.00.710.939 I ggml_metal_init: use bfloat            = true
0.00.710.940 I ggml_metal_init: hasUnifiedMemory      = true
0.00.710.941 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.728.563 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.782.253 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.782.260 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.782.281 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.787.436 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.787.439 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.787.439 I llama_init_from_model: graph nodes  = 967
0.00.787.440 I llama_init_from_model: graph splits = 2
0.00.787.450 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.787.572 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.787.572 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.845.917 I main: llama threadpool init, n_threads = 4
0.00.845.967 I 
0.00.845.987 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.845.989 I 
0.00.846.142 I sampler seed: 1234
0.00.846.147 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.846.197 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.846.200 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.846.200 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.635.776 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50786.84 tokens per second)
0.01.635.777 I llama_perf_context_print:        load time =     836.29 ms
0.01.635.778 I llama_perf_context_print: prompt eval time =      53.23 ms /     7 tokens (    7.60 ms per token,   131.51 tokens per second)
0.01.635.779 I llama_perf_context_print:        eval time =     733.41 ms /    63 runs   (   11.64 ms per token,    85.90 tokens per second)
0.01.635.779 I llama_perf_context_print:       total time =     790.59 ms /    70 tokens
0.01.636.013 I ggml_metal_free: deallocating

real	0m1.652s
user	0m0.109s
sys	0m0.219s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.974 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.636 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.641 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.643 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.643 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.643 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.644 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.644 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.645 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.645 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.645 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.646 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.646 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.646 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.647 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.648 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.648 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.649 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.560 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.632 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.524 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.525 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.525 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.526 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.526 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.526 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.527 I llama_model_loader: - type  f32:  194 tensors
0.00.025.527 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.528 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.528 I print_info: file format = GGUF V3 (latest)
0.00.025.529 I print_info: file type   = Q5_0
0.00.025.530 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.946 I load: special tokens cache size = 25
0.00.040.368 I load: token to piece cache size = 0.2984 MB
0.00.040.385 I print_info: arch             = gptneox
0.00.040.386 I print_info: vocab_only       = 0
0.00.040.386 I print_info: n_ctx_train      = 2048
0.00.040.386 I print_info: n_embd           = 2048
0.00.040.386 I print_info: n_layer          = 24
0.00.040.391 I print_info: n_head           = 16
0.00.040.391 I print_info: n_head_kv        = 16
0.00.040.391 I print_info: n_rot            = 32
0.00.040.391 I print_info: n_swa            = 0
0.00.040.394 I print_info: n_embd_head_k    = 128
0.00.040.394 I print_info: n_embd_head_v    = 128
0.00.040.395 I print_info: n_gqa            = 1
0.00.040.395 I print_info: n_embd_k_gqa     = 2048
0.00.040.396 I print_info: n_embd_v_gqa     = 2048
0.00.040.396 I print_info: f_norm_eps       = 1.0e-05
0.00.040.397 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.397 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.397 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.397 I print_info: f_logit_scale    = 0.0e+00
0.00.040.398 I print_info: n_ff             = 8192
0.00.040.398 I print_info: n_expert         = 0
0.00.040.398 I print_info: n_expert_used    = 0
0.00.040.398 I print_info: causal attn      = 1
0.00.040.398 I print_info: pooling type     = 0
0.00.040.399 I print_info: rope type        = 2
0.00.040.399 I print_info: rope scaling     = linear
0.00.040.399 I print_info: freq_base_train  = 10000.0
0.00.040.399 I print_info: freq_scale_train = 1
0.00.040.400 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.400 I print_info: rope_finetuned   = unknown
0.00.040.400 I print_info: ssm_d_conv       = 0
0.00.040.400 I print_info: ssm_d_inner      = 0
0.00.040.400 I print_info: ssm_d_state      = 0
0.00.040.400 I print_info: ssm_dt_rank      = 0
0.00.040.400 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.400 I print_info: model type       = 1.4B
0.00.040.401 I print_info: model params     = 1.41 B
0.00.040.401 I print_info: general.name     = 1.4B
0.00.040.401 I print_info: vocab type       = BPE
0.00.040.402 I print_info: n_vocab          = 50304
0.00.040.402 I print_info: n_merges         = 50009
0.00.040.402 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.402 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.402 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.402 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.402 I print_info: LF token         = 187 ''
0.00.040.403 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.403 I print_info: max token length = 1024
0.00.040.403 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.664.627 I load_tensors: offloading 24 repeating layers to GPU
0.00.664.646 I load_tensors: offloading output layer to GPU
0.00.664.647 I load_tensors: offloaded 25/25 layers to GPU
0.00.664.681 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.664.682 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.665.718 I llama_init_from_model: n_seq_max     = 1
0.00.665.720 I llama_init_from_model: n_ctx         = 128
0.00.665.721 I llama_init_from_model: n_ctx_per_seq = 128
0.00.665.721 I llama_init_from_model: n_batch       = 128
0.00.665.721 I llama_init_from_model: n_ubatch      = 128
0.00.665.722 I llama_init_from_model: flash_attn    = 0
0.00.665.723 I llama_init_from_model: freq_base     = 10000.0
0.00.665.723 I llama_init_from_model: freq_scale    = 1
0.00.665.724 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.665.725 I ggml_metal_init: allocating
0.00.665.787 I ggml_metal_init: found device: Apple M4
0.00.665.796 I ggml_metal_init: picking default device: Apple M4
0.00.666.671 I ggml_metal_init: using embedded metal library
0.00.671.603 I ggml_metal_init: GPU name:   Apple M4
0.00.671.607 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.671.607 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.671.608 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.671.609 I ggml_metal_init: simdgroup reduction   = true
0.00.671.609 I ggml_metal_init: simdgroup matrix mul. = true
0.00.671.609 I ggml_metal_init: has residency sets    = true
0.00.671.609 I ggml_metal_init: has bfloat            = true
0.00.671.610 I ggml_metal_init: use bfloat            = true
0.00.671.610 I ggml_metal_init: hasUnifiedMemory      = true
0.00.671.612 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.686.466 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.689.055 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.689.059 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.689.084 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.691.595 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.691.597 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.691.597 I llama_init_from_model: graph nodes  = 967
0.00.691.598 I llama_init_from_model: graph splits = 2
0.00.691.600 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.691.600 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.107 I 
0.00.717.147 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.717.160 I perplexity: tokenizing the input ..
0.00.721.390 I perplexity: tokenization took 4.228 ms
0.00.721.394 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.854.946 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.856.281 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.856.294 I llama_perf_context_print:        load time =     708.13 ms
0.00.856.295 I llama_perf_context_print: prompt eval time =     133.32 ms /   128 tokens (    1.04 ms per token,   960.09 tokens per second)
0.00.856.296 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.856.296 I llama_perf_context_print:       total time =     139.19 ms /   129 tokens
0.00.856.689 I ggml_metal_free: deallocating

real	0m0.871s
user	0m0.073s
sys	0m0.112s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.010.708 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.285 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.289 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.291 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.292 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.292 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.293 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.293 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.294 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.294 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.295 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.295 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.295 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.296 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.296 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.298 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.298 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.298 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.067 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.111 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.855 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.856 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.857 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.857 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.857 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.857 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.858 I llama_model_loader: - type  f32:  194 tensors
0.00.025.858 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.859 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.859 I print_info: file format = GGUF V3 (latest)
0.00.025.860 I print_info: file type   = Q5_1
0.00.025.861 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.022 I load: special tokens cache size = 25
0.00.040.380 I load: token to piece cache size = 0.2984 MB
0.00.040.395 I print_info: arch             = gptneox
0.00.040.396 I print_info: vocab_only       = 0
0.00.040.396 I print_info: n_ctx_train      = 2048
0.00.040.396 I print_info: n_embd           = 2048
0.00.040.396 I print_info: n_layer          = 24
0.00.040.399 I print_info: n_head           = 16
0.00.040.400 I print_info: n_head_kv        = 16
0.00.040.400 I print_info: n_rot            = 32
0.00.040.400 I print_info: n_swa            = 0
0.00.040.400 I print_info: n_embd_head_k    = 128
0.00.040.401 I print_info: n_embd_head_v    = 128
0.00.040.401 I print_info: n_gqa            = 1
0.00.040.402 I print_info: n_embd_k_gqa     = 2048
0.00.040.403 I print_info: n_embd_v_gqa     = 2048
0.00.040.403 I print_info: f_norm_eps       = 1.0e-05
0.00.040.404 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.404 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.404 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.404 I print_info: f_logit_scale    = 0.0e+00
0.00.040.405 I print_info: n_ff             = 8192
0.00.040.406 I print_info: n_expert         = 0
0.00.040.407 I print_info: n_expert_used    = 0
0.00.040.407 I print_info: causal attn      = 1
0.00.040.407 I print_info: pooling type     = 0
0.00.040.407 I print_info: rope type        = 2
0.00.040.407 I print_info: rope scaling     = linear
0.00.040.407 I print_info: freq_base_train  = 10000.0
0.00.040.408 I print_info: freq_scale_train = 1
0.00.040.409 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.409 I print_info: rope_finetuned   = unknown
0.00.040.409 I print_info: ssm_d_conv       = 0
0.00.040.410 I print_info: ssm_d_inner      = 0
0.00.040.410 I print_info: ssm_d_state      = 0
0.00.040.410 I print_info: ssm_dt_rank      = 0
0.00.040.410 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.410 I print_info: model type       = 1.4B
0.00.040.410 I print_info: model params     = 1.41 B
0.00.040.410 I print_info: general.name     = 1.4B
0.00.040.411 I print_info: vocab type       = BPE
0.00.040.411 I print_info: n_vocab          = 50304
0.00.040.411 I print_info: n_merges         = 50009
0.00.040.411 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.412 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.412 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.412 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.412 I print_info: LF token         = 187 ''
0.00.040.413 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.413 I print_info: max token length = 1024
0.00.040.413 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.608.970 I load_tensors: offloading 24 repeating layers to GPU
0.00.608.986 I load_tensors: offloading output layer to GPU
0.00.608.987 I load_tensors: offloaded 25/25 layers to GPU
0.00.609.019 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.609.020 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.610.497 I llama_init_from_model: n_seq_max     = 1
0.00.610.507 I llama_init_from_model: n_ctx         = 2048
0.00.610.508 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.610.508 I llama_init_from_model: n_batch       = 2048
0.00.610.509 I llama_init_from_model: n_ubatch      = 512
0.00.610.509 I llama_init_from_model: flash_attn    = 0
0.00.610.511 I llama_init_from_model: freq_base     = 10000.0
0.00.610.512 I llama_init_from_model: freq_scale    = 1
0.00.610.514 I ggml_metal_init: allocating
0.00.610.583 I ggml_metal_init: found device: Apple M4
0.00.610.607 I ggml_metal_init: picking default device: Apple M4
0.00.612.315 I ggml_metal_init: using embedded metal library
0.00.618.950 I ggml_metal_init: GPU name:   Apple M4
0.00.618.953 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.618.954 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.618.955 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.618.955 I ggml_metal_init: simdgroup reduction   = true
0.00.618.955 I ggml_metal_init: simdgroup matrix mul. = true
0.00.618.956 I ggml_metal_init: has residency sets    = true
0.00.618.956 I ggml_metal_init: has bfloat            = true
0.00.618.956 I ggml_metal_init: use bfloat            = true
0.00.618.957 I ggml_metal_init: hasUnifiedMemory      = true
0.00.618.958 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.637.113 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.690.273 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.690.280 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.690.304 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.694.372 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.694.374 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.694.374 I llama_init_from_model: graph nodes  = 967
0.00.694.374 I llama_init_from_model: graph splits = 2
0.00.694.383 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.694.509 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.694.509 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.755.770 I main: llama threadpool init, n_threads = 4
0.00.755.815 I 
0.00.755.833 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.755.835 I 
0.00.756.006 I sampler seed: 1234
0.00.756.011 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.756.026 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.756.026 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.756.026 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.611.433 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 49033.15 tokens per second)
0.01.611.434 I llama_perf_context_print:        load time =     744.33 ms
0.01.611.435 I llama_perf_context_print: prompt eval time =      51.95 ms /     7 tokens (    7.42 ms per token,   134.74 tokens per second)
0.01.611.437 I llama_perf_context_print:        eval time =     800.93 ms /    63 runs   (   12.71 ms per token,    78.66 tokens per second)
0.01.611.438 I llama_perf_context_print:       total time =     856.39 ms /    70 tokens
0.01.611.749 I ggml_metal_free: deallocating

real	0m1.631s
user	0m0.111s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.052 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.227 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.233 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.235 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.236 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.236 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.236 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.237 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.238 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.238 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.238 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.239 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.239 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.240 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.240 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.242 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.242 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.243 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.052 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.018 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.769 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.771 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.771 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.771 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.772 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.772 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.773 I llama_model_loader: - type  f32:  194 tensors
0.00.025.773 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.773 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.774 I print_info: file format = GGUF V3 (latest)
0.00.025.775 I print_info: file type   = Q5_1
0.00.025.776 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.344 I load: special tokens cache size = 25
0.00.042.042 I load: token to piece cache size = 0.2984 MB
0.00.042.060 I print_info: arch             = gptneox
0.00.042.061 I print_info: vocab_only       = 0
0.00.042.061 I print_info: n_ctx_train      = 2048
0.00.042.061 I print_info: n_embd           = 2048
0.00.042.061 I print_info: n_layer          = 24
0.00.042.065 I print_info: n_head           = 16
0.00.042.066 I print_info: n_head_kv        = 16
0.00.042.066 I print_info: n_rot            = 32
0.00.042.066 I print_info: n_swa            = 0
0.00.042.067 I print_info: n_embd_head_k    = 128
0.00.042.067 I print_info: n_embd_head_v    = 128
0.00.042.067 I print_info: n_gqa            = 1
0.00.042.068 I print_info: n_embd_k_gqa     = 2048
0.00.042.070 I print_info: n_embd_v_gqa     = 2048
0.00.042.071 I print_info: f_norm_eps       = 1.0e-05
0.00.042.071 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.072 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.072 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.072 I print_info: f_logit_scale    = 0.0e+00
0.00.042.073 I print_info: n_ff             = 8192
0.00.042.073 I print_info: n_expert         = 0
0.00.042.073 I print_info: n_expert_used    = 0
0.00.042.073 I print_info: causal attn      = 1
0.00.042.073 I print_info: pooling type     = 0
0.00.042.073 I print_info: rope type        = 2
0.00.042.074 I print_info: rope scaling     = linear
0.00.042.074 I print_info: freq_base_train  = 10000.0
0.00.042.074 I print_info: freq_scale_train = 1
0.00.042.074 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.075 I print_info: rope_finetuned   = unknown
0.00.042.075 I print_info: ssm_d_conv       = 0
0.00.042.075 I print_info: ssm_d_inner      = 0
0.00.042.077 I print_info: ssm_d_state      = 0
0.00.042.078 I print_info: ssm_dt_rank      = 0
0.00.042.078 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.078 I print_info: model type       = 1.4B
0.00.042.078 I print_info: model params     = 1.41 B
0.00.042.078 I print_info: general.name     = 1.4B
0.00.042.079 I print_info: vocab type       = BPE
0.00.042.079 I print_info: n_vocab          = 50304
0.00.042.079 I print_info: n_merges         = 50009
0.00.042.079 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.079 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.080 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.080 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.080 I print_info: LF token         = 187 ''
0.00.042.080 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.080 I print_info: max token length = 1024
0.00.042.081 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.623.161 I load_tensors: offloading 24 repeating layers to GPU
0.00.623.175 I load_tensors: offloading output layer to GPU
0.00.623.175 I load_tensors: offloaded 25/25 layers to GPU
0.00.623.214 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.623.215 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.625.058 I llama_init_from_model: n_seq_max     = 1
0.00.625.064 I llama_init_from_model: n_ctx         = 128
0.00.625.064 I llama_init_from_model: n_ctx_per_seq = 128
0.00.625.065 I llama_init_from_model: n_batch       = 128
0.00.625.065 I llama_init_from_model: n_ubatch      = 128
0.00.625.065 I llama_init_from_model: flash_attn    = 0
0.00.625.067 I llama_init_from_model: freq_base     = 10000.0
0.00.625.067 I llama_init_from_model: freq_scale    = 1
0.00.625.067 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.625.073 I ggml_metal_init: allocating
0.00.625.167 I ggml_metal_init: found device: Apple M4
0.00.625.188 I ggml_metal_init: picking default device: Apple M4
0.00.627.151 I ggml_metal_init: using embedded metal library
0.00.633.881 I ggml_metal_init: GPU name:   Apple M4
0.00.633.886 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.633.887 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.633.888 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.633.889 I ggml_metal_init: simdgroup reduction   = true
0.00.633.889 I ggml_metal_init: simdgroup matrix mul. = true
0.00.633.889 I ggml_metal_init: has residency sets    = true
0.00.633.889 I ggml_metal_init: has bfloat            = true
0.00.633.890 I ggml_metal_init: use bfloat            = true
0.00.633.890 I ggml_metal_init: hasUnifiedMemory      = true
0.00.633.893 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.651.210 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.654.708 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.654.712 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.654.741 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.658.162 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.658.164 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.658.164 I llama_init_from_model: graph nodes  = 967
0.00.658.165 I llama_init_from_model: graph splits = 2
0.00.658.168 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.658.168 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.672 I 
0.00.689.766 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.795 I perplexity: tokenizing the input ..
0.00.696.878 I perplexity: tokenization took 7.081 ms
0.00.696.885 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.845.511 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.846.865 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.846.882 I llama_perf_context_print:        load time =     679.61 ms
0.00.846.883 I llama_perf_context_print: prompt eval time =     147.69 ms /   128 tokens (    1.15 ms per token,   866.70 tokens per second)
0.00.846.883 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.846.884 I llama_perf_context_print:       total time =     157.22 ms /   129 tokens
0.00.847.292 I ggml_metal_free: deallocating

real	0m0.863s
user	0m0.082s
sys	0m0.150s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.913 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.729 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.735 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.737 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.737 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.738 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.738 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.738 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.739 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.740 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.740 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.740 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.741 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.741 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.743 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.747 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.747 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.747 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.588 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.588 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.313 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.315 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.315 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.316 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.316 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.316 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.317 I llama_model_loader: - type  f32:  194 tensors
0.00.024.317 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.318 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.318 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.320 I print_info: file format = GGUF V3 (latest)
0.00.024.321 I print_info: file type   = Q2_K - Medium
0.00.024.322 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.411 I load: special tokens cache size = 25
0.00.038.660 I load: token to piece cache size = 0.2984 MB
0.00.038.678 I print_info: arch             = gptneox
0.00.038.679 I print_info: vocab_only       = 0
0.00.038.679 I print_info: n_ctx_train      = 2048
0.00.038.679 I print_info: n_embd           = 2048
0.00.038.679 I print_info: n_layer          = 24
0.00.038.684 I print_info: n_head           = 16
0.00.038.685 I print_info: n_head_kv        = 16
0.00.038.686 I print_info: n_rot            = 32
0.00.038.686 I print_info: n_swa            = 0
0.00.038.686 I print_info: n_embd_head_k    = 128
0.00.038.687 I print_info: n_embd_head_v    = 128
0.00.038.687 I print_info: n_gqa            = 1
0.00.038.688 I print_info: n_embd_k_gqa     = 2048
0.00.038.688 I print_info: n_embd_v_gqa     = 2048
0.00.038.689 I print_info: f_norm_eps       = 1.0e-05
0.00.038.689 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.689 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.690 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.690 I print_info: f_logit_scale    = 0.0e+00
0.00.038.690 I print_info: n_ff             = 8192
0.00.038.691 I print_info: n_expert         = 0
0.00.038.691 I print_info: n_expert_used    = 0
0.00.038.691 I print_info: causal attn      = 1
0.00.038.691 I print_info: pooling type     = 0
0.00.038.691 I print_info: rope type        = 2
0.00.038.691 I print_info: rope scaling     = linear
0.00.038.692 I print_info: freq_base_train  = 10000.0
0.00.038.692 I print_info: freq_scale_train = 1
0.00.038.692 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.692 I print_info: rope_finetuned   = unknown
0.00.038.692 I print_info: ssm_d_conv       = 0
0.00.038.692 I print_info: ssm_d_inner      = 0
0.00.038.692 I print_info: ssm_d_state      = 0
0.00.038.693 I print_info: ssm_dt_rank      = 0
0.00.038.693 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.693 I print_info: model type       = 1.4B
0.00.038.693 I print_info: model params     = 1.41 B
0.00.038.693 I print_info: general.name     = 1.4B
0.00.038.694 I print_info: vocab type       = BPE
0.00.038.694 I print_info: n_vocab          = 50304
0.00.038.694 I print_info: n_merges         = 50009
0.00.038.694 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.694 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.695 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.695 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.695 I print_info: LF token         = 187 ''
0.00.038.695 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.695 I print_info: max token length = 1024
0.00.038.696 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.341.109 I load_tensors: offloading 24 repeating layers to GPU
0.00.341.127 I load_tensors: offloading output layer to GPU
0.00.341.127 I load_tensors: offloaded 25/25 layers to GPU
0.00.341.160 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.341.164 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.342.737 I llama_init_from_model: n_seq_max     = 1
0.00.342.740 I llama_init_from_model: n_ctx         = 2048
0.00.342.741 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.342.741 I llama_init_from_model: n_batch       = 2048
0.00.342.742 I llama_init_from_model: n_ubatch      = 512
0.00.342.742 I llama_init_from_model: flash_attn    = 0
0.00.342.744 I llama_init_from_model: freq_base     = 10000.0
0.00.342.745 I llama_init_from_model: freq_scale    = 1
0.00.342.747 I ggml_metal_init: allocating
0.00.342.847 I ggml_metal_init: found device: Apple M4
0.00.342.864 I ggml_metal_init: picking default device: Apple M4
0.00.344.501 I ggml_metal_init: using embedded metal library
0.00.350.009 I ggml_metal_init: GPU name:   Apple M4
0.00.350.024 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.350.024 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.350.025 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.350.026 I ggml_metal_init: simdgroup reduction   = true
0.00.350.027 I ggml_metal_init: simdgroup matrix mul. = true
0.00.350.027 I ggml_metal_init: has residency sets    = true
0.00.350.027 I ggml_metal_init: has bfloat            = true
0.00.350.028 I ggml_metal_init: use bfloat            = true
0.00.350.029 I ggml_metal_init: hasUnifiedMemory      = true
0.00.350.034 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.372.077 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.432.946 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.432.959 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.432.992 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.437.892 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.437.902 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.437.903 I llama_init_from_model: graph nodes  = 967
0.00.437.903 I llama_init_from_model: graph splits = 2
0.00.437.909 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.438.039 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.438.039 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.495.810 I main: llama threadpool init, n_threads = 4
0.00.495.861 I 
0.00.495.882 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.495.883 I 
0.00.496.060 I sampler seed: 1234
0.00.496.065 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.496.111 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.496.112 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.496.112 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.170.971 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52244.30 tokens per second)
0.01.170.972 I llama_perf_context_print:        load time =     486.17 ms
0.01.170.972 I llama_perf_context_print: prompt eval time =      35.45 ms /     7 tokens (    5.06 ms per token,   197.46 tokens per second)
0.01.170.973 I llama_perf_context_print:        eval time =     636.58 ms /    63 runs   (   10.10 ms per token,    98.97 tokens per second)
0.01.170.973 I llama_perf_context_print:       total time =     675.88 ms /    70 tokens
0.01.171.212 I ggml_metal_free: deallocating

real	0m1.188s
user	0m0.113s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.938 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.927 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.935 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.936 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.937 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.937 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.937 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.938 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.940 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.940 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.941 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.942 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.942 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.943 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.943 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.945 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.945 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.945 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.707 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.683 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.383 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.384 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.385 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.385 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.385 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.386 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.386 I llama_model_loader: - type  f32:  194 tensors
0.00.025.387 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.387 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.387 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.388 I print_info: file format = GGUF V3 (latest)
0.00.025.390 I print_info: file type   = Q2_K - Medium
0.00.025.391 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.778 I load: special tokens cache size = 25
0.00.040.150 I load: token to piece cache size = 0.2984 MB
0.00.040.168 I print_info: arch             = gptneox
0.00.040.169 I print_info: vocab_only       = 0
0.00.040.169 I print_info: n_ctx_train      = 2048
0.00.040.169 I print_info: n_embd           = 2048
0.00.040.169 I print_info: n_layer          = 24
0.00.040.173 I print_info: n_head           = 16
0.00.040.174 I print_info: n_head_kv        = 16
0.00.040.174 I print_info: n_rot            = 32
0.00.040.174 I print_info: n_swa            = 0
0.00.040.174 I print_info: n_embd_head_k    = 128
0.00.040.174 I print_info: n_embd_head_v    = 128
0.00.040.175 I print_info: n_gqa            = 1
0.00.040.175 I print_info: n_embd_k_gqa     = 2048
0.00.040.176 I print_info: n_embd_v_gqa     = 2048
0.00.040.177 I print_info: f_norm_eps       = 1.0e-05
0.00.040.177 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.177 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.177 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.177 I print_info: f_logit_scale    = 0.0e+00
0.00.040.178 I print_info: n_ff             = 8192
0.00.040.179 I print_info: n_expert         = 0
0.00.040.180 I print_info: n_expert_used    = 0
0.00.040.180 I print_info: causal attn      = 1
0.00.040.180 I print_info: pooling type     = 0
0.00.040.180 I print_info: rope type        = 2
0.00.040.180 I print_info: rope scaling     = linear
0.00.040.181 I print_info: freq_base_train  = 10000.0
0.00.040.181 I print_info: freq_scale_train = 1
0.00.040.181 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.181 I print_info: rope_finetuned   = unknown
0.00.040.181 I print_info: ssm_d_conv       = 0
0.00.040.181 I print_info: ssm_d_inner      = 0
0.00.040.181 I print_info: ssm_d_state      = 0
0.00.040.181 I print_info: ssm_dt_rank      = 0
0.00.040.182 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.184 I print_info: model type       = 1.4B
0.00.040.184 I print_info: model params     = 1.41 B
0.00.040.184 I print_info: general.name     = 1.4B
0.00.040.185 I print_info: vocab type       = BPE
0.00.040.185 I print_info: n_vocab          = 50304
0.00.040.185 I print_info: n_merges         = 50009
0.00.040.185 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.186 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.186 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.186 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.187 I print_info: LF token         = 187 ''
0.00.040.188 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.188 I print_info: max token length = 1024
0.00.040.188 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.330.979 I load_tensors: offloading 24 repeating layers to GPU
0.00.330.995 I load_tensors: offloading output layer to GPU
0.00.330.995 I load_tensors: offloaded 25/25 layers to GPU
0.00.331.032 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.331.034 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.332.489 I llama_init_from_model: n_seq_max     = 1
0.00.332.492 I llama_init_from_model: n_ctx         = 128
0.00.332.493 I llama_init_from_model: n_ctx_per_seq = 128
0.00.332.493 I llama_init_from_model: n_batch       = 128
0.00.332.494 I llama_init_from_model: n_ubatch      = 128
0.00.332.494 I llama_init_from_model: flash_attn    = 0
0.00.332.496 I llama_init_from_model: freq_base     = 10000.0
0.00.332.496 I llama_init_from_model: freq_scale    = 1
0.00.332.497 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.332.506 I ggml_metal_init: allocating
0.00.332.614 I ggml_metal_init: found device: Apple M4
0.00.332.629 I ggml_metal_init: picking default device: Apple M4
0.00.334.281 I ggml_metal_init: using embedded metal library
0.00.339.863 I ggml_metal_init: GPU name:   Apple M4
0.00.339.881 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.339.882 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.339.882 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.339.883 I ggml_metal_init: simdgroup reduction   = true
0.00.339.884 I ggml_metal_init: simdgroup matrix mul. = true
0.00.339.884 I ggml_metal_init: has residency sets    = true
0.00.339.884 I ggml_metal_init: has bfloat            = true
0.00.339.884 I ggml_metal_init: use bfloat            = true
0.00.339.886 I ggml_metal_init: hasUnifiedMemory      = true
0.00.339.891 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.361.138 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.364.748 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.364.755 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.364.789 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.368.120 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.368.122 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.368.122 I llama_init_from_model: graph nodes  = 967
0.00.368.122 I llama_init_from_model: graph splits = 2
0.00.368.126 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.368.126 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.397.694 I 
0.00.397.782 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.397.810 I perplexity: tokenizing the input ..
0.00.404.586 I perplexity: tokenization took 6.773 ms
0.00.404.593 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.536.576 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.537.882 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.537.896 I llama_perf_context_print:        load time =     388.75 ms
0.00.537.897 I llama_perf_context_print: prompt eval time =     131.05 ms /   128 tokens (    1.02 ms per token,   976.75 tokens per second)
0.00.537.899 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.537.899 I llama_perf_context_print:       total time =     140.21 ms /   129 tokens
0.00.538.250 I ggml_metal_free: deallocating

real	0m0.552s
user	0m0.082s
sys	0m0.083s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.801 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.436 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.441 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.442 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.443 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.443 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.444 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.444 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.445 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.445 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.446 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.446 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.447 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.447 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.448 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.449 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.450 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.450 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.130 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.102 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.817 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.818 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.819 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.819 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.819 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.820 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.820 I llama_model_loader: - type  f32:  194 tensors
0.00.023.820 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.821 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.821 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.821 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.822 I print_info: file format = GGUF V3 (latest)
0.00.023.822 I print_info: file type   = Q3_K - Medium
0.00.023.823 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.031.636 I load: special tokens cache size = 25
0.00.037.844 I load: token to piece cache size = 0.2984 MB
0.00.037.859 I print_info: arch             = gptneox
0.00.037.860 I print_info: vocab_only       = 0
0.00.037.860 I print_info: n_ctx_train      = 2048
0.00.037.860 I print_info: n_embd           = 2048
0.00.037.861 I print_info: n_layer          = 24
0.00.037.863 I print_info: n_head           = 16
0.00.037.864 I print_info: n_head_kv        = 16
0.00.037.864 I print_info: n_rot            = 32
0.00.037.865 I print_info: n_swa            = 0
0.00.037.865 I print_info: n_embd_head_k    = 128
0.00.037.865 I print_info: n_embd_head_v    = 128
0.00.037.866 I print_info: n_gqa            = 1
0.00.037.866 I print_info: n_embd_k_gqa     = 2048
0.00.037.867 I print_info: n_embd_v_gqa     = 2048
0.00.037.868 I print_info: f_norm_eps       = 1.0e-05
0.00.037.868 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.869 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.870 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.870 I print_info: f_logit_scale    = 0.0e+00
0.00.037.870 I print_info: n_ff             = 8192
0.00.037.871 I print_info: n_expert         = 0
0.00.037.871 I print_info: n_expert_used    = 0
0.00.037.871 I print_info: causal attn      = 1
0.00.037.871 I print_info: pooling type     = 0
0.00.037.871 I print_info: rope type        = 2
0.00.037.871 I print_info: rope scaling     = linear
0.00.037.872 I print_info: freq_base_train  = 10000.0
0.00.037.872 I print_info: freq_scale_train = 1
0.00.037.872 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.872 I print_info: rope_finetuned   = unknown
0.00.037.872 I print_info: ssm_d_conv       = 0
0.00.037.872 I print_info: ssm_d_inner      = 0
0.00.037.873 I print_info: ssm_d_state      = 0
0.00.037.874 I print_info: ssm_dt_rank      = 0
0.00.037.874 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.874 I print_info: model type       = 1.4B
0.00.037.875 I print_info: model params     = 1.41 B
0.00.037.875 I print_info: general.name     = 1.4B
0.00.037.875 I print_info: vocab type       = BPE
0.00.037.875 I print_info: n_vocab          = 50304
0.00.037.876 I print_info: n_merges         = 50009
0.00.037.876 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.876 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.876 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.876 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.876 I print_info: LF token         = 187 ''
0.00.037.877 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.877 I print_info: max token length = 1024
0.00.037.877 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.443.334 I load_tensors: offloading 24 repeating layers to GPU
0.00.443.346 I load_tensors: offloading output layer to GPU
0.00.443.347 I load_tensors: offloaded 25/25 layers to GPU
0.00.443.382 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.443.391 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.444.963 I llama_init_from_model: n_seq_max     = 1
0.00.444.965 I llama_init_from_model: n_ctx         = 2048
0.00.444.966 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.444.966 I llama_init_from_model: n_batch       = 2048
0.00.444.967 I llama_init_from_model: n_ubatch      = 512
0.00.444.967 I llama_init_from_model: flash_attn    = 0
0.00.444.969 I llama_init_from_model: freq_base     = 10000.0
0.00.444.969 I llama_init_from_model: freq_scale    = 1
0.00.444.972 I ggml_metal_init: allocating
0.00.445.062 I ggml_metal_init: found device: Apple M4
0.00.445.076 I ggml_metal_init: picking default device: Apple M4
0.00.446.662 I ggml_metal_init: using embedded metal library
0.00.452.172 I ggml_metal_init: GPU name:   Apple M4
0.00.452.178 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.452.178 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.452.179 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.452.180 I ggml_metal_init: simdgroup reduction   = true
0.00.452.180 I ggml_metal_init: simdgroup matrix mul. = true
0.00.452.181 I ggml_metal_init: has residency sets    = true
0.00.452.181 I ggml_metal_init: has bfloat            = true
0.00.452.181 I ggml_metal_init: use bfloat            = true
0.00.452.182 I ggml_metal_init: hasUnifiedMemory      = true
0.00.452.184 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.472.421 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.532.932 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.532.940 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.532.976 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.538.027 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.538.030 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.538.030 I llama_init_from_model: graph nodes  = 967
0.00.538.030 I llama_init_from_model: graph splits = 2
0.00.538.035 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.538.158 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.538.159 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.592.156 I main: llama threadpool init, n_threads = 4
0.00.592.208 I 
0.00.592.229 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.592.231 I 
0.00.592.414 I sampler seed: 1234
0.00.592.419 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.592.464 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.592.468 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.592.468 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.326.524 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50569.80 tokens per second)
0.01.326.525 I llama_perf_context_print:        load time =     582.62 ms
0.01.326.527 I llama_perf_context_print: prompt eval time =      40.52 ms /     7 tokens (    5.79 ms per token,   172.75 tokens per second)
0.01.326.528 I llama_perf_context_print:        eval time =     690.63 ms /    63 runs   (   10.96 ms per token,    91.22 tokens per second)
0.01.326.530 I llama_perf_context_print:       total time =     735.10 ms /    70 tokens
0.01.326.764 I ggml_metal_free: deallocating

real	0m1.343s
user	0m0.111s
sys	0m0.194s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.330 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.205 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.019.212 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.213 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.214 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.214 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.214 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.215 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.216 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.220 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.220 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.220 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.220 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.223 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.224 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.226 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.226 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.226 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.057 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.087 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.961 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.962 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.963 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.963 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.963 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.964 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.027.964 I llama_model_loader: - type  f32:  194 tensors
0.00.027.965 I llama_model_loader: - type q3_K:   25 tensors
0.00.027.965 I llama_model_loader: - type q4_K:   71 tensors
0.00.027.965 I llama_model_loader: - type q5_K:    1 tensors
0.00.027.965 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.966 I print_info: file format = GGUF V3 (latest)
0.00.027.967 I print_info: file type   = Q3_K - Medium
0.00.027.968 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.036.352 I load: special tokens cache size = 25
0.00.042.915 I load: token to piece cache size = 0.2984 MB
0.00.042.933 I print_info: arch             = gptneox
0.00.042.934 I print_info: vocab_only       = 0
0.00.042.934 I print_info: n_ctx_train      = 2048
0.00.042.934 I print_info: n_embd           = 2048
0.00.042.934 I print_info: n_layer          = 24
0.00.042.938 I print_info: n_head           = 16
0.00.042.939 I print_info: n_head_kv        = 16
0.00.042.939 I print_info: n_rot            = 32
0.00.042.939 I print_info: n_swa            = 0
0.00.042.939 I print_info: n_embd_head_k    = 128
0.00.042.939 I print_info: n_embd_head_v    = 128
0.00.042.940 I print_info: n_gqa            = 1
0.00.042.941 I print_info: n_embd_k_gqa     = 2048
0.00.042.941 I print_info: n_embd_v_gqa     = 2048
0.00.042.942 I print_info: f_norm_eps       = 1.0e-05
0.00.042.942 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.942 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.942 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.942 I print_info: f_logit_scale    = 0.0e+00
0.00.042.943 I print_info: n_ff             = 8192
0.00.042.943 I print_info: n_expert         = 0
0.00.042.943 I print_info: n_expert_used    = 0
0.00.042.944 I print_info: causal attn      = 1
0.00.042.944 I print_info: pooling type     = 0
0.00.042.944 I print_info: rope type        = 2
0.00.042.944 I print_info: rope scaling     = linear
0.00.042.944 I print_info: freq_base_train  = 10000.0
0.00.042.945 I print_info: freq_scale_train = 1
0.00.042.945 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.945 I print_info: rope_finetuned   = unknown
0.00.042.945 I print_info: ssm_d_conv       = 0
0.00.042.945 I print_info: ssm_d_inner      = 0
0.00.042.945 I print_info: ssm_d_state      = 0
0.00.042.949 I print_info: ssm_dt_rank      = 0
0.00.042.949 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.949 I print_info: model type       = 1.4B
0.00.042.949 I print_info: model params     = 1.41 B
0.00.042.949 I print_info: general.name     = 1.4B
0.00.042.950 I print_info: vocab type       = BPE
0.00.042.950 I print_info: n_vocab          = 50304
0.00.042.951 I print_info: n_merges         = 50009
0.00.042.953 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.953 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.953 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.953 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.954 I print_info: LF token         = 187 ''
0.00.042.954 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.954 I print_info: max token length = 1024
0.00.042.956 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.444.319 I load_tensors: offloading 24 repeating layers to GPU
0.00.444.335 I load_tensors: offloading output layer to GPU
0.00.444.336 I load_tensors: offloaded 25/25 layers to GPU
0.00.444.378 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.444.379 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.445.980 I llama_init_from_model: n_seq_max     = 1
0.00.445.983 I llama_init_from_model: n_ctx         = 128
0.00.445.983 I llama_init_from_model: n_ctx_per_seq = 128
0.00.445.984 I llama_init_from_model: n_batch       = 128
0.00.445.984 I llama_init_from_model: n_ubatch      = 128
0.00.445.984 I llama_init_from_model: flash_attn    = 0
0.00.445.987 I llama_init_from_model: freq_base     = 10000.0
0.00.445.987 I llama_init_from_model: freq_scale    = 1
0.00.445.988 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.445.990 I ggml_metal_init: allocating
0.00.446.121 I ggml_metal_init: found device: Apple M4
0.00.446.136 I ggml_metal_init: picking default device: Apple M4
0.00.447.814 I ggml_metal_init: using embedded metal library
0.00.453.488 I ggml_metal_init: GPU name:   Apple M4
0.00.453.506 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.453.507 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.453.507 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.453.508 I ggml_metal_init: simdgroup reduction   = true
0.00.453.508 I ggml_metal_init: simdgroup matrix mul. = true
0.00.453.509 I ggml_metal_init: has residency sets    = true
0.00.453.509 I ggml_metal_init: has bfloat            = true
0.00.453.509 I ggml_metal_init: use bfloat            = true
0.00.453.511 I ggml_metal_init: hasUnifiedMemory      = true
0.00.453.517 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.475.243 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.478.855 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.478.859 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.478.890 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.482.187 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.482.189 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.482.190 I llama_init_from_model: graph nodes  = 967
0.00.482.190 I llama_init_from_model: graph splits = 2
0.00.482.193 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.482.193 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.509.448 I 
0.00.509.543 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.509.571 I perplexity: tokenizing the input ..
0.00.516.751 I perplexity: tokenization took 7.171 ms
0.00.516.757 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.648.772 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.650.099 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.650.119 I llama_perf_context_print:        load time =     497.11 ms
0.00.650.121 I llama_perf_context_print: prompt eval time =     131.47 ms /   128 tokens (    1.03 ms per token,   973.61 tokens per second)
0.00.650.122 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.650.122 I llama_perf_context_print:       total time =     140.68 ms /   129 tokens
0.00.650.513 I ggml_metal_free: deallocating

real	0m0.665s
user	0m0.082s
sys	0m0.113s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.630 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.261 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.266 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.268 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.268 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.269 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.269 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.269 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.270 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.271 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.271 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.271 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.272 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.272 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.273 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.274 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.274 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.275 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.969 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.967 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.699 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.701 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.701 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.701 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.701 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.702 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.702 I llama_model_loader: - type  f32:  194 tensors
0.00.024.702 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.703 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.703 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.703 I print_info: file format = GGUF V3 (latest)
0.00.024.704 I print_info: file type   = Q4_K - Medium
0.00.024.706 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.555 I load: special tokens cache size = 25
0.00.038.684 I load: token to piece cache size = 0.2984 MB
0.00.038.693 I print_info: arch             = gptneox
0.00.038.694 I print_info: vocab_only       = 0
0.00.038.694 I print_info: n_ctx_train      = 2048
0.00.038.694 I print_info: n_embd           = 2048
0.00.038.694 I print_info: n_layer          = 24
0.00.038.697 I print_info: n_head           = 16
0.00.038.698 I print_info: n_head_kv        = 16
0.00.038.698 I print_info: n_rot            = 32
0.00.038.698 I print_info: n_swa            = 0
0.00.038.698 I print_info: n_embd_head_k    = 128
0.00.038.698 I print_info: n_embd_head_v    = 128
0.00.038.699 I print_info: n_gqa            = 1
0.00.038.700 I print_info: n_embd_k_gqa     = 2048
0.00.038.701 I print_info: n_embd_v_gqa     = 2048
0.00.038.701 I print_info: f_norm_eps       = 1.0e-05
0.00.038.702 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.702 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.704 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.704 I print_info: f_logit_scale    = 0.0e+00
0.00.038.705 I print_info: n_ff             = 8192
0.00.038.705 I print_info: n_expert         = 0
0.00.038.706 I print_info: n_expert_used    = 0
0.00.038.706 I print_info: causal attn      = 1
0.00.038.706 I print_info: pooling type     = 0
0.00.038.706 I print_info: rope type        = 2
0.00.038.706 I print_info: rope scaling     = linear
0.00.038.707 I print_info: freq_base_train  = 10000.0
0.00.038.707 I print_info: freq_scale_train = 1
0.00.038.709 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.709 I print_info: rope_finetuned   = unknown
0.00.038.709 I print_info: ssm_d_conv       = 0
0.00.038.709 I print_info: ssm_d_inner      = 0
0.00.038.709 I print_info: ssm_d_state      = 0
0.00.038.709 I print_info: ssm_dt_rank      = 0
0.00.038.709 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.710 I print_info: model type       = 1.4B
0.00.038.710 I print_info: model params     = 1.41 B
0.00.038.710 I print_info: general.name     = 1.4B
0.00.038.711 I print_info: vocab type       = BPE
0.00.038.711 I print_info: n_vocab          = 50304
0.00.038.711 I print_info: n_merges         = 50009
0.00.038.711 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.711 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.711 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.712 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.712 I print_info: LF token         = 187 ''
0.00.038.712 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.712 I print_info: max token length = 1024
0.00.038.713 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.537.563 I load_tensors: offloading 24 repeating layers to GPU
0.00.537.579 I load_tensors: offloading output layer to GPU
0.00.537.579 I load_tensors: offloaded 25/25 layers to GPU
0.00.537.618 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.537.619 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.539.222 I llama_init_from_model: n_seq_max     = 1
0.00.539.226 I llama_init_from_model: n_ctx         = 2048
0.00.539.226 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.539.227 I llama_init_from_model: n_batch       = 2048
0.00.539.227 I llama_init_from_model: n_ubatch      = 512
0.00.539.228 I llama_init_from_model: flash_attn    = 0
0.00.539.230 I llama_init_from_model: freq_base     = 10000.0
0.00.539.230 I llama_init_from_model: freq_scale    = 1
0.00.539.240 I ggml_metal_init: allocating
0.00.539.340 I ggml_metal_init: found device: Apple M4
0.00.539.354 I ggml_metal_init: picking default device: Apple M4
0.00.540.973 I ggml_metal_init: using embedded metal library
0.00.547.797 I ggml_metal_init: GPU name:   Apple M4
0.00.547.801 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.547.802 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.547.802 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.547.803 I ggml_metal_init: simdgroup reduction   = true
0.00.547.803 I ggml_metal_init: simdgroup matrix mul. = true
0.00.547.804 I ggml_metal_init: has residency sets    = true
0.00.547.804 I ggml_metal_init: has bfloat            = true
0.00.547.804 I ggml_metal_init: use bfloat            = true
0.00.547.805 I ggml_metal_init: hasUnifiedMemory      = true
0.00.547.807 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.565.989 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.625.791 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.625.797 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.625.819 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.630.610 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.630.612 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.630.613 I llama_init_from_model: graph nodes  = 967
0.00.630.613 I llama_init_from_model: graph splits = 2
0.00.630.619 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.630.735 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.630.735 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.353 I main: llama threadpool init, n_threads = 4
0.00.689.405 I 
0.00.689.425 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.425 I 
0.00.689.602 I sampler seed: 1234
0.00.689.607 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.689.623 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.689.624 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.689.624 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.450.987 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49374.13 tokens per second)
0.01.450.987 I llama_perf_context_print:        load time =     678.99 ms
0.01.450.988 I llama_perf_context_print: prompt eval time =      57.57 ms /     7 tokens (    8.22 ms per token,   121.58 tokens per second)
0.01.450.989 I llama_perf_context_print:        eval time =     700.81 ms /    63 runs   (   11.12 ms per token,    89.90 tokens per second)
0.01.450.989 I llama_perf_context_print:       total time =     762.37 ms /    70 tokens
0.01.451.281 I ggml_metal_free: deallocating

real	0m1.472s
user	0m0.110s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.901 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.742 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.749 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.751 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.751 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.752 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.752 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.752 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.753 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.754 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.754 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.754 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.755 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.755 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.756 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.758 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.758 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.758 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.517 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.568 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.479 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.480 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.481 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.481 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.482 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.482 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.483 I llama_model_loader: - type  f32:  194 tensors
0.00.024.483 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.483 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.484 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.484 I print_info: file format = GGUF V3 (latest)
0.00.024.485 I print_info: file type   = Q4_K - Medium
0.00.024.487 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.859 I load: special tokens cache size = 25
0.00.039.319 I load: token to piece cache size = 0.2984 MB
0.00.039.337 I print_info: arch             = gptneox
0.00.039.338 I print_info: vocab_only       = 0
0.00.039.338 I print_info: n_ctx_train      = 2048
0.00.039.338 I print_info: n_embd           = 2048
0.00.039.339 I print_info: n_layer          = 24
0.00.039.342 I print_info: n_head           = 16
0.00.039.343 I print_info: n_head_kv        = 16
0.00.039.343 I print_info: n_rot            = 32
0.00.039.343 I print_info: n_swa            = 0
0.00.039.343 I print_info: n_embd_head_k    = 128
0.00.039.343 I print_info: n_embd_head_v    = 128
0.00.039.344 I print_info: n_gqa            = 1
0.00.039.345 I print_info: n_embd_k_gqa     = 2048
0.00.039.345 I print_info: n_embd_v_gqa     = 2048
0.00.039.346 I print_info: f_norm_eps       = 1.0e-05
0.00.039.346 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.346 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.347 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.347 I print_info: f_logit_scale    = 0.0e+00
0.00.039.347 I print_info: n_ff             = 8192
0.00.039.348 I print_info: n_expert         = 0
0.00.039.348 I print_info: n_expert_used    = 0
0.00.039.348 I print_info: causal attn      = 1
0.00.039.348 I print_info: pooling type     = 0
0.00.039.348 I print_info: rope type        = 2
0.00.039.352 I print_info: rope scaling     = linear
0.00.039.352 I print_info: freq_base_train  = 10000.0
0.00.039.353 I print_info: freq_scale_train = 1
0.00.039.353 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.353 I print_info: rope_finetuned   = unknown
0.00.039.353 I print_info: ssm_d_conv       = 0
0.00.039.353 I print_info: ssm_d_inner      = 0
0.00.039.353 I print_info: ssm_d_state      = 0
0.00.039.353 I print_info: ssm_dt_rank      = 0
0.00.039.354 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.354 I print_info: model type       = 1.4B
0.00.039.354 I print_info: model params     = 1.41 B
0.00.039.354 I print_info: general.name     = 1.4B
0.00.039.355 I print_info: vocab type       = BPE
0.00.039.355 I print_info: n_vocab          = 50304
0.00.039.355 I print_info: n_merges         = 50009
0.00.039.355 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.356 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.356 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.356 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.356 I print_info: LF token         = 187 ''
0.00.039.356 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.357 I print_info: max token length = 1024
0.00.039.357 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.510.301 I load_tensors: offloading 24 repeating layers to GPU
0.00.510.316 I load_tensors: offloading output layer to GPU
0.00.510.316 I load_tensors: offloaded 25/25 layers to GPU
0.00.510.349 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.510.350 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.511.699 I llama_init_from_model: n_seq_max     = 1
0.00.511.702 I llama_init_from_model: n_ctx         = 128
0.00.511.703 I llama_init_from_model: n_ctx_per_seq = 128
0.00.511.703 I llama_init_from_model: n_batch       = 128
0.00.511.704 I llama_init_from_model: n_ubatch      = 128
0.00.511.704 I llama_init_from_model: flash_attn    = 0
0.00.511.706 I llama_init_from_model: freq_base     = 10000.0
0.00.511.707 I llama_init_from_model: freq_scale    = 1
0.00.511.707 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.511.710 I ggml_metal_init: allocating
0.00.511.804 I ggml_metal_init: found device: Apple M4
0.00.511.819 I ggml_metal_init: picking default device: Apple M4
0.00.513.378 I ggml_metal_init: using embedded metal library
0.00.519.586 I ggml_metal_init: GPU name:   Apple M4
0.00.519.595 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.519.596 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.519.597 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.519.597 I ggml_metal_init: simdgroup reduction   = true
0.00.519.598 I ggml_metal_init: simdgroup matrix mul. = true
0.00.519.598 I ggml_metal_init: has residency sets    = true
0.00.519.598 I ggml_metal_init: has bfloat            = true
0.00.519.598 I ggml_metal_init: use bfloat            = true
0.00.519.600 I ggml_metal_init: hasUnifiedMemory      = true
0.00.519.604 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.539.396 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.542.963 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.542.967 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.543.003 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.546.269 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.546.270 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.546.271 I llama_init_from_model: graph nodes  = 967
0.00.546.271 I llama_init_from_model: graph splits = 2
0.00.546.275 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.546.275 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.575.051 I 
0.00.575.146 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.575.174 I perplexity: tokenizing the input ..
0.00.582.548 I perplexity: tokenization took 7.37 ms
0.00.582.561 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.724.382 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.725.794 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.725.808 I llama_perf_context_print:        load time =     566.14 ms
0.00.725.808 I llama_perf_context_print: prompt eval time =     140.93 ms /   128 tokens (    1.10 ms per token,   908.27 tokens per second)
0.00.725.809 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.725.811 I llama_perf_context_print:       total time =     150.76 ms /   129 tokens
0.00.726.214 I ggml_metal_free: deallocating

real	0m0.740s
user	0m0.080s
sys	0m0.121s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.856 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.616 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.621 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.623 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.624 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.624 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.624 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.625 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.626 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.626 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.626 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.627 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.627 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.627 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.628 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.630 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.630 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.630 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.442 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.485 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.245 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.247 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.247 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.247 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.248 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.248 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.249 I llama_model_loader: - type  f32:  194 tensors
0.00.024.249 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.249 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.250 I print_info: file format = GGUF V3 (latest)
0.00.024.250 I print_info: file type   = Q5_K - Medium
0.00.024.252 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.460 I load: special tokens cache size = 25
0.00.038.767 I load: token to piece cache size = 0.2984 MB
0.00.038.781 I print_info: arch             = gptneox
0.00.038.782 I print_info: vocab_only       = 0
0.00.038.782 I print_info: n_ctx_train      = 2048
0.00.038.783 I print_info: n_embd           = 2048
0.00.038.783 I print_info: n_layer          = 24
0.00.038.785 I print_info: n_head           = 16
0.00.038.786 I print_info: n_head_kv        = 16
0.00.038.786 I print_info: n_rot            = 32
0.00.038.786 I print_info: n_swa            = 0
0.00.038.787 I print_info: n_embd_head_k    = 128
0.00.038.787 I print_info: n_embd_head_v    = 128
0.00.038.787 I print_info: n_gqa            = 1
0.00.038.788 I print_info: n_embd_k_gqa     = 2048
0.00.038.789 I print_info: n_embd_v_gqa     = 2048
0.00.038.790 I print_info: f_norm_eps       = 1.0e-05
0.00.038.790 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.790 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.791 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.791 I print_info: f_logit_scale    = 0.0e+00
0.00.038.791 I print_info: n_ff             = 8192
0.00.038.792 I print_info: n_expert         = 0
0.00.038.793 I print_info: n_expert_used    = 0
0.00.038.794 I print_info: causal attn      = 1
0.00.038.794 I print_info: pooling type     = 0
0.00.038.794 I print_info: rope type        = 2
0.00.038.794 I print_info: rope scaling     = linear
0.00.038.794 I print_info: freq_base_train  = 10000.0
0.00.038.798 I print_info: freq_scale_train = 1
0.00.038.798 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.798 I print_info: rope_finetuned   = unknown
0.00.038.799 I print_info: ssm_d_conv       = 0
0.00.038.799 I print_info: ssm_d_inner      = 0
0.00.038.799 I print_info: ssm_d_state      = 0
0.00.038.799 I print_info: ssm_dt_rank      = 0
0.00.038.800 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.800 I print_info: model type       = 1.4B
0.00.038.800 I print_info: model params     = 1.41 B
0.00.038.800 I print_info: general.name     = 1.4B
0.00.038.801 I print_info: vocab type       = BPE
0.00.038.801 I print_info: n_vocab          = 50304
0.00.038.801 I print_info: n_merges         = 50009
0.00.038.801 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.801 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.802 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.803 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.803 I print_info: LF token         = 187 ''
0.00.038.803 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.803 I print_info: max token length = 1024
0.00.038.804 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.589.986 I load_tensors: offloading 24 repeating layers to GPU
0.00.590.027 I load_tensors: offloading output layer to GPU
0.00.590.028 I load_tensors: offloaded 25/25 layers to GPU
0.00.590.059 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.590.065 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.591.289 I llama_init_from_model: n_seq_max     = 1
0.00.591.291 I llama_init_from_model: n_ctx         = 2048
0.00.591.291 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.591.292 I llama_init_from_model: n_batch       = 2048
0.00.591.292 I llama_init_from_model: n_ubatch      = 512
0.00.591.293 I llama_init_from_model: flash_attn    = 0
0.00.591.294 I llama_init_from_model: freq_base     = 10000.0
0.00.591.294 I llama_init_from_model: freq_scale    = 1
0.00.591.295 I ggml_metal_init: allocating
0.00.591.328 I ggml_metal_init: found device: Apple M4
0.00.591.341 I ggml_metal_init: picking default device: Apple M4
0.00.592.604 I ggml_metal_init: using embedded metal library
0.00.598.800 I ggml_metal_init: GPU name:   Apple M4
0.00.598.803 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.804 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.805 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.805 I ggml_metal_init: simdgroup reduction   = true
0.00.598.805 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.806 I ggml_metal_init: has residency sets    = true
0.00.598.806 I ggml_metal_init: has bfloat            = true
0.00.598.806 I ggml_metal_init: use bfloat            = true
0.00.598.807 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.810 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.615.749 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.673.660 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.673.668 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.673.692 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.677.659 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.677.661 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.677.662 I llama_init_from_model: graph nodes  = 967
0.00.677.662 I llama_init_from_model: graph splits = 2
0.00.677.666 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.677.794 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.677.795 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.948 I main: llama threadpool init, n_threads = 4
0.00.743.997 I 
0.00.744.017 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.744.019 I 
0.00.744.203 I sampler seed: 1234
0.00.744.208 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.744.236 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.744.236 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.744.236 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.590.583 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51787.02 tokens per second)
0.01.590.583 I llama_perf_context_print:        load time =     734.37 ms
0.01.590.584 I llama_perf_context_print: prompt eval time =      52.70 ms /     7 tokens (    7.53 ms per token,   132.82 tokens per second)
0.01.590.585 I llama_perf_context_print:        eval time =     790.65 ms /    63 runs   (   12.55 ms per token,    79.68 tokens per second)
0.01.590.585 I llama_perf_context_print:       total time =     847.36 ms /    70 tokens
0.01.590.858 I ggml_metal_free: deallocating

real	0m1.607s
user	0m0.108s
sys	0m0.217s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.747 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.672 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.678 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.684 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.685 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.685 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.686 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.686 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.687 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.687 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.688 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.688 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.688 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.688 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.689 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.691 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.691 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.691 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.512 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.503 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.259 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.261 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.261 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.262 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.262 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.262 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.263 I llama_model_loader: - type  f32:  194 tensors
0.00.026.264 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.264 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.265 I print_info: file format = GGUF V3 (latest)
0.00.026.269 I print_info: file type   = Q5_K - Medium
0.00.026.270 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.762 I load: special tokens cache size = 25
0.00.041.107 I load: token to piece cache size = 0.2984 MB
0.00.041.125 I print_info: arch             = gptneox
0.00.041.126 I print_info: vocab_only       = 0
0.00.041.126 I print_info: n_ctx_train      = 2048
0.00.041.127 I print_info: n_embd           = 2048
0.00.041.127 I print_info: n_layer          = 24
0.00.041.130 I print_info: n_head           = 16
0.00.041.131 I print_info: n_head_kv        = 16
0.00.041.131 I print_info: n_rot            = 32
0.00.041.131 I print_info: n_swa            = 0
0.00.041.132 I print_info: n_embd_head_k    = 128
0.00.041.132 I print_info: n_embd_head_v    = 128
0.00.041.132 I print_info: n_gqa            = 1
0.00.041.133 I print_info: n_embd_k_gqa     = 2048
0.00.041.133 I print_info: n_embd_v_gqa     = 2048
0.00.041.137 I print_info: f_norm_eps       = 1.0e-05
0.00.041.137 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.137 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.137 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.137 I print_info: f_logit_scale    = 0.0e+00
0.00.041.138 I print_info: n_ff             = 8192
0.00.041.138 I print_info: n_expert         = 0
0.00.041.138 I print_info: n_expert_used    = 0
0.00.041.139 I print_info: causal attn      = 1
0.00.041.139 I print_info: pooling type     = 0
0.00.041.139 I print_info: rope type        = 2
0.00.041.139 I print_info: rope scaling     = linear
0.00.041.139 I print_info: freq_base_train  = 10000.0
0.00.041.140 I print_info: freq_scale_train = 1
0.00.041.140 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.140 I print_info: rope_finetuned   = unknown
0.00.041.140 I print_info: ssm_d_conv       = 0
0.00.041.140 I print_info: ssm_d_inner      = 0
0.00.041.140 I print_info: ssm_d_state      = 0
0.00.041.141 I print_info: ssm_dt_rank      = 0
0.00.041.141 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.141 I print_info: model type       = 1.4B
0.00.041.141 I print_info: model params     = 1.41 B
0.00.041.141 I print_info: general.name     = 1.4B
0.00.041.142 I print_info: vocab type       = BPE
0.00.041.142 I print_info: n_vocab          = 50304
0.00.041.142 I print_info: n_merges         = 50009
0.00.041.143 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.143 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.143 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.143 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.143 I print_info: LF token         = 187 ''
0.00.041.144 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.144 I print_info: max token length = 1024
0.00.041.144 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.596.648 I load_tensors: offloading 24 repeating layers to GPU
0.00.596.665 I load_tensors: offloading output layer to GPU
0.00.596.666 I load_tensors: offloaded 25/25 layers to GPU
0.00.596.701 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.596.702 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.598.319 I llama_init_from_model: n_seq_max     = 1
0.00.598.323 I llama_init_from_model: n_ctx         = 128
0.00.598.323 I llama_init_from_model: n_ctx_per_seq = 128
0.00.598.324 I llama_init_from_model: n_batch       = 128
0.00.598.324 I llama_init_from_model: n_ubatch      = 128
0.00.598.325 I llama_init_from_model: flash_attn    = 0
0.00.598.326 I llama_init_from_model: freq_base     = 10000.0
0.00.598.327 I llama_init_from_model: freq_scale    = 1
0.00.598.328 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.598.330 I ggml_metal_init: allocating
0.00.598.386 I ggml_metal_init: found device: Apple M4
0.00.598.399 I ggml_metal_init: picking default device: Apple M4
0.00.599.694 I ggml_metal_init: using embedded metal library
0.00.606.073 I ggml_metal_init: GPU name:   Apple M4
0.00.606.076 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.606.077 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.606.078 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.606.078 I ggml_metal_init: simdgroup reduction   = true
0.00.606.079 I ggml_metal_init: simdgroup matrix mul. = true
0.00.606.079 I ggml_metal_init: has residency sets    = true
0.00.606.079 I ggml_metal_init: has bfloat            = true
0.00.606.080 I ggml_metal_init: use bfloat            = true
0.00.606.081 I ggml_metal_init: hasUnifiedMemory      = true
0.00.606.083 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.623.055 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.626.516 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.626.522 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.626.565 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.629.788 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.629.789 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.629.790 I llama_init_from_model: graph nodes  = 967
0.00.629.790 I llama_init_from_model: graph splits = 2
0.00.629.794 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.629.794 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.338 I 
0.00.667.420 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.448 I perplexity: tokenizing the input ..
0.00.673.911 I perplexity: tokenization took 6.461 ms
0.00.673.915 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.816.323 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.817.659 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.817.668 I llama_perf_context_print:        load time =     656.58 ms
0.00.817.669 I llama_perf_context_print: prompt eval time =     142.03 ms /   128 tokens (    1.11 ms per token,   901.24 tokens per second)
0.00.817.670 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.817.670 I llama_perf_context_print:       total time =     150.33 ms /   129 tokens
0.00.818.096 I ggml_metal_free: deallocating

real	0m0.834s
user	0m0.078s
sys	0m0.145s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.813 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.425 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.429 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.430 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.431 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.431 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.431 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.432 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.433 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.433 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.433 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.434 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.434 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.434 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.437 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.438 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.439 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.439 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.190 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.183 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.924 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.925 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.925 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.925 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.926 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.926 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.926 I llama_model_loader: - type  f32:  194 tensors
0.00.023.926 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.927 I print_info: file format = GGUF V3 (latest)
0.00.023.927 I print_info: file type   = Q6_K
0.00.023.928 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.824 I load: special tokens cache size = 25
0.00.038.018 I load: token to piece cache size = 0.2984 MB
0.00.038.032 I print_info: arch             = gptneox
0.00.038.033 I print_info: vocab_only       = 0
0.00.038.034 I print_info: n_ctx_train      = 2048
0.00.038.034 I print_info: n_embd           = 2048
0.00.038.034 I print_info: n_layer          = 24
0.00.038.037 I print_info: n_head           = 16
0.00.038.038 I print_info: n_head_kv        = 16
0.00.038.038 I print_info: n_rot            = 32
0.00.038.038 I print_info: n_swa            = 0
0.00.038.038 I print_info: n_embd_head_k    = 128
0.00.038.039 I print_info: n_embd_head_v    = 128
0.00.038.039 I print_info: n_gqa            = 1
0.00.038.041 I print_info: n_embd_k_gqa     = 2048
0.00.038.042 I print_info: n_embd_v_gqa     = 2048
0.00.038.042 I print_info: f_norm_eps       = 1.0e-05
0.00.038.043 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.043 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.043 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.043 I print_info: f_logit_scale    = 0.0e+00
0.00.038.044 I print_info: n_ff             = 8192
0.00.038.044 I print_info: n_expert         = 0
0.00.038.044 I print_info: n_expert_used    = 0
0.00.038.044 I print_info: causal attn      = 1
0.00.038.044 I print_info: pooling type     = 0
0.00.038.044 I print_info: rope type        = 2
0.00.038.046 I print_info: rope scaling     = linear
0.00.038.046 I print_info: freq_base_train  = 10000.0
0.00.038.046 I print_info: freq_scale_train = 1
0.00.038.046 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.047 I print_info: rope_finetuned   = unknown
0.00.038.047 I print_info: ssm_d_conv       = 0
0.00.038.047 I print_info: ssm_d_inner      = 0
0.00.038.047 I print_info: ssm_d_state      = 0
0.00.038.047 I print_info: ssm_dt_rank      = 0
0.00.038.047 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.047 I print_info: model type       = 1.4B
0.00.038.047 I print_info: model params     = 1.41 B
0.00.038.048 I print_info: general.name     = 1.4B
0.00.038.048 I print_info: vocab type       = BPE
0.00.038.049 I print_info: n_vocab          = 50304
0.00.038.049 I print_info: n_merges         = 50009
0.00.038.049 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.049 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.049 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.049 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.049 I print_info: LF token         = 187 ''
0.00.038.050 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.050 I print_info: max token length = 1024
0.00.038.050 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.632.327 I load_tensors: offloading 24 repeating layers to GPU
0.00.632.345 I load_tensors: offloading output layer to GPU
0.00.632.345 I load_tensors: offloaded 25/25 layers to GPU
0.00.632.379 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.632.381 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.634.045 I llama_init_from_model: n_seq_max     = 1
0.00.634.047 I llama_init_from_model: n_ctx         = 2048
0.00.634.048 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.634.048 I llama_init_from_model: n_batch       = 2048
0.00.634.049 I llama_init_from_model: n_ubatch      = 512
0.00.634.049 I llama_init_from_model: flash_attn    = 0
0.00.634.051 I llama_init_from_model: freq_base     = 10000.0
0.00.634.051 I llama_init_from_model: freq_scale    = 1
0.00.634.052 I ggml_metal_init: allocating
0.00.634.079 I ggml_metal_init: found device: Apple M4
0.00.634.089 I ggml_metal_init: picking default device: Apple M4
0.00.635.373 I ggml_metal_init: using embedded metal library
0.00.641.770 I ggml_metal_init: GPU name:   Apple M4
0.00.641.774 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.641.775 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.641.776 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.641.776 I ggml_metal_init: simdgroup reduction   = true
0.00.641.776 I ggml_metal_init: simdgroup matrix mul. = true
0.00.641.777 I ggml_metal_init: has residency sets    = true
0.00.641.777 I ggml_metal_init: has bfloat            = true
0.00.641.777 I ggml_metal_init: use bfloat            = true
0.00.641.778 I ggml_metal_init: hasUnifiedMemory      = true
0.00.641.780 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.658.780 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.712.141 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.712.150 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.712.180 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.716.401 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.716.403 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.716.403 I llama_init_from_model: graph nodes  = 967
0.00.716.404 I llama_init_from_model: graph splits = 2
0.00.716.413 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.716.534 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.716.535 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.784.499 I main: llama threadpool init, n_threads = 4
0.00.784.550 I 
0.00.784.571 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.574 I 
0.00.784.752 I sampler seed: 1234
0.00.784.757 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.784.772 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.784.775 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.784.775 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.671.325 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53910.40 tokens per second)
0.01.671.325 I llama_perf_context_print:        load time =     774.97 ms
0.01.671.326 I llama_perf_context_print: prompt eval time =      57.44 ms /     7 tokens (    8.21 ms per token,   121.87 tokens per second)
0.01.671.328 I llama_perf_context_print:        eval time =     826.24 ms /    63 runs   (   13.11 ms per token,    76.25 tokens per second)
0.01.671.328 I llama_perf_context_print:       total time =     887.54 ms /    70 tokens
0.01.671.577 I ggml_metal_free: deallocating

real	0m1.689s
user	0m0.108s
sys	0m0.209s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4840 (3ffbbd5c) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.733 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.471 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.478 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.479 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.480 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.480 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.481 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.481 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.482 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.482 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.483 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.483 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.484 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.484 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.485 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.486 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.487 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.487 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.320 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.341 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.229 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.230 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.231 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.231 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.231 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.232 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.232 I llama_model_loader: - type  f32:  194 tensors
0.00.024.233 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.233 I print_info: file format = GGUF V3 (latest)
0.00.024.234 I print_info: file type   = Q6_K
0.00.024.235 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.622 I load: special tokens cache size = 25
0.00.039.035 I load: token to piece cache size = 0.2984 MB
0.00.039.053 I print_info: arch             = gptneox
0.00.039.054 I print_info: vocab_only       = 0
0.00.039.054 I print_info: n_ctx_train      = 2048
0.00.039.054 I print_info: n_embd           = 2048
0.00.039.054 I print_info: n_layer          = 24
0.00.039.058 I print_info: n_head           = 16
0.00.039.059 I print_info: n_head_kv        = 16
0.00.039.059 I print_info: n_rot            = 32
0.00.039.059 I print_info: n_swa            = 0
0.00.039.059 I print_info: n_embd_head_k    = 128
0.00.039.060 I print_info: n_embd_head_v    = 128
0.00.039.060 I print_info: n_gqa            = 1
0.00.039.061 I print_info: n_embd_k_gqa     = 2048
0.00.039.061 I print_info: n_embd_v_gqa     = 2048
0.00.039.062 I print_info: f_norm_eps       = 1.0e-05
0.00.039.062 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.062 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.063 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.063 I print_info: f_logit_scale    = 0.0e+00
0.00.039.063 I print_info: n_ff             = 8192
0.00.039.064 I print_info: n_expert         = 0
0.00.039.064 I print_info: n_expert_used    = 0
0.00.039.066 I print_info: causal attn      = 1
0.00.039.066 I print_info: pooling type     = 0
0.00.039.066 I print_info: rope type        = 2
0.00.039.066 I print_info: rope scaling     = linear
0.00.039.066 I print_info: freq_base_train  = 10000.0
0.00.039.067 I print_info: freq_scale_train = 1
0.00.039.067 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.067 I print_info: rope_finetuned   = unknown
0.00.039.069 I print_info: ssm_d_conv       = 0
0.00.039.069 I print_info: ssm_d_inner      = 0
0.00.039.069 I print_info: ssm_d_state      = 0
0.00.039.069 I print_info: ssm_dt_rank      = 0
0.00.039.069 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.070 I print_info: model type       = 1.4B
0.00.039.070 I print_info: model params     = 1.41 B
0.00.039.070 I print_info: general.name     = 1.4B
0.00.039.071 I print_info: vocab type       = BPE
0.00.039.071 I print_info: n_vocab          = 50304
0.00.039.071 I print_info: n_merges         = 50009
0.00.039.071 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.071 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.072 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.076 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.077 I print_info: LF token         = 187 ''
0.00.039.077 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.077 I print_info: max token length = 1024
0.00.039.077 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.590.212 I load_tensors: offloading 24 repeating layers to GPU
0.00.590.219 I load_tensors: offloading output layer to GPU
0.00.590.219 I load_tensors: offloaded 25/25 layers to GPU
0.00.590.251 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.590.253 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.591.586 I llama_init_from_model: n_seq_max     = 1
0.00.591.588 I llama_init_from_model: n_ctx         = 128
0.00.591.589 I llama_init_from_model: n_ctx_per_seq = 128
0.00.591.589 I llama_init_from_model: n_batch       = 128
0.00.591.589 I llama_init_from_model: n_ubatch      = 128
0.00.591.590 I llama_init_from_model: flash_attn    = 0
0.00.591.591 I llama_init_from_model: freq_base     = 10000.0
0.00.591.591 I llama_init_from_model: freq_scale    = 1
0.00.591.592 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.591.594 I ggml_metal_init: allocating
0.00.591.713 I ggml_metal_init: found device: Apple M4
0.00.591.727 I ggml_metal_init: picking default device: Apple M4
0.00.593.058 I ggml_metal_init: using embedded metal library
0.00.598.923 I ggml_metal_init: GPU name:   Apple M4
0.00.598.927 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.928 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.929 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.929 I ggml_metal_init: simdgroup reduction   = true
0.00.598.929 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.930 I ggml_metal_init: has residency sets    = true
0.00.598.930 I ggml_metal_init: has bfloat            = true
0.00.598.930 I ggml_metal_init: use bfloat            = true
0.00.598.931 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.933 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.615.426 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.618.876 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.618.880 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.618.906 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.622.164 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.622.165 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.622.166 I llama_init_from_model: graph nodes  = 967
0.00.622.166 I llama_init_from_model: graph splits = 2
0.00.622.169 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.622.169 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.858 I 
0.00.655.952 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.655.980 I perplexity: tokenizing the input ..
0.00.662.724 I perplexity: tokenization took 6.742 ms
0.00.662.728 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.793.583 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.794.932 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.794.950 I llama_perf_context_print:        load time =     647.12 ms
0.00.794.951 I llama_perf_context_print: prompt eval time =     130.63 ms /   128 tokens (    1.02 ms per token,   979.89 tokens per second)
0.00.794.952 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.794.952 I llama_perf_context_print:       total time =     139.10 ms /   129 tokens
0.00.795.381 I ggml_metal_free: deallocating

real	0m0.809s
user	0m0.077s
sys	0m0.130s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4840 (3ffbbd5c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x145804d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x145805470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x145805a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x145805fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145806580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x145806b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1458070e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x145807690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x145807c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x145808140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x145808640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x145808b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x145809660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x145809e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14580a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14580ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14580b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14580bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14580c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14580ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14580d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14580d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14580dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14580e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14580ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14580f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14580f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1458104d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145810a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x145810cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x145811170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x145811430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145811cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145812200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1458124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x145812960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145812e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1458132a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145813740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145813be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x145814080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x145814520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1458149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x145814e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145815120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145815730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x145815d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x145816660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x145816c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145817280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x145817890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x145817ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1458184b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x145818ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1458192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x145819750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x145819bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x145819eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14581a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14581acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14581af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14581b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14581b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14581bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14581c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14581c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14581cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14581cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14581d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14581d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14581ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14581e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14581e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14581ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14581f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14581f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14581fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x145820180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1458206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x145820c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x145821170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1458216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x145821c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x145822160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1458226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x145822c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x145823150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1458236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x145823bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x145824140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x145824690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x145824be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x145825130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x145825680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x145825bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x145826120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x145826670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x145816350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x145826ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x145827290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1458277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x145827d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x145828280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1458287d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x145828d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x145829270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1458297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145829d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14582a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14582a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14582ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14582b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14582b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14582bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14582c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14582c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14582ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14582cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14582d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14582d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14582dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14582e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14582e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14582ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14582ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14582f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14582f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14582fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1458301a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x145830640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x145830ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x145830f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x145831420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1458318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x145831d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x145832200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1458326a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x145832b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x145832fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x145833480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x145833920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x145833dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x145834260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x145834700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x145834ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x145835040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1458354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x145835980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x145835e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1458362c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x145836760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x145836c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1458370a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x145837540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1458379e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145837e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x145838320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1458387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x145838c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x145839100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1458395a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145839a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x145839ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14583a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14583a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14583acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14583b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14583b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14583baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14583bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14583c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14583c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14583cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14583d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14583d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14583db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14583dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14583e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14583e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14583ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14583f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14583f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14583fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x145840000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1458404a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x145840940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x145840de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x145841280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x145841720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x145841bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x145842060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x145842500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1458429a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x145842ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x145843440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x145843990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x145843ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1458441a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1458447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x145844dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1458453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x145845bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x145846060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x145846320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145846930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x145846f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145847730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145847bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x145848070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145848510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x145848cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x145849210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x145849760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x145849cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14584a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14584a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14584aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14584b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14584b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14584bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14584c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14584c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14584cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14584d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14584d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14584dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14584e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14584e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14584ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14584f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14584f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14584fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1458501a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1458506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x145850c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x145851190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1458516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x145851c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x145852180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1458526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x145852c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x145853170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1458536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x145853c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x145854160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1458546b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x145854c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x145855150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1458556a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x145855bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x145856140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x145856690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145856be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x145857130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x145857680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x145857bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x145858120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x145858670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x145858bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145859110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x145859660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145859bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14585a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14585a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14585aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14585b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14585b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14585bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14585bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14585c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14585c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14585cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14585d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14585d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14585db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14585dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14585e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14585e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14585edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14585f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14585f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14585fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x145860040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x1458604e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x145860980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x145860e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x1458612c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x145861760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x145861c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1458620a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x145862540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1458629e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x145862f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x145863650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x145863d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x145864490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x145864bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x145864e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x145865660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x145865920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x145865f30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.722.927 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.722.931 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x108804ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x108804f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1088053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x108805830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x108805ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x108806110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x108806580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1088069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x108806e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1088073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x108807850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x108807ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1088089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1088091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1088099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10880a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10880a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10880af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10880b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10880be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10880c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10880cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10880d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10880da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10880e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10880e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10880e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10880eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10880f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10880f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10880f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10880fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x108810280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x108810540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1088109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x108810e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x108811290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x108811700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x108811b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x108811fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x108812450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1088128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x108812d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1088131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x108813610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x108813a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x108813ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x108814360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1088147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x108814c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1088150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x108815520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x108815990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x108815e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x108816270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1088166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x108816c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x108817150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1088175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x108817a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x108817ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x108818310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x108818780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x108818bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x108819060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1088194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x108819940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x108819db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10881a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10881a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10881ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10881af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10881b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10881b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10881bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10881c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10881c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10881ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10881ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10881d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10881d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10881dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10881e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10881e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10881e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10881ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10881f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10881f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10881fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10881ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1088203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x108820830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x108820ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x108821110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x108821580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1088219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x108821e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1088222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x108822740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x108822bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x108823020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x108823490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x108823900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x108823d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1088241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x108824650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x108824ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x108824f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1088253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x108825810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x108825c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1088260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x108826560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1088269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x108826e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1088272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x108827720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x108827b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x108828000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x108828470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1088288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x108828d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1088291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x108829630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x108829aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x108829f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10882a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10882a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10882ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10882b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10882b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10882b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10882be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10882c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10882c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10882cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10882cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10882d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10882d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10882dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10882e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10882e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10882ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10882eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10882f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10882f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10882fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1088300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x108830520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x108830990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x108830e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x108831270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1088316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x108831b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x108831fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x108832430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1088328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x108832d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x108833180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1088335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x108833a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x108833ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x108834340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1088347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x108834c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x108835090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x108835cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x108835f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x108836240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1088366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x108836b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x108836f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x108837400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x108837870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x108837ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x108838150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1088385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x108838a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x108838ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x108839310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x108839780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x108839bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10883a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10883a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10883a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10883adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10883b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10883b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10883bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10883bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10883c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10883c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10883ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10883d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10883d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10883da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10883de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10883e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10883e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10883ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10883f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10883f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10883fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10883ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x108840390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x108840800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x108840c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1088410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x108841600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x108841b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x108842680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x108842940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x108842f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1088434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x108843a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x108844040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x108844600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x108844bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x108845180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x108845740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x108845d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1088462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x108846880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x108846e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x108847400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1088479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x108847f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x108848540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x108848b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1088490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x108849680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x108849c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10884a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10884a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10884ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10884b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10884b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10884bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10884c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10884ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10884d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10884d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10884db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10884e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10884e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10884ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10884f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10884f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10884fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1088503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x108850980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x108850f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x108851500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x108851ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x108852080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x108852640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x108852c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1088531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x108853780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x108853d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x108854300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1088548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x108854e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x108855440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x108855a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x108855fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x108856580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x108856b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x108857040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x108857540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x108857a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x108857f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x108858440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x108858940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x108858e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x108859340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x108859840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x108859d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10885a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10885a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10885ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10885b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x10885b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x10885bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x10885c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x10885c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x10885ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x10885cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x10885d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x10885d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x10885de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x10885e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10885e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10885f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10885f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x108860090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1088607b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x108860a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x108861260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x108861520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x108861b30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x144704e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1447052c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x144705730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x144705ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x144706010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x144706480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1447068f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x144706d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1447071d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x144707640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x144707ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1447081d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x144708cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1447094a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x144709cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14470a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14470aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14470b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14470b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14470c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14470c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14470cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14470d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14470dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14470e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14470e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14470e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14470edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14470f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14470f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14470fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x144710070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1447104e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1447107a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x144710c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x144711080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1447114f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x144711960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x144711dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x144712240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1447126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x144712b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x144712f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x144713400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x144713870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x144713ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x144714150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1447145c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x144714a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x144714ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x144715310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x144715780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x144715bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x144716060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1447164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x144716940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x144716eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1447173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x144717820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x144717c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x144718100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x144718570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1447189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x144718e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1447192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x144719730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x144719ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14471a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14471a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14471a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14471ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14471b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14471b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14471bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14471bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14471c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14471c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14471cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14471d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14471d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14471d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14471de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14471e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14471e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14471eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14471eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14471f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14471f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14471fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1447201b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x144720620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x144720a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x144720f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x144721370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1447217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x144721c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1447220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x144722530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x144722d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1447232b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x144723860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x144723e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1447243c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x144724970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x144724f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1447254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x144725a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x144726030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1447265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x144726b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x144727140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1447276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x144727ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x144728250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x144728750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x144728c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x144729150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x144729650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x144729b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14472a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14472a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14472aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14472af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14472b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14472b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14472be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14472c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14472c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14472cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14472d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14472d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14472dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14472e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14472e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14472eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14472f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14472f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14472fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14472ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x144730450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x144730950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x144730e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x144731350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x144731850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x144731d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x144732250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x144732750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x144732c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x144733150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x144733650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x144733b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x144734050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x144734550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x144734a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x144734f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x144735450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x144735950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x144735e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x144736350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x144736850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x144736d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x144737250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x144737750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x144737c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x144738150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x144738650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x144738b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x144739050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x144739550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x144739a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x144739f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14473a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14473a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14473ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14473b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14473b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14473bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14473c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14473c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14473cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14473d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14473d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14473db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14473e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14473e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14473ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14473ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14473f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14473f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14473fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x144740350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x144740850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x144740d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x144741250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x144741800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x144741db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x144742360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x144742910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x144742f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x144743530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x144743b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x144744330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1447447d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x144744a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1447450a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1447456b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x144745ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x144746340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1447467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x144746c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x144747430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x144747980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x144747ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x144748420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x144748970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x144748ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x144749410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x144749960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x144749eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14474a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14474a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14474aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14474b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14474b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14474be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14474c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14474c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14474ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14474d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14474d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14474de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14474e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14474e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14474ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14474f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14474f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14474fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1447503a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1447508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x144750e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x144751390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1447518e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x144751e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x144752380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1447528d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x144752e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x144753370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1447538c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x144753e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x144754360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1447548b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x144754e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x144755350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1447558a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x144755df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x144756340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x144756890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x144756de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x144757330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x144757880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x144757dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x144758320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x144758870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x144758dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x144759310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x144759860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x144759db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14475a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14475a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14475ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14475b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14475b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14475b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14475be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14475c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14475c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14475cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14475d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14475d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14475d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14475de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14475e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14475e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x14475ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x14475f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x14475f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x14475fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x14475fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x144760370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x144760810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x144760cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x144761150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1447616a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x144761dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1447624e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x144762c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x144763320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1447635e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x144763dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x144764090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1447646a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.820s
user	0m0.281s
sys	0m0.328s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4840 (3ffbbd5c)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13e610aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13e6111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13e611790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13e611d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13e6122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13e6128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13e612e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13e613400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13e6139b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13e613eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13e6143b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13e6148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13e6153d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13e615b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13e616390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13e616ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13e6171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13e6178f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13e618010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13e6187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13e618f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e619620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13e619d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13e61a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13e61ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13e61afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e61b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13e61c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e61c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e61ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13e61cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13e61d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13e61da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13e61df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e61e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13e61e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e61eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e61f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13e61f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13e61f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e61fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e620290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e620730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e620bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e620e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e6214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e621ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13e6223d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e6229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e622ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13e623600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e623c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13e624220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13e624830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13e625020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13e6254c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13e625960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13e625c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13e626230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13e626a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13e626ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13e627180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13e627620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13e627ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e627f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13e628400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e6288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13e628d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13e6291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13e629680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13e629b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13e629fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e62a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e62a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e62af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e62b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e62b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e62bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e62c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e62c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e62cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13e62d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e62d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e62ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13e62e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e62e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e62eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e62f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e62f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13e62feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e630400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e630950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e630ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e6313f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e631940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13e631e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e6323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13e6220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e632850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13e633000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13e633550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e633aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13e633ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e634540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13e634a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e634fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13e635530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13e635a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13e635fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13e636520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13e636a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13e636fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e637510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13e6379b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13e637e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e6382f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13e638790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13e638c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e6390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13e639570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e639a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13e639eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e63a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13e63a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e63ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e63b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13e63b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e63ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e63bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13e63c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e63c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e63ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13e63d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e63d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13e63dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13e63df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13e63e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e63e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13e63ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e63f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e63f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e63fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13e63ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13e640470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13e640910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13e640db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13e641250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13e6416f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e641b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e642030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13e6424d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13e642970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13e642e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13e6432b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13e643750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e643bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13e644090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13e644530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13e6449d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e644e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13e645310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13e6457b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e645c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e6460f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13e646590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e646a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13e646ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13e647370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e647810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e647cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e648150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13e6485f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13e648a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13e648f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13e6493d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13e649870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13e649d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13e64a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13e64a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13e64aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e64af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13e64b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13e64b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e64bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e64c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13e64c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13e64cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13e64cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13e64d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13e64d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e64ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e64e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e64e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e64ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13e64f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13e64f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e64fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13e64ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13e650520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13e650b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13e651140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e651930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e651dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13e652090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13e6526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13e652cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e6534a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13e653940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e653de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e654280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e654a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e654f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13e6554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13e655a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13e655f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e6564c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13e656a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13e656f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13e6574b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13e657a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13e657f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13e6584a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13e6589f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13e658f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e659490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e6599e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13e659f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e65a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e65a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13e65af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e65b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13e65b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e65bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13e65c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e65c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13e65cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e65d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13e65d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13e65def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13e65e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e65e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e65eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e65f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13e65f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e65fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13e660420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13e660970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13e660ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13e661410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e661960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13e661eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e662400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13e662950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13e662ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13e6633f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e663940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e663e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e6643e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e664930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e664e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e6653d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13e665920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13e665e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13e6663c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13e666910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13e666e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13e6673b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13e667850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13e667cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13e668190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13e668630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13e668ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13e668f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13e669410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13e6698b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13e669d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13e66a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13e66a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13e66ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13e66afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13e66b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13e66b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13e66bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13e66c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13e66c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13e66cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13e66d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13e66d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13e66d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13e66de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13e66e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13e66e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13e66eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13e66f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13e66fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13e670200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13e670920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13e670be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13e6713d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13e671690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e671ca0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.099.840 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.845 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13e705780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13e705bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13e706060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13e7064d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13e706940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13e706db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13e707220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13e707690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13e707b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13e707f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13e7083e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13e708ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13e7095f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13e709da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13e70a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13e70acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13e70b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13e70bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13e70c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13e70c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13e70d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e70d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13e70dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13e70e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13e70ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13e70efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e70f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13e70f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e70fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e70ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13e710440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13e710970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13e710de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13e7110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e711510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13e711980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e711df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e712260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13e7126d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13e712b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e712fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e713420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e713890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e713d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e714170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e7145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e714a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13e714ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e715330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e7157a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13e715c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e716080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13e7164f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13e716960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13e716dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13e717240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13e7177b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13e717cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13e718120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13e718590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13e718a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13e718e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13e7192e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13e719750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e719bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13e71a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e71a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13e71a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13e71ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13e71b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13e71b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13e71bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e71bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e71c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e71c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e71cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e71d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e71d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e71d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e71de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e71e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13e71e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e71eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e71f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13e71f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e71f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e71fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e7201d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e720640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13e720ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e720f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e721390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e721800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e721c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e7220e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13e722550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e7229c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13e722e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e7232a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13e723710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13e723b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e723ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13e724460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e7248d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13e724d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e7251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13e725620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13e725a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13e725f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13e726370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13e7267e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13e726c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e7270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13e727530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13e7279a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e727e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13e728280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13e7286f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e728b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13e728fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e729440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13e7298b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e729d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13e72a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e72a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e72aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13e72aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e72b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e72b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13e72bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e72c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e72c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13e72c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e72cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13e72d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13e72d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13e72db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e72dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13e72e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e72e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e72ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e72f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13e72f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13e72fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13e72fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13e730330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13e7307a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13e730c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e731080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e7314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13e731960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13e731dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13e732240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13e7326b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13e732b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e732f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13e733400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13e733870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13e733ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e734150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13e7345c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13e734a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e734ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e735310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13e735780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e735bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13e736820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13e736ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e736da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e737210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e737680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13e737af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13e737f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13e7383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13e738840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13e738cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13e739120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13e739590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13e739a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13e739e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e73a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13e73a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13e73abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e73b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e73b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13e73b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13e73bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13e73c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13e73c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13e73cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e73cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e73d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e73d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e73dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13e73e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13e73e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e73e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13e73ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13e73f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13e73f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13e73fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e740010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e740570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13e740a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13e740ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13e741360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e7417d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13e741c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e742160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e742670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e7431e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e7434a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13e743a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13e744020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13e7445e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e744ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13e745160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13e745720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13e745ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13e7462a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13e746860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13e746e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13e7473e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13e7479a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e747f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e748520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13e748ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e7490a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e749660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13e749c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e74a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13e74a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e74ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13e74b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e74b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13e74bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e74c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13e74ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13e74cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13e74d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e74db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e74e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e74e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13e74eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e74f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13e74f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13e74fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13e7503a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13e750960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e750f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13e7514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e751aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13e752060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13e752620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13e752be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e7531a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e753760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e753d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e7542e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e7548a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e754e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13e755420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13e7559e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13e755fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13e756560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13e756b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13e7570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13e7576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13e757ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13e7580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13e7585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13e758aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13e758fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13e7594a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13e7599a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13e759ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13e75a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13e75a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13e75ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13e75b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13e75b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13e75bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13e75c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13e75c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13e75cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13e75d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13e75d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13e75daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13e75dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13e75e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13e75e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13e75eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13e75f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13e75fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13e7604d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13e760bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13e761310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13e7615d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13e761dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13e762080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e762690 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1400046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x140004b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x140004fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x140005430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1400058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x140005d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x140006180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1400065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x140006a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x140006fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x140007440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x140007ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1400085e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x140008d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1400095a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x140009cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14000a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14000ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14000b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14000b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14000c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14000c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14000cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14000d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14000dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14000e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14000e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14000e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14000ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14000f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14000f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14000fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14000fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x140010130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1400105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x140010a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x140010e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1400112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x140011760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x140011bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x140012040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1400124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x140012920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x140012d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x140013200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x140013670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x140013ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x140013f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1400143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x140014830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x140014ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x140015110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x140015580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1400159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x140015e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1400162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x140016840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x140016d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1400171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x140017620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x140017a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x140017f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x140018370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1400187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x140018c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1400190c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x140019530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1400199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x140019e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14001a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14001a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14001ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14001afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14001b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14001b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14001bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14001c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14001c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14001ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14001cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14001d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14001d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14001dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14001e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14001e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14001e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14001edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14001f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14001f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14001fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14001ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x140020420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x140020890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x140020d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x140021170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1400215e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x140021a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x140021ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x140022720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x140022c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1400231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1400237a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x140023d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x140024300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1400248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x140024e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x140025410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1400259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x140025f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x140026520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x140026ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x140027080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x140027630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x140027be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1400280e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1400285e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x140028ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x140028fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1400294e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1400299e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x140029ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14002a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14002a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14002ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14002b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14002b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14002bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14002c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14002c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14002cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14002d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14002d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14002dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14002dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14002e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14002e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14002eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14002f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14002f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14002fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1400302e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1400307e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x140030ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1400311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1400316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x140031be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1400320e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1400325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x140032ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x140032fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1400334e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1400339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x140033ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1400343e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1400348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x140034de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1400352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1400357e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x140035ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1400361e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1400366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x140036be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1400370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1400375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x140037ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x140037fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1400384e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1400389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x140038ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1400393e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1400398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x140039de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14003a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14003a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14003ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14003b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14003b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14003bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14003c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14003c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14003cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14003cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14003d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14003d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14003dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14003e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14003e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14003ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14003f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14003f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14003fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1400401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1400406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x140040be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x140041190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x140041740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x140041cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1400422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1400428b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x140042ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1400434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x140043cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x140044160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x140044420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x140044a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x140045040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x140045830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x140045cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x140046170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x140046610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x140046dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x140047310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x140047860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x140047db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x140048300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x140048850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x140048da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1400492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x140049840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x140049d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14004a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14004a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14004ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14004b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14004b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14004bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14004c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14004c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14004cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14004d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14004d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14004dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14004e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14004e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14004ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14004f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14004f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14004fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x140050280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1400507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x140050d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x140051270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1400517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x140051d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x140052260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1400527b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x140052d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x140053250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1400537a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x140053cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x140054240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x140054790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x140054ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x140055230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x140055780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x140055cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x140056220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x140056770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x140056cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x140057210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x140057760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x140057cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x140058200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x140058750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x140058ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1400591f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x140059740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x140059be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14005a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14005a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14005a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14005ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14005b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14005b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14005bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14005c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14005c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14005ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14005cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14005d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14005d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14005dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14005e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x14005e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x14005ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x14005ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x14005f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x14005f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x14005fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1400601a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x140060640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x140060ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x140061030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x140061750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x140061e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x140062590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x140062cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x140062f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x140063760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x140063a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x140064030 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.983s
user	0m0.231s
sys	0m0.187s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.07 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.51 sec*proc (2 tests)

Total Test time (real) =   1.52 sec
        1.54 real         0.52 user         0.20 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.28 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.31 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.58 sec*proc (2 tests)

Total Test time (real) =   0.60 sec
        0.60 real         0.13 user         0.09 sys
```
