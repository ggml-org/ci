Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.544s
user	0m0.871s
sys	0m1.218s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target sha256
[  5%] Built target sha1
[  5%] Built target build_info
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Linking CXX executable ../../bin/llama-gguf
[ 24%] Linking CXX executable ../../bin/llama-gguf-hash
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 30%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 35%] Linking CXX executable ../../bin/llama-simple
[ 35%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llava
[ 36%] Built target llama-simple
[ 36%] Built target test-c
[ 36%] Built target llama-quantize-stats
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library libllava_shared.dylib
[ 37%] Built target llama-simple-chat
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 42%] Built target llava_shared
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-sampling
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Built target test-log
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Built target test-arg-parser
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-chat-template
[ 58%] Linking CXX executable ../bin/test-gguf
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-backend-ops
[ 63%] Built target test-chat-template
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Linking CXX executable ../bin/test-rope
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-autorelease
[ 64%] Built target test-gguf
[ 64%] Built target test-barrier
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-quantize-perf
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Built target test-rope
[ 65%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Built target llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-embedding
[ 73%] Built target llama-eval-callback
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-batched
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-infill
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Built target llama-bench
[ 79%] Linking CXX executable ../../bin/llama-lookup
[ 79%] Built target llama-lookahead
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Built target llama-lookup
[ 83%] Built target llama-lookup-create
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-cli
[ 83%] Built target llama-parallel
[ 84%] Generating loading.html.hpp
[ 84%] Generating index.html.gz.hpp
[ 84%] Built target llama-passkey
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Built target llama-perplexity
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Built target llama-retrieval
[ 86%] Built target llama-quantize
[ 86%] Linking CXX executable ../../bin/llama-run
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Built target llama-run
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-speculative
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-tts
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Built target llama-gen-docs
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Built target llama-cvector-generator
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 98%] Linking CXX executable ../../bin/llama-export-lora
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.995s
user	0m5.966s
sys	0m9.643s

main: quantize time =  2198.28 ms
main:    total time =  2198.28 ms

main: quantize time =  1332.08 ms
main:    total time =  1332.08 ms

main: quantize time =  1314.31 ms
main:    total time =  1314.31 ms

main: quantize time =  2130.04 ms
main:    total time =  2130.04 ms

main: quantize time =  2851.79 ms
main:    total time =  2851.79 ms

main: quantize time =  4977.50 ms
main:    total time =  4977.50 ms

main: quantize time =  5766.84 ms
main:    total time =  5766.84 ms

main: quantize time =  6851.89 ms
main:    total time =  6851.89 ms

main: quantize time =  6124.30 ms
main:    total time =  6124.30 ms

main: quantize time =  4551.16 ms
main:    total time =  4551.16 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.166 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.277 I main: llama backend init
0.00.000.283 I main: load the model and apply lora adapter, if any
0.00.041.978 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.054.477 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.054.490 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.054.493 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.054.494 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.054.495 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.054.509 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.054.510 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.054.514 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.054.514 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.054.515 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.054.517 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.054.517 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.054.518 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.054.519 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.054.524 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.054.525 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.054.525 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.063.009 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.065.160 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.073.150 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.073.153 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.073.154 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.073.154 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.073.155 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.073.155 I llama_model_loader: - type  f32:  194 tensors
0.00.073.156 I llama_model_loader: - type  f16:   98 tensors
0.00.073.157 I print_info: file format = GGUF V3 (latest)
0.00.073.158 I print_info: file type   = all F32 (guessed)
0.00.073.160 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.103.494 I load: special tokens cache size = 25
0.00.110.743 I load: token to piece cache size = 0.2984 MB
0.00.110.747 I print_info: arch             = gptneox
0.00.110.747 I print_info: vocab_only       = 0
0.00.110.747 I print_info: n_ctx_train      = 2048
0.00.110.747 I print_info: n_embd           = 2048
0.00.110.747 I print_info: n_layer          = 24
0.00.110.751 I print_info: n_head           = 16
0.00.110.752 I print_info: n_head_kv        = 16
0.00.110.753 I print_info: n_rot            = 32
0.00.110.753 I print_info: n_swa            = 0
0.00.110.754 I print_info: n_embd_head_k    = 128
0.00.110.754 I print_info: n_embd_head_v    = 128
0.00.110.754 I print_info: n_gqa            = 1
0.00.110.755 I print_info: n_embd_k_gqa     = 2048
0.00.110.756 I print_info: n_embd_v_gqa     = 2048
0.00.110.756 I print_info: f_norm_eps       = 1.0e-05
0.00.110.757 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.110.757 I print_info: f_clamp_kqv      = 0.0e+00
0.00.110.757 I print_info: f_max_alibi_bias = 0.0e+00
0.00.110.759 I print_info: f_logit_scale    = 0.0e+00
0.00.110.759 I print_info: n_ff             = 8192
0.00.110.760 I print_info: n_expert         = 0
0.00.110.760 I print_info: n_expert_used    = 0
0.00.110.760 I print_info: causal attn      = 1
0.00.110.760 I print_info: pooling type     = 0
0.00.110.760 I print_info: rope type        = 2
0.00.110.760 I print_info: rope scaling     = linear
0.00.110.761 I print_info: freq_base_train  = 10000.0
0.00.110.761 I print_info: freq_scale_train = 1
0.00.110.761 I print_info: n_ctx_orig_yarn  = 2048
0.00.110.761 I print_info: rope_finetuned   = unknown
0.00.110.761 I print_info: ssm_d_conv       = 0
0.00.110.762 I print_info: ssm_d_inner      = 0
0.00.110.762 I print_info: ssm_d_state      = 0
0.00.110.762 I print_info: ssm_dt_rank      = 0
0.00.110.762 I print_info: ssm_dt_b_c_rms   = 0
0.00.110.762 I print_info: model type       = 1.4B
0.00.110.762 I print_info: model params     = 1.41 B
0.00.110.763 I print_info: general.name     = 1.4B
0.00.110.763 I print_info: vocab type       = BPE
0.00.110.763 I print_info: n_vocab          = 50304
0.00.110.764 I print_info: n_merges         = 50009
0.00.110.767 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.110.768 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.110.768 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.110.768 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.110.768 I print_info: LF token         = 128 'Ä'
0.00.110.769 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.110.769 I print_info: max token length = 1024
0.00.113.428 I load_tensors: offloading 24 repeating layers to GPU
0.00.113.428 I load_tensors: offloading output layer to GPU
0.00.113.428 I load_tensors: offloaded 25/25 layers to GPU
0.00.113.448 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.113.449 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.113.758 I llama_init_from_model: n_seq_max     = 1
0.00.113.759 I llama_init_from_model: n_ctx         = 2048
0.00.113.760 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.113.760 I llama_init_from_model: n_batch       = 2048
0.00.113.760 I llama_init_from_model: n_ubatch      = 512
0.00.113.760 I llama_init_from_model: flash_attn    = 0
0.00.113.761 I llama_init_from_model: freq_base     = 10000.0
0.00.113.761 I llama_init_from_model: freq_scale    = 1
0.00.113.761 I ggml_metal_init: allocating
0.00.113.764 I ggml_metal_init: found device: Apple M4
0.00.113.766 I ggml_metal_init: picking default device: Apple M4
0.00.114.439 I ggml_metal_init: using embedded metal library
0.00.283.267 I ggml_metal_init: GPU name:   Apple M4
0.00.283.280 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.283.281 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.283.281 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.283.282 I ggml_metal_init: simdgroup reduction   = true
0.00.283.283 I ggml_metal_init: simdgroup matrix mul. = true
0.00.283.283 I ggml_metal_init: has bfloat            = true
0.00.283.283 I ggml_metal_init: use bfloat            = true
0.00.283.285 I ggml_metal_init: hasUnifiedMemory      = true
0.00.283.290 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.329.801 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.360.508 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.360.525 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.360.579 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.362.811 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.362.815 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.362.816 I llama_init_from_model: graph nodes  = 967
0.00.362.816 I llama_init_from_model: graph splits = 2
0.00.362.820 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.363.154 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.363.156 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.446.723 I main: llama threadpool init, n_threads = 4
0.00.446.762 I 
0.00.446.788 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.446.788 I 
0.00.446.871 I sampler seed: 1234
0.00.446.877 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.446.910 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.446.912 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.446.912 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.301.624 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55295.95 tokens per second)
0.02.301.625 I llama_perf_context_print:        load time =     404.73 ms
0.02.301.625 I llama_perf_context_print: prompt eval time =      54.65 ms /     7 tokens (    7.81 ms per token,   128.09 tokens per second)
0.02.301.626 I llama_perf_context_print:        eval time =    1797.08 ms /    63 runs   (   28.53 ms per token,    35.06 tokens per second)
0.02.301.627 I llama_perf_context_print:       total time =    1854.90 ms /    70 tokens
0.02.301.838 I ggml_metal_free: deallocating

real	0m2.630s
user	0m0.163s
sys	0m0.120s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.891 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.456 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.461 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.467 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.468 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.468 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.469 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.471 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.472 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.472 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.473 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.473 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.473 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.474 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.474 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.477 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.477 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.477 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.204 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.260 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.219 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.221 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.221 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.221 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.222 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.222 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.223 I llama_model_loader: - type  f32:  194 tensors
0.00.035.223 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.223 I print_info: file format = GGUF V3 (latest)
0.00.035.224 I print_info: file type   = Q8_0
0.00.035.225 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.056.907 I load: special tokens cache size = 25
0.00.062.937 I load: token to piece cache size = 0.2984 MB
0.00.062.942 I print_info: arch             = gptneox
0.00.062.942 I print_info: vocab_only       = 0
0.00.062.942 I print_info: n_ctx_train      = 2048
0.00.062.943 I print_info: n_embd           = 2048
0.00.062.944 I print_info: n_layer          = 24
0.00.062.950 I print_info: n_head           = 16
0.00.062.951 I print_info: n_head_kv        = 16
0.00.062.951 I print_info: n_rot            = 32
0.00.062.952 I print_info: n_swa            = 0
0.00.062.952 I print_info: n_embd_head_k    = 128
0.00.062.952 I print_info: n_embd_head_v    = 128
0.00.062.952 I print_info: n_gqa            = 1
0.00.062.953 I print_info: n_embd_k_gqa     = 2048
0.00.062.954 I print_info: n_embd_v_gqa     = 2048
0.00.062.954 I print_info: f_norm_eps       = 1.0e-05
0.00.062.955 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.955 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.956 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.957 I print_info: f_logit_scale    = 0.0e+00
0.00.062.957 I print_info: n_ff             = 8192
0.00.062.958 I print_info: n_expert         = 0
0.00.062.959 I print_info: n_expert_used    = 0
0.00.062.959 I print_info: causal attn      = 1
0.00.062.959 I print_info: pooling type     = 0
0.00.062.959 I print_info: rope type        = 2
0.00.062.959 I print_info: rope scaling     = linear
0.00.062.960 I print_info: freq_base_train  = 10000.0
0.00.062.960 I print_info: freq_scale_train = 1
0.00.062.960 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.960 I print_info: rope_finetuned   = unknown
0.00.062.960 I print_info: ssm_d_conv       = 0
0.00.062.960 I print_info: ssm_d_inner      = 0
0.00.062.960 I print_info: ssm_d_state      = 0
0.00.062.961 I print_info: ssm_dt_rank      = 0
0.00.062.961 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.961 I print_info: model type       = 1.4B
0.00.062.961 I print_info: model params     = 1.41 B
0.00.062.962 I print_info: general.name     = 1.4B
0.00.062.962 I print_info: vocab type       = BPE
0.00.062.963 I print_info: n_vocab          = 50304
0.00.062.963 I print_info: n_merges         = 50009
0.00.062.963 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.963 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.963 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.963 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.964 I print_info: LF token         = 128 'Ä'
0.00.062.964 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.964 I print_info: max token length = 1024
0.00.065.423 I load_tensors: offloading 24 repeating layers to GPU
0.00.065.424 I load_tensors: offloading output layer to GPU
0.00.065.424 I load_tensors: offloaded 25/25 layers to GPU
0.00.065.435 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.436 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.065.805 I llama_init_from_model: n_seq_max     = 1
0.00.065.806 I llama_init_from_model: n_ctx         = 2048
0.00.065.806 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.065.806 I llama_init_from_model: n_batch       = 2048
0.00.065.806 I llama_init_from_model: n_ubatch      = 512
0.00.065.807 I llama_init_from_model: flash_attn    = 0
0.00.065.807 I llama_init_from_model: freq_base     = 10000.0
0.00.065.807 I llama_init_from_model: freq_scale    = 1
0.00.065.808 I ggml_metal_init: allocating
0.00.065.811 I ggml_metal_init: found device: Apple M4
0.00.065.813 I ggml_metal_init: picking default device: Apple M4
0.00.066.561 I ggml_metal_init: using embedded metal library
0.00.069.338 I ggml_metal_init: GPU name:   Apple M4
0.00.069.340 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.340 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.341 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.341 I ggml_metal_init: simdgroup reduction   = true
0.00.069.341 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.341 I ggml_metal_init: has bfloat            = true
0.00.069.341 I ggml_metal_init: use bfloat            = true
0.00.069.342 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.342 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.415 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.105.021 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.105.036 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.105.075 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.106.204 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.106.207 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.106.207 I llama_init_from_model: graph nodes  = 967
0.00.106.207 I llama_init_from_model: graph splits = 2
0.00.106.212 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.106.334 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.106.335 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.352.983 I main: llama threadpool init, n_threads = 4
0.01.353.018 I 
0.01.353.043 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.353.044 I 
0.01.353.201 I sampler seed: 1234
0.01.353.205 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.353.216 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.353.218 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.353.218 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.447.598 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50426.14 tokens per second)
0.02.447.598 I llama_perf_context_print:        load time =    1343.09 ms
0.02.447.599 I llama_perf_context_print: prompt eval time =      42.67 ms /     7 tokens (    6.10 ms per token,   164.05 tokens per second)
0.02.447.600 I llama_perf_context_print:        eval time =    1048.97 ms /    63 runs   (   16.65 ms per token,    60.06 tokens per second)
0.02.447.601 I llama_perf_context_print:       total time =    1094.62 ms /    70 tokens
0.02.447.903 I ggml_metal_free: deallocating

real	0m2.469s
user	0m0.115s
sys	0m0.207s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.013.813 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.924 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.029.931 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.933 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.933 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.934 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.934 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.934 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.936 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.936 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.936 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.937 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.937 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.937 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.938 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.940 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.940 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.940 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.884 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.028 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.095 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.096 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.097 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.097 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.097 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.097 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.039.098 I llama_model_loader: - type  f32:  194 tensors
0.00.039.098 I llama_model_loader: - type q4_0:   97 tensors
0.00.039.098 I llama_model_loader: - type q6_K:    1 tensors
0.00.039.099 I print_info: file format = GGUF V3 (latest)
0.00.039.100 I print_info: file type   = Q4_0
0.00.039.100 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.062.394 I load: special tokens cache size = 25
0.00.069.766 I load: token to piece cache size = 0.2984 MB
0.00.069.769 I print_info: arch             = gptneox
0.00.069.769 I print_info: vocab_only       = 0
0.00.069.769 I print_info: n_ctx_train      = 2048
0.00.069.770 I print_info: n_embd           = 2048
0.00.069.770 I print_info: n_layer          = 24
0.00.069.774 I print_info: n_head           = 16
0.00.069.775 I print_info: n_head_kv        = 16
0.00.069.776 I print_info: n_rot            = 32
0.00.069.776 I print_info: n_swa            = 0
0.00.069.776 I print_info: n_embd_head_k    = 128
0.00.069.776 I print_info: n_embd_head_v    = 128
0.00.069.777 I print_info: n_gqa            = 1
0.00.069.778 I print_info: n_embd_k_gqa     = 2048
0.00.069.778 I print_info: n_embd_v_gqa     = 2048
0.00.069.780 I print_info: f_norm_eps       = 1.0e-05
0.00.069.781 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.781 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.781 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.783 I print_info: f_logit_scale    = 0.0e+00
0.00.069.783 I print_info: n_ff             = 8192
0.00.069.783 I print_info: n_expert         = 0
0.00.069.784 I print_info: n_expert_used    = 0
0.00.069.784 I print_info: causal attn      = 1
0.00.069.784 I print_info: pooling type     = 0
0.00.069.784 I print_info: rope type        = 2
0.00.069.785 I print_info: rope scaling     = linear
0.00.069.785 I print_info: freq_base_train  = 10000.0
0.00.069.785 I print_info: freq_scale_train = 1
0.00.069.786 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.786 I print_info: rope_finetuned   = unknown
0.00.069.786 I print_info: ssm_d_conv       = 0
0.00.069.786 I print_info: ssm_d_inner      = 0
0.00.069.786 I print_info: ssm_d_state      = 0
0.00.069.786 I print_info: ssm_dt_rank      = 0
0.00.069.788 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.788 I print_info: model type       = 1.4B
0.00.069.788 I print_info: model params     = 1.41 B
0.00.069.788 I print_info: general.name     = 1.4B
0.00.069.789 I print_info: vocab type       = BPE
0.00.069.789 I print_info: n_vocab          = 50304
0.00.069.789 I print_info: n_merges         = 50009
0.00.069.790 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.790 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.790 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.791 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.791 I print_info: LF token         = 128 'Ä'
0.00.069.792 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.792 I print_info: max token length = 1024
0.00.071.718 I load_tensors: offloading 24 repeating layers to GPU
0.00.071.719 I load_tensors: offloading output layer to GPU
0.00.071.719 I load_tensors: offloaded 25/25 layers to GPU
0.00.071.729 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.071.730 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.072.031 I llama_init_from_model: n_seq_max     = 1
0.00.072.032 I llama_init_from_model: n_ctx         = 2048
0.00.072.032 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.072.032 I llama_init_from_model: n_batch       = 2048
0.00.072.032 I llama_init_from_model: n_ubatch      = 512
0.00.072.032 I llama_init_from_model: flash_attn    = 0
0.00.072.033 I llama_init_from_model: freq_base     = 10000.0
0.00.072.033 I llama_init_from_model: freq_scale    = 1
0.00.072.033 I ggml_metal_init: allocating
0.00.072.036 I ggml_metal_init: found device: Apple M4
0.00.072.038 I ggml_metal_init: picking default device: Apple M4
0.00.072.810 I ggml_metal_init: using embedded metal library
0.00.075.792 I ggml_metal_init: GPU name:   Apple M4
0.00.075.794 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.795 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.795 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.795 I ggml_metal_init: simdgroup reduction   = true
0.00.075.795 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.796 I ggml_metal_init: has bfloat            = true
0.00.075.796 I ggml_metal_init: use bfloat            = true
0.00.075.796 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.797 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.791 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.113.338 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.113.345 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.113.367 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.114.474 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.114.475 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.114.476 I llama_init_from_model: graph nodes  = 967
0.00.114.476 I llama_init_from_model: graph splits = 2
0.00.114.480 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.114.611 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.114.611 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.550 I main: llama threadpool init, n_threads = 4
0.00.717.590 I 
0.00.717.616 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.717.617 I 
0.00.717.844 I sampler seed: 1234
0.00.717.849 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.717.891 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.717.891 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.717.891 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.393.164 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60016.91 tokens per second)
0.01.393.165 I llama_perf_context_print:        load time =     703.73 ms
0.01.393.166 I llama_perf_context_print: prompt eval time =      42.84 ms /     7 tokens (    6.12 ms per token,   163.38 tokens per second)
0.01.393.166 I llama_perf_context_print:        eval time =     629.51 ms /    63 runs   (    9.99 ms per token,   100.08 tokens per second)
0.01.393.171 I llama_perf_context_print:       total time =     675.62 ms /    70 tokens
0.01.393.366 I ggml_metal_free: deallocating

real	0m1.409s
user	0m0.120s
sys	0m0.148s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.721 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.636 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.026.641 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.642 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.642 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.643 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.643 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.643 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.644 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.644 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.645 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.646 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.646 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.646 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.647 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.651 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.651 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.652 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.472 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.525 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.278 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.279 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.279 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.280 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.280 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.280 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.035.281 I llama_model_loader: - type  f32:  194 tensors
0.00.035.281 I llama_model_loader: - type q4_1:   97 tensors
0.00.035.281 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.282 I print_info: file format = GGUF V3 (latest)
0.00.035.282 I print_info: file type   = Q4_1
0.00.035.283 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.056.727 I load: special tokens cache size = 25
0.00.062.748 I load: token to piece cache size = 0.2984 MB
0.00.062.751 I print_info: arch             = gptneox
0.00.062.751 I print_info: vocab_only       = 0
0.00.062.751 I print_info: n_ctx_train      = 2048
0.00.062.751 I print_info: n_embd           = 2048
0.00.062.752 I print_info: n_layer          = 24
0.00.062.754 I print_info: n_head           = 16
0.00.062.755 I print_info: n_head_kv        = 16
0.00.062.755 I print_info: n_rot            = 32
0.00.062.755 I print_info: n_swa            = 0
0.00.062.757 I print_info: n_embd_head_k    = 128
0.00.062.757 I print_info: n_embd_head_v    = 128
0.00.062.758 I print_info: n_gqa            = 1
0.00.062.759 I print_info: n_embd_k_gqa     = 2048
0.00.062.760 I print_info: n_embd_v_gqa     = 2048
0.00.062.765 I print_info: f_norm_eps       = 1.0e-05
0.00.062.765 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.766 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.766 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.766 I print_info: f_logit_scale    = 0.0e+00
0.00.062.768 I print_info: n_ff             = 8192
0.00.062.769 I print_info: n_expert         = 0
0.00.062.769 I print_info: n_expert_used    = 0
0.00.062.769 I print_info: causal attn      = 1
0.00.062.769 I print_info: pooling type     = 0
0.00.062.769 I print_info: rope type        = 2
0.00.062.769 I print_info: rope scaling     = linear
0.00.062.770 I print_info: freq_base_train  = 10000.0
0.00.062.771 I print_info: freq_scale_train = 1
0.00.062.771 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.771 I print_info: rope_finetuned   = unknown
0.00.062.771 I print_info: ssm_d_conv       = 0
0.00.062.771 I print_info: ssm_d_inner      = 0
0.00.062.771 I print_info: ssm_d_state      = 0
0.00.062.772 I print_info: ssm_dt_rank      = 0
0.00.062.772 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.772 I print_info: model type       = 1.4B
0.00.062.772 I print_info: model params     = 1.41 B
0.00.062.773 I print_info: general.name     = 1.4B
0.00.062.773 I print_info: vocab type       = BPE
0.00.062.773 I print_info: n_vocab          = 50304
0.00.062.774 I print_info: n_merges         = 50009
0.00.062.774 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.774 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.774 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.774 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.775 I print_info: LF token         = 128 'Ä'
0.00.062.775 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.775 I print_info: max token length = 1024
0.00.064.598 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.599 I load_tensors: offloading output layer to GPU
0.00.064.599 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.604 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.064.605 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.064.875 I llama_init_from_model: n_seq_max     = 1
0.00.064.876 I llama_init_from_model: n_ctx         = 2048
0.00.064.877 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.064.877 I llama_init_from_model: n_batch       = 2048
0.00.064.877 I llama_init_from_model: n_ubatch      = 512
0.00.064.877 I llama_init_from_model: flash_attn    = 0
0.00.064.877 I llama_init_from_model: freq_base     = 10000.0
0.00.064.878 I llama_init_from_model: freq_scale    = 1
0.00.064.878 I ggml_metal_init: allocating
0.00.064.881 I ggml_metal_init: found device: Apple M4
0.00.064.883 I ggml_metal_init: picking default device: Apple M4
0.00.065.500 I ggml_metal_init: using embedded metal library
0.00.067.989 I ggml_metal_init: GPU name:   Apple M4
0.00.067.991 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.991 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.992 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.992 I ggml_metal_init: simdgroup reduction   = true
0.00.067.992 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.992 I ggml_metal_init: has bfloat            = true
0.00.067.993 I ggml_metal_init: use bfloat            = true
0.00.067.993 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.994 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.816 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.099.189 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.099.200 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.099.221 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.100.272 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.100.273 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.100.274 I llama_init_from_model: graph nodes  = 967
0.00.100.276 I llama_init_from_model: graph splits = 2
0.00.100.280 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.100.415 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.415 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.802.653 I main: llama threadpool init, n_threads = 4
0.00.802.706 I 
0.00.802.725 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.802.725 I 
0.00.803.005 I sampler seed: 1234
0.00.803.009 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.803.051 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.803.062 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.803.062 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.522.940 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61471.86 tokens per second)
0.01.522.940 I llama_perf_context_print:        load time =     793.93 ms
0.01.522.941 I llama_perf_context_print: prompt eval time =      39.51 ms /     7 tokens (    5.64 ms per token,   177.17 tokens per second)
0.01.522.941 I llama_perf_context_print:        eval time =     677.43 ms /    63 runs   (   10.75 ms per token,    93.00 tokens per second)
0.01.522.942 I llama_perf_context_print:       total time =     720.29 ms /    70 tokens
0.01.523.146 I ggml_metal_free: deallocating

real	0m1.540s
user	0m0.112s
sys	0m0.144s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.013.154 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.686 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.020.691 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.693 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.693 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.698 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.698 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.698 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.702 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.702 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.703 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.703 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.704 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.704 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.704 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.707 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.707 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.708 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.454 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.488 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.265 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.266 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.266 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.267 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.267 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.267 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.029.268 I llama_model_loader: - type  f32:  194 tensors
0.00.029.268 I llama_model_loader: - type q5_0:   97 tensors
0.00.029.268 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.269 I print_info: file format = GGUF V3 (latest)
0.00.029.269 I print_info: file type   = Q5_0
0.00.029.274 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.047.894 I load: special tokens cache size = 25
0.00.053.911 I load: token to piece cache size = 0.2984 MB
0.00.053.915 I print_info: arch             = gptneox
0.00.053.915 I print_info: vocab_only       = 0
0.00.053.915 I print_info: n_ctx_train      = 2048
0.00.053.915 I print_info: n_embd           = 2048
0.00.053.916 I print_info: n_layer          = 24
0.00.053.919 I print_info: n_head           = 16
0.00.053.920 I print_info: n_head_kv        = 16
0.00.053.920 I print_info: n_rot            = 32
0.00.053.920 I print_info: n_swa            = 0
0.00.053.920 I print_info: n_embd_head_k    = 128
0.00.053.921 I print_info: n_embd_head_v    = 128
0.00.053.921 I print_info: n_gqa            = 1
0.00.053.922 I print_info: n_embd_k_gqa     = 2048
0.00.053.923 I print_info: n_embd_v_gqa     = 2048
0.00.053.923 I print_info: f_norm_eps       = 1.0e-05
0.00.053.924 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.924 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.925 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.926 I print_info: f_logit_scale    = 0.0e+00
0.00.053.926 I print_info: n_ff             = 8192
0.00.053.927 I print_info: n_expert         = 0
0.00.053.927 I print_info: n_expert_used    = 0
0.00.053.929 I print_info: causal attn      = 1
0.00.053.930 I print_info: pooling type     = 0
0.00.053.930 I print_info: rope type        = 2
0.00.053.930 I print_info: rope scaling     = linear
0.00.053.931 I print_info: freq_base_train  = 10000.0
0.00.053.931 I print_info: freq_scale_train = 1
0.00.053.931 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.932 I print_info: rope_finetuned   = unknown
0.00.053.932 I print_info: ssm_d_conv       = 0
0.00.053.932 I print_info: ssm_d_inner      = 0
0.00.053.932 I print_info: ssm_d_state      = 0
0.00.053.932 I print_info: ssm_dt_rank      = 0
0.00.053.932 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.933 I print_info: model type       = 1.4B
0.00.053.937 I print_info: model params     = 1.41 B
0.00.053.937 I print_info: general.name     = 1.4B
0.00.053.938 I print_info: vocab type       = BPE
0.00.053.938 I print_info: n_vocab          = 50304
0.00.053.938 I print_info: n_merges         = 50009
0.00.053.938 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.939 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.939 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.939 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.939 I print_info: LF token         = 128 'Ä'
0.00.053.939 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.940 I print_info: max token length = 1024
0.00.055.958 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.958 I load_tensors: offloading output layer to GPU
0.00.055.958 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.968 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.055.969 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.056.253 I llama_init_from_model: n_seq_max     = 1
0.00.056.254 I llama_init_from_model: n_ctx         = 2048
0.00.056.254 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.056.254 I llama_init_from_model: n_batch       = 2048
0.00.056.254 I llama_init_from_model: n_ubatch      = 512
0.00.056.254 I llama_init_from_model: flash_attn    = 0
0.00.056.255 I llama_init_from_model: freq_base     = 10000.0
0.00.056.255 I llama_init_from_model: freq_scale    = 1
0.00.056.255 I ggml_metal_init: allocating
0.00.056.258 I ggml_metal_init: found device: Apple M4
0.00.056.260 I ggml_metal_init: picking default device: Apple M4
0.00.056.839 I ggml_metal_init: using embedded metal library
0.00.059.207 I ggml_metal_init: GPU name:   Apple M4
0.00.059.209 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.209 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.209 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.210 I ggml_metal_init: simdgroup reduction   = true
0.00.059.210 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.210 I ggml_metal_init: has bfloat            = true
0.00.059.210 I ggml_metal_init: use bfloat            = true
0.00.059.210 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.211 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.120 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.111 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.117 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.135 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.218 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.219 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.219 I llama_init_from_model: graph nodes  = 967
0.00.089.220 I llama_init_from_model: graph splits = 2
0.00.089.222 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.353 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.353 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.777.897 I main: llama threadpool init, n_threads = 4
0.00.777.933 I 
0.00.777.953 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.777.954 I 
0.00.778.179 I sampler seed: 1234
0.00.778.184 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.778.234 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.778.236 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.778.236 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.567.113 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58388.16 tokens per second)
0.01.567.114 I llama_perf_context_print:        load time =     764.74 ms
0.01.567.115 I llama_perf_context_print: prompt eval time =      43.16 ms /     7 tokens (    6.17 ms per token,   162.19 tokens per second)
0.01.567.115 I llama_perf_context_print:        eval time =     742.69 ms /    63 runs   (   11.79 ms per token,    84.83 tokens per second)
0.01.567.116 I llama_perf_context_print:       total time =     789.22 ms /    70 tokens
0.01.567.345 I ggml_metal_free: deallocating

real	0m1.585s
user	0m0.107s
sys	0m0.155s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.008.781 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.707 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.712 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.713 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.719 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.720 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.720 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.720 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.722 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.722 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.722 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.723 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.723 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.723 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.724 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.726 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.726 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.726 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.475 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.469 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.127 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.128 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.128 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.129 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.129 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.129 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.130 I llama_model_loader: - type  f32:  194 tensors
0.00.025.130 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.130 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.131 I print_info: file format = GGUF V3 (latest)
0.00.025.131 I print_info: file type   = Q5_1
0.00.025.132 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.624 I load: special tokens cache size = 25
0.00.049.601 I load: token to piece cache size = 0.2984 MB
0.00.049.604 I print_info: arch             = gptneox
0.00.049.604 I print_info: vocab_only       = 0
0.00.049.605 I print_info: n_ctx_train      = 2048
0.00.049.605 I print_info: n_embd           = 2048
0.00.049.605 I print_info: n_layer          = 24
0.00.049.608 I print_info: n_head           = 16
0.00.049.608 I print_info: n_head_kv        = 16
0.00.049.608 I print_info: n_rot            = 32
0.00.049.609 I print_info: n_swa            = 0
0.00.049.609 I print_info: n_embd_head_k    = 128
0.00.049.609 I print_info: n_embd_head_v    = 128
0.00.049.610 I print_info: n_gqa            = 1
0.00.049.613 I print_info: n_embd_k_gqa     = 2048
0.00.049.613 I print_info: n_embd_v_gqa     = 2048
0.00.049.614 I print_info: f_norm_eps       = 1.0e-05
0.00.049.614 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.614 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.615 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.615 I print_info: f_logit_scale    = 0.0e+00
0.00.049.615 I print_info: n_ff             = 8192
0.00.049.616 I print_info: n_expert         = 0
0.00.049.616 I print_info: n_expert_used    = 0
0.00.049.616 I print_info: causal attn      = 1
0.00.049.616 I print_info: pooling type     = 0
0.00.049.616 I print_info: rope type        = 2
0.00.049.617 I print_info: rope scaling     = linear
0.00.049.617 I print_info: freq_base_train  = 10000.0
0.00.049.617 I print_info: freq_scale_train = 1
0.00.049.618 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.618 I print_info: rope_finetuned   = unknown
0.00.049.619 I print_info: ssm_d_conv       = 0
0.00.049.620 I print_info: ssm_d_inner      = 0
0.00.049.620 I print_info: ssm_d_state      = 0
0.00.049.620 I print_info: ssm_dt_rank      = 0
0.00.049.620 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.620 I print_info: model type       = 1.4B
0.00.049.621 I print_info: model params     = 1.41 B
0.00.049.621 I print_info: general.name     = 1.4B
0.00.049.621 I print_info: vocab type       = BPE
0.00.049.621 I print_info: n_vocab          = 50304
0.00.049.622 I print_info: n_merges         = 50009
0.00.049.622 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.622 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.626 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.626 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.627 I print_info: LF token         = 128 'Ä'
0.00.049.627 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.627 I print_info: max token length = 1024
0.00.051.602 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.602 I load_tensors: offloading output layer to GPU
0.00.051.603 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.613 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.614 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.907 I llama_init_from_model: n_seq_max     = 1
0.00.051.908 I llama_init_from_model: n_ctx         = 2048
0.00.051.908 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.908 I llama_init_from_model: n_batch       = 2048
0.00.051.909 I llama_init_from_model: n_ubatch      = 512
0.00.051.909 I llama_init_from_model: flash_attn    = 0
0.00.051.909 I llama_init_from_model: freq_base     = 10000.0
0.00.051.909 I llama_init_from_model: freq_scale    = 1
0.00.051.910 I ggml_metal_init: allocating
0.00.051.913 I ggml_metal_init: found device: Apple M4
0.00.051.915 I ggml_metal_init: picking default device: Apple M4
0.00.052.472 I ggml_metal_init: using embedded metal library
0.00.054.797 I ggml_metal_init: GPU name:   Apple M4
0.00.054.798 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.798 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.799 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.799 I ggml_metal_init: simdgroup reduction   = true
0.00.054.799 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.800 I ggml_metal_init: has bfloat            = true
0.00.054.800 I ggml_metal_init: use bfloat            = true
0.00.054.800 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.801 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.397 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.090 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.099 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.124 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.102 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.103 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.104 I llama_init_from_model: graph nodes  = 967
0.00.084.104 I llama_init_from_model: graph splits = 2
0.00.084.108 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.238 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.239 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.968 I main: llama threadpool init, n_threads = 4
0.00.704.013 I 
0.00.704.063 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.704.064 I 
0.00.704.303 I sampler seed: 1234
0.00.704.310 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.704.352 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.704.353 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.704.353 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.545.930 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59068.22 tokens per second)
0.01.545.930 I llama_perf_context_print:        load time =     695.18 ms
0.01.545.931 I llama_perf_context_print: prompt eval time =      46.21 ms /     7 tokens (    6.60 ms per token,   151.49 tokens per second)
0.01.545.932 I llama_perf_context_print:        eval time =     792.39 ms /    63 runs   (   12.58 ms per token,    79.51 tokens per second)
0.01.545.932 I llama_perf_context_print:       total time =     841.97 ms /    70 tokens
0.01.546.196 I ggml_metal_free: deallocating

real	0m1.563s
user	0m0.107s
sys	0m0.150s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.010.831 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.356 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.361 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.367 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.368 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.368 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.368 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.369 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.371 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.371 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.371 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.372 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.372 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.373 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.376 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.377 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.378 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.378 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.063 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.053 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.757 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.758 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.758 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.758 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.759 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.759 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.760 I llama_model_loader: - type  f32:  194 tensors
0.00.025.760 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.760 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.760 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.761 I print_info: file format = GGUF V3 (latest)
0.00.025.762 I print_info: file type   = Q2_K - Medium
0.00.025.762 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.290 I load: special tokens cache size = 25
0.00.050.048 I load: token to piece cache size = 0.2984 MB
0.00.050.051 I print_info: arch             = gptneox
0.00.050.051 I print_info: vocab_only       = 0
0.00.050.052 I print_info: n_ctx_train      = 2048
0.00.050.052 I print_info: n_embd           = 2048
0.00.050.052 I print_info: n_layer          = 24
0.00.050.054 I print_info: n_head           = 16
0.00.050.055 I print_info: n_head_kv        = 16
0.00.050.055 I print_info: n_rot            = 32
0.00.050.056 I print_info: n_swa            = 0
0.00.050.056 I print_info: n_embd_head_k    = 128
0.00.050.056 I print_info: n_embd_head_v    = 128
0.00.050.057 I print_info: n_gqa            = 1
0.00.050.058 I print_info: n_embd_k_gqa     = 2048
0.00.050.058 I print_info: n_embd_v_gqa     = 2048
0.00.050.061 I print_info: f_norm_eps       = 1.0e-05
0.00.050.061 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.062 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.062 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.062 I print_info: f_logit_scale    = 0.0e+00
0.00.050.063 I print_info: n_ff             = 8192
0.00.050.063 I print_info: n_expert         = 0
0.00.050.063 I print_info: n_expert_used    = 0
0.00.050.063 I print_info: causal attn      = 1
0.00.050.063 I print_info: pooling type     = 0
0.00.050.064 I print_info: rope type        = 2
0.00.050.070 I print_info: rope scaling     = linear
0.00.050.072 I print_info: freq_base_train  = 10000.0
0.00.050.072 I print_info: freq_scale_train = 1
0.00.050.072 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.073 I print_info: rope_finetuned   = unknown
0.00.050.073 I print_info: ssm_d_conv       = 0
0.00.050.073 I print_info: ssm_d_inner      = 0
0.00.050.073 I print_info: ssm_d_state      = 0
0.00.050.073 I print_info: ssm_dt_rank      = 0
0.00.050.073 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.074 I print_info: model type       = 1.4B
0.00.050.074 I print_info: model params     = 1.41 B
0.00.050.074 I print_info: general.name     = 1.4B
0.00.050.076 I print_info: vocab type       = BPE
0.00.050.076 I print_info: n_vocab          = 50304
0.00.050.076 I print_info: n_merges         = 50009
0.00.050.076 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.076 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.077 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.077 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.077 I print_info: LF token         = 128 'Ä'
0.00.050.077 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.078 I print_info: max token length = 1024
0.00.051.868 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.868 I load_tensors: offloading output layer to GPU
0.00.051.868 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.878 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.880 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.206 I llama_init_from_model: n_seq_max     = 1
0.00.052.206 I llama_init_from_model: n_ctx         = 2048
0.00.052.206 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.207 I llama_init_from_model: n_batch       = 2048
0.00.052.207 I llama_init_from_model: n_ubatch      = 512
0.00.052.207 I llama_init_from_model: flash_attn    = 0
0.00.052.207 I llama_init_from_model: freq_base     = 10000.0
0.00.052.207 I llama_init_from_model: freq_scale    = 1
0.00.052.208 I ggml_metal_init: allocating
0.00.052.211 I ggml_metal_init: found device: Apple M4
0.00.052.213 I ggml_metal_init: picking default device: Apple M4
0.00.052.769 I ggml_metal_init: using embedded metal library
0.00.055.097 I ggml_metal_init: GPU name:   Apple M4
0.00.055.098 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.099 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.099 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.099 I ggml_metal_init: simdgroup reduction   = true
0.00.055.099 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.100 I ggml_metal_init: has bfloat            = true
0.00.055.100 I ggml_metal_init: use bfloat            = true
0.00.055.100 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.101 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.510 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.589 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.594 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.612 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.647 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.649 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.649 I llama_init_from_model: graph nodes  = 967
0.00.084.649 I llama_init_from_model: graph splits = 2
0.00.084.652 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.773 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.773 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.433.812 I main: llama threadpool init, n_threads = 4
0.00.433.851 I 
0.00.433.877 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.433.877 I 
0.00.434.100 I sampler seed: 1234
0.00.434.106 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.434.117 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.434.119 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.434.119 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.109.568 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56664.01 tokens per second)
0.01.109.570 I llama_perf_context_print:        load time =     422.98 ms
0.01.109.570 I llama_perf_context_print: prompt eval time =      35.84 ms /     7 tokens (    5.12 ms per token,   195.32 tokens per second)
0.01.109.572 I llama_perf_context_print:        eval time =     636.86 ms /    63 runs   (   10.11 ms per token,    98.92 tokens per second)
0.01.109.573 I llama_perf_context_print:       total time =     675.76 ms /    70 tokens
0.01.109.785 I ggml_metal_free: deallocating

real	0m1.128s
user	0m0.107s
sys	0m0.107s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.760 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.578 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.589 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.591 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.592 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.592 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.592 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.593 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.594 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.594 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.594 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.595 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.595 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.595 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.596 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.597 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.597 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.597 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.427 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.452 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.403 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.405 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.405 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.406 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.406 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.406 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.407 I llama_model_loader: - type  f32:  194 tensors
0.00.025.407 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.408 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.408 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.408 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.409 I print_info: file format = GGUF V3 (latest)
0.00.025.409 I print_info: file type   = Q3_K - Medium
0.00.025.411 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.007 I load: special tokens cache size = 25
0.00.050.880 I load: token to piece cache size = 0.2984 MB
0.00.050.883 I print_info: arch             = gptneox
0.00.050.883 I print_info: vocab_only       = 0
0.00.050.884 I print_info: n_ctx_train      = 2048
0.00.050.884 I print_info: n_embd           = 2048
0.00.050.884 I print_info: n_layer          = 24
0.00.050.887 I print_info: n_head           = 16
0.00.050.888 I print_info: n_head_kv        = 16
0.00.050.888 I print_info: n_rot            = 32
0.00.050.888 I print_info: n_swa            = 0
0.00.050.888 I print_info: n_embd_head_k    = 128
0.00.050.889 I print_info: n_embd_head_v    = 128
0.00.050.889 I print_info: n_gqa            = 1
0.00.050.890 I print_info: n_embd_k_gqa     = 2048
0.00.050.890 I print_info: n_embd_v_gqa     = 2048
0.00.050.891 I print_info: f_norm_eps       = 1.0e-05
0.00.050.891 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.891 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.892 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.892 I print_info: f_logit_scale    = 0.0e+00
0.00.050.892 I print_info: n_ff             = 8192
0.00.050.893 I print_info: n_expert         = 0
0.00.050.893 I print_info: n_expert_used    = 0
0.00.050.893 I print_info: causal attn      = 1
0.00.050.894 I print_info: pooling type     = 0
0.00.050.894 I print_info: rope type        = 2
0.00.050.895 I print_info: rope scaling     = linear
0.00.050.896 I print_info: freq_base_train  = 10000.0
0.00.050.897 I print_info: freq_scale_train = 1
0.00.050.897 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.897 I print_info: rope_finetuned   = unknown
0.00.050.897 I print_info: ssm_d_conv       = 0
0.00.050.897 I print_info: ssm_d_inner      = 0
0.00.050.897 I print_info: ssm_d_state      = 0
0.00.050.897 I print_info: ssm_dt_rank      = 0
0.00.050.898 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.898 I print_info: model type       = 1.4B
0.00.050.898 I print_info: model params     = 1.41 B
0.00.050.898 I print_info: general.name     = 1.4B
0.00.050.899 I print_info: vocab type       = BPE
0.00.050.899 I print_info: n_vocab          = 50304
0.00.050.900 I print_info: n_merges         = 50009
0.00.050.900 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.901 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.902 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.902 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.902 I print_info: LF token         = 128 'Ä'
0.00.050.902 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.903 I print_info: max token length = 1024
0.00.052.534 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.535 I load_tensors: offloading output layer to GPU
0.00.052.535 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.541 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.542 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.841 I llama_init_from_model: n_seq_max     = 1
0.00.052.842 I llama_init_from_model: n_ctx         = 2048
0.00.052.842 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.842 I llama_init_from_model: n_batch       = 2048
0.00.052.842 I llama_init_from_model: n_ubatch      = 512
0.00.052.843 I llama_init_from_model: flash_attn    = 0
0.00.052.843 I llama_init_from_model: freq_base     = 10000.0
0.00.052.844 I llama_init_from_model: freq_scale    = 1
0.00.052.845 I ggml_metal_init: allocating
0.00.052.848 I ggml_metal_init: found device: Apple M4
0.00.052.850 I ggml_metal_init: picking default device: Apple M4
0.00.053.474 I ggml_metal_init: using embedded metal library
0.00.055.844 I ggml_metal_init: GPU name:   Apple M4
0.00.055.846 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.847 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.847 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.847 I ggml_metal_init: simdgroup reduction   = true
0.00.055.849 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.849 I ggml_metal_init: has bfloat            = true
0.00.055.850 I ggml_metal_init: use bfloat            = true
0.00.055.850 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.851 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.973 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.413 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.425 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.445 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.377 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.379 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.379 I llama_init_from_model: graph nodes  = 967
0.00.086.380 I llama_init_from_model: graph splits = 2
0.00.086.383 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.511 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.512 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.513.522 I main: llama threadpool init, n_threads = 4
0.00.513.574 I 
0.00.513.608 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.513.610 I 
0.00.513.847 I sampler seed: 1234
0.00.513.852 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.513.889 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.513.892 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.513.892 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.254.795 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49754.73 tokens per second)
0.01.254.795 I llama_perf_context_print:        load time =     504.75 ms
0.01.254.796 I llama_perf_context_print: prompt eval time =      40.66 ms /     7 tokens (    5.81 ms per token,   172.16 tokens per second)
0.01.254.797 I llama_perf_context_print:        eval time =     697.53 ms /    63 runs   (   11.07 ms per token,    90.32 tokens per second)
0.01.254.798 I llama_perf_context_print:       total time =     741.28 ms /    70 tokens
0.01.255.036 I ggml_metal_free: deallocating

real	0m1.272s
user	0m0.109s
sys	0m0.101s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.988 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.732 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.738 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.741 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.741 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.742 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.742 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.742 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.743 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.744 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.744 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.745 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.745 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.745 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.746 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.747 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.748 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.748 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.622 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.621 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.377 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.378 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.379 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.379 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.379 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.380 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.380 I llama_model_loader: - type  f32:  194 tensors
0.00.025.380 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.381 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.381 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.381 I print_info: file format = GGUF V3 (latest)
0.00.025.382 I print_info: file type   = Q4_K - Medium
0.00.025.383 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.044 I load: special tokens cache size = 25
0.00.050.931 I load: token to piece cache size = 0.2984 MB
0.00.050.936 I print_info: arch             = gptneox
0.00.050.936 I print_info: vocab_only       = 0
0.00.050.936 I print_info: n_ctx_train      = 2048
0.00.050.936 I print_info: n_embd           = 2048
0.00.050.936 I print_info: n_layer          = 24
0.00.050.941 I print_info: n_head           = 16
0.00.050.942 I print_info: n_head_kv        = 16
0.00.050.942 I print_info: n_rot            = 32
0.00.050.942 I print_info: n_swa            = 0
0.00.050.942 I print_info: n_embd_head_k    = 128
0.00.050.942 I print_info: n_embd_head_v    = 128
0.00.050.943 I print_info: n_gqa            = 1
0.00.050.944 I print_info: n_embd_k_gqa     = 2048
0.00.050.944 I print_info: n_embd_v_gqa     = 2048
0.00.050.945 I print_info: f_norm_eps       = 1.0e-05
0.00.050.945 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.945 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.945 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.946 I print_info: f_logit_scale    = 0.0e+00
0.00.050.946 I print_info: n_ff             = 8192
0.00.050.946 I print_info: n_expert         = 0
0.00.050.946 I print_info: n_expert_used    = 0
0.00.050.947 I print_info: causal attn      = 1
0.00.050.947 I print_info: pooling type     = 0
0.00.050.947 I print_info: rope type        = 2
0.00.050.947 I print_info: rope scaling     = linear
0.00.050.948 I print_info: freq_base_train  = 10000.0
0.00.050.948 I print_info: freq_scale_train = 1
0.00.050.948 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.948 I print_info: rope_finetuned   = unknown
0.00.050.948 I print_info: ssm_d_conv       = 0
0.00.050.949 I print_info: ssm_d_inner      = 0
0.00.050.949 I print_info: ssm_d_state      = 0
0.00.050.949 I print_info: ssm_dt_rank      = 0
0.00.050.949 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.949 I print_info: model type       = 1.4B
0.00.050.950 I print_info: model params     = 1.41 B
0.00.050.950 I print_info: general.name     = 1.4B
0.00.050.950 I print_info: vocab type       = BPE
0.00.050.950 I print_info: n_vocab          = 50304
0.00.050.951 I print_info: n_merges         = 50009
0.00.050.951 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.951 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.951 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.951 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.952 I print_info: LF token         = 128 'Ä'
0.00.050.955 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.955 I print_info: max token length = 1024
0.00.052.972 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.972 I load_tensors: offloading output layer to GPU
0.00.052.973 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.983 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.985 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.275 I llama_init_from_model: n_seq_max     = 1
0.00.053.276 I llama_init_from_model: n_ctx         = 2048
0.00.053.276 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.276 I llama_init_from_model: n_batch       = 2048
0.00.053.276 I llama_init_from_model: n_ubatch      = 512
0.00.053.276 I llama_init_from_model: flash_attn    = 0
0.00.053.277 I llama_init_from_model: freq_base     = 10000.0
0.00.053.277 I llama_init_from_model: freq_scale    = 1
0.00.053.277 I ggml_metal_init: allocating
0.00.053.281 I ggml_metal_init: found device: Apple M4
0.00.053.283 I ggml_metal_init: picking default device: Apple M4
0.00.053.913 I ggml_metal_init: using embedded metal library
0.00.056.337 I ggml_metal_init: GPU name:   Apple M4
0.00.056.338 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.339 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.339 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.340 I ggml_metal_init: simdgroup reduction   = true
0.00.056.340 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.340 I ggml_metal_init: has bfloat            = true
0.00.056.340 I ggml_metal_init: use bfloat            = true
0.00.056.341 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.342 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.636 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.495 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.501 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.522 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.586 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.588 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.589 I llama_init_from_model: graph nodes  = 967
0.00.087.589 I llama_init_from_model: graph splits = 2
0.00.087.592 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.721 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.721 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.625.511 I main: llama threadpool init, n_threads = 4
0.00.625.552 I 
0.00.625.575 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.625.575 I 
0.00.625.806 I sampler seed: 1234
0.00.625.811 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.625.822 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.625.822 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.625.822 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.376.836 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56037.88 tokens per second)
0.01.376.836 I llama_perf_context_print:        load time =     616.52 ms
0.01.376.837 I llama_perf_context_print: prompt eval time =      47.13 ms /     7 tokens (    6.73 ms per token,   148.54 tokens per second)
0.01.376.839 I llama_perf_context_print:        eval time =     700.77 ms /    63 runs   (   11.12 ms per token,    89.90 tokens per second)
0.01.376.839 I llama_perf_context_print:       total time =     751.33 ms /    70 tokens
0.01.377.057 I ggml_metal_free: deallocating

real	0m1.394s
user	0m0.109s
sys	0m0.150s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.010.826 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.217 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.222 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.223 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.224 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.225 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.226 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.226 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.228 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.229 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.229 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.229 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.230 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.230 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.231 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.233 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.233 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.234 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.016 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.072 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.792 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.793 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.794 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.794 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.794 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.795 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.795 I llama_model_loader: - type  f32:  194 tensors
0.00.026.795 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.796 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.796 I print_info: file format = GGUF V3 (latest)
0.00.026.797 I print_info: file type   = Q5_K - Medium
0.00.026.798 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.470 I load: special tokens cache size = 25
0.00.051.230 I load: token to piece cache size = 0.2984 MB
0.00.051.233 I print_info: arch             = gptneox
0.00.051.233 I print_info: vocab_only       = 0
0.00.051.233 I print_info: n_ctx_train      = 2048
0.00.051.234 I print_info: n_embd           = 2048
0.00.051.234 I print_info: n_layer          = 24
0.00.051.237 I print_info: n_head           = 16
0.00.051.238 I print_info: n_head_kv        = 16
0.00.051.238 I print_info: n_rot            = 32
0.00.051.240 I print_info: n_swa            = 0
0.00.051.240 I print_info: n_embd_head_k    = 128
0.00.051.241 I print_info: n_embd_head_v    = 128
0.00.051.241 I print_info: n_gqa            = 1
0.00.051.242 I print_info: n_embd_k_gqa     = 2048
0.00.051.243 I print_info: n_embd_v_gqa     = 2048
0.00.051.243 I print_info: f_norm_eps       = 1.0e-05
0.00.051.244 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.244 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.244 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.244 I print_info: f_logit_scale    = 0.0e+00
0.00.051.245 I print_info: n_ff             = 8192
0.00.051.245 I print_info: n_expert         = 0
0.00.051.245 I print_info: n_expert_used    = 0
0.00.051.246 I print_info: causal attn      = 1
0.00.051.247 I print_info: pooling type     = 0
0.00.051.248 I print_info: rope type        = 2
0.00.051.249 I print_info: rope scaling     = linear
0.00.051.249 I print_info: freq_base_train  = 10000.0
0.00.051.249 I print_info: freq_scale_train = 1
0.00.051.250 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.250 I print_info: rope_finetuned   = unknown
0.00.051.250 I print_info: ssm_d_conv       = 0
0.00.051.250 I print_info: ssm_d_inner      = 0
0.00.051.250 I print_info: ssm_d_state      = 0
0.00.051.254 I print_info: ssm_dt_rank      = 0
0.00.051.255 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.255 I print_info: model type       = 1.4B
0.00.051.255 I print_info: model params     = 1.41 B
0.00.051.255 I print_info: general.name     = 1.4B
0.00.051.257 I print_info: vocab type       = BPE
0.00.051.257 I print_info: n_vocab          = 50304
0.00.051.257 I print_info: n_merges         = 50009
0.00.051.258 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.258 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.258 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.258 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.258 I print_info: LF token         = 128 'Ä'
0.00.051.259 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.259 I print_info: max token length = 1024
0.00.052.797 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.798 I load_tensors: offloading output layer to GPU
0.00.052.798 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.808 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.809 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.092 I llama_init_from_model: n_seq_max     = 1
0.00.053.093 I llama_init_from_model: n_ctx         = 2048
0.00.053.093 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.093 I llama_init_from_model: n_batch       = 2048
0.00.053.094 I llama_init_from_model: n_ubatch      = 512
0.00.053.094 I llama_init_from_model: flash_attn    = 0
0.00.053.094 I llama_init_from_model: freq_base     = 10000.0
0.00.053.094 I llama_init_from_model: freq_scale    = 1
0.00.053.095 I ggml_metal_init: allocating
0.00.053.098 I ggml_metal_init: found device: Apple M4
0.00.053.100 I ggml_metal_init: picking default device: Apple M4
0.00.053.656 I ggml_metal_init: using embedded metal library
0.00.055.942 I ggml_metal_init: GPU name:   Apple M4
0.00.055.944 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.944 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.944 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.945 I ggml_metal_init: simdgroup reduction   = true
0.00.055.945 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.945 I ggml_metal_init: has bfloat            = true
0.00.055.945 I ggml_metal_init: use bfloat            = true
0.00.055.946 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.946 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.643 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.642 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.650 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.672 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.728 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.729 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.730 I llama_init_from_model: graph nodes  = 967
0.00.085.730 I llama_init_from_model: graph splits = 2
0.00.085.733 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.858 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.858 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.695.345 I main: llama threadpool init, n_threads = 4
0.00.695.389 I 
0.00.695.410 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.695.410 I 
0.00.695.645 I sampler seed: 1234
0.00.695.650 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.695.691 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.695.693 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.695.693 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.545.011 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58484.35 tokens per second)
0.01.545.012 I llama_perf_context_print:        load time =     684.51 ms
0.01.545.013 I llama_perf_context_print: prompt eval time =      55.51 ms /     7 tokens (    7.93 ms per token,   126.09 tokens per second)
0.01.545.014 I llama_perf_context_print:        eval time =     790.84 ms /    63 runs   (   12.55 ms per token,    79.66 tokens per second)
0.01.545.014 I llama_perf_context_print:       total time =     849.67 ms /    70 tokens
0.01.545.220 I ggml_metal_free: deallocating

real	0m1.563s
user	0m0.108s
sys	0m0.156s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.793 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.934 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.939 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.945 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.946 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.946 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.946 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.947 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.948 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.948 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.948 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.949 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.949 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.949 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.950 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.951 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.951 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.952 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.693 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.739 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.508 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.509 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.510 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.510 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.510 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.511 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.511 I llama_model_loader: - type  f32:  194 tensors
0.00.025.511 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.512 I print_info: file format = GGUF V3 (latest)
0.00.025.512 I print_info: file type   = Q6_K
0.00.025.513 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.260 I load: special tokens cache size = 25
0.00.050.005 I load: token to piece cache size = 0.2984 MB
0.00.050.008 I print_info: arch             = gptneox
0.00.050.008 I print_info: vocab_only       = 0
0.00.050.008 I print_info: n_ctx_train      = 2048
0.00.050.008 I print_info: n_embd           = 2048
0.00.050.008 I print_info: n_layer          = 24
0.00.050.012 I print_info: n_head           = 16
0.00.050.012 I print_info: n_head_kv        = 16
0.00.050.013 I print_info: n_rot            = 32
0.00.050.013 I print_info: n_swa            = 0
0.00.050.013 I print_info: n_embd_head_k    = 128
0.00.050.013 I print_info: n_embd_head_v    = 128
0.00.050.014 I print_info: n_gqa            = 1
0.00.050.015 I print_info: n_embd_k_gqa     = 2048
0.00.050.015 I print_info: n_embd_v_gqa     = 2048
0.00.050.016 I print_info: f_norm_eps       = 1.0e-05
0.00.050.016 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.016 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.018 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.019 I print_info: f_logit_scale    = 0.0e+00
0.00.050.019 I print_info: n_ff             = 8192
0.00.050.020 I print_info: n_expert         = 0
0.00.050.020 I print_info: n_expert_used    = 0
0.00.050.021 I print_info: causal attn      = 1
0.00.050.022 I print_info: pooling type     = 0
0.00.050.022 I print_info: rope type        = 2
0.00.050.022 I print_info: rope scaling     = linear
0.00.050.022 I print_info: freq_base_train  = 10000.0
0.00.050.023 I print_info: freq_scale_train = 1
0.00.050.024 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.024 I print_info: rope_finetuned   = unknown
0.00.050.024 I print_info: ssm_d_conv       = 0
0.00.050.025 I print_info: ssm_d_inner      = 0
0.00.050.025 I print_info: ssm_d_state      = 0
0.00.050.025 I print_info: ssm_dt_rank      = 0
0.00.050.025 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.025 I print_info: model type       = 1.4B
0.00.050.026 I print_info: model params     = 1.41 B
0.00.050.026 I print_info: general.name     = 1.4B
0.00.050.026 I print_info: vocab type       = BPE
0.00.050.026 I print_info: n_vocab          = 50304
0.00.050.027 I print_info: n_merges         = 50009
0.00.050.030 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.031 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.031 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.035 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.036 I print_info: LF token         = 128 'Ä'
0.00.050.038 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.038 I print_info: max token length = 1024
0.00.052.071 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.071 I load_tensors: offloading output layer to GPU
0.00.052.072 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.082 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.083 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.378 I llama_init_from_model: n_seq_max     = 1
0.00.052.379 I llama_init_from_model: n_ctx         = 2048
0.00.052.379 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.379 I llama_init_from_model: n_batch       = 2048
0.00.052.379 I llama_init_from_model: n_ubatch      = 512
0.00.052.379 I llama_init_from_model: flash_attn    = 0
0.00.052.380 I llama_init_from_model: freq_base     = 10000.0
0.00.052.380 I llama_init_from_model: freq_scale    = 1
0.00.052.381 I ggml_metal_init: allocating
0.00.052.384 I ggml_metal_init: found device: Apple M4
0.00.052.386 I ggml_metal_init: picking default device: Apple M4
0.00.052.970 I ggml_metal_init: using embedded metal library
0.00.055.329 I ggml_metal_init: GPU name:   Apple M4
0.00.055.330 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.331 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.331 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.331 I ggml_metal_init: simdgroup reduction   = true
0.00.055.332 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.332 I ggml_metal_init: has bfloat            = true
0.00.055.332 I ggml_metal_init: use bfloat            = true
0.00.055.332 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.333 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.276 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.226 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.236 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.267 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.087.315 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.087.317 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.087.317 I llama_init_from_model: graph nodes  = 967
0.00.087.318 I llama_init_from_model: graph splits = 2
0.00.087.321 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.449 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.450 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.771.792 I main: llama threadpool init, n_threads = 4
0.00.771.835 I 
0.00.771.875 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.771.877 I 
0.00.772.106 I sampler seed: 1234
0.00.772.110 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.772.146 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.772.167 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.772.167 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.652.701 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61954.62 tokens per second)
0.01.652.702 I llama_perf_context_print:        load time =     762.99 ms
0.01.652.702 I llama_perf_context_print: prompt eval time =      54.40 ms /     7 tokens (    7.77 ms per token,   128.68 tokens per second)
0.01.652.703 I llama_perf_context_print:        eval time =     823.25 ms /    63 runs   (   13.07 ms per token,    76.53 tokens per second)
0.01.652.703 I llama_perf_context_print:       total time =     880.91 ms /    70 tokens
0.01.652.908 I ggml_metal_free: deallocating

real	0m1.671s
user	0m0.108s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.774 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.248 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.039.947 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.956 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.966 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.968 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.968 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.969 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.970 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.971 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.972 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.975 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.976 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.977 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.977 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.978 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.982 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.982 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.983 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.340 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.327 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.524 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.526 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.527 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.527 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.528 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.528 I llama_model_loader: - type  f32:  194 tensors
0.00.057.529 I llama_model_loader: - type  f16:   98 tensors
0.00.057.529 I print_info: file format = GGUF V3 (latest)
0.00.057.531 I print_info: file type   = all F32 (guessed)
0.00.057.532 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.084.947 I load: special tokens cache size = 25
0.00.091.432 I load: token to piece cache size = 0.2984 MB
0.00.091.436 I print_info: arch             = gptneox
0.00.091.436 I print_info: vocab_only       = 0
0.00.091.436 I print_info: n_ctx_train      = 2048
0.00.091.436 I print_info: n_embd           = 2048
0.00.091.436 I print_info: n_layer          = 24
0.00.091.440 I print_info: n_head           = 16
0.00.091.440 I print_info: n_head_kv        = 16
0.00.091.441 I print_info: n_rot            = 32
0.00.091.441 I print_info: n_swa            = 0
0.00.091.441 I print_info: n_embd_head_k    = 128
0.00.091.443 I print_info: n_embd_head_v    = 128
0.00.091.443 I print_info: n_gqa            = 1
0.00.091.444 I print_info: n_embd_k_gqa     = 2048
0.00.091.445 I print_info: n_embd_v_gqa     = 2048
0.00.091.445 I print_info: f_norm_eps       = 1.0e-05
0.00.091.446 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.091.446 I print_info: f_clamp_kqv      = 0.0e+00
0.00.091.446 I print_info: f_max_alibi_bias = 0.0e+00
0.00.091.446 I print_info: f_logit_scale    = 0.0e+00
0.00.091.447 I print_info: n_ff             = 8192
0.00.091.447 I print_info: n_expert         = 0
0.00.091.447 I print_info: n_expert_used    = 0
0.00.091.447 I print_info: causal attn      = 1
0.00.091.447 I print_info: pooling type     = 0
0.00.091.448 I print_info: rope type        = 2
0.00.091.448 I print_info: rope scaling     = linear
0.00.091.448 I print_info: freq_base_train  = 10000.0
0.00.091.449 I print_info: freq_scale_train = 1
0.00.091.449 I print_info: n_ctx_orig_yarn  = 2048
0.00.091.449 I print_info: rope_finetuned   = unknown
0.00.091.449 I print_info: ssm_d_conv       = 0
0.00.091.450 I print_info: ssm_d_inner      = 0
0.00.091.450 I print_info: ssm_d_state      = 0
0.00.091.450 I print_info: ssm_dt_rank      = 0
0.00.091.450 I print_info: ssm_dt_b_c_rms   = 0
0.00.091.450 I print_info: model type       = 1.4B
0.00.091.451 I print_info: model params     = 1.41 B
0.00.091.452 I print_info: general.name     = 1.4B
0.00.091.453 I print_info: vocab type       = BPE
0.00.091.453 I print_info: n_vocab          = 50304
0.00.091.453 I print_info: n_merges         = 50009
0.00.091.453 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.091.453 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.091.453 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.091.454 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.091.454 I print_info: LF token         = 128 'Ä'
0.00.091.454 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.091.455 I print_info: max token length = 1024
0.00.094.016 I load_tensors: offloading 24 repeating layers to GPU
0.00.094.017 I load_tensors: offloading output layer to GPU
0.00.094.017 I load_tensors: offloaded 25/25 layers to GPU
0.00.094.027 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.028 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.094.293 I llama_init_from_model: n_seq_max     = 1
0.00.094.293 I llama_init_from_model: n_ctx         = 128
0.00.094.294 I llama_init_from_model: n_ctx_per_seq = 128
0.00.094.294 I llama_init_from_model: n_batch       = 128
0.00.094.294 I llama_init_from_model: n_ubatch      = 128
0.00.094.294 I llama_init_from_model: flash_attn    = 0
0.00.094.295 I llama_init_from_model: freq_base     = 10000.0
0.00.094.295 I llama_init_from_model: freq_scale    = 1
0.00.094.295 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.295 I ggml_metal_init: allocating
0.00.094.298 I ggml_metal_init: found device: Apple M4
0.00.094.300 I ggml_metal_init: picking default device: Apple M4
0.00.094.918 I ggml_metal_init: using embedded metal library
0.00.097.466 I ggml_metal_init: GPU name:   Apple M4
0.00.097.468 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.468 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.469 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.469 I ggml_metal_init: simdgroup reduction   = true
0.00.097.469 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.469 I ggml_metal_init: has bfloat            = true
0.00.097.469 I ggml_metal_init: use bfloat            = true
0.00.097.470 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.470 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.743 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.107.999 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.002 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.015 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.108.872 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.108.874 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.108.874 I llama_init_from_model: graph nodes  = 967
0.00.108.874 I llama_init_from_model: graph splits = 2
0.00.108.875 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.108.875 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.215.324 I 
0.01.215.409 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.215.454 I perplexity: tokenizing the input ..
0.01.230.165 I perplexity: tokenization took 14.708 ms
0.01.230.172 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.352.247 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.353.943 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.353.991 I llama_perf_context_print:        load time =    1191.05 ms
0.01.353.992 I llama_perf_context_print: prompt eval time =     121.17 ms /   128 tokens (    0.95 ms per token,  1056.34 tokens per second)
0.01.353.993 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.353.996 I llama_perf_context_print:       total time =     138.68 ms /   129 tokens
0.01.354.766 I ggml_metal_free: deallocating

real	0m1.573s
user	0m0.126s
sys	0m0.213s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.139 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.491 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.622 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.629 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.630 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.631 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.632 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.632 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.632 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.633 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.633 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.634 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.634 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.635 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.635 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.635 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.638 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.638 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.638 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.975 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.394 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.739 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.741 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.741 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.742 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.742 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.742 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.743 I llama_model_loader: - type  f32:  194 tensors
0.00.033.743 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.744 I print_info: file format = GGUF V3 (latest)
0.00.033.745 I print_info: file type   = Q8_0
0.00.033.746 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.057.046 I load: special tokens cache size = 25
0.00.063.404 I load: token to piece cache size = 0.2984 MB
0.00.063.407 I print_info: arch             = gptneox
0.00.063.408 I print_info: vocab_only       = 0
0.00.063.408 I print_info: n_ctx_train      = 2048
0.00.063.408 I print_info: n_embd           = 2048
0.00.063.408 I print_info: n_layer          = 24
0.00.063.411 I print_info: n_head           = 16
0.00.063.412 I print_info: n_head_kv        = 16
0.00.063.413 I print_info: n_rot            = 32
0.00.063.413 I print_info: n_swa            = 0
0.00.063.413 I print_info: n_embd_head_k    = 128
0.00.063.413 I print_info: n_embd_head_v    = 128
0.00.063.414 I print_info: n_gqa            = 1
0.00.063.415 I print_info: n_embd_k_gqa     = 2048
0.00.063.416 I print_info: n_embd_v_gqa     = 2048
0.00.063.416 I print_info: f_norm_eps       = 1.0e-05
0.00.063.417 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.417 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.417 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.417 I print_info: f_logit_scale    = 0.0e+00
0.00.063.418 I print_info: n_ff             = 8192
0.00.063.418 I print_info: n_expert         = 0
0.00.063.418 I print_info: n_expert_used    = 0
0.00.063.418 I print_info: causal attn      = 1
0.00.063.419 I print_info: pooling type     = 0
0.00.063.419 I print_info: rope type        = 2
0.00.063.422 I print_info: rope scaling     = linear
0.00.063.423 I print_info: freq_base_train  = 10000.0
0.00.063.423 I print_info: freq_scale_train = 1
0.00.063.423 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.423 I print_info: rope_finetuned   = unknown
0.00.063.423 I print_info: ssm_d_conv       = 0
0.00.063.424 I print_info: ssm_d_inner      = 0
0.00.063.424 I print_info: ssm_d_state      = 0
0.00.063.424 I print_info: ssm_dt_rank      = 0
0.00.063.424 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.424 I print_info: model type       = 1.4B
0.00.063.425 I print_info: model params     = 1.41 B
0.00.063.425 I print_info: general.name     = 1.4B
0.00.063.430 I print_info: vocab type       = BPE
0.00.063.430 I print_info: n_vocab          = 50304
0.00.063.430 I print_info: n_merges         = 50009
0.00.063.431 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.432 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.432 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.432 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.432 I print_info: LF token         = 128 'Ä'
0.00.063.432 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.433 I print_info: max token length = 1024
0.00.065.655 I load_tensors: offloading 24 repeating layers to GPU
0.00.065.655 I load_tensors: offloading output layer to GPU
0.00.065.655 I load_tensors: offloaded 25/25 layers to GPU
0.00.065.666 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.667 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.065.956 I llama_init_from_model: n_seq_max     = 1
0.00.065.957 I llama_init_from_model: n_ctx         = 128
0.00.065.958 I llama_init_from_model: n_ctx_per_seq = 128
0.00.065.958 I llama_init_from_model: n_batch       = 128
0.00.065.958 I llama_init_from_model: n_ubatch      = 128
0.00.065.958 I llama_init_from_model: flash_attn    = 0
0.00.065.958 I llama_init_from_model: freq_base     = 10000.0
0.00.065.959 I llama_init_from_model: freq_scale    = 1
0.00.065.959 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.065.959 I ggml_metal_init: allocating
0.00.065.963 I ggml_metal_init: found device: Apple M4
0.00.065.965 I ggml_metal_init: picking default device: Apple M4
0.00.066.584 I ggml_metal_init: using embedded metal library
0.00.069.134 I ggml_metal_init: GPU name:   Apple M4
0.00.069.135 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.136 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.136 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.136 I ggml_metal_init: simdgroup reduction   = true
0.00.069.137 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.137 I ggml_metal_init: has bfloat            = true
0.00.069.137 I ggml_metal_init: use bfloat            = true
0.00.069.137 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.138 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.346 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.080.765 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.080.770 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.080.787 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.081.812 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.081.813 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.081.814 I llama_init_from_model: graph nodes  = 967
0.00.081.814 I llama_init_from_model: graph splits = 2
0.00.081.815 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.081.816 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.961.722 I 
0.00.961.780 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.961.792 I perplexity: tokenizing the input ..
0.00.969.654 I perplexity: tokenization took 7.86 ms
0.00.969.657 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.093.499 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.094.674 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.094.693 I llama_perf_context_print:        load time =     950.22 ms
0.01.094.694 I llama_perf_context_print: prompt eval time =     123.62 ms /   128 tokens (    0.97 ms per token,  1035.46 tokens per second)
0.01.094.695 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.094.695 I llama_perf_context_print:       total time =     132.97 ms /   129 tokens
0.01.095.104 I ggml_metal_free: deallocating

real	0m1.112s
user	0m0.092s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.398 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.180 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.185 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.188 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.188 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.189 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.189 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.189 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.190 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.190 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.191 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.191 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.192 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.192 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.193 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.194 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.194 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.195 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.873 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.893 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.524 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.525 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.525 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.525 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.526 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.526 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.526 I llama_model_loader: - type  f32:  194 tensors
0.00.025.527 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.527 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.527 I print_info: file format = GGUF V3 (latest)
0.00.025.528 I print_info: file type   = Q4_0
0.00.025.529 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.036 I load: special tokens cache size = 25
0.00.049.776 I load: token to piece cache size = 0.2984 MB
0.00.049.779 I print_info: arch             = gptneox
0.00.049.779 I print_info: vocab_only       = 0
0.00.049.780 I print_info: n_ctx_train      = 2048
0.00.049.780 I print_info: n_embd           = 2048
0.00.049.780 I print_info: n_layer          = 24
0.00.049.783 I print_info: n_head           = 16
0.00.049.784 I print_info: n_head_kv        = 16
0.00.049.784 I print_info: n_rot            = 32
0.00.049.784 I print_info: n_swa            = 0
0.00.049.785 I print_info: n_embd_head_k    = 128
0.00.049.785 I print_info: n_embd_head_v    = 128
0.00.049.786 I print_info: n_gqa            = 1
0.00.049.786 I print_info: n_embd_k_gqa     = 2048
0.00.049.787 I print_info: n_embd_v_gqa     = 2048
0.00.049.788 I print_info: f_norm_eps       = 1.0e-05
0.00.049.788 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.788 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.788 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.789 I print_info: f_logit_scale    = 0.0e+00
0.00.049.789 I print_info: n_ff             = 8192
0.00.049.790 I print_info: n_expert         = 0
0.00.049.790 I print_info: n_expert_used    = 0
0.00.049.790 I print_info: causal attn      = 1
0.00.049.790 I print_info: pooling type     = 0
0.00.049.790 I print_info: rope type        = 2
0.00.049.790 I print_info: rope scaling     = linear
0.00.049.792 I print_info: freq_base_train  = 10000.0
0.00.049.792 I print_info: freq_scale_train = 1
0.00.049.793 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.793 I print_info: rope_finetuned   = unknown
0.00.049.793 I print_info: ssm_d_conv       = 0
0.00.049.793 I print_info: ssm_d_inner      = 0
0.00.049.795 I print_info: ssm_d_state      = 0
0.00.049.796 I print_info: ssm_dt_rank      = 0
0.00.049.796 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.796 I print_info: model type       = 1.4B
0.00.049.796 I print_info: model params     = 1.41 B
0.00.049.797 I print_info: general.name     = 1.4B
0.00.049.797 I print_info: vocab type       = BPE
0.00.049.797 I print_info: n_vocab          = 50304
0.00.049.797 I print_info: n_merges         = 50009
0.00.049.798 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.798 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.799 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.799 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.803 I print_info: LF token         = 128 'Ä'
0.00.049.803 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.803 I print_info: max token length = 1024
0.00.051.564 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.565 I load_tensors: offloading output layer to GPU
0.00.051.565 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.570 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.571 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.858 I llama_init_from_model: n_seq_max     = 1
0.00.051.858 I llama_init_from_model: n_ctx         = 128
0.00.051.859 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.859 I llama_init_from_model: n_batch       = 128
0.00.051.859 I llama_init_from_model: n_ubatch      = 128
0.00.051.859 I llama_init_from_model: flash_attn    = 0
0.00.051.859 I llama_init_from_model: freq_base     = 10000.0
0.00.051.860 I llama_init_from_model: freq_scale    = 1
0.00.051.860 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.861 I ggml_metal_init: allocating
0.00.051.864 I ggml_metal_init: found device: Apple M4
0.00.051.866 I ggml_metal_init: picking default device: Apple M4
0.00.052.424 I ggml_metal_init: using embedded metal library
0.00.054.729 I ggml_metal_init: GPU name:   Apple M4
0.00.054.730 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.731 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.731 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.731 I ggml_metal_init: simdgroup reduction   = true
0.00.054.731 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.731 I ggml_metal_init: has bfloat            = true
0.00.054.732 I ggml_metal_init: use bfloat            = true
0.00.054.732 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.733 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.543 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.743 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.745 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.759 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.692 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.693 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.693 I llama_init_from_model: graph nodes  = 967
0.00.067.693 I llama_init_from_model: graph splits = 2
0.00.067.694 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.695 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.696.000 I 
0.00.696.071 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.696.098 I perplexity: tokenizing the input ..
0.00.703.924 I perplexity: tokenization took 7.823 ms
0.00.703.927 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.825.536 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.826.877 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.826.906 I llama_perf_context_print:        load time =     685.59 ms
0.00.826.907 I llama_perf_context_print: prompt eval time =     121.38 ms /   128 tokens (    0.95 ms per token,  1054.55 tokens per second)
0.00.826.908 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.826.908 I llama_perf_context_print:       total time =     130.91 ms /   129 tokens
0.00.827.284 I ggml_metal_free: deallocating

real	0m0.842s
user	0m0.076s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.564 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.464 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.019.473 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.475 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.475 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.476 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.476 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.476 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.477 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.477 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.477 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.478 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.478 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.478 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.479 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.480 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.480 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.481 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.366 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.388 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.297 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.299 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.299 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.300 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.300 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.300 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.028.301 I llama_model_loader: - type  f32:  194 tensors
0.00.028.301 I llama_model_loader: - type q4_1:   97 tensors
0.00.028.302 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.302 I print_info: file format = GGUF V3 (latest)
0.00.028.303 I print_info: file type   = Q4_1
0.00.028.308 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.048.568 I load: special tokens cache size = 25
0.00.054.554 I load: token to piece cache size = 0.2984 MB
0.00.054.558 I print_info: arch             = gptneox
0.00.054.559 I print_info: vocab_only       = 0
0.00.054.559 I print_info: n_ctx_train      = 2048
0.00.054.559 I print_info: n_embd           = 2048
0.00.054.559 I print_info: n_layer          = 24
0.00.054.564 I print_info: n_head           = 16
0.00.054.564 I print_info: n_head_kv        = 16
0.00.054.565 I print_info: n_rot            = 32
0.00.054.565 I print_info: n_swa            = 0
0.00.054.565 I print_info: n_embd_head_k    = 128
0.00.054.565 I print_info: n_embd_head_v    = 128
0.00.054.566 I print_info: n_gqa            = 1
0.00.054.567 I print_info: n_embd_k_gqa     = 2048
0.00.054.569 I print_info: n_embd_v_gqa     = 2048
0.00.054.569 I print_info: f_norm_eps       = 1.0e-05
0.00.054.570 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.570 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.572 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.572 I print_info: f_logit_scale    = 0.0e+00
0.00.054.572 I print_info: n_ff             = 8192
0.00.054.573 I print_info: n_expert         = 0
0.00.054.573 I print_info: n_expert_used    = 0
0.00.054.573 I print_info: causal attn      = 1
0.00.054.573 I print_info: pooling type     = 0
0.00.054.573 I print_info: rope type        = 2
0.00.054.574 I print_info: rope scaling     = linear
0.00.054.574 I print_info: freq_base_train  = 10000.0
0.00.054.574 I print_info: freq_scale_train = 1
0.00.054.574 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.575 I print_info: rope_finetuned   = unknown
0.00.054.575 I print_info: ssm_d_conv       = 0
0.00.054.575 I print_info: ssm_d_inner      = 0
0.00.054.575 I print_info: ssm_d_state      = 0
0.00.054.576 I print_info: ssm_dt_rank      = 0
0.00.054.576 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.577 I print_info: model type       = 1.4B
0.00.054.577 I print_info: model params     = 1.41 B
0.00.054.577 I print_info: general.name     = 1.4B
0.00.054.578 I print_info: vocab type       = BPE
0.00.054.578 I print_info: n_vocab          = 50304
0.00.054.578 I print_info: n_merges         = 50009
0.00.054.578 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.578 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.579 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.579 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.579 I print_info: LF token         = 128 'Ä'
0.00.054.579 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.580 I print_info: max token length = 1024
0.00.056.708 I load_tensors: offloading 24 repeating layers to GPU
0.00.056.708 I load_tensors: offloading output layer to GPU
0.00.056.708 I load_tensors: offloaded 25/25 layers to GPU
0.00.056.719 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.056.720 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.057.005 I llama_init_from_model: n_seq_max     = 1
0.00.057.006 I llama_init_from_model: n_ctx         = 128
0.00.057.006 I llama_init_from_model: n_ctx_per_seq = 128
0.00.057.006 I llama_init_from_model: n_batch       = 128
0.00.057.006 I llama_init_from_model: n_ubatch      = 128
0.00.057.006 I llama_init_from_model: flash_attn    = 0
0.00.057.007 I llama_init_from_model: freq_base     = 10000.0
0.00.057.007 I llama_init_from_model: freq_scale    = 1
0.00.057.007 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.057.008 I ggml_metal_init: allocating
0.00.057.011 I ggml_metal_init: found device: Apple M4
0.00.057.013 I ggml_metal_init: picking default device: Apple M4
0.00.057.613 I ggml_metal_init: using embedded metal library
0.00.060.081 I ggml_metal_init: GPU name:   Apple M4
0.00.060.082 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.083 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.083 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.083 I ggml_metal_init: simdgroup reduction   = true
0.00.060.084 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.084 I ggml_metal_init: has bfloat            = true
0.00.060.084 I ggml_metal_init: use bfloat            = true
0.00.060.084 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.085 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.600 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.070.968 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.070.970 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.070.984 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.071.944 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.071.946 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.071.946 I llama_init_from_model: graph nodes  = 967
0.00.071.946 I llama_init_from_model: graph splits = 2
0.00.071.948 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.071.948 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.531 I 
0.00.665.568 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.600 I perplexity: tokenizing the input ..
0.00.673.548 I perplexity: tokenization took 7.946 ms
0.00.673.551 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.796.593 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.797.899 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.797.927 I llama_perf_context_print:        load time =     656.96 ms
0.00.797.928 I llama_perf_context_print: prompt eval time =     122.82 ms /   128 tokens (    0.96 ms per token,  1042.21 tokens per second)
0.00.797.928 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.797.929 I llama_perf_context_print:       total time =     132.40 ms /   129 tokens
0.00.798.436 I ggml_metal_free: deallocating

real	0m0.813s
user	0m0.079s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.296 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.155 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.160 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.162 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.162 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.163 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.163 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.163 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.164 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.164 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.165 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.165 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.166 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.167 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.167 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.170 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.170 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.171 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.896 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.956 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.633 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.635 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.635 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.635 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.636 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.636 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.637 I llama_model_loader: - type  f32:  194 tensors
0.00.025.637 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.637 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.638 I print_info: file format = GGUF V3 (latest)
0.00.025.638 I print_info: file type   = Q5_0
0.00.025.643 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.052 I load: special tokens cache size = 25
0.00.049.811 I load: token to piece cache size = 0.2984 MB
0.00.049.815 I print_info: arch             = gptneox
0.00.049.815 I print_info: vocab_only       = 0
0.00.049.815 I print_info: n_ctx_train      = 2048
0.00.049.816 I print_info: n_embd           = 2048
0.00.049.816 I print_info: n_layer          = 24
0.00.049.818 I print_info: n_head           = 16
0.00.049.819 I print_info: n_head_kv        = 16
0.00.049.819 I print_info: n_rot            = 32
0.00.049.820 I print_info: n_swa            = 0
0.00.049.822 I print_info: n_embd_head_k    = 128
0.00.049.822 I print_info: n_embd_head_v    = 128
0.00.049.822 I print_info: n_gqa            = 1
0.00.049.823 I print_info: n_embd_k_gqa     = 2048
0.00.049.824 I print_info: n_embd_v_gqa     = 2048
0.00.049.825 I print_info: f_norm_eps       = 1.0e-05
0.00.049.825 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.825 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.825 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.825 I print_info: f_logit_scale    = 0.0e+00
0.00.049.826 I print_info: n_ff             = 8192
0.00.049.826 I print_info: n_expert         = 0
0.00.049.826 I print_info: n_expert_used    = 0
0.00.049.827 I print_info: causal attn      = 1
0.00.049.827 I print_info: pooling type     = 0
0.00.049.827 I print_info: rope type        = 2
0.00.049.827 I print_info: rope scaling     = linear
0.00.049.829 I print_info: freq_base_train  = 10000.0
0.00.049.831 I print_info: freq_scale_train = 1
0.00.049.831 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.831 I print_info: rope_finetuned   = unknown
0.00.049.832 I print_info: ssm_d_conv       = 0
0.00.049.832 I print_info: ssm_d_inner      = 0
0.00.049.832 I print_info: ssm_d_state      = 0
0.00.049.832 I print_info: ssm_dt_rank      = 0
0.00.049.833 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.835 I print_info: model type       = 1.4B
0.00.049.835 I print_info: model params     = 1.41 B
0.00.049.835 I print_info: general.name     = 1.4B
0.00.049.836 I print_info: vocab type       = BPE
0.00.049.836 I print_info: n_vocab          = 50304
0.00.049.836 I print_info: n_merges         = 50009
0.00.049.837 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.837 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.837 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.837 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.837 I print_info: LF token         = 128 'Ä'
0.00.049.838 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.838 I print_info: max token length = 1024
0.00.051.837 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.837 I load_tensors: offloading output layer to GPU
0.00.051.838 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.848 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.850 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.244 I llama_init_from_model: n_seq_max     = 1
0.00.052.244 I llama_init_from_model: n_ctx         = 128
0.00.052.245 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.245 I llama_init_from_model: n_batch       = 128
0.00.052.245 I llama_init_from_model: n_ubatch      = 128
0.00.052.245 I llama_init_from_model: flash_attn    = 0
0.00.052.245 I llama_init_from_model: freq_base     = 10000.0
0.00.052.246 I llama_init_from_model: freq_scale    = 1
0.00.052.246 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.246 I ggml_metal_init: allocating
0.00.052.249 I ggml_metal_init: found device: Apple M4
0.00.052.251 I ggml_metal_init: picking default device: Apple M4
0.00.052.812 I ggml_metal_init: using embedded metal library
0.00.055.157 I ggml_metal_init: GPU name:   Apple M4
0.00.055.159 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.159 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.159 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.160 I ggml_metal_init: simdgroup reduction   = true
0.00.055.160 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.160 I ggml_metal_init: has bfloat            = true
0.00.055.160 I ggml_metal_init: use bfloat            = true
0.00.055.161 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.161 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.674 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.257 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.259 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.275 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.135 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.136 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.136 I llama_init_from_model: graph nodes  = 967
0.00.067.137 I llama_init_from_model: graph splits = 2
0.00.067.138 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.138 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.987 I 
0.00.763.014 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.763.029 I perplexity: tokenizing the input ..
0.00.771.090 I perplexity: tokenization took 8.059 ms
0.00.771.094 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.905.860 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.907.024 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.907.051 I llama_perf_context_print:        load time =     752.69 ms
0.00.907.052 I llama_perf_context_print: prompt eval time =     134.54 ms /   128 tokens (    1.05 ms per token,   951.40 tokens per second)
0.00.907.053 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.907.053 I llama_perf_context_print:       total time =     144.07 ms /   129 tokens
0.00.907.539 I ggml_metal_free: deallocating

real	0m0.923s
user	0m0.076s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.828 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.115 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.119 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.121 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.122 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.122 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.122 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.123 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.124 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.124 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.124 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.125 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.125 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.125 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.126 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.129 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.129 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.130 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.947 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.977 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.782 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.783 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.783 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.784 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.784 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.784 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.785 I llama_model_loader: - type  f32:  194 tensors
0.00.024.785 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.785 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.786 I print_info: file format = GGUF V3 (latest)
0.00.024.786 I print_info: file type   = Q5_1
0.00.024.787 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.922 I load: special tokens cache size = 25
0.00.049.726 I load: token to piece cache size = 0.2984 MB
0.00.049.729 I print_info: arch             = gptneox
0.00.049.729 I print_info: vocab_only       = 0
0.00.049.729 I print_info: n_ctx_train      = 2048
0.00.049.729 I print_info: n_embd           = 2048
0.00.049.730 I print_info: n_layer          = 24
0.00.049.733 I print_info: n_head           = 16
0.00.049.734 I print_info: n_head_kv        = 16
0.00.049.734 I print_info: n_rot            = 32
0.00.049.734 I print_info: n_swa            = 0
0.00.049.737 I print_info: n_embd_head_k    = 128
0.00.049.737 I print_info: n_embd_head_v    = 128
0.00.049.738 I print_info: n_gqa            = 1
0.00.049.739 I print_info: n_embd_k_gqa     = 2048
0.00.049.739 I print_info: n_embd_v_gqa     = 2048
0.00.049.740 I print_info: f_norm_eps       = 1.0e-05
0.00.049.741 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.742 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.742 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.742 I print_info: f_logit_scale    = 0.0e+00
0.00.049.743 I print_info: n_ff             = 8192
0.00.049.743 I print_info: n_expert         = 0
0.00.049.743 I print_info: n_expert_used    = 0
0.00.049.743 I print_info: causal attn      = 1
0.00.049.743 I print_info: pooling type     = 0
0.00.049.743 I print_info: rope type        = 2
0.00.049.744 I print_info: rope scaling     = linear
0.00.049.744 I print_info: freq_base_train  = 10000.0
0.00.049.744 I print_info: freq_scale_train = 1
0.00.049.745 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.749 I print_info: rope_finetuned   = unknown
0.00.049.749 I print_info: ssm_d_conv       = 0
0.00.049.749 I print_info: ssm_d_inner      = 0
0.00.049.750 I print_info: ssm_d_state      = 0
0.00.049.751 I print_info: ssm_dt_rank      = 0
0.00.049.751 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.751 I print_info: model type       = 1.4B
0.00.049.751 I print_info: model params     = 1.41 B
0.00.049.751 I print_info: general.name     = 1.4B
0.00.049.752 I print_info: vocab type       = BPE
0.00.049.752 I print_info: n_vocab          = 50304
0.00.049.752 I print_info: n_merges         = 50009
0.00.049.753 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.756 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.756 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.756 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.756 I print_info: LF token         = 128 'Ä'
0.00.049.757 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.757 I print_info: max token length = 1024
0.00.051.742 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.742 I load_tensors: offloading output layer to GPU
0.00.051.743 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.753 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.754 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.044 I llama_init_from_model: n_seq_max     = 1
0.00.052.044 I llama_init_from_model: n_ctx         = 128
0.00.052.045 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.045 I llama_init_from_model: n_batch       = 128
0.00.052.045 I llama_init_from_model: n_ubatch      = 128
0.00.052.045 I llama_init_from_model: flash_attn    = 0
0.00.052.045 I llama_init_from_model: freq_base     = 10000.0
0.00.052.046 I llama_init_from_model: freq_scale    = 1
0.00.052.046 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.046 I ggml_metal_init: allocating
0.00.052.049 I ggml_metal_init: found device: Apple M4
0.00.052.051 I ggml_metal_init: picking default device: Apple M4
0.00.052.627 I ggml_metal_init: using embedded metal library
0.00.054.943 I ggml_metal_init: GPU name:   Apple M4
0.00.054.944 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.945 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.945 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.945 I ggml_metal_init: simdgroup reduction   = true
0.00.054.946 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.946 I ggml_metal_init: has bfloat            = true
0.00.054.946 I ggml_metal_init: use bfloat            = true
0.00.054.946 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.947 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.574 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.860 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.862 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.876 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.714 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.715 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.715 I llama_init_from_model: graph nodes  = 967
0.00.066.716 I llama_init_from_model: graph splits = 2
0.00.066.717 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.717 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.140 I 
0.00.668.170 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.668.181 I perplexity: tokenizing the input ..
0.00.676.175 I perplexity: tokenization took 7.992 ms
0.00.676.178 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.810.411 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.811.614 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.811.637 I llama_perf_context_print:        load time =     659.31 ms
0.00.811.638 I llama_perf_context_print: prompt eval time =     134.01 ms /   128 tokens (    1.05 ms per token,   955.17 tokens per second)
0.00.811.638 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.811.639 I llama_perf_context_print:       total time =     143.50 ms /   129 tokens
0.00.812.084 I ggml_metal_free: deallocating

real	0m0.826s
user	0m0.077s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.962 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.692 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.697 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.699 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.699 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.699 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.700 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.700 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.701 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.701 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.702 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.702 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.702 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.703 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.703 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.705 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.705 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.706 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.426 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.442 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.174 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.175 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.176 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.176 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.176 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.177 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.177 I llama_model_loader: - type  f32:  194 tensors
0.00.026.177 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.178 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.178 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.178 I print_info: file format = GGUF V3 (latest)
0.00.026.179 I print_info: file type   = Q2_K - Medium
0.00.026.181 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.640 I load: special tokens cache size = 25
0.00.050.285 I load: token to piece cache size = 0.2984 MB
0.00.050.288 I print_info: arch             = gptneox
0.00.050.288 I print_info: vocab_only       = 0
0.00.050.288 I print_info: n_ctx_train      = 2048
0.00.050.288 I print_info: n_embd           = 2048
0.00.050.288 I print_info: n_layer          = 24
0.00.050.291 I print_info: n_head           = 16
0.00.050.292 I print_info: n_head_kv        = 16
0.00.050.292 I print_info: n_rot            = 32
0.00.050.292 I print_info: n_swa            = 0
0.00.050.293 I print_info: n_embd_head_k    = 128
0.00.050.293 I print_info: n_embd_head_v    = 128
0.00.050.294 I print_info: n_gqa            = 1
0.00.050.295 I print_info: n_embd_k_gqa     = 2048
0.00.050.295 I print_info: n_embd_v_gqa     = 2048
0.00.050.296 I print_info: f_norm_eps       = 1.0e-05
0.00.050.296 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.297 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.297 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.297 I print_info: f_logit_scale    = 0.0e+00
0.00.050.298 I print_info: n_ff             = 8192
0.00.050.299 I print_info: n_expert         = 0
0.00.050.299 I print_info: n_expert_used    = 0
0.00.050.299 I print_info: causal attn      = 1
0.00.050.301 I print_info: pooling type     = 0
0.00.050.301 I print_info: rope type        = 2
0.00.050.301 I print_info: rope scaling     = linear
0.00.050.302 I print_info: freq_base_train  = 10000.0
0.00.050.302 I print_info: freq_scale_train = 1
0.00.050.302 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.303 I print_info: rope_finetuned   = unknown
0.00.050.303 I print_info: ssm_d_conv       = 0
0.00.050.303 I print_info: ssm_d_inner      = 0
0.00.050.303 I print_info: ssm_d_state      = 0
0.00.050.303 I print_info: ssm_dt_rank      = 0
0.00.050.303 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.304 I print_info: model type       = 1.4B
0.00.050.304 I print_info: model params     = 1.41 B
0.00.050.305 I print_info: general.name     = 1.4B
0.00.050.306 I print_info: vocab type       = BPE
0.00.050.306 I print_info: n_vocab          = 50304
0.00.050.306 I print_info: n_merges         = 50009
0.00.050.307 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.307 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.307 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.307 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.308 I print_info: LF token         = 128 'Ä'
0.00.050.308 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.308 I print_info: max token length = 1024
0.00.052.116 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.116 I load_tensors: offloading output layer to GPU
0.00.052.116 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.127 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.128 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.052.412 I llama_init_from_model: n_seq_max     = 1
0.00.052.413 I llama_init_from_model: n_ctx         = 128
0.00.052.413 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.413 I llama_init_from_model: n_batch       = 128
0.00.052.414 I llama_init_from_model: n_ubatch      = 128
0.00.052.414 I llama_init_from_model: flash_attn    = 0
0.00.052.414 I llama_init_from_model: freq_base     = 10000.0
0.00.052.414 I llama_init_from_model: freq_scale    = 1
0.00.052.415 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.415 I ggml_metal_init: allocating
0.00.052.417 I ggml_metal_init: found device: Apple M4
0.00.052.419 I ggml_metal_init: picking default device: Apple M4
0.00.052.968 I ggml_metal_init: using embedded metal library
0.00.055.293 I ggml_metal_init: GPU name:   Apple M4
0.00.055.294 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.295 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.295 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.295 I ggml_metal_init: simdgroup reduction   = true
0.00.055.295 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.296 I ggml_metal_init: has bfloat            = true
0.00.055.296 I ggml_metal_init: use bfloat            = true
0.00.055.296 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.297 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.880 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.106 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.109 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.133 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.995 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.997 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.997 I llama_init_from_model: graph nodes  = 967
0.00.065.997 I llama_init_from_model: graph splits = 2
0.00.065.998 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.998 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.371.219 I 
0.00.371.243 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.371.254 I perplexity: tokenizing the input ..
0.00.378.664 I perplexity: tokenization took 7.409 ms
0.00.378.667 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.511.213 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.512.388 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.512.414 I llama_perf_context_print:        load time =     360.25 ms
0.00.512.415 I llama_perf_context_print: prompt eval time =     132.32 ms /   128 tokens (    1.03 ms per token,   967.34 tokens per second)
0.00.512.416 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.512.417 I llama_perf_context_print:       total time =     141.19 ms /   129 tokens
0.00.512.758 I ggml_metal_free: deallocating

real	0m0.528s
user	0m0.075s
sys	0m0.064s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.674 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.951 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.956 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.959 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.959 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.960 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.960 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.960 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.961 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.962 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.962 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.962 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.963 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.963 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.964 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.965 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.966 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.966 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.694 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.689 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.431 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.432 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.432 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.433 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.433 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.433 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.434 I llama_model_loader: - type  f32:  194 tensors
0.00.024.434 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.434 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.435 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.435 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.435 I print_info: file format = GGUF V3 (latest)
0.00.024.436 I print_info: file type   = Q3_K - Medium
0.00.024.437 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.668 I load: special tokens cache size = 25
0.00.049.403 I load: token to piece cache size = 0.2984 MB
0.00.049.406 I print_info: arch             = gptneox
0.00.049.406 I print_info: vocab_only       = 0
0.00.049.407 I print_info: n_ctx_train      = 2048
0.00.049.407 I print_info: n_embd           = 2048
0.00.049.407 I print_info: n_layer          = 24
0.00.049.410 I print_info: n_head           = 16
0.00.049.411 I print_info: n_head_kv        = 16
0.00.049.411 I print_info: n_rot            = 32
0.00.049.411 I print_info: n_swa            = 0
0.00.049.411 I print_info: n_embd_head_k    = 128
0.00.049.411 I print_info: n_embd_head_v    = 128
0.00.049.412 I print_info: n_gqa            = 1
0.00.049.413 I print_info: n_embd_k_gqa     = 2048
0.00.049.413 I print_info: n_embd_v_gqa     = 2048
0.00.049.414 I print_info: f_norm_eps       = 1.0e-05
0.00.049.414 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.414 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.415 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.415 I print_info: f_logit_scale    = 0.0e+00
0.00.049.415 I print_info: n_ff             = 8192
0.00.049.416 I print_info: n_expert         = 0
0.00.049.416 I print_info: n_expert_used    = 0
0.00.049.416 I print_info: causal attn      = 1
0.00.049.416 I print_info: pooling type     = 0
0.00.049.416 I print_info: rope type        = 2
0.00.049.416 I print_info: rope scaling     = linear
0.00.049.417 I print_info: freq_base_train  = 10000.0
0.00.049.417 I print_info: freq_scale_train = 1
0.00.049.417 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.418 I print_info: rope_finetuned   = unknown
0.00.049.418 I print_info: ssm_d_conv       = 0
0.00.049.418 I print_info: ssm_d_inner      = 0
0.00.049.419 I print_info: ssm_d_state      = 0
0.00.049.419 I print_info: ssm_dt_rank      = 0
0.00.049.421 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.421 I print_info: model type       = 1.4B
0.00.049.422 I print_info: model params     = 1.41 B
0.00.049.422 I print_info: general.name     = 1.4B
0.00.049.422 I print_info: vocab type       = BPE
0.00.049.423 I print_info: n_vocab          = 50304
0.00.049.423 I print_info: n_merges         = 50009
0.00.049.423 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.423 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.423 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.424 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.428 I print_info: LF token         = 128 'Ä'
0.00.049.428 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.429 I print_info: max token length = 1024
0.00.051.354 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.354 I load_tensors: offloading output layer to GPU
0.00.051.354 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.364 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.366 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.659 I llama_init_from_model: n_seq_max     = 1
0.00.051.660 I llama_init_from_model: n_ctx         = 128
0.00.051.660 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.660 I llama_init_from_model: n_batch       = 128
0.00.051.660 I llama_init_from_model: n_ubatch      = 128
0.00.051.661 I llama_init_from_model: flash_attn    = 0
0.00.051.661 I llama_init_from_model: freq_base     = 10000.0
0.00.051.661 I llama_init_from_model: freq_scale    = 1
0.00.051.661 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.662 I ggml_metal_init: allocating
0.00.051.666 I ggml_metal_init: found device: Apple M4
0.00.051.668 I ggml_metal_init: picking default device: Apple M4
0.00.052.210 I ggml_metal_init: using embedded metal library
0.00.054.547 I ggml_metal_init: GPU name:   Apple M4
0.00.054.549 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.549 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.549 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.550 I ggml_metal_init: simdgroup reduction   = true
0.00.054.550 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.550 I ggml_metal_init: has bfloat            = true
0.00.054.550 I ggml_metal_init: use bfloat            = true
0.00.054.550 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.551 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.148 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.415 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.418 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.432 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.427 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.428 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.428 I llama_init_from_model: graph nodes  = 967
0.00.066.429 I llama_init_from_model: graph splits = 2
0.00.066.430 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.430 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.480.849 I 
0.00.480.893 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.480.909 I perplexity: tokenizing the input ..
0.00.488.953 I perplexity: tokenization took 8.043 ms
0.00.488.964 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.621.254 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.622.408 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.622.432 I llama_perf_context_print:        load time =     472.17 ms
0.00.622.435 I llama_perf_context_print: prompt eval time =     132.05 ms /   128 tokens (    1.03 ms per token,   969.35 tokens per second)
0.00.622.436 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.622.437 I llama_perf_context_print:       total time =     141.58 ms /   129 tokens
0.00.622.892 I ggml_metal_free: deallocating

real	0m0.636s
user	0m0.077s
sys	0m0.086s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.838 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.950 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.955 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.957 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.957 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.958 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.958 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.959 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.959 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.960 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.960 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.961 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.961 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.961 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.962 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.963 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.964 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.964 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.679 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.708 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.411 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.412 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.412 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.413 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.413 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.413 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.414 I llama_model_loader: - type  f32:  194 tensors
0.00.024.414 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.414 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.414 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.415 I print_info: file format = GGUF V3 (latest)
0.00.024.415 I print_info: file type   = Q4_K - Medium
0.00.024.416 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.043.748 I load: special tokens cache size = 25
0.00.049.455 I load: token to piece cache size = 0.2984 MB
0.00.049.458 I print_info: arch             = gptneox
0.00.049.458 I print_info: vocab_only       = 0
0.00.049.459 I print_info: n_ctx_train      = 2048
0.00.049.459 I print_info: n_embd           = 2048
0.00.049.459 I print_info: n_layer          = 24
0.00.049.462 I print_info: n_head           = 16
0.00.049.463 I print_info: n_head_kv        = 16
0.00.049.463 I print_info: n_rot            = 32
0.00.049.463 I print_info: n_swa            = 0
0.00.049.463 I print_info: n_embd_head_k    = 128
0.00.049.464 I print_info: n_embd_head_v    = 128
0.00.049.464 I print_info: n_gqa            = 1
0.00.049.465 I print_info: n_embd_k_gqa     = 2048
0.00.049.466 I print_info: n_embd_v_gqa     = 2048
0.00.049.466 I print_info: f_norm_eps       = 1.0e-05
0.00.049.467 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.467 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.467 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.467 I print_info: f_logit_scale    = 0.0e+00
0.00.049.468 I print_info: n_ff             = 8192
0.00.049.468 I print_info: n_expert         = 0
0.00.049.469 I print_info: n_expert_used    = 0
0.00.049.469 I print_info: causal attn      = 1
0.00.049.469 I print_info: pooling type     = 0
0.00.049.469 I print_info: rope type        = 2
0.00.049.469 I print_info: rope scaling     = linear
0.00.049.472 I print_info: freq_base_train  = 10000.0
0.00.049.473 I print_info: freq_scale_train = 1
0.00.049.473 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.473 I print_info: rope_finetuned   = unknown
0.00.049.473 I print_info: ssm_d_conv       = 0
0.00.049.473 I print_info: ssm_d_inner      = 0
0.00.049.473 I print_info: ssm_d_state      = 0
0.00.049.473 I print_info: ssm_dt_rank      = 0
0.00.049.474 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.474 I print_info: model type       = 1.4B
0.00.049.474 I print_info: model params     = 1.41 B
0.00.049.475 I print_info: general.name     = 1.4B
0.00.049.475 I print_info: vocab type       = BPE
0.00.049.475 I print_info: n_vocab          = 50304
0.00.049.476 I print_info: n_merges         = 50009
0.00.049.476 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.476 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.476 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.476 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.477 I print_info: LF token         = 128 'Ä'
0.00.049.477 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.477 I print_info: max token length = 1024
0.00.051.497 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.498 I load_tensors: offloading output layer to GPU
0.00.051.498 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.508 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.509 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.799 I llama_init_from_model: n_seq_max     = 1
0.00.051.800 I llama_init_from_model: n_ctx         = 128
0.00.051.800 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.800 I llama_init_from_model: n_batch       = 128
0.00.051.801 I llama_init_from_model: n_ubatch      = 128
0.00.051.801 I llama_init_from_model: flash_attn    = 0
0.00.051.801 I llama_init_from_model: freq_base     = 10000.0
0.00.051.801 I llama_init_from_model: freq_scale    = 1
0.00.051.802 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.802 I ggml_metal_init: allocating
0.00.051.805 I ggml_metal_init: found device: Apple M4
0.00.051.807 I ggml_metal_init: picking default device: Apple M4
0.00.052.373 I ggml_metal_init: using embedded metal library
0.00.054.697 I ggml_metal_init: GPU name:   Apple M4
0.00.054.699 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.699 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.699 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.700 I ggml_metal_init: simdgroup reduction   = true
0.00.054.700 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.700 I ggml_metal_init: has bfloat            = true
0.00.054.700 I ggml_metal_init: use bfloat            = true
0.00.054.700 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.701 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.425 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.673 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.675 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.690 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.651 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.652 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.652 I llama_init_from_model: graph nodes  = 967
0.00.066.653 I llama_init_from_model: graph splits = 2
0.00.066.654 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.654 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.540.355 I 
0.00.540.386 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.540.397 I perplexity: tokenizing the input ..
0.00.548.171 I perplexity: tokenization took 7.772 ms
0.00.548.175 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.682.659 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.683.819 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.683.845 I llama_perf_context_print:        load time =     531.51 ms
0.00.683.846 I llama_perf_context_print: prompt eval time =     134.26 ms /   128 tokens (    1.05 ms per token,   953.39 tokens per second)
0.00.683.847 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.683.847 I llama_perf_context_print:       total time =     143.49 ms /   129 tokens
0.00.684.368 I ggml_metal_free: deallocating

real	0m0.699s
user	0m0.078s
sys	0m0.091s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.051 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.892 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.897 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.898 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.899 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.900 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.900 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.900 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.901 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.901 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.903 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.903 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.904 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.904 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.905 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.906 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.907 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.908 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.562 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.546 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.210 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.211 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.211 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.212 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.212 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.212 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.213 I llama_model_loader: - type  f32:  194 tensors
0.00.025.213 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.213 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.214 I print_info: file format = GGUF V3 (latest)
0.00.025.214 I print_info: file type   = Q5_K - Medium
0.00.025.215 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.533 I load: special tokens cache size = 25
0.00.049.442 I load: token to piece cache size = 0.2984 MB
0.00.049.445 I print_info: arch             = gptneox
0.00.049.445 I print_info: vocab_only       = 0
0.00.049.446 I print_info: n_ctx_train      = 2048
0.00.049.446 I print_info: n_embd           = 2048
0.00.049.446 I print_info: n_layer          = 24
0.00.049.449 I print_info: n_head           = 16
0.00.049.450 I print_info: n_head_kv        = 16
0.00.049.450 I print_info: n_rot            = 32
0.00.049.450 I print_info: n_swa            = 0
0.00.049.450 I print_info: n_embd_head_k    = 128
0.00.049.450 I print_info: n_embd_head_v    = 128
0.00.049.451 I print_info: n_gqa            = 1
0.00.049.452 I print_info: n_embd_k_gqa     = 2048
0.00.049.453 I print_info: n_embd_v_gqa     = 2048
0.00.049.453 I print_info: f_norm_eps       = 1.0e-05
0.00.049.454 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.454 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.454 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.455 I print_info: f_logit_scale    = 0.0e+00
0.00.049.456 I print_info: n_ff             = 8192
0.00.049.456 I print_info: n_expert         = 0
0.00.049.457 I print_info: n_expert_used    = 0
0.00.049.457 I print_info: causal attn      = 1
0.00.049.457 I print_info: pooling type     = 0
0.00.049.457 I print_info: rope type        = 2
0.00.049.457 I print_info: rope scaling     = linear
0.00.049.458 I print_info: freq_base_train  = 10000.0
0.00.049.458 I print_info: freq_scale_train = 1
0.00.049.458 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.459 I print_info: rope_finetuned   = unknown
0.00.049.459 I print_info: ssm_d_conv       = 0
0.00.049.459 I print_info: ssm_d_inner      = 0
0.00.049.459 I print_info: ssm_d_state      = 0
0.00.049.461 I print_info: ssm_dt_rank      = 0
0.00.049.461 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.461 I print_info: model type       = 1.4B
0.00.049.462 I print_info: model params     = 1.41 B
0.00.049.462 I print_info: general.name     = 1.4B
0.00.049.462 I print_info: vocab type       = BPE
0.00.049.463 I print_info: n_vocab          = 50304
0.00.049.463 I print_info: n_merges         = 50009
0.00.049.463 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.463 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.463 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.464 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.464 I print_info: LF token         = 128 'Ä'
0.00.049.464 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.465 I print_info: max token length = 1024
0.00.051.406 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.406 I load_tensors: offloading output layer to GPU
0.00.051.406 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.417 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.418 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.700 I llama_init_from_model: n_seq_max     = 1
0.00.051.700 I llama_init_from_model: n_ctx         = 128
0.00.051.701 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.701 I llama_init_from_model: n_batch       = 128
0.00.051.701 I llama_init_from_model: n_ubatch      = 128
0.00.051.701 I llama_init_from_model: flash_attn    = 0
0.00.051.701 I llama_init_from_model: freq_base     = 10000.0
0.00.051.702 I llama_init_from_model: freq_scale    = 1
0.00.051.702 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.702 I ggml_metal_init: allocating
0.00.051.705 I ggml_metal_init: found device: Apple M4
0.00.051.707 I ggml_metal_init: picking default device: Apple M4
0.00.052.246 I ggml_metal_init: using embedded metal library
0.00.054.567 I ggml_metal_init: GPU name:   Apple M4
0.00.054.568 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.568 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.569 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.569 I ggml_metal_init: simdgroup reduction   = true
0.00.054.569 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.569 I ggml_metal_init: has bfloat            = true
0.00.054.569 I ggml_metal_init: use bfloat            = true
0.00.054.570 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.570 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.248 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.566 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.568 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.582 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.499 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.500 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.501 I llama_init_from_model: graph nodes  = 967
0.00.065.501 I llama_init_from_model: graph splits = 2
0.00.065.502 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.502 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.643.554 I 
0.00.643.587 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.643.600 I perplexity: tokenizing the input ..
0.00.651.806 I perplexity: tokenization took 8.204 ms
0.00.651.814 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.792.353 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.793.531 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.793.563 I llama_perf_context_print:        load time =     633.50 ms
0.00.793.564 I llama_perf_context_print: prompt eval time =     140.31 ms /   128 tokens (    1.10 ms per token,   912.25 tokens per second)
0.00.793.565 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.793.570 I llama_perf_context_print:       total time =     150.01 ms /   129 tokens
0.00.794.024 I ggml_metal_free: deallocating

real	0m0.810s
user	0m0.076s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.838 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.742 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.746 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.748 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.748 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.749 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.749 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.749 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.750 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.750 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.751 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.751 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.751 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.752 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.752 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.755 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.755 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.756 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.504 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.534 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.267 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.268 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.269 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.269 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.269 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.270 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.270 I llama_model_loader: - type  f32:  194 tensors
0.00.024.270 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.271 I print_info: file format = GGUF V3 (latest)
0.00.024.272 I print_info: file type   = Q6_K
0.00.024.272 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.489 I load: special tokens cache size = 25
0.00.049.326 I load: token to piece cache size = 0.2984 MB
0.00.049.328 I print_info: arch             = gptneox
0.00.049.329 I print_info: vocab_only       = 0
0.00.049.329 I print_info: n_ctx_train      = 2048
0.00.049.329 I print_info: n_embd           = 2048
0.00.049.329 I print_info: n_layer          = 24
0.00.049.332 I print_info: n_head           = 16
0.00.049.333 I print_info: n_head_kv        = 16
0.00.049.333 I print_info: n_rot            = 32
0.00.049.333 I print_info: n_swa            = 0
0.00.049.334 I print_info: n_embd_head_k    = 128
0.00.049.334 I print_info: n_embd_head_v    = 128
0.00.049.335 I print_info: n_gqa            = 1
0.00.049.336 I print_info: n_embd_k_gqa     = 2048
0.00.049.336 I print_info: n_embd_v_gqa     = 2048
0.00.049.337 I print_info: f_norm_eps       = 1.0e-05
0.00.049.337 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.337 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.337 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.338 I print_info: f_logit_scale    = 0.0e+00
0.00.049.338 I print_info: n_ff             = 8192
0.00.049.339 I print_info: n_expert         = 0
0.00.049.339 I print_info: n_expert_used    = 0
0.00.049.339 I print_info: causal attn      = 1
0.00.049.339 I print_info: pooling type     = 0
0.00.049.339 I print_info: rope type        = 2
0.00.049.341 I print_info: rope scaling     = linear
0.00.049.341 I print_info: freq_base_train  = 10000.0
0.00.049.342 I print_info: freq_scale_train = 1
0.00.049.342 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.342 I print_info: rope_finetuned   = unknown
0.00.049.342 I print_info: ssm_d_conv       = 0
0.00.049.342 I print_info: ssm_d_inner      = 0
0.00.049.343 I print_info: ssm_d_state      = 0
0.00.049.343 I print_info: ssm_dt_rank      = 0
0.00.049.343 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.343 I print_info: model type       = 1.4B
0.00.049.344 I print_info: model params     = 1.41 B
0.00.049.344 I print_info: general.name     = 1.4B
0.00.049.344 I print_info: vocab type       = BPE
0.00.049.345 I print_info: n_vocab          = 50304
0.00.049.345 I print_info: n_merges         = 50009
0.00.049.345 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.345 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.345 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.346 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.346 I print_info: LF token         = 128 'Ä'
0.00.049.346 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.346 I print_info: max token length = 1024
0.00.051.372 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.372 I load_tensors: offloading output layer to GPU
0.00.051.372 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.383 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.384 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.051.672 I llama_init_from_model: n_seq_max     = 1
0.00.051.673 I llama_init_from_model: n_ctx         = 128
0.00.051.673 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.673 I llama_init_from_model: n_batch       = 128
0.00.051.673 I llama_init_from_model: n_ubatch      = 128
0.00.051.673 I llama_init_from_model: flash_attn    = 0
0.00.051.674 I llama_init_from_model: freq_base     = 10000.0
0.00.051.674 I llama_init_from_model: freq_scale    = 1
0.00.051.674 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.675 I ggml_metal_init: allocating
0.00.051.678 I ggml_metal_init: found device: Apple M4
0.00.051.680 I ggml_metal_init: picking default device: Apple M4
0.00.052.251 I ggml_metal_init: using embedded metal library
0.00.054.576 I ggml_metal_init: GPU name:   Apple M4
0.00.054.577 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.577 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.578 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.578 I ggml_metal_init: simdgroup reduction   = true
0.00.054.578 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.578 I ggml_metal_init: has bfloat            = true
0.00.054.578 I ggml_metal_init: use bfloat            = true
0.00.054.579 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.579 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.173 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.457 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.459 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.485 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.410 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.410 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.411 I llama_init_from_model: graph nodes  = 967
0.00.066.411 I llama_init_from_model: graph splits = 2
0.00.066.412 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.412 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.366.155 I 
0.00.366.183 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.366.193 I perplexity: tokenizing the input ..
0.00.373.708 I perplexity: tokenization took 7.514 ms
0.00.373.712 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.514.014 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.515.166 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.515.191 I llama_perf_context_print:        load time =     357.31 ms
0.00.515.192 I llama_perf_context_print: prompt eval time =     140.07 ms /   128 tokens (    1.09 ms per token,   913.81 tokens per second)
0.00.515.193 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.515.193 I llama_perf_context_print:       total time =     149.04 ms /   129 tokens
0.00.515.571 I ggml_metal_free: deallocating

real	0m0.528s
user	0m0.077s
sys	0m0.076s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.247 I build: 4467 (cbea4ba1) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.460 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.961 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.971 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.974 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.975 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.976 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.977 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.977 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.979 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.980 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.981 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.982 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.983 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.983 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.984 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.987 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.988 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.989 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.920 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.882 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.075 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.078 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.078 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.079 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.079 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.080 I llama_model_loader: - type  f32:  194 tensors
0.00.053.080 I llama_model_loader: - type  f16:   98 tensors
0.00.053.081 I print_info: file format = GGUF V3 (latest)
0.00.053.082 I print_info: file type   = all F32 (guessed)
0.00.053.090 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.080.344 I load: special tokens cache size = 25
0.00.087.010 I load: token to piece cache size = 0.2984 MB
0.00.087.013 I print_info: arch             = gptneox
0.00.087.013 I print_info: vocab_only       = 0
0.00.087.013 I print_info: n_ctx_train      = 2048
0.00.087.014 I print_info: n_embd           = 2048
0.00.087.014 I print_info: n_layer          = 24
0.00.087.016 I print_info: n_head           = 16
0.00.087.017 I print_info: n_head_kv        = 16
0.00.087.018 I print_info: n_rot            = 32
0.00.087.018 I print_info: n_swa            = 0
0.00.087.018 I print_info: n_embd_head_k    = 128
0.00.087.018 I print_info: n_embd_head_v    = 128
0.00.087.019 I print_info: n_gqa            = 1
0.00.087.021 I print_info: n_embd_k_gqa     = 2048
0.00.087.021 I print_info: n_embd_v_gqa     = 2048
0.00.087.022 I print_info: f_norm_eps       = 1.0e-05
0.00.087.022 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.087.022 I print_info: f_clamp_kqv      = 0.0e+00
0.00.087.022 I print_info: f_max_alibi_bias = 0.0e+00
0.00.087.023 I print_info: f_logit_scale    = 0.0e+00
0.00.087.023 I print_info: n_ff             = 8192
0.00.087.024 I print_info: n_expert         = 0
0.00.087.024 I print_info: n_expert_used    = 0
0.00.087.024 I print_info: causal attn      = 1
0.00.087.024 I print_info: pooling type     = 0
0.00.087.024 I print_info: rope type        = 2
0.00.087.025 I print_info: rope scaling     = linear
0.00.087.025 I print_info: freq_base_train  = 10000.0
0.00.087.025 I print_info: freq_scale_train = 1
0.00.087.025 I print_info: n_ctx_orig_yarn  = 2048
0.00.087.026 I print_info: rope_finetuned   = unknown
0.00.087.026 I print_info: ssm_d_conv       = 0
0.00.087.026 I print_info: ssm_d_inner      = 0
0.00.087.026 I print_info: ssm_d_state      = 0
0.00.087.026 I print_info: ssm_dt_rank      = 0
0.00.087.026 I print_info: ssm_dt_b_c_rms   = 0
0.00.087.027 I print_info: model type       = 1.4B
0.00.087.027 I print_info: model params     = 1.41 B
0.00.087.027 I print_info: general.name     = 1.4B
0.00.087.028 I print_info: vocab type       = BPE
0.00.087.028 I print_info: n_vocab          = 50304
0.00.087.028 I print_info: n_merges         = 50009
0.00.087.028 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.087.029 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.087.029 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.087.029 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.087.029 I print_info: LF token         = 128 'Ä'
0.00.087.031 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.087.031 I print_info: max token length = 1024
0.00.089.710 I load_tensors: offloading 24 repeating layers to GPU
0.00.089.710 I load_tensors: offloading output layer to GPU
0.00.089.710 I load_tensors: offloaded 25/25 layers to GPU
0.00.089.721 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.089.722 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.090.009 I llama_init_from_model: n_seq_max     = 1
0.00.090.010 I llama_init_from_model: n_ctx         = 128
0.00.090.010 I llama_init_from_model: n_ctx_per_seq = 128
0.00.090.010 I llama_init_from_model: n_batch       = 128
0.00.090.010 I llama_init_from_model: n_ubatch      = 128
0.00.090.010 I llama_init_from_model: flash_attn    = 0
0.00.090.011 I llama_init_from_model: freq_base     = 10000.0
0.00.090.011 I llama_init_from_model: freq_scale    = 1
0.00.090.011 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.012 I ggml_metal_init: allocating
0.00.090.014 I ggml_metal_init: found device: Apple M4
0.00.090.016 I ggml_metal_init: picking default device: Apple M4
0.00.090.621 I ggml_metal_init: using embedded metal library
0.00.093.211 I ggml_metal_init: GPU name:   Apple M4
0.00.093.213 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.213 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.214 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.214 I ggml_metal_init: simdgroup reduction   = true
0.00.093.214 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.214 I ggml_metal_init: has bfloat            = true
0.00.093.214 I ggml_metal_init: use bfloat            = true
0.00.093.215 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.215 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.618 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.873 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.103.876 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.103.889 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.104.759 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.104.760 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.104.761 I llama_init_from_model: graph nodes  = 967
0.00.104.761 I llama_init_from_model: graph splits = 2
0.00.104.762 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.104.762 I 
0.00.104.788 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.104.789 I compute_imatrix: tokenizing the input ..
0.00.111.385 I compute_imatrix: tokenization took 6.595 ms
0.00.111.387 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.634.220 I compute_imatrix: 1.52 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.636.727 I llama_perf_context_print:        load time =    1611.76 ms
0.01.636.729 I llama_perf_context_print: prompt eval time =    1522.17 ms /   128 tokens (   11.89 ms per token,    84.09 tokens per second)
0.01.636.730 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.636.732 I llama_perf_context_print:       total time =    1614.26 ms /   129 tokens
0.01.637.332 I ggml_metal_free: deallocating

real	0m1.818s
user	0m0.167s
sys	0m0.234s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4467 (cbea4ba1)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a20a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a20a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a20af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a20b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a20bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a20c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a20c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a20cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a20d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a20d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a20dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a20e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a20ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a20f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a20fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a2102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a2109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a2110e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a211800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a211fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a2126f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a212e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a213530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a213dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a2144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a2147b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a214dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a215a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a215f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a216230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a2166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a216990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a217220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a217760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a217a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a217ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a218360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a218800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a218ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a219140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a2195e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a219a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a219f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a21a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a21a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a21ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a21b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a21bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a21c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a21c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a21cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a21d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a21da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a21e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a21e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a21ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a21f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a21f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a21fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a220210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a2204d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a220970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a220e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a2212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a221750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a221bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a222090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a222530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a2229d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a222e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a223310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a2237b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a223c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14a2241a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14a2246f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14a224c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14a225190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14a2256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14a225c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14a226180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14a2266d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14a226c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14a227170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14a2276c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14a227c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14a228160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14a2286b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14a228c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14a229150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14a2296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14a229bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14a22a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14a22a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14a22abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14a22b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14a22b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14a22bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14a21b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14a22c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14a22c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14a22cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14a22d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14a22d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14a22dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14a22e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14a22e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14a22ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14a22f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14a22f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14a22fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14a230260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14a2307b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14a230d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a2311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a231640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a231ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a231f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a232420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a2328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a232d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a233200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a2336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a233b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a233fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a234480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a234920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a234dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a235260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a235700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a235ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a236040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a2364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a236980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a236e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a2372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a237760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a237c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a2380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a238540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a2389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a238e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a239320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a2397c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a239c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a23a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a23a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a23aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a23aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a23b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a23b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a23bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a23c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a23c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a23caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a23cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a23d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a23d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a23dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a23e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a23e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a23eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a23efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a23f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a23f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a23fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a240220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a2406c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a240b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a241000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a2414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a241940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a241de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a242280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a242720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a242bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a243060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a243500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a2439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a243e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a2442e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a244780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a244c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a2450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a245560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a245a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a245ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a246340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a2467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a246c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a247120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a2475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a247a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a247f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a248450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a2489a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a248ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a249440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a249700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a249d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a24a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a24a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14a24b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14a24b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a24b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a24be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14a24c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a24cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a24d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a24d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a24da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a24e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a24e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a24ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a24f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a24f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a24fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a250200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a250750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a250ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a2511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a251740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a251c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a2521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a252730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a252c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a2531d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a253720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a253c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a2541c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a254710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a254c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a2551b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a255700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a255c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a2561a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a2566f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a256c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a257190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a2576e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a257c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a258180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a2586d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a258c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a259170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a2596c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a259c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a25a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a25a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a25ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a25b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a25b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a25bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a25c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a25c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a25cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a25d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a25d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a25dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a25e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a25e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a25ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a25f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a25f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a25fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a260100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a260650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a260ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14a261040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14a2614e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a261980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a261e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a2622c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a262760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a262c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a2630a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a263540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a2639e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a263e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a264320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a2647c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a264c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a265100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a265650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a265d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a266490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a266bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a2672d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a267590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14a267d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a268040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a268650 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.142.820 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.142.823 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a268300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a24bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a2499c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a24a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a21d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a21d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a21f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a24c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a214a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a21b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a21be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a21c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a21a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a21caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a213a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a21fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a22c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a267850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a216c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a216f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a24c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a24abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a215080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a215340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a215600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a268ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a268d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a269030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a2692f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a2695b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a269870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a269b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a269df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a26a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a26a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a26a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a26a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a26abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a26ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a26b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a26b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a26b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a26b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a26bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a26bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a26c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a26c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a26c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a26c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a26ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a26cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a26d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a26d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a26d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a26da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a26dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a26dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a26e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a26e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a26e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a26eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a26edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a26f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a26f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a26f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a26f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a26fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a26fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a2700f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a2703b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a270670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a270930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a270bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14a270eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14a271170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14a271430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14a2716f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14a2719b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14a271c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14a271f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14a2721f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14a2724b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14a272770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14a272a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14a272cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14a272fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14a273270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14a273530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14a2737f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14a273ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14a273d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14a274030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14a2742f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14a2745b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14a274870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14a274b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14a274df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14a2750b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14a275370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14a275630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14a2758f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14a275bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14a275e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14a276130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14a2763f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14a2766b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14a276970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14a276c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14a276ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14a2771b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14a277470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14a277730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14a2779f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a277cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a277f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a278230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a2784f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a2787b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a278a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a278d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a278ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a2792b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a279570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a279830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a279af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a279db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a27a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a27a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a27a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a27a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a27ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a27ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a27b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a27b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a27b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a27b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a27bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a27beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a27c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a27c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a27c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a27c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a27cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a27cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a27d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a27d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a27d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a27da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a27dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a27dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a27e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a27e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a27e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a27eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a27ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a27f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a27f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a27f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a27f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a27fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a27fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a2800b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a280370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a280630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a2808f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a280bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a280e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a281130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a2813f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a2816b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a281970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a281c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a281ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a2821b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a282470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a282730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a2829f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a282cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a282f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a283230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a2834f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a2837b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a283a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a283d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a283ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a2842b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a284570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a284830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a284af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a284db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a285070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a285330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a2855f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a2858b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a285b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a285e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a2860f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a2863b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a286670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a286930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a286bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14a286eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14a287170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a287430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a2876f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14a2879b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a287c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a287f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a2881f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a288690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a288e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a289100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a2893c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a289830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a289ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a28a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a28a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a28a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a28ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a28b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a28b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a28bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a28c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a28c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a28c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a28cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a28d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a28d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a28dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a28df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a28e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a28e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a28ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a28f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a28f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a28f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a28fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a2902b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a290720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a290b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a291000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a291470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a2918e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a291d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a2921c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a292630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a292aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a292f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a293380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a2937f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a293c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a2940d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a294540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a2949b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a294e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a295290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a295700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a295b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a295fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a296450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a2968c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a296d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a2971a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a297610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a297a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a297ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a298360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14a2987d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14a298c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a2990b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a299520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a299990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a299e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a29a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a29a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a29ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a29afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a29b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a29b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a29bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a29c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a29c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a29ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a29d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a29dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a29e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a29ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a29ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14a29f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a29f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a29fdb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13e7044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13e704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13e704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13e705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13e7056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13e705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13e705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13e7063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13e706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13e706db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13e707220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13e7078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13e7083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13e708b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13e709380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13e709aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13e70a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13e70a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13e70b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13e70b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13e70bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13e70c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13e70cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13e70d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13e70db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13e70de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13e70e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13e70e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13e70e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13e70ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13e70f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13e70f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13e70fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13e70ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13e710380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13e7107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13e710c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13e7110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13e711540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13e7119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13e711e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13e712290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13e712700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13e712b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13e712fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13e713450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13e7138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13e713d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13e7141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13e714610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13e714a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13e714ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13e715360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13e7157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13e715c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13e7160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13e716620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13e716b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13e716f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13e717400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13e717870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13e717ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13e718150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13e7185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13e718a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13e718ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13e719310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13e719780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13e719bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13e71a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13e71a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13e71a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13e71adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13e71b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13e71b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13e71bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13e71bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13e71c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13e71c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13e71ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13e71d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13e71d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13e71da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13e71de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13e71e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13e71e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13e71ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13e71f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13e71f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13e71f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13e71fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13e720200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13e720670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13e720ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13e720f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13e7213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13e721830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13e721ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13e722110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13e722580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13e7229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13e722e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13e7232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13e723b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13e723e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13e724290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13e724700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13e724b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13e724fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13e725450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13e7258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13e725d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13e7261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13e726610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13e726a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13e726ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13e727360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13e7277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13e727c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13e7280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13e728520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13e728990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13e728e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13e729270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13e7296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13e729b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13e729fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13e72a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13e72a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13e72ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13e72b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13e72b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13e72ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13e72bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13e72c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13e72c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13e72cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13e72d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13e72d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13e72d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13e72dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13e72e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13e72e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13e72eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13e72efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13e72f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13e72f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13e72fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13e730160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13e7305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13e730a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13e730eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13e731320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13e731790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13e731c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13e732070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13e7324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13e732950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13e732dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13e733230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13e7336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13e733b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13e733f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13e7343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13e734860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13e734cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13e735140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13e7355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13e735a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13e735e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13e736300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13e736770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13e736be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13e737050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13e7374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13e737930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13e737da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13e738210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13e738680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13e738af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13e738f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13e7393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13e739840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13e739cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13e73a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13e73a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13e73aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13e73ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13e73b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13e73b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13e73bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13e73c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13e73c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13e73c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13e73cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13e73d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13e73d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13e73dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13e73df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13e73e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13e73e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13e73ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13e73f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13e73f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13e73f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13e73fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13e7402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13e740730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13e740ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13e741010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13e741b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13e741e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13e742110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13e742580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13e7429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13e742e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13e7432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13e743740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13e743bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13e744020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13e744490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13e744900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13e744d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13e7451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13e745650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13e745ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13e745f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13e7463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13e746810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13e746c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13e7470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13e747560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13e7479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13e747e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13e7482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13e748720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13e748b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13e749000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13e749470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13e7498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13e749d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13e74a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13e74a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13e74aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13e74af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13e74b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13e74b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13e74bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13e74c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13e74c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13e74c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13e74ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13e74d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13e74d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13e74db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13e74dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13e74e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13e74e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13e74ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13e74f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13e74f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13e74fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13e74fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13e750360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13e7507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13e750c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13e7510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13e751520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13e751990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13e751e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13e752270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13e7526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13e752b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13e752fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13e753430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13e7538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13e753d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13e754180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13e7545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13e754a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13e754ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13e755340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13e7557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13e756220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13e756940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13e757060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13e757780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13e757a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13e757eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13e7584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13e758ac0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.819s
user	0m0.295s
sys	0m0.317s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4467 (cbea4ba1)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121f0c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121f0c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121f0cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121f0d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121f0d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121f0de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121f0e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121f0e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121f0ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121f0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121f0f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121f0fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121f10940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121f110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121f11900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121f12020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121f12740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121f12e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121f13580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121f13d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121f14470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121f14b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121f152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121f15b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121f16270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121f16530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121f16b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121f177b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121f17cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121f17fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121f18450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121f18710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121f18fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121f194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121f197a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121f19c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121f1a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121f1a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121f1aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121f1aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121f1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121f1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121f1bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121f1c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121f1c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121f1ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121f1d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121f1d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121f1df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121f1e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121f1eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121f1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121f1f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121f1fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121f20590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121f20a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121f20ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121f21190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121f217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121f21f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121f22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121f226f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121f22b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121f23030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121f234d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121f23970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121f23e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121f242b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121f24750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121f24bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121f25090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121f25530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121f259d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121f25f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x121f26470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x121f269c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x121f26f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x121f27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x121f279b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x121f27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x121f28450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x121f289a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x121f28ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x121f29440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x121f29990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x121f29ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x121f2a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x121f2a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x121f2aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x121f2b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x121f2b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x121f2bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x121f2c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x121f2c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x121f2ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x121f2d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x121f2d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x121f1d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x121f2ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x121f2e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x121f2eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x121f2f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x121f2f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x121f2fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x121f30000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x121f30550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x121f30aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x121f30ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x121f31540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x121f31a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x121f31fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x121f32530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x121f32a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x121f32f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x121f333c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x121f33860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x121f33d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x121f341a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x121f34640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x121f34ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x121f34f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x121f35420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x121f358c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x121f35d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x121f36200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x121f366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x121f36b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x121f36fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x121f37480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x121f37920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x121f37dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x121f38260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x121f38700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x121f38ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x121f39040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x121f394e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x121f39980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x121f39e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x121f3a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x121f3a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x121f3ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x121f3b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x121f3b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x121f3b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x121f3be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x121f3c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x121f3c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x121f3cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x121f3d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x121f3d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x121f3da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x121f3dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x121f3e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x121f3e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x121f3ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x121f3f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x121f3f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x121f3faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x121f3ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x121f403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x121f40880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x121f40d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x121f411c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x121f41660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x121f41b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x121f41fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x121f42440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x121f428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x121f42d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x121f43220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x121f436c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x121f43b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x121f44000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x121f444a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x121f44940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x121f44de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x121f45280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x121f45720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x121f45bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x121f46060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x121f46500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x121f469a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x121f46e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x121f472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x121f47780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x121f47c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x121f480c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x121f48560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x121f48a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x121f48ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x121f49340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x121f497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x121f49c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x121f4a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x121f4a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x121f4ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x121f4b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x121f4b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x121f4ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x121f4c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x121f4c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x121f4cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x121f4d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x121f4d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x121f4dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x121f4e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x121f4ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x121f4eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x121f4f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x121f4f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x121f4ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x121f504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x121f50a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x121f50f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x121f514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x121f51a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x121f51f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x121f524d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x121f52a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x121f52f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x121f534c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x121f53a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x121f53f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x121f544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x121f54a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x121f54f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x121f554a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x121f559f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x121f55f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x121f56490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x121f569e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x121f56f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x121f57480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x121f579d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x121f57f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x121f58470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x121f589c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x121f58f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x121f59460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x121f599b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x121f59f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x121f5a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x121f5a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x121f5aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x121f5b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x121f5b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x121f5bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x121f5c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x121f5c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x121f5ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x121f5d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x121f5d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x121f5dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x121f5e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x121f5e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x121f5eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x121f5f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x121f5f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x121f5fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x121f603f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x121f60940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x121f60e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x121f613e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x121f61930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x121f61e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x121f623d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x121f62920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x121f62dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x121f63260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x121f63700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x121f63ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x121f64040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x121f644e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x121f64980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x121f64e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x121f652c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x121f65760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x121f65c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x121f660a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x121f66540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x121f669e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x121f66e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x121f673d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x121f67af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x121f68210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x121f68930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x121f69050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x121f69310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x121f69b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x121f69dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x121f6a3d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.100.764 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.775 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123004ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123005150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1230055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123005a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123005ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123006310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123006780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x123006bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x123007060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1230074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123007940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123008020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123008b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1230092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x123009b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12300a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12300a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12300b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12300b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12300bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12300c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12300cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12300d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12300dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12300e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12300e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12300e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12300ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12300f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12300f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12300fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12300ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1230103d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123010690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123010b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123010f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1230113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123011850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123011cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123012130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1230125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123012a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123012e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1230132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123013760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123013bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123014040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1230144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123014920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123014d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123015200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123015670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x123015ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x123015f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1230163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x123016830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x123016da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1230172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123017710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x123017b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x123017ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123018460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1230188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123018d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1230191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123019620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123019a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x123019f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12301a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12301a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12301ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12301b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12301b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12301b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12301be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12301c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12301c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12301cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12301cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12301d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12301d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12301dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12301e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12301e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12301ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12301eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12301f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12301f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12301fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1230200a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123020510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123020980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x123020df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123021260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1230216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123021b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123021fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123022420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123022890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123022d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123023170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1230235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123023a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123023ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x123024330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1230247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x123024c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x123025080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1230254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x123025960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x123025dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x123026240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1230266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123026b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123026f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123027400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123027870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x123027ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123028150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1230285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123028a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123028ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123029310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123029780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123029bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12302a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12302a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12302a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12302adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12302b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12302b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12302bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12302bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12302c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12302c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12302ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12302d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12302d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12302da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12302de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12302e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12302e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12302ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12302f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12302f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12302f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12302fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123030200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123030670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123030ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123030f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1230313c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123031830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123031ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123032110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x123032580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1230329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123032e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1230332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123033740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x123033bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123034020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x123034490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x123034900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x123034d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1230351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123035e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1230360d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123036390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123036800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123036c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1230370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123037550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1230379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123037e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1230382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123038710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123038b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123038ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123039460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1230398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123039d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12303a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12303a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12303aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12303af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12303b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12303b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12303bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12303c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12303c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12303c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12303ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12303d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12303d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12303db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12303dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12303e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12303e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12303ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12303f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12303f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12303fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123040070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1230404e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x123040950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123040dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123041230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123041750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123041c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1230427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123042a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123043050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x123043610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123043bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123044190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123044750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123044d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1230452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123045890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123045e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123046410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1230469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123046f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123047550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123047b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1230480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123048690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123048c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123049210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1230497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123049d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12304a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12304a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12304aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12304b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12304ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12304c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12304c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12304cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12304d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12304d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12304dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12304e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12304e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12304ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12304f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12304f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12304ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x123050510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x123050ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123051090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x123051650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x123051c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1230521d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x123052790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x123052d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x123053310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1230538d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x123053e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123054450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x123054a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x123054fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x123055590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x123055b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123056110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1230566d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x123056c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x123057190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123057690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x123057b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123058090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123058590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123058a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123058f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123059490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123059990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123059e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12305a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12305a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12305ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12305b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12305b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12305c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12305c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12305cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12305d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12305d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12305e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12305e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12305ea80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x121f6a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x121f4bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x121f4b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x121f4c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x121f1f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x121f1ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x121f21450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x121f4ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x121f167f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x121f1d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x121f1dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x121f1e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x121f1c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x121f1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x121f157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x121f0b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x121f20060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x121f21a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x121f2e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x121f695d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x121f189d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x121f18c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x121f4e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x121f4c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x121f16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x121f170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x121f17380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x121f6a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x121f6aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x121f6adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x121f6b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x121f6b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x121f6b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x121f6b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x121f6bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x121f6be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x121f6c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x121f6c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x121f6c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x121f6c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x121f6cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x121f6ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x121f6d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x121f6d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x121f6d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x121f6d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x121f6dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x121f6df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x121f6e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x121f6e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x121f6e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x121f6ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x121f6ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x121f6efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x121f6f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x121f6f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x121f6f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x121f6fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x121f6fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x121f70030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x121f702f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x121f705b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x121f70870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x121f70b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x121f70df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x121f710b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x121f71370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x121f71630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x121f718f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x121f71bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x121f71e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x121f72130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x121f723f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x121f726b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x114004280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1140046f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x114004b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x114004fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x114005440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1140058b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x114005d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x114006190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x114006600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x114006a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x114006ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x114007350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1140077c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x114007c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1140080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x114008510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x114008980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x114008df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x114009260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1140096d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x114009b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x114009fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11400a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11400a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11400ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11400b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11400b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11400ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11400bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11400c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11400cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11400d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11400d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11400ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11400e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11400e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11400ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11400f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11400fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x114010010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x114010510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x114010a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x114010f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x114011410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x114011910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x114011e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x114012310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x114012810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x114012d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x114013210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x114013710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x114013c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x114014110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x114014610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x114014b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x114015010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x114015510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x114015a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x114015f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x114016410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x114016910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x114016e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x114017310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x114017810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x114017d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x114018210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x114018710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x114018c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x114019110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x114019610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x114019b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11401a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11401a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11401aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11401af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11401b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11401b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11401be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11401c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11401c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11401cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11401d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11401d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11401dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11401e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11401e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11401eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11401f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11401f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11401fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11401ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x114020410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x114020910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x114020e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x114021310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x114021810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x114021d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x114022210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x114022710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x114022c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x114023110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x114023610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x114023b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x114024010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x114024510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x114024a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x114024f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x114025410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x114025910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x114025e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x114026310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x114026810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x114026d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x114027210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x114027710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x114027c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x114028110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x114028610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x114028b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x114029010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1140295c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x114029b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11402a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11402a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11402ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11402b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11402b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11402c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11402c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11402c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11402ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11402d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11402dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11402e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11402e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11402ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11402f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11402f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11402fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1140301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x114030730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x114030c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1140311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x114031720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x114031c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1140321c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x114032710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x114032c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1140331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x114033700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x114033c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1140341a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1140346f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x114034c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x114035190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1140356e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x114035c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x114036180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1140366d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x114036c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x114037170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1140376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x114037c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x114038160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1140386b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x114038c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x114039150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1140396a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x114039bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11403a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11403a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11403abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11403b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11403b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11403bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11403c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11403c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11403cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11403d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11403d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11403dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11403e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11403e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11403eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11403f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11403f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11403fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1140400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x114040630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x114040b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1140410d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x114041620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x114041b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x114042010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1140424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x114042950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x114042df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x114043290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x114043730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x114043bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x114044070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x114044510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1140449b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x114044e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1140452f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x114045790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x114045c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1140460d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x114046620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x114046d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x114047460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x114047b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1140482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x114048560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x114048d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x114049010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x114049620 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.923s
user	0m0.243s
sys	0m0.138s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
